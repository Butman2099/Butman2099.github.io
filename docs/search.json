[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Моделювання складних систем у Python",
    "section": "",
    "text": "Передмова\nЦе книга з моделювання складних систем засобами мови програмування Python."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Вступ",
    "section": "",
    "text": "Задекларований перехід економіки України на інноваційний шлях розвитку в умовах обмеженості ресурсів, у першу чергу фінансових, вимагає зосередження їх на найбільш перспективних напрямках, де імовірність досягнення конкурентного успіху є найбільшою. Як свідчить практика, такого роду оцінки доцільно виконувати із застосуванням економіко-математичних моделей. Множинність шляхів вибору моделей і методів актуалізує необхідність розуміння основних тенденцій розвитку наукового знання і ключових досягнень.\nГоворячи мовою синергетики, необхідно виділити такі параметри порядку — провідні змінні, які з плином часу починають визначати динаміку і розвиток складної системи, і підпорядковувати собі її інші параметри, які призведуть до ефективного спрощення складного об’єкта.\nВсе більш очевидною є революція, що почалася в природничих та гуманітарних науках і пов’язана з вивченням феномена самоорганізації і дослідженням мережних структур. Мережна парадигма складності обумовлена великим значенням, яке придбали такі об’єкти, і тим, що на початку ХХІ століття очевидною стала разюча аналогія в топології мережних структур, що виникають завдяки активному використанню інформаційно-телекомунікаційних, гуманітарних, управлінських, військових технологій.\nМережі стали одним з двигунів економіки. У своїй історії людство долало різні бар’єри, створюючи нові матеріали, технології, об’єкти. Однак “мережна економіка” зіткнулася з обмеженнями самої людини — так званим “когнітивним бар’єром”. Як показали психологи, людина в змозі активно, творчо взаємодіяти з 5-7 людьми (з рештою опосередковано або стандартно, незалежно від того, скільки у нього друзів у соціальній мережі). Вона може одночасно стежити за 5-7 змінними (незалежно від того, наскільки великий обсяг інформації їй доступний). Приймаючи рішення, вона може зважити 5-7 факторів (скільки б даних у неї не було).\nЗліт нової економіки в США в 1990-х роках, пов’язаний багато в чому з інтернет- компаніями, породив ілюзії, що капіталізація мережних структур пропорційна числу зв’язків між вузлами, тобто квадрату числа вузлів \\(N^2\\). Однак, коли на початку 2000-х років міхур «нової економіки» лопнув (криза «доткомів - .com»), то виявилося, що реальна капіталізація мала зовсім іншу залежність від числа об’єктів, пов’язаних мережею — \\(N\\ln{N}\\). Інакше кажучи, не «всі зв’язуються з усіма», а майже всі взаємодіють з декількома дуже великими вузлами- хабами, які вже тісно пов’язуються між собою. Подібним же чином виявляється влаштована інфраструктура більшості складних систем, незалежно від їх природи.\nЧерговим викликом для нас стала нова індустріальна революція, актуалізована давоським форумом 2016 року. Згідно з опитуванням 800 лідерів технологічних компаній, проведеним спеціально для форуму в Давосі, ключовими драйверами змін стануть хмарні технології, розвиток способів збору і аналізу Big Data, краудсорсінг, шерінгова економіка і біотехнології.\nОчевидно, що революційні вимоги Індустрії 4.0 потребують нових парадигм моделювання соціально-економічних систем. На наш погляд, такою парадигмою може стати мережна парадигма складних систем.\nМережні технології змінили обчислювальну математику, системний аналіз, інформаційні технології. В останні роки було реалізовано кілька грандіозних мережних проектів, в яких поставлена задача вирішувалася завдяки спільним діям сотень тисяч або навіть мільйонів комп’ютерів. Це і криптографічні проблеми, і пошук ліків проти раку, заснований на математичному моделюванні взаємодії різних речовин з клітинами. Це розподілений аналіз даних космічних експериментів та обробка результатів, отриманих на Великому адронному колайдері.\nУ зв’язку з новою мережною парадигмою складності перед фахівцями з моделювання виникають принципово нові, актуальні задачі. Ось, на наш погляд, тільки деякі з них:\n\nдослідження надійності, робастності кіберфізичних мереж відносно випадкових помилок та направлених атак;\nаналіз когнітивних можливостей складних систем;\nмоделювання мультиплексних мереж;\nвплив нанотехнологій на формування наноекономіки та ін.\n\nСьогодні вже зрозуміло, що відповідних інновацій вимагає і система освіти. На початку комп’ютерної ери основну цінність і проблеми становили власне комп’ютери (hardware) і акцент робився на підготовку фахівців з обчислювальної техніки. Потім величезне значення набуло програмне забезпечення (software) і була розпочата підготовка дослідників у цій галузі (computer science) та інженерів-програмістів (computer engineering). У даний час на перший план виходять фахівці з мережних технологій (NetWare). Саме таких фахівців треба починати готувати у провідних національних університетах. За оцінками експертів з Індустрії 4.0 основними трендами освітньої сфери на двадцятирічному горизонті стануть: поширення цінностей мережної культури, прагматизація освіти, автоматизація рутинних інтелектуальних операцій, розвиток індустрії поліпшення когнітивних здібностей, боротьба за таланти, зростання значущості глобальних людських цінностей, уваги до природи і дбайливого поводження з ресурсами.\nМета курсу моделювання складних систем — формування системи теоретичних знань та практичних навичок щодо моделювання структурних і динамічних властивостей систем різної природи як засобу дослідження та управління складними явищами на макро-, мезо- й мікрорівнях. Протягом останніх десяти-п’ятнадцяти років відбулися відчутні зміни в розумінні фундаментальних закономірностей складних систем. Виявилось, що такі складні системи мають універсальні емерджентні властивості, які не знаходять адекватного розуміння у рамках традиційних парадигм. Тому для аналізу все активніше використовуються сучасні методи та моделі фундаментальних наук, які у поєднанні з новітніми досягненнями в галузі інформаційних технологій та досить ємними базами даних (мільйони записів навіть в базах некомерційного призначення) забезпечили значний прогрес у розумінні та квантифікації природи цих систем. Сюди відносяться методи фрактального і мультифрактального аналізу, дослідження рекурентних властивостей динамічних систем, нелінійної динаміки, теорії хаосу і біфуркацій тощо. З’явились нові «кількісні» напрямки економіки: математична економіка, фізична економіка, еконофізика та ін. Деякі з таких моделей, не знайшовши поки що відображення у навчальній літературі, включені до цієї роботи.\nНаукову основу курсу складають теоретичні моделі, математичний апарат, сучасні концепції та парадигми, які визначають підходи до вивчення характеристик складних систем. Курс базується на знаннях, одержаних при вивченні дисциплін математичного циклу, основ теорії систем та системного аналізу, моделювання, фінансового аналізу, макро- і мікроекономіки.\nЗавдання курсу — оволодіння теоретичними знаннями та інструментарієм моделювання складних систем, вивчення підходів до дослідження й аналізу, методів прогнозування їхнього розвитку, управління розвитком та функціонуванням складних систем у різних умовах функціонування.\nПредметом курсу є математичні моделі і методи дослідження складних систем різної природи.\nУ результаті вивчення дисципліни студент повинен:\n\nзнати структурні та динамічні характеристики складної системи; моделі прогнозування характеристик системи; основні методи оцінки якості функціонування; методи оцінки структурних змін; методи дослідження та моделювання складних природних та штучних систем;\nвміти здійснювати класифікацію характеристик складної системи, проводити порівняльний аналіз методів прогнозування; оцінити якість функціонування ієрархічної системи; визначити катастрофічні зміни в системі, які описуються рівняннями динаміки, визначити джерела структурних катастроф;\nдослідити та проаналізувати комплекс моделей складної системи;\nбути ознайомленим з сучасними напрямками розвитку сучасних теорій та парадигм, які використовуються для дослідження якісних характеристик динамічних систем.\n\nСистема контролю якості навчання студентів містить такі заходи:\n\nмодульний контроль;\nпроведення контрольних робіт;\nконтроль теоретичних знань у ході практичних занять;\nвиконання індивідуальних завдань на лабораторних заняттях.\n\nОрганізація самостійної роботи студентів передбачає підготовку до семінарських занять, проведення індивідуальних консультацій, виконання курсової роботи.\nВідсутність альтернативних підручників і методичних посібників з курсу моделювання складних систем спонукала авторів розробити методичні вказівки до виконання практичних робіт з даного курсу. Вони спираються на деяке авторське бачення, певні висновки мають дискусійний характер. Автори будуть вдячні за бачення, спрямоване на покращення змісту та методики викладання дисципліни."
  },
  {
    "objectID": "lab_1.html#теоретичні-відомості",
    "href": "lab_1.html#теоретичні-відомості",
    "title": "2  Лабораторна робота № 1",
    "section": "2.1 Теоретичні відомості",
    "text": "2.1 Теоретичні відомості\n\n2.1.1 Аналіз динаміки прибутків, модулів прибутків та волатильностей\nОстаннім часом вчені все більше цікавляться економічними часовими рядами, і відбувається це за кількох причин, зокрема: (1) економічні часові ряди, такі як індекси акцій, курсів валют, залежать від розвитку великої кількості взаємодіючих систем, і є прикладами складних систем, що широко вивчаються у науці; (2) з’явилась велика кількість доступних баз з даними про економічні системи, що містять інформацію з різними часовими шкалами (починаючи з 1 хвилини і закінчуючи 1 роком). Внаслідок цього вже на даний час існує також велика кількість розроблених методів (зокрема, у статистичній фізиці), спрямованих на отримання характеристик цін акцій чи курсів валют, що еволюціонують у часі.\nДослідження, проведені над часовими рядами, показують, що стохастичний процес, який лежить у основі зміни ціни, характеризується кількома ознаками. Розподіл зміни ціни має виділений хвіст порівняно із Гаусовим розподілом. Функція автокореляції зміни ціни спадає експоненційно з певним характерним часом. Однак, виявляється, що амплітуда зміни ціни, виміряна за абсолютними значеннями чи квадратами цін, показує степеневі кореляції з довго часовою персистентністю аж до кількох місяців, або навіть років. Такі довгочасові залежності краще моделюються з використанням «додаткового процесу», що в економічній літературі часто називається волатильністю. Волатильність змін ціни акції є мірою того, як сильно ринок схильний до флуктуацій, тобто відхилень ціни від попередніх значень.\nПершим кроком при проведенні аналізу є побудова оцінювача волатильності. Ми будемо отримувати волатильність як локальне середнє модуля зміни ціни.\nРозуміння статистичних властивостей волатильності має також важливе практичне застосування. Волатильність є інтересом торговців, оскільки визначає ризик і є ключовим входом практично до всіх моделей цін опціонів (вторинного цінного паперу), включаючи і класичну модель Блека-Шоулза. Без задовільних методів оцінювання волатильності трейдерам було б надзвичайно важко визначати ситуації, в яких опціони попадають в недооцінку чи переоцінку.\n\n\n2.1.2 Визначення волатильності\nТермін волатильність представляє узагальнену міру величини ринкових флуктуацій (відхилень). У літературі існує досить багато визначень волатильності, проте ми будемо використовувати наступне: волатильність є локальним середнім модуля зміни ціни на відповідному часовому інтервалі \\(T\\), що є рухомим параметром нашої оцінки. Для індексу \\(X(t)\\) визначимо зміну ціни \\(G(t)\\) як зміну логарифмів індексів,\n\\[\nG(t) = \\ln{X(t+\\Delta t) - \\ln{X(t)}} \\cong \\frac{X(t+\\Delta t) - X(t)}{X(t)}, \\tag{1}\n\\]\nде \\(\\Delta t\\) є часовим інтервалом затримки. Величину (1) називають прибутковістю (return). Якщо використовувати границі, то малі зміни \\(X(t)\\) приблизно відповідають змінам, визначеним другою рівністю. Ми лише підраховуємо час роботи ринку, викидаємо ночі, вихідні та свята із набору даних, тобто, вважається, що ринок працює без перерв.\nМодуль \\(G(t)\\) описує амплітуду флуктуацій. У порівнянні із значеннями \\(G(t)\\) їх модуль не показує глобальних трендів, але великі значення \\(G(t)\\) відповідають крахам та великим миттєвим змінам на ринках.\nВизначимо волатильність як середнє від \\(G(t)\\) для часових вікон \\(T = n \\cdot \\Delta t\\), тобто\n\\[\nV_{T} = \\frac{1}{n}\\sum_{t^{'}=t}^{t+n-1}\\left| G(t^{'}) \\right|, \\tag{2}\n\\]\nде \\(n\\) є цілим числом. Таке визначення може бути ще узагальнене заміною \\(G(t)\\) на \\(\\left| G(t) \\right|^{\\gamma}\\), де \\(\\gamma &gt; 1\\) дає більш виражені великі значення \\(G(t)\\), в той час як \\(0 &lt; \\gamma &lt; 1\\) виділяє малі значення \\(G(t)\\).\nУ цьому визначенні волатильності використовується два параметри, \\(\\Delta t\\) та \\(n\\). Параметр \\(n\\) є шаблонним (чи модельним) часовим інтервалом для даних, а параметр \\(\\Delta t\\) є кроком переміщення часового вікна. Зауважимо, що вказане визначення волатильності має внутрішню помилку, а саме: вибір більшого часового інтервалу \\(T\\) веде до збільшення точності визначення волатильності. Однак, велике значення \\(T\\) також включає погане розбиття часу на інтервали, що веде, у свою чергу, до врахування не всієї прихованої у ряді інформації.\n\n\n2.1.3 Визначення кореляцій\nДля визначення кореляцій часового ряду використовується функція автокореляції. Саме поняття автокореляції означає кореляцію часового ряду самого з собою (між попередніми та наступними значеннями). Автокореляцію іноді називають послідовною кореляцією, що означає кореляцію між членами ряду чисел, розташованих у певному порядку. Також синонімами цього терміну є лагова кореляція та персистентність. Наприклад, часто зустрічається автокореляція геофізичних процесів, що означає перенесення залишкового процесу на наступні часові проміжки.\nПозитивно автокорельований часовий ряд часто називають персистентним, що значить існування тенденції слідування великих значень за великими та малих за малими, інакше позитивно корельований часовий ряд можна назвати інертним.\nВізьмемо \\(N\\) пар спостережень двох змінних \\(x\\) та \\(y\\). Коефіцієнт кореляції між парами \\(x\\) та \\(y\\) визначається як\n\\[\n    r = \\frac{\\sum \\left( x_i - \\bar{x} \\right) \\left( y_i - \\bar{y} \\right)}{\\sqrt{\\sum \\left( x_i - \\bar{x} \\right)^{2}} \\sqrt{\\sum \\left( y_i - \\bar{y} \\right)^{2}}}, \\tag{3}\n\\]\nде сума знаходиться по всім \\(N\\) спостереженням.\nТаким же чином можна визначати й автокореляцію, або ж кореляцію всередині досліджуваного часового ряду. Для автокореляції першого порядку береться лаг (часова затримка), рівний 1 часовій одиниці. Таким чином, автокореляція першого порядку використовує перші \\(N−1\\) спостережень \\(x_t, t = 1,..., N−1\\), та наступні \\(N−1\\) спостережень \\(x_t , t = 2,..., N\\).\nКореляція між \\(x_t\\) та \\(x_t + 1\\) визначається наступним чином:\n\\[\nr_1 = \\frac{\\sum_{t=1}^{N-1} \\left( x_t - \\bar{x} \\right) \\left( x_{t+1} - \\bar{x} \\right)}{\\sum_{t=1}^{N}\\left( x_t - \\bar{x} \\right)^2}, \\tag{4}\n\\]\nде \\(x\\) — це середнє для досліджуваного періоду.\nРівняння (4) може бути узагальнене для отримання кореляції між спостереженнями, розділеними \\(k\\) часовими інтервалами:\n\\[\n    r_k = \\frac{\\sum_{t=1}^{N-k} \\left( x_t - \\bar{x} \\right) \\left( x_{t+k} - \\bar{x} \\right)}{\\sum_{t=1}^{N}\\left( x_t - \\bar{x} \\right)^2}. \\tag{4}\n\\]\nЗначення \\(r_k\\) називається коефіцієнтом автокореляції з лагом \\(k\\). Графік функції автокореляції як залежності \\(r_k\\) від \\(k\\) також називають корелограмою."
  },
  {
    "objectID": "lab_1.html#хід-роботи",
    "href": "lab_1.html#хід-роботи",
    "title": "2  Лабораторна робота № 1",
    "section": "2.2 Хід роботи",
    "text": "2.2 Хід роботи\nДля подальшої роботи з моделювання складних систем візьмемо з основу бібліотеку yfinance, що дозволяє працювати з даними фінансових ринків засобами мови програмування Python.\n\n\n\n\n\n\nПримітка\n\n\n\nYahoo!, Y!Finance, and Yahoo! finance є зареєстрованими товарними знаками Yahoo, Inc.\nyfinance не є афілійованим, схваленим або перевіреним Yahoo, Inc. Це інструмент з відкритим вихідним кодом, який використовує загальнодоступні API Yahoo, і призначений для дослідницьких та освітніх цілей.\nВи повинні звернутися до умов використання Yahoo! (сюди, сюди і сюди) для отримання детальної інформації про ваші права на використання фактично завантажених даних. Пам’ятайте — фінансовий API Yahoo! призначений лише для особистого використання.\n\n\nДля встановлення бібліотеки yfinance можете скористатися наступною командою:\n\n!pip install yfinance --upgrade --no-cache-dir\n\nГітхаб репозиторій містить більше інформації по самій бібліотеці та помилкам, що можуть виникнути та їх потенційним рішенням.\n\n2.2.1 Вступ до модуля Ticker()\nПерш за все імпортуємо бібліотеку yfinance за допомогою наступної команди:\n\nimport yfinance as yf\n\nМодуль Ticker() дозволяє отримувати ринкові та метадані для цінного паперу, використовуючи Python:\n\nmsft = yf.Ticker(\"MSFT\")\nprint(msft)\n\nyfinance.Ticker object &lt;MSFT&gt;\n\n\nМожна вилучити всю інформацію по досліджуваному індексу:\n\n# отримати інформацію по індексу\nprint(msft.info)\n\n{'address1': 'One Microsoft Way', 'city': 'Redmond', 'state': 'WA', 'zip': '98052-6399', 'country': 'United States', 'phone': '425 882 8080', 'website': 'https://www.microsoft.com', 'industry': 'Software—Infrastructure', 'industryDisp': 'Software—Infrastructure', 'sector': 'Technology', 'sectorDisp': 'Technology', 'longBusinessSummary': 'Microsoft Corporation develops and supports software, services, devices and solutions worldwide. The Productivity and Business Processes segment offers office, exchange, SharePoint, Microsoft Teams, office 365 Security and Compliance, Microsoft viva, and Microsoft 365 copilot; and office consumer services, such as Microsoft 365 consumer subscriptions, Office licensed on-premises, and other office services. This segment also provides LinkedIn; and dynamics business solutions, including Dynamics 365, a set of intelligent, cloud-based applications across ERP, CRM, power apps, and power automate; and on-premises ERP and CRM applications. The Intelligent Cloud segment provides server products and cloud services, such as azure and other cloud services; SQL and windows server, visual studio, system center, and related client access licenses, as well as nuance and GitHub; and enterprise services including enterprise support services, industry solutions, and nuance professional services. The More Personal Computing segment offers Windows, including windows OEM licensing and other non-volume licensing of the Windows operating system; Windows commercial comprising volume licensing of the Windows operating system, windows cloud services, and other Windows commercial offerings; patent licensing; and windows Internet of Things; and devices, such as surface, HoloLens, and PC accessories. Additionally, this segment provides gaming, which includes Xbox hardware and content, and first- and third-party content; Xbox game pass and other subscriptions, cloud gaming, advertising, third-party disc royalties, and other cloud services; and search and news advertising, which includes Bing, Microsoft News and Edge, and third-party affiliates. The company sells its products through OEMs, distributors, and resellers; and directly through digital marketplaces, online, and retail stores. The company was founded in 1975 and is headquartered in Redmond, Washington.', 'fullTimeEmployees': 221000, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Satya  Nadella', 'age': 55, 'title': 'Chairman & CEO', 'yearBorn': 1967, 'fiscalYear': 2022, 'totalPay': 12676750, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Bradford L. Smith LCA', 'age': 63, 'title': 'Pres & Vice Chairman', 'yearBorn': 1959, 'fiscalYear': 2022, 'totalPay': 4655274, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Amy E. Hood', 'age': 50, 'title': 'Exec. VP & CFO', 'yearBorn': 1972, 'fiscalYear': 2022, 'totalPay': 4637915, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Judson  Althoff', 'age': 49, 'title': 'Exec. VP & Chief Commercial Officer', 'yearBorn': 1973, 'fiscalYear': 2022, 'totalPay': 4428268, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Christopher David Young', 'age': 50, 'title': 'Exec. VP of Bus. Devel., Strategy & Ventures', 'yearBorn': 1972, 'fiscalYear': 2022, 'totalPay': 4588876, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Alice L. Jolla', 'age': 55, 'title': 'Corp. VP & Chief Accounting Officer', 'yearBorn': 1967, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. James Kevin Scott', 'age': 50, 'title': 'Exec. VP of AI & CTO', 'yearBorn': 1972, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Brett  Iversen', 'title': 'Gen. Mang. of Investor Relations', 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Frank X. Shaw', 'title': 'Corp. VP for Corp. Communications', 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Christopher C. Capossela', 'age': 52, 'title': 'Exec. VP & Chief Marketing Officer', 'yearBorn': 1970, 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 6, 'boardRisk': 3, 'compensationRisk': 3, 'shareHolderRightsRisk': 2, 'overallRisk': 2, 'governanceEpochDate': 1693526400, 'compensationAsOfEpochDate': 1672444800, 'maxAge': 86400, 'priceHint': 2, 'previousClose': 333.55, 'open': 333.38, 'dayLow': 330.36, 'dayHigh': 334.46, 'regularMarketPreviousClose': 333.55, 'regularMarketOpen': 333.38, 'regularMarketDayLow': 330.36, 'regularMarketDayHigh': 334.46, 'dividendRate': 2.72, 'dividendYield': 0.0082, 'exDividendDate': 1692144000, 'payoutRatio': 0.2748, 'fiveYearAvgDividendYield': 1.05, 'beta': 0.904564, 'trailingPE': 34.171665, 'forwardPE': 26.183836, 'volume': 7259479, 'regularMarketVolume': 7259479, 'averageVolume': 26337729, 'averageVolume10days': 19143070, 'averageDailyVolume10Day': 19143070, 'bid': 331.6, 'ask': 331.68, 'bidSize': 1200, 'askSize': 2200, 'marketCap': 2455089971200, 'fiftyTwoWeekLow': 213.43, 'fiftyTwoWeekHigh': 366.78, 'priceToSalesTrailing12Months': 11.5852585, 'fiftyDayAverage': 332.6776, 'twoHundredDayAverage': 289.0401, 'trailingAnnualDividendRate': 2.72, 'trailingAnnualDividendYield': 0.0081547, 'currency': 'USD', 'enterpriseValue': 2446382596096, 'profitMargins': 0.34146, 'floatShares': 7423448423, 'sharesOutstanding': 7429760000, 'sharesShort': 31028035, 'sharesShortPriorMonth': 42497800, 'sharesShortPreviousMonthDate': 1689292800, 'dateShortInterest': 1692057600, 'sharesPercentSharesOut': 0.0042, 'heldPercentInsiders': 0.00052, 'heldPercentInstitutions': 0.73212, 'shortRatio': 1.0, 'shortPercentOfFloat': 0.0042, 'impliedSharesOutstanding': 7429760000, 'bookValue': 27.748, 'priceToBook': 11.908607, 'lastFiscalYearEnd': 1688083200, 'nextFiscalYearEnd': 1719705600, 'mostRecentQuarter': 1688083200, 'earningsQuarterlyGrowth': 0.2, 'netIncomeToCommon': 72361000960, 'trailingEps': 9.67, 'forwardEps': 12.62, 'pegRatio': 2.11, 'lastSplitFactor': '2:1', 'lastSplitDate': 1045526400, 'enterpriseToRevenue': 11.544, 'enterpriseToEbitda': 23.979, '52WeekChange': 0.29237866, 'SandP52WeekChange': 0.12989366, 'lastDividendValue': 0.68, 'lastDividendDate': 1692144000, 'exchange': 'NMS', 'quoteType': 'EQUITY', 'symbol': 'MSFT', 'underlyingSymbol': 'MSFT', 'shortName': 'Microsoft Corporation', 'longName': 'Microsoft Corporation', 'firstTradeDateEpochUtc': 511108200, 'timeZoneFullName': 'America/New_York', 'timeZoneShortName': 'EDT', 'uuid': 'b004b3ec-de24-385e-b2c1-923f10d3fb62', 'messageBoardId': 'finmb_21835', 'gmtOffSetMilliseconds': -14400000, 'currentPrice': 330.44, 'targetHighPrice': 440.0, 'targetLowPrice': 232.0, 'targetMeanPrice': 387.17, 'targetMedianPrice': 400.0, 'recommendationMean': 1.8, 'recommendationKey': 'buy', 'numberOfAnalystOpinions': 44, 'totalCash': 111256002560, 'totalCashPerShare': 14.974, 'ebitda': 102022995968, 'totalDebt': 79441002496, 'quickRatio': 1.536, 'currentRatio': 1.769, 'totalRevenue': 211914997760, 'debtToEquity': 38.522, 'revenuePerShare': 28.46, 'returnOnAssets': 0.14245, 'returnOnEquity': 0.38824, 'freeCashflow': 47268999168, 'operatingCashflow': 87581999104, 'earningsGrowth': 0.202, 'revenueGrowth': 0.083, 'grossMargins': 0.6892, 'ebitdaMargins': 0.48143002, 'operatingMargins': 0.41772997, 'financialCurrency': 'USD', 'trailingPegRatio': 2.4421}\n\n\nМожна вилучити ринкові значення за максимальний період часу:\n\n# отримати ринкові історичні значення індексу\nmsft.history(period=\"max\")\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n1986-03-13 00:00:00-05:00\n0.055004\n0.063093\n0.055004\n0.060396\n1031788800\n0.0\n0.0\n\n\n1986-03-14 00:00:00-05:00\n0.060396\n0.063632\n0.060396\n0.062553\n308160000\n0.0\n0.0\n\n\n1986-03-17 00:00:00-05:00\n0.062553\n0.064172\n0.062553\n0.063632\n133171200\n0.0\n0.0\n\n\n1986-03-18 00:00:00-05:00\n0.063632\n0.064172\n0.061475\n0.062014\n67766400\n0.0\n0.0\n\n\n1986-03-19 00:00:00-05:00\n0.062014\n0.062553\n0.060396\n0.060936\n47894400\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-08-30 00:00:00-04:00\n328.670013\n329.809998\n326.450012\n328.790009\n15222100\n0.0\n0.0\n\n\n2023-08-31 00:00:00-04:00\n329.200012\n330.910004\n326.779999\n327.760010\n26411000\n0.0\n0.0\n\n\n2023-09-01 00:00:00-04:00\n331.309998\n331.989990\n326.779999\n328.660004\n14931200\n0.0\n0.0\n\n\n2023-09-05 00:00:00-04:00\n329.000000\n334.850006\n328.660004\n333.549988\n18539400\n0.0\n0.0\n\n\n2023-09-06 00:00:00-04:00\n333.380005\n334.459991\n330.359985\n330.440002\n7259479\n0.0\n0.0\n\n\n\n\n9447 rows × 7 columns\n\n\n\nОкрім цього, yfinance дозволяє отримати інформацію по дивідентам та сплітам фінансового індексу:\n\n# показувати дії (дивіденди, спліти)\nmsft.actions\n\n\n\n\n\n\n\n\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n1987-09-21 00:00:00-04:00\n0.00\n2.0\n\n\n1990-04-16 00:00:00-04:00\n0.00\n2.0\n\n\n1991-06-27 00:00:00-04:00\n0.00\n1.5\n\n\n1992-06-15 00:00:00-04:00\n0.00\n1.5\n\n\n1994-05-23 00:00:00-04:00\n0.00\n2.0\n\n\n...\n...\n...\n\n\n2022-08-17 00:00:00-04:00\n0.62\n0.0\n\n\n2022-11-16 00:00:00-05:00\n0.68\n0.0\n\n\n2023-02-15 00:00:00-05:00\n0.68\n0.0\n\n\n2023-05-17 00:00:00-04:00\n0.68\n0.0\n\n\n2023-08-16 00:00:00-04:00\n0.68\n0.0\n\n\n\n\n88 rows × 2 columns\n\n\n\n\n# продемонструвати дивіденти\nmsft.dividends\n\nDate\n2003-02-19 00:00:00-05:00    0.08\n2003-10-15 00:00:00-04:00    0.16\n2004-08-23 00:00:00-04:00    0.08\n2004-11-15 00:00:00-05:00    3.08\n2005-02-15 00:00:00-05:00    0.08\n                             ... \n2022-08-17 00:00:00-04:00    0.62\n2022-11-16 00:00:00-05:00    0.68\n2023-02-15 00:00:00-05:00    0.68\n2023-05-17 00:00:00-04:00    0.68\n2023-08-16 00:00:00-04:00    0.68\nName: Dividends, Length: 79, dtype: float64\n\n\n\n# продемонструвати спліти\nmsft.splits\n\nDate\n1987-09-21 00:00:00-04:00    2.0\n1990-04-16 00:00:00-04:00    2.0\n1991-06-27 00:00:00-04:00    1.5\n1992-06-15 00:00:00-04:00    1.5\n1994-05-23 00:00:00-04:00    2.0\n1996-12-09 00:00:00-05:00    2.0\n1998-02-23 00:00:00-05:00    2.0\n1999-03-29 00:00:00-05:00    2.0\n2003-02-18 00:00:00-05:00    2.0\nName: Stock Splits, dtype: float64\n\n\nДля методу history() доступні наступні аргументи:\n\nperiod: період даних для завантаження (або використовуйте параметр period, або використовуйте start і end). Допустимі періоди: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max.\ninterval: інтервал даних (внутрішньоденні дані не можуть перевищувати 60 днів) Допустимі інтервали 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo.\nstart: Якщо не використовується період — завантажте рядок дати початку у форматі (YYYY-MM-DD) або datetime.\nend: Якщо не використовується період — завантажте рядок дати закінчення (YYYY-MM-DD) або datetime.\nprepost: Включати в результати попередні та пост ринкові дані (За замовчуванням False).\nauto_adjust: Автоматично налаштовувати всі OHLC (ціни відкриття, закриття, найбільшу та найменшу) (За замовчуванням True).\nactions: Завантажувати події дивідендів та дроблення акцій (За замовчуванням True).\n\n\n\n2.2.2 Одночасне вивантаження декількох ринкових активів\nЯк і до цього, ви також можете завантажувати дані для кількох тикерів одночасно.\n\ndata = yf.download(\"SPY AAPL\", \n                   start=\"2017-01-01\", \n                   end=\"2017-04-30\") # вивантажуємо дані, \n                                     # зберігаємо до змінної data\n\ndata.head() # виводимо перші 5 рядків нашого масиву даних \n\n[*********************100%%**********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-03\n27.059305\n201.337387\n29.037500\n225.240005\n29.082500\n225.830002\n28.690001\n223.880005\n28.950001\n225.039993\n115127600\n91366500\n\n\n2017-01-04\n27.029020\n202.535187\n29.004999\n226.580002\n29.127501\n226.750000\n28.937500\n225.610001\n28.962500\n225.619995\n84472400\n78744400\n\n\n2017-01-05\n27.166475\n202.374207\n29.152500\n226.399994\n29.215000\n226.580002\n28.952499\n225.479996\n28.980000\n226.270004\n88774400\n78379000\n\n\n2017-01-06\n27.469332\n203.098328\n29.477501\n227.210007\n29.540001\n227.750000\n29.117500\n225.899994\n29.195000\n226.529999\n127007600\n71559900\n\n\n2017-01-09\n27.720938\n202.427948\n29.747499\n226.460007\n29.857500\n227.070007\n29.485001\n226.419998\n29.487499\n226.910004\n134247600\n46939700\n\n\n\n\n\n\n\nДля отримання конкретно цін закриття індексу SPY, вам варто використовувати наступну команду: data['Close']['SPY'] Але, якщо вам потребується згрупувати дані по їх символу, можна скористатися наступним записом:\n\ndata = yf.download(\"SPY AAPL\", \n                   start=\"2017-01-01\", \n                   end=\"2017-04-30\",\n                   group_by=\"ticker\")\n\n[*********************100%%**********************]  2 of 2 completed\n\n\nТепер для звернення до цін закриття індексу SPY, вам треба використовувати наступний запис: data['SPY']['Close'].\nМетод download() приймає додатковий параметр — threads для швидшої обробки великої кількості фінансових індексів одночасно.\nДля подальшої роботи у даній лабораторній та подальших нас ще цікавитимуть наступні бібліотеки:\n\nmatplotlib: комплексна бібліотека для створення статичних, анімованих та інтерактивних візуалізацій на Python. Matplotlib робить прості речі простими, а складні — можливими.\npandas: програмна бібліотека, написана для мови програмування Python для маніпулювання та аналізу даних. Зокрема, вона пропонує структури даних та операції для маніпулювання числовими таблицями та часовими рядами.\nnumpy: бібліотека, що додає підтримку великих багатовимірних масивів і матриць, а також велику колекцію високорівневих математичних функцій для роботи з цими масивами.\nneurokit2: зручна бібліотека, що забезпечує легкий доступ до розширених процедур обробки біосигналів. Дослідники та клініцисти без глибоких знань програмування або біомедичної обробки сигналів можуть аналізувати фізіологічні дані за допомогою лише двох рядків коду. Перевага даної бібліотеки полягає в тому, що вона надає функціонал, який можна використовувати не лише для біомедичних сигналів, але й для фінансових, фізичних тощо.\n\nВстановити кожну з даних бібліотек можна в наступний спосіб: !pip install *назва бібліотеки*:\n\n!pip install matplotlib pandas numpy neurokit2\n\nДалі нам треба буде визначити стиль рисунків для виведення та збереження. Зробити це можна в наступний спосіб. Для використання подальшого стилю рисунків потребується встановити наступну бібліотеку:\n\n# для встановлення останньої версії (із PyPI)\n!pip install SciencePlots\n\nІмпортуємо кожну із зазначених бібліотек:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport numpy as np\nimport neurokit2 as nk\nimport scienceplots\n\n# магічна команда для вбудування рисунків у середовище jupyter блокнотів\n%matplotlib inline  \n\nUsageError: unrecognized arguments: # магічна команда для вбудування рисунків у середовище jupyter блокнотів\n\n\nВиконаємо налаштування стилю наших подальших рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків \n                                      # за замовчуванням\n        \n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nПредставлені налаштування є орієнтовними і можуть змінюватись у ході наступних лабораторних. Ви можете встановлювати власні налаштування. На сайті бібліотеки matplotlib можна ознайомитись з усіма можливими командами.\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи yfinance:\n\nsymbol = 'BTC-USD'       # Символ індексу\nstart = \"2015-09-01\"     # Дата початку зчитування даних\nend = \"2020-03-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиведемо досліджуваний ряд\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)\nax.set_ylabel(ylabel)\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 2.1: Динаміка щоденних змін індексу Біткоїна\n\n\n\n\nВидно, що ряд нестаціонарний, що викликає певні ускладнення для подальшого аналізу. Тому перейдемо до прибутковостей, які вже є стаціонарними, а їх нормалізація стандартним відхиленням дозволяє легко порівнювати їх розподіл з розподілом Гауса.\nПрибутковості розраховуватимуться згідно формулі (1). У Python ми використовуватимемо метод pct_change() для знаходження прибутковостей, що доступний нам завдяки бібліотеці pandas.\nСтандартизовані прибутковості визначаються наступним шляхом:\n\\[\ng(t) = \\frac{G(t) - \\mu}{\\sigma},  \n\\]\nде \\(\\mu\\) відповідає середньому значенню прибутковостей за досліджуваний часовий інтервал, а \\(\\sigma\\) представляє стандартне відхилення.\n\nret = time_ser.copy()      # копіюємо значення вихідного ряду для збереження \n                           # його від змін\n\nret = ret.pct_change()     # знаходимо прибутковості\nret -= ret.mean()          # вилучаємо середнє \nret /= ret.std()           # ділимо на стандартнє відхилення\n\nret = ret.dropna().values  # видаляємо всі можливі нульові значення \n\nВиводимо отриманий результат\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index[1:], ret)           # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Додаємо підпис для вісі Ох\nax.set_ylabel(ylabel + ' прибутковості')   # Додаємо підпис для вісі Оу\nax.axhline(y = 3.0, color = 'r', linestyle = '--')  # Додаємо горизонтальну лінію, що роз-\n                                                    # межує 3 сигма події\nax.axhline(y = -3.0, color = 'r', linestyle = '--') # Додаємо горизонтальну лінію, що роз-\n                                                    # межує -3 сигма події\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'Прибутковості{symbol}.jpg')  # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 2.2: Нормалізовані прибутковості досліджуваного часового ряду\n\n\n\n\nЗверніть увагу, що флуктуації нормалізованих прибутковостей досить часто перевищують величину \\(\\pm 3\\sigma\\), що, як відомо, надзвичайно рідко спостерігається для незалежних подій. Цей факт можна відобразити шляхом порівняння функції розподілу нормалізованих флуктуацій з розподілом Гауса (рис. Рисунок 2.3). Очевидно, що хвости розподілу вихідного ряду містять значні флуктуації, вони досить помітні (часто кажуть “важкі” у порівнянні з самою “головою” розподілу).\nДля побудови нормального розподілу скористаємось бібліотекою scipy. Встановити її можна по аналогії з попередніми бібліотеками.\n\n# Для встановлення останньої версії scipy\n!pip install scipy\n\n\nfrom scipy.stats import norm # імпорт модуля norm для побудови Гаусового розподілу\n\nФункція щільності ймовірності для norm має наступний вид:\n\\[\n    f(x) = \\frac{\\exp{(-x^2/2)}}{\\sqrt{2\\pi}}\n\\]\nдля дійсних значень \\(x\\).\n\nmu, sigma = norm.fit(ret)\n\nx = np.linspace(ret.min(), ret.max(), 10000) # Генеруємо 10000 значень для побудови \n                                             # Гаусового розподілу\np = norm.pdf(x, mu, sigma)                   # Отримання значень функції щільності\n\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(x, p, label='Гаус')                # Додаємо дані до графіку\nax.hist(ret, bins=50,                      # Побудова гістограми для прибутковостей\n        density=True, \n        alpha=0.6, \n        color='g',\n        label='прибутковості '+symbol)\n\nax.legend()                                # Додаємо легенду\nax.set_xlabel(\"g\")                         # Додаємо підпис для вісі Ох\nax.set_ylabel(r\"$f(g)$\")                   # Додаємо підпис для вісі Оу\nax.set_yscale('log')\n\n\nplt.savefig(f'Гаус + прибутковості {symbol}.jpg')       # Зберігаємо графік \nplt.show();                                             # Виводимо графік\n\n\n\n\nРисунок 2.3: Порівняння функцій розподілу нормалізованих прибутковостей з нормальним розподілом\n\n\n\n\nЯк ми можемо бачити, підігнана крива Гауса відхиляється від істинної частоти настання подій, що перевищують \\(\\pm 3\\sigma\\). Отже, ми можемо стверджувати, що наші прибутковості не є незалежними. Підтвердження цьому факту будемо шукати шляхом вивчення кореляційних властивостей нашого часового ряду.\nДля простоти обчислень скористаємось функцією signal_autocor() бібліотеки neurokit2. Виглядає дана функція наступним чином:\nsignal_autocor(signal, lag=None, demean=True, method='auto', show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) - Вектор значень.\nlag (int) — Часовий лаг. Якщо вказано, буде повернуто одне значення автокореляції сигналу з його власним лагом.\ndemean (bool) — Якщо має значення True, від сигналу буде відніматися середнє значення сигналу перед обчисленням автокореляції.\nmethod (str) — Використання \"auto\" запускає scipy.signal.correlate для використання швидшого алгоритму. Інші методи зберігаються з причин застарілості, але не рекомендуються. Інші методи включають \"correlation\" (за допомогою np.correlate()) або \"fft\" (швидке перетворення Фур’є).\nshow (bool) — якщо значення True, побудувати графік автокореляції для всіх значень затримки.\n\nПовертає\n\nr (float) - крос-кореляція сигналу з самим собою на різних часових лагах. Мінімальний часовий лаг дорівнює 0, максимальний часовий лаг дорівнює довжині сигналу. Або значення кореляції на певному часовому лазі, якщо лаг не дорівнює None.\ninfo (dict) - Словник, що містить додаткову інформацію, наприклад, довірчий інтервал.\n\n\n# розрахунок автокореляції\n\nr_init, _ = nk.signal_autocor(time_ser.values, \n                              method='correlation')  # для вихідних значень ряду                                                                    \nr_ret, _ = nk.signal_autocor(ret, \n                             method='correlation')   # для прибутковостей\nr_vol, _ = nk.signal_autocor(np.abs(ret), \n                             method='correlation')   # для модулів прибутковостей\n\nr_range = np.arange(1, len(r_ret) + 1)               # генерація лагів\n\n\nfig, ax = plt.subplots()                    # Створюємо порожній графік\n\nax.plot(r_range, r_init[1:], label=symbol)  # Додаємо дані до графіку\nax.plot(r_range, r_ret, label=r'$g(t)$')                          \nax.plot(r_range, r_vol, label=r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Autocorrelation r\")          # Додаємо підпис для вісі Оу\nax.set_ylim(-1.1, 1.1)                      # Встановлюємо обмеження по вісі Oy\n\nplt.savefig(f'Автокореляції {symbol}.jpg')  # Зберігаємо графік \nplt.show();                                 # Виводимо графік\n\n\n\n\nРисунок 2.4: Зміна з часом парних автокореляційних функцій для вихідного ряду x, нормалізованих прибутковостей g та їх модулів mod(g)\n\n\n\n\nАле, досліджуючи складні системи варто пам’ятати, що їх складність є варіативною. Тому і внутрішні кореляції системи на різних часових лагах також варіюються з плином часу. Із цього випливає, що подальши розрахунки варто виконувати не для всього ряду, а для його фрагментів.\nПодальші розрахунки здійснюватимуться в рамках алгоритму ковзного (рухомого) вікна. Для цього виділятиметься частина часового ряду (вікно), для якої розраховуватимуться міри складності, потім вікно зміщуватиметься разом з часовим рядом на заздалегідь визначену величину, і процедура повторюватиметься до тих пір, поки значення всього ряду не будуть вичерпані. Далі, порівнюючи динаміку фактичного часового ряду і відповідних мір складності, ми матимемо змогу судити про характерні зміни в динаміці міри складності зі зміною досліджуваної системи. Якщо та чи інша міра складності поводиться певним чином для всіх періодів крахів, наприклад, зменшується або збільшується під час передкризовий або передкритичний період, то вона може служити їх індикатором або провісником.\nРозглянемо як поводитиме себе функція автокореляцій та волатильність в рамках алгоритму ковзного вікна.\nСпочатку визначимо параметри\n\nret_type = 4 # вид ряду: \n             # 1 - вихідний, \n             # 2 - детрендований (різниця між теп. значенням та попереднім)\n             # 3 - прибутковості звичайні, \n             # 4 - стандартизовані прибутковості, \n             # 5 - абсолютні значення (волатильності),\n             # 6 - стандартизований вихідний ряд \n\nlength = len(time_ser) # довжина всього ряду\n\nwindow = 250    # довжина вікна - період у межах якого розраховуватимуться наші індикатори\ntstep = 1       # крок зміщення вікна\nvolatility = [] # масив значень волатильностей \nautocorr = []   # масив значень автокореляції при змінній lag\n\nДалі розпочнемо розрахунки. Для відслідковування прогресу зміщення ковзного вікна скористаємось бібліотекою tqdm. Її можна встановити аналогічно попереднім бібліотекам.\n\n!pip install tqdm\n\nІмпортуємо модуль для візуалізації прогресу\n\nfrom tqdm import tqdm\n\nі тепер приступимо до виконання віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # Фрагменти довжиною window  \n                                              # з кроком tstep\n                                              \n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент\n\n                                          # подальшому відбираємо потрібний тип ряду                                         \n    if ret_type == 1:                     # вихідні значення \n        pass\n    elif ret_type == 2:                   # різниці\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:                   # прибутковості\n        fragm = fragm.pct_change()\n    elif ret_type == 4:                   # стандартизовані прибутковості\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:                   # абсолютні значення прибутковостей\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        fragm = fragm.abs()\n    elif ret_type == 6:                   # стандартизований вихідний ряд\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values         # видаляємо зайві нульові значення, якщо є\n    \n    # розрахунок віконної автокореляції\n    r_window, _ = nk.signal_autocor(fragm, method='correlation') \n\n    # розрахунок волатильності по модулям прибутковостей                                     \n    vol_window = np.mean(np.abs(fragm))\n\n    # збереження результатів до масивів\n    volatility.append(vol_window)\n    autocorr.append(r_window[1])\n\n100%|██████████| 1393/1393 [00:01&lt;00:00, 884.25it/s]\n\n\nЗбережемо результати в окремих текстових файлах\n\n# збереження результатів ковзної автокореляції\nnp.savetxt(f\"autocorr_name={symbol}_ \\\n            window={window}_step={tstep}_ \\\n            rettype={ret_type}.txt\", autocorr)\n\n# збереження результатів ковзної волатильності\nnp.savetxt(f\"volatility_name={symbol}_ \\\n            window={window}_step={tstep}_ \\\n            rettype={ret_type}.txt\", volatility)\n\nНарешті порівняємо динаміку вихідного ряду і розрахованих похідних. Для цього врахуємо, що автокореляцію і волатильність ми рахували для рухомого вікна. Результати представлено на рис. Рисунок 2.5.\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.10))\nax4.spines.right.set_position((\"axes\", 1.19))\n\np1, = ax.plot(time_ser.index[window:length:tstep], \n              time_ser.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window+1:length:tstep], \n               ret[window:length:tstep], \"r--\", label=r\"$G(t)$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], \n               autocorr, \"g-\", label=r\"$A$\")\np4, = ax4.plot(time_ser.index[window:length:tstep],\n               volatility, \"m-\", label=r\"$V$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax4.legend(handles=[p1, p2, p3, p4])\n\nplt.savefig(f\"all_name={symbol}_ret={ret_type}_\\\n            wind={window}_step={tstep}.jpg\")\nplt.show();\n\n\n\n\nРисунок 2.5: Динаміка індексу Біткоїна, нормалізованих прибутковостей, віконних автокореляції та волатильності\n\n\n\n\nАналізуючи графік, можна зробити висновок, що у певні моменти спостерігалися стрибки волатильності (як і автокореляції) із поступовим зменшенням її до попереднього рівня, що може бути внаслідок збурень у процесі роботи ринку. Аналіз таких збурень, їх частоти та сили, дозволяє виявляти приховані закономірності роботи ринку."
  },
  {
    "objectID": "lab_1.html#висновок",
    "href": "lab_1.html#висновок",
    "title": "2  Лабораторна робота № 1",
    "section": "2.3 Висновок",
    "text": "2.3 Висновок\nТаким чином, аналіз флуктуацій прибутковостей та волатильностей шляхом побудови функції автокореляції та розподілу ймовірності дозволяє отримати певні висновки, що можуть допомогти в роботі із аналізованими часовими рядами і ринком, з якого взято зазначені часові ряди. Зокрема, у даному випадку, можна давати рекомендації аналітикам, що працюють на фінансових ринках."
  },
  {
    "objectID": "lab_1.html#завдання-для-самостійної-роботи",
    "href": "lab_1.html#завдання-для-самостійної-роботи",
    "title": "2  Лабораторна робота № 1",
    "section": "2.4 Завдання для самостійної роботи",
    "text": "2.4 Завдання для самостійної роботи\n\nОтримати індекс часового ряду у викладача\nПровести дослідження згідно інструкції\nДослідити зміни розрахованих величин для вікон 100 і 500, з кроком 1. Порівняти результати\nЗробити висновки"
  },
  {
    "objectID": "lab_1.html#контрольні-питання",
    "href": "lab_1.html#контрольні-питання",
    "title": "2  Лабораторна робота № 1",
    "section": "2.5 Контрольні питання",
    "text": "2.5 Контрольні питання\n\nПорівняйте вид залежностей флуктуацій цін і прибутковостей. Чому при розрахунках користуються не цінами, а прибутковостями?\nЯку характеристику ряду визначає волатильність?\nУ чому причина різних залежностей для прибутковостей та їх модулів?"
  },
  {
    "objectID": "lab_1.html#додаток",
    "href": "lab_1.html#додаток",
    "title": "2  Лабораторна робота № 1",
    "section": "2.6 Додаток",
    "text": "2.6 Додаток\nДля обрання часового індексу часового ряду використаємо дані, що розміщенні на сайті Yahoo! Finance. Оскільки окремі фінансові показники не завжди є доступними, будемо використовувати список компаній, що входять до індексу DJIA. За вказаним посиланням та номером у списку групи оберіть компанію, що входить до індексу та проведіть відповідні розрахунки. Порівняйте отримані результати з Біткоїном."
  },
  {
    "objectID": "lab_2.html#теоретичні-відомості",
    "href": "lab_2.html#теоретичні-відомості",
    "title": "3  Лабораторна робота № 2",
    "section": "3.1 Теоретичні відомості",
    "text": "3.1 Теоретичні відомості\nДослідження складних систем, як природних, так і штучних, показали, що в їх основі лежать нелінійні процеси, ретельне вивчення яких необхідне для розуміння і моделювання складних систем. У останні десятиліття набір традиційних (лінійних) методик дослідження був істотно розширений нелінійними методами, одержаними з теорії нелінійної динаміки і хаосу; багато досліджень були присвячені оцінці нелінійних характеристик і властивостей процесів, що протікають в природі (скейлінг, фрактальна розмірність). Проте більшість методів нелінійного аналізу вимагає або достатньо довгих, або стаціонарних рядів даних, які досить важко одержати з природи. Більш того, було показано, що дані методи дають задовільні результати для моделей реальних систем, що ідеалізуються. Ці чинники вимагали розробки нових методик нелінійного аналізу даних.\nСтан природних або штучних систем, як правило, змінюється в часі. Вивчення цих, часто складних, процесів — важлива задача в багатьох дисциплінах, дозволяє зрозуміти і описати їх суть, наприклад, для прогнозування стану на деякий час в майбутнє. Метою таких досліджень є знаходження математичних моделей, які б достатньо відповідали реальним процесам і могли б бути використані для розв’язання поставлених задач.\nРозглянемо ідею і коротко теорію рекурентного аналізу, наведемо деякі приклади, розглянемо його можливі області застосування при аналізі і прогнозування складних фінансово-економічних систем.\n\n3.1.1 Фазовий простір та його реконструкція\nСтан системи описується її змінними стану\n\\[\nx^1(t),x^2(t),...,x^d(t)\n\\]\nде верхній індекс — номер змінної. Набір із \\(d\\) змінних стану у момент часу \\(t\\) складає вектор стану \\(\\vec x(t)\\) в \\(d\\)-вимірному фазовому просторі. Даний вектор переміщається в часі в напрямі,визначуваному його вектором швидкості:\n\\[\n\\dot{\\vec x}(t)=\\partial_t\\vec x(t)=\\vec F(t)\n\\]\nПослідовність векторів \\(\\vec x(t)\\) утворює траєкторію у фазовому просторі, причому поле швидкості \\(\\vec F\\) дотичне до цієї траєкторії. Еволюція траєкторії описує динаміку системи і її атрактор. Знаючи \\(\\vec F\\), можна одержати інформацію про стан системи в момент \\(t\\) шляхом інтегрування виразу. Оскільки форма траєкторії дозволяє судити про характер процесу (періодичні або хаотичні процеси мають характерні фазові портрети), то для визначення стану системи не обов’язково проводити інтегрування, достатньо побудувати графічне відображення траєкторії.\nПри дослідженні складних систем часто немає інформації про всі змінні стану, або не все з них можливо виміряти. Як правило, є єдине спостереження, проведене через дискретний часовий інтервал \\(\\Delta t\\). Таким чином, вимірювання записуються у вигляді ряду \\(u_i(t)\\) i , де \\(t=i\\cdot \\Delta t\\). Інтервал \\(\\Delta t\\) може бути постійним, проте це не завжди можливо і створює проблеми для застосування стандартних методів аналізу даних, що вимагають рівномірної шкали спостережень.\nВзаємодії і їх кількість в складних системах такі, що навіть по одній змінній стану можна судити про динаміку всієї системи в цілому (даний факт був встановлений групою американських учених при вивченні турбулентності). Таким чином, еквівалентна фазова траєкторія, що зберігає структури оригінальної фазової траєкторії, може бути відновлена з одного спостереження або часового ряду за теоремою Такенса (Takens) методом часових затримок:\n\\[\n\\widehat{\\vec x}(t)=(u_i,u_{i+\\tau},...,u_{i+(m-1)\\tau})\n\\]\nде \\(m\\) — розмірність вкладення, \\(\\tau\\) — часова затримка (реальна часова затримка визначається як \\(\\tau \\cdot \\Delta t\\)). Топологічні структури відновленої траєкторії зберігаються, якщо \\(m \\geq 2 \\cdot d+1\\), де \\(d\\) — розмірність атрактора. На практиці більшості випадків атрактор може бути відновлений і при \\(m \\leq 2d\\). Затримка, як правило, вибирається апріорно.\nІснує кілька підходів до вибору мінімально достатньої розмірності \\(m\\), крім аналітичного. Високу ефективність показали методи, засновані на концепції фальшивих найближчих точок (false nearest neighbours, FNN). Суть її заключається у тому, що при зменшенні розмірності вкладення відбувається збільшення кількості фальшивих точок, що потрапляють в околицю будь-якої точки фазового простору. Звідси витікає простий метод — визначення кількості FNN як функції від розмірності. Існують і інші методи, засновані на цій концепції — наприклад, визначення відносин відстаней між одними і тими ж сусідніми точками при різних \\(m\\). Розмірність атрактора також може бути визначена за допомогою крос-кореляційних сум.\n\n\n\n\n \n\n\nРисунок 3.1: Відрізок траєкторії у фазовому просторі системи Рьослера \\(i\\) (a); відповідний рекурентний графік (b). Вектор фазового простору в точці \\(j\\), який потрапляє в околицю (сіре коло в (a)) заданого вектора фазового простору вектора в точці \\(i\\) вважається точкою рекурентності (чорна точка на траєкторії в (a)). Вона позначається чорною точкою на рекурентній діаграмі у позиції \\((i, j)\\). Вектор фазового простору за межами околу (порожнє коло в (a)) призводить до білої точки в рекурентній діаграмі\n\n\n\n\n3.1.2 Рекурентний аналіз\nПроцесам в природі властива яскраво виражена рекурентна поведінка, така, як періодичність або іррегулярна циклічність. Більш того, рекурентність (повторюваність) станів в значенні проходження подальшої траєкторії достатньо близько до попередньої є фундаментальною властивістю дисипативних динамічних систем. Ця властивість була відмічена ще в 80-х роках XIX століття французьким математиком Пуанкаре (Poincare) і згодом сформульовано у вигляді “теореми рекурентності”, опублікованої в 1890 р.:\n\n\n\n\n\n\nПримітка\n\n\n\nЯкщо система зводить свою динаміку до обмеженої підмножини фазового простору, то система майже напевно, тобто з вірогідністю, практично рівною 1, скільки завгодно близько повертається до якого-небудь спочатку заданого режиму.\n\n\nСуть цієї фундаментальної властивості у тому, що, не дивлячись на те, що навіть саме мале збурення в складній динамічній системі може привести систему до експоненціального відхилення від її стану, через деякий час система прагне повернутися до стану, деяким чином близького до попереднього, і проходить при цьому подібні етапи еволюції.\nПереконатися в цьому можна за допомогою графічного зображення траєкторії системи у фазовому просторі. Проте можливості такого аналізу сильно обмежені. Як правило, розмірність фазового простору складної динамічної системи більша трьох, що робить практично незручним його розгляд напряму; єдина можливість — проекції в дво- і тривимірні простори, що часто не дає вірного уявлення про фазовий портрет.\nУ 1987 р. Екман (Eckmann) і співавтори запропонували спосіб відображення \\(m\\)-вимірної фазової траєкторії станів системи \\(\\vec x(t)\\) завдовжки \\(N\\) на двовимірну квадратну двійкову матрицю розміром \\(N \\times N\\) , в якій 1 (чорна точка) відповідає повторенню стану при деякому часі \\(i\\) в деякий інший час \\(j\\), а обидві координатні осі є осями часу. Таке представлення було назване рекурентною картою або діаграмою (recurrence plot, RP), оскільки воно фіксує інформацію про рекурентну поведінку системи.\nМатематично вищесказане описується як\n\\[\nR_{i,j}^{m,\\varepsilon_i}=\\Theta(\\varepsilon_i-\\| \\vec x_i - \\vec x_j \\|), \\cdot \\vec x \\in \\Re^m, \\cdot i, j=1...N\n\\]\nде \\(N\\) — кількість даних станів, \\(x_i, \\varepsilon_i\\) — розмір околиці точки \\(\\vec x\\) у момент \\(i\\), \\(\\| \\cdot \\|\\) — норма і \\(\\Theta(\\cdot)\\) — функція Хевісайда.\nНепрактично і, як правило, неможливо знайти повну рекурентність у значенні \\(\\vec x_i \\equiv \\vec x_j\\) (стан динамічної, а особливо — хаотичної системи не повторюється повністю еквівалентно початковому стану, а підходить до нього скільки завгодно близько). Таким чином, рекурентність визначається як достатня близькість стану \\(\\vec x_j\\) до стану \\(\\vec x_i\\). Іншими словами, рекурентними є стани \\(\\vec x_j\\), які потрапляють в \\(m\\)-вимірну околицю з радіусом \\(\\varepsilon_i\\) і центром в \\(\\vec x_i\\). Ці точки \\(\\vec x_j\\) називаються рекурентними точками (recurrence points).\nОскільки \\(R_{i,i}=1\\), \\(i=1,...,N\\) за визначенням, то рекурентна діаграма завжди міститьчорну діагональну лінію — лінію ідентичності (line of identity, LOI) під кутом \\(\\pi/4\\) до осей координат. Довільно узята рекурентна точка не несе якої-небудь корисної інформації про стани в часи \\(i\\) і \\(j\\). Тільки вся сукупність рекурентних точок дозволяє відновити властивості системи.\nЗовнішній вигляд рекурентної діаграми дозволяє судити про характер процесів, які протікають в системі, наявності і впливі шуму, станів повторення і завмирання (ламінарності), здійсненні в ході еволюції системи різких змін стану (екстремальних подій).\n\n\n    \nРисунок 3.2: Типові динамічні ряди і їх рекурентні карти\n\n\n\n\n3.1.3 Аналіз діаграм\nОчевидно, що процеси різної поведінки даватимуть рекурентні діаграми з різним рисунком. Таким чином, візуальна оцінка діаграм може дати уявлення про еволюцію досліджуваної траєкторії. Виділяють два основних класи структури зображення: топологія (typology), що представляється крупномасштабними структурами, і текстура (texture), що формується дрібномасштабними структурами.\nТопологія дає загальне уявлення про характер процесу. Виділяють чотири основні класи:\n\nоднорідні рекурентні діаграми типові для стаціонарних і автономних систем, в яких час релаксації малий у порівнянні з довжиною ряду;\nперіодичні структури, що повторюються (діагональні лінії, узори у шаховому порядку) відповідають різним осцилюючим системам з періодичністю в динаміці;\nдрейф відповідає системам з параметрами, що поволі змінюються, що робить білими лівий верхній і правий нижній кути рекурентної діаграми;\nрізкі зміни в динаміці системи, рівно як і екстремальні ситуації, обумовлюють появу білих областей або смуг.\n\nРекурентні діаграми спрощують виявлення екстремальних і рідкісних подій.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 3.3: Характернi топологiї рекурентних дiаграм: (а) — однорiдна (нормально розподiлений шум); (b) — перiодична (генератор Ван дер Поля); (c) — дрейф (вiдображення Iкеди з накладеною послiдовнiстю, що лiнiйно росте); (d) — контрастнi областi або смуги (узагальнений броунiвський рух)\n\n\nДокладний розгляд рекурентних діаграм дозволяє виявити дрібномасштабні структури — текстуру, яка складається з простих точок, діагональних, горизонтальних і вертикальних ліній. Комбінації вертикальних і горизонтальних ліній формують прямокутні кластери точок.\n\nсамотні, окремо розташовані рекурентні точки з’являються в тому разі, коли відповідні стани рідкісні, або нестійкі в часі, або викликані сильною флуктуацією. При цьому вони не є ознаками випадковості або шуму;\nдіагональні лінії \\(R_{i+k, j+k}=1\\) (при \\(k = 1...l\\) де \\(l\\) — довжина діагональної лінії) з’являються у разі, коли сегмент траєкторії у фазовому просторі пролягає паралельно іншому сегменту, тобто траєкторія повторює саму себе, повертаючись в одну і ту ж область фазового простору у різний час. Довжина таких ліній визначається часом, протягом якого сегменти траєкторії залишаються паралельними; напрям (кут нахилу) ліній характеризує внутрішній час підпроцесів, відповідних даним сегментам траєкторії. Проходження ліній паралельно лінії ідентичності (під кутом \\(\\pi/4\\) до осей координат) свідчить про однаковий напрям сегментів траєкторії, перпендикулярно — про протилежний («відображені» сегменти), що може також бути ознакою реконструкції фазового простору з невідповідною розмірністю вкладення. Нерегулярна поява діагональних ліній є ознакою хаотичного процесу;\nвертикальні (горизонтальні) лінії \\(R_{i, j+k}=1\\) (при \\(k = 1...\\upsilon\\), де \\(\\upsilon\\) — довжина вертикальної або горизонтальної лінії) виділяють проміжки часу, в котрі стан системи не змінюється або змінюється трохи (система як би «заморожена» на цей час), що є ознакою «ламінарних» станів.\n\n\n\n\nРисунок 3.4: Основнi концепцiї рекурентного аналiзу. Вiдображена дiаграма рекурентностi базується на часовому ряду, що було реконструйовано до 11 реконструйованих векторiв, вiд \\(\\vec{X}(0)\\) до \\(\\vec{X}(10)\\). Видiлено дiагональну лiнiю довжиною \\(d = 3\\), вертикальна лiнiя довжиною \\(v = 3\\) i бiлу вертикальну лiнiю довжиною \\(w = 5\\)"
  },
  {
    "objectID": "lab_2.html#хід-роботи",
    "href": "lab_2.html#хід-роботи",
    "title": "3  Лабораторна робота № 2",
    "section": "3.2 Хід роботи",
    "text": "3.2 Хід роботи\nСпочатку побудуємо дво- та тривимірні фазові портрети як для модельних значень, так і для реальних. Використовуватимемо бібліотеки neurokit2 для побудови атракторів та рекурентного аналізу.\n\n3.2.1 Процедура реконструкції фазового простору\nДля побудови фазового портрету скористаємось методами complexity_attractor() та complexity_embedding() бібліотеки neuralkit2. Синтаксис complexity_attractor() виглядає наступним чином:\ncomplexity_attractor(embedded='lorenz', alpha='time', color='last_dim', shadows=True, linewidth=1, **kwargs)\nПараметри\n\nembedded (Union[str, np.ndarray]) — результат функції complexity_embedding(). Також може бути рядком, наприклад, \"lorenz\" (атрактор Лоренца) або \"rossler\" (атрактор Рьосслера).\nalpha (Union[str, float]) — прозорість ліній. Якщо \"time\", то лінії будуть прозорими як функція часу (повільно).\ncolor (str) — Колір графіку. Якщо \"last_dim\", буде використано останній вимір (максимум 4-й) вбудованих даних, коли розмірність більша за 2. Корисно для візуалізації глибини (для 3-вимірного вбудовування), або четвертого виміру, але працюватиме це повільно.\nshadows (bool) — якщо значення True, 2D-проекції буде додано до бокових сторін 3D-атрактора.\nlinewidth (float) — задає товщину лінії.\n****kwargs** — До палітри кольорів (наприклад, name=\"plasma\") або до симулятора системи Лоренца передаються додаткові аргументи ключових слів, такі як duration (за замовчуванням = 100), sampling_rate (за замовчуванням = 10), sigma (за замовчуванням = 10), beta (за замовчуванням = 8/3), rho (за замовчуванням = 28).\n\nЯк вже зазначалося, побудова фазового простору, на основі якого і проводитиметься рекурентний аналіз, вимагає реконструкції. Виконати реконструкції фазового простору із одновимірного часового ряду можна із використанням методу часових затримок.\nМетод часових затримок є однією з ключових концепцій науки про складність, що ми використовуватимемо і в подальших лабораторних. Він базується на ідеї, що динамічна система може бути описана вектором чисел, який називається її “станом”, що має на меті забезпечити повний опис системи в певний момент часу. Множина всіх можливих станів називається “простором станів”.\nТеорема Такенса (1981) припускає, що послідовність вимірювань динамічної системи містить у собі всю інформацію, необхідну для повної реконструкції простору станів. Метод часових затримок намагається визначити стан \\(s\\) системи в певний момент часу \\(t\\), шукаючи в минулій історії спостережень схожі стани, і, вивчаючи еволюцію схожих станів, виводити інформацію про майбутнє системи.\nЯк візуалізувати динаміку системи? Послідовність значень стану в часі називається траєкторією. Залежно від системи, різні траєкторії можуть еволюціонувати до спільної підмножини простору станів, яка називається атрактором. Наявність та поведінка атракторів дає інтуїтивне уявлення про досліджувану динамічну систему.\nОдже, згідно Такенсу, ідея полягає в тому, щоб на основі одиничних вимірювань системи, отримати \\(m\\)-розмірні реконструйовані часові вкладення\n\\[\n    \\vec{y}_i = \\left( y_i, y_{i+\\tau}, ... , y_{i+(m-1)\\tau} \\right), \\tag{1}\n\\]\nде \\(i\\) проходить в діапазоні \\(1,..., N-(m-1)\\tau\\); значення \\(\\tau\\) представляє часову затримку, а \\(m\\) — це розмірність вкладень (кількість змінних, що включає кожна траєкторія).\nКод для реконструкції фазового простору може виглядати наступним чином:\nY = np.zeros((dimension, N - (dimension - 1) * delay)) # ініціалізуємо масив нулів,\n                                                       # що будуть представляти траєкторії\nfor i in range(dimension):\n    Y[i] = signal[i * delay : i * delay + Y.shape[1]]  # заповнюємо кожну траєкторію \n\nembedded = Y.T                                          \nreturn embedded                                        # повертаємо результат \nДля реконструкції фазового простору використовуватимемо метод complexity_embedding(). Його синтаксис виглядає наступним чином:\ncomplexity_embedding(signal, delay=1, dimension=3, show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень. Також може бути рядком, наприклад, \"lorenz\" (атрактор Лоренца), \"rossler\" (атрактор Росслера) або \"clifford\" (атрактор Кліффорда) для отримання попередньо визначеного атрактора.\ndelay (int) — часова затримка (часто позначається \\(\\tau\\) іноді називають запізненням). Ще розглянемо метод complexity_delay() для оцінки оптимального значення цього параметра.\ndimension (int) — розмірність вкладень (\\(m\\), іноді позначається як \\(d\\) або порядок). Далі звернемось до методу complexity_dimension(), щоб оцінити оптимальне значення для цього параметра.\nshow (bool) — Побудувати графік реконструйованого атрактора.\n****kwargs** — інші аргументи, що передаються до complexity_attractor().\n\nПовертає\n\narray — реконструйований атрактор розміру length - (dimension - 1) * delay\n\nДалі імпортуємо необхідні для подальшої роботи модулі\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport scienceplots\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виводу\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nТепер розглянемо можливість використання методу часових затримок і отриманих в подальшому атракторів у якості індикатора складності. Як і в попередній роботі, для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи yfinance:\n\nsymbol = 'BTC-USD'       # Символ індексу\nstart = \"2015-09-01\"     # Дата початку зчитування даних\nend = \"2020-03-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nСпочатку оберемо вид ряду: 1. вихідний ряд 2. детермінований (різниця між теперішнім та попереднім значенням) 3. прибутковості звичайні 4. стандартизовані прибутковості 5. абсолютні значення (волатильності) 6. стандартизований ряд\nДля подальших розрахунків накращим варіантом буде вибір стандартизованого вихідного ряду або прибутковостей, оскільки значення вихідного часового ряду відрізняються на декілька порядків, і можуть сильно перевищувати встановлений параметр \\(\\varepsilon\\). Тобто, для вихідних значень, що сильно різняться між собою, увесь часовий діапазон буде розглядатися як нерекурентний.\nСпочатку визначимо функції для виконання перетворення ряду:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nі тепер виконаємо перетворення, використовуючи дану функцію:\n\nsignal = time_ser.copy()\nret_type = 6    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rec = transformation(signal, ret_type) \n\nОскільки ми не матимемо змоги візуалізувати багатовимірний фазовий простір (\\(m&gt;3\\)), ми послуговуватимемось значеннями \\(m=2\\) та \\(m=3\\). Значення \\(\\tau\\) будемо варіювати як із власних переконань, так і з опорою на функціонал бібліотеки neuralkit2.\nСкористаємось методом complexity_simulate() для генерації різних тестових сигналів.\n\nsignal_random_walk = nk.complexity_simulate(duration=30, \n                                            sampling_rate=100, \n                                            method=\"randomwalk\") # симуляція випадкового блукання\n\n\nnk.complexity_attractor(embedded=nk.complexity_embedding(signal_random_walk, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"orange\"); \n\n\n\n\nРисунок 3.5: Двовимірний фазовий портрет випадкового блукання\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_random_walk, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"orange\");\n\n\n\n\nРисунок 3.6: Тривимірний фазовий портрет випадкового блукання\n\n\n\n\n\nsignal_ornstein = nk.complexity_simulate(duration=30, \n                                        sampling_rate=100, \n                                        method=\"ornstein\") # симуляція системи Орнштайна\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_ornstein, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРисунок 3.7: Двовимірний фазовий портрет системи Орнштайна\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_ornstein, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРисунок 3.8: Двовимірний фазовий портрет системи Орнштайна\n\n\n\n\n\nnk.complexity_attractor(color = \"last_dim\", alpha=\"time\", duration=1);\n\n\n\n\nРисунок 3.9: Тривимірний фазовий портрет атрактора Лоренца\n\n\n\n\n\nnk.complexity_attractor(\"rossler\", color = \"blue\", alpha=1, sampling_rate=5000);\n\n\n\n\nРисунок 3.10: Тривимірний фазовий портрет атрактора Рьосслера\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"lime\"); \n\n\n\n\nРисунок 3.11: Двовимірний фазовий портрет вихідних значень досліджуваного ряду Біткоїна\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"lime\"); \n\n\n\n\nРисунок 3.12: Тривимірний фазовий портрет вихідних значень досліджуваного ряду Біткоїна\n\n\n\n\nУ зазначених вище прикладах прикладах ми обирали параметри \\(m\\) і \\(\\tau\\) згідно нашим власним міркуванням. Але, як правило, при виконанні серйозного дослідження, що матиме прикладне застосування, лише власних переконань буває недостатньо. У нашому випадку бажано було б, щоб зазначені параметри обирались автоматично, опираючись на конкретну статистичну процедуру. Бібліотека neurokit2 представляє функціонал для автоматичного підбору параметрів розмірності та часової затримки. Коротко їх опишемо.\n\n\n3.2.2 Автоматизований підбір параметра часової затримки, \\(\\tau\\)\nЧасова затримка (Tau \\(\\tau\\) також відома як Lag) є одним з двох критичних параметрів, що беруть участь у процедурі реконструкції фазового простору. Він відповідає затримці у відліках між вихідним сигналом і його затриманою версією (версіями). Іншими словами, скільки відліків ми розглядаємо між певним станом сигналу та його найближчим минулим станом.\nЯкщо \\(\\tau\\) менше оптимального теоретичного значення, послідовні координати стану системи корельовані і атрактор недостатньо розгорнутий. І навпаки, коли \\(\\tau\\) більше, ніж повинно бути, послідовні координати майже незалежні, що призводить до некорельованої та неструктурованої хмари точок.\nВибір параметрів затримки та розмірності представляє нетривіальну задачу. Один з підходів полягає у їх (напів)незалежному виборі (оскільки вибір розмірності часто вимагає затримки) за допомогою функцій complexity_delay() та complexity_dimension(). Однак, існують методи спільного оцінювання, які намагаються знайти оптимальну затримку та розмірність одночасно.\nЗауважте також, що деякі автори (наприклад, Розенштейн, 1994) пропонують спочатку визначити оптимальну розмірність вбудовування, а потім розглядати оптимальне значення затримки як оптимальну затримку між першою та останньою координатами затримки (іншими словами, фактична затримка має дорівнювати оптимальній затримці, поділеній на оптимальну розмірність вбудовування мінус 1).\nДекілька авторів запропонували різні методи для вибору затримки:\n\nФрейзер і Свінні (1986) пропонують використовувати перший локальний мінімум взаємної інформації між затриманим і незатриманим часовими рядами, ефективно визначаючи значення Tau, для якого вони діляться найменшою інформацією (і де атрактор є найменш надлишковим). На відміну від автокореляції, взаємна інформація враховує також нелінійні кореляції.\nТейлер (1990) запропонував вибирати таке значення Tau, при якому автокореляція між сигналом та його зміщенною версією при Tau вперше перетинає значення \\(1/\\exp\\). Методи, що базуються на автокореляції, мають перевагу в короткому часі обчислень, коли вони обчислюються за допомогою алгоритму швидкого перетворення Фур’є (fast Fourier transform, FFT).\nКасдаглі (1991) пропонує замість цього брати перший нульовий перетин автокореляції.\nРозенштейн (1993) пропонує апроксимувати точку, де функція автокореляцій падає до \\(\\left( 1-1/\\exp \\right)\\) від свого максимального значення.\nРозенштейн (1994) пропонує наближатися до точки, близької до 40% нахилу середнього зміщення від діагоналі.\nКім (1999) пропонує оцінювати Tau за допомогою кореляційного інтегралу, який називається C-C методом, і який, як виявилося, узгоджується з результатами, отриманими за допомогою методу взаємної інформації. Цей метод використовує статистику в реконструйованому фазовому просторі, а не аналізує часову еволюцію ряду. Однак час обчислень для цього методу значно довший через необхідність порівнювати кожну унікальну пару парних векторів у реконструйованому сигналі на кожну затримку.\nЛайл (2021) описує “Реконструкцію симетричного проекційного атрактора” (Symmetric Projection Attractor Reconstruction, SPAR), де \\(1/3\\) від домінуючої частоти (тобто довжини середнього “циклу”) може бути підходящим значенням для приблизно періодичних даних, і робить атрактор чутливим до морфологічних змін. Див. також доповідь Астона. Цей метод також є найшвидшим, але може не підходити для аперіодичних сигналів. Аргумент алгоритму (за замовчуванням \"fft\").\n\nМожна також зазначити наступний метод для об’єднаного підбору параметрів затримки та розмірності:\n\nГаутама (2003) зазначає, що на практиці часто використовують фіксовану часову затримку і відповідно регулюють розмірність вбудовування. Оскільки це може призвести до великих значень \\(m\\) (а отже, до вкладених даних великого розміру) і, відповідно, до повільної обробки, вони описують метод оптимізації для спільного визначення \\(m\\) і \\(\\tau\\) на основі показника entropy ratio.\n\nРозглянемо оптимальні значення розмірності та затримки для часового сигналу Біткоїна:\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"fraser1986\")\n\n\n\n\nРисунок 3.13: Оптимальне значення розмірності на основі методу Фрейзера і Свінні для часового ряду Біткоїна\n\n\n\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"theiler1990\")\n\n\n\n\nРисунок 3.14: Оптимальне значення розмірності на основі методу Тейлера для часового ряду Біткоїна\n\n\n\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=500, show=True,\n                                        method=\"casdagli1991\")\n\ndelay\n\nЯк можна бачити по прикладу вище, не всі методи надають адекватну оцінку розмірності нашого сигналу. Спробуємо привести вихідні значення Біткоїна до прибутковостей та повторити процедуру Касдаглі ще раз.\n\nret_type = 4 \nret = transformation(signal, ret_type)\n\n\ndelay, parameters = nk.complexity_delay(ret, \n                                        delay_max=300, show=True,\n                                        method=\"casdagli1991\")\n\n\n\n\nРисунок 3.15: Оптимальне значення розмірності на основі методу Касдаглі для прибутковостей Біткоїна\n\n\n\n\nЦього разу нам вдалося досягти оптимального результату, але приклад вище демонструє, що кожна процедура має свої виключення.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"rosenstein1993\")\n\n\n\n\nРисунок 3.16: Оптимальне значення розмірності на основі методу Розенштайна (1993) для часового ряду Біткоїна\n\n\n\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"rosenstein1994\")\n\n\n\n\nРисунок 3.17: Оптимальне значення розмірності на основі методу Розенштайна (1994) для часового ряду Біткоїна\n\n\n\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"lyle2021\")\n\n\n\n\nРисунок 3.18: Оптимальне значення розмірності на основі методу Лайла для часового ряду Біткоїна\n\n\n\n\nТепер подивимось як це виглядатиме для об’єднаного підбору параметрів\n\ndelay, parameters = nk.complexity_delay(for_rec,\n    delay_max=np.arange(1, 30, 1), # діапазон значень затримки\n    dimension_max=20,              # максимальна розмірність вкладень\n    method=\"gautama2003\",\n    surrogate_n=5,                 # Кількість сурогатних сигналів \n                                   # для генерації\n    surrogate_method=\"random\",     # Спосіб генерації сигналів\n    show=True)\n \n\n\n\n\nРисунок 3.19: Оптимальне значення розмірності та затримки на основі методу Гаутами для часового ряду Біткоїна\n\n\n\n\n\ndimension = parameters[\"Dimension\"]\ndimension\n\n20\n\n\n\n\n3.2.3 Автоматизований підбір параметра розмірності вкладень, \\(m\\)\nЗа дану процедуру відповідає метод complexity dimension(). Її синтаксис виглядає наступним чином:\ncomplexity_dimension(signal, delay=1, dimension_max=20, method='afnn', show=False, **kwargs)\nХоча зазвичай використовують \\(m=2\\) або \\(m=3\\), але різні автори пропонують наступні процедури підбору:\n\nКореляційна розмірність (Correlation Dimension, CD): Одним з перших методів оцінки оптимального \\(m\\) був розрахунок кореляційної розмірності для вкладень різного розміру і пошук насичення (тобто плато) в її значенні при збільшенні розміру векторів. Одне з обмежень полягає в тому, що насичення буде також мати місце, коли даних недостатньо для адекватного заповнення простору високої розмірності (зауважте, що в загальному випадку не рекомендується мати настільки великі вбудовування, оскільки це значно скорочує довжину сигналу).\nНайближчі хибні сусіди (False Nearest Neighbour, FNN): Метод, запропонований Кеннелом та ін., базується на припущенні, що дві точки, які є близькими одна до одної в достатній розмірності вбудовування, повинні залишатися близькими при збільшенні розмірності. Алгоритм перевіряє сусідів при збільшенні розмірності вкладень, поки не знайде лише незначну кількість хибних сусідів при переході від розмірності \\(m\\) до \\(m+1\\). Це відповідає найнижчій розмірності вбудовування, яка, як передбачається, дає розгорнуту реконструкцію просторово-часового стану. Цей метод може не спрацювати в зашумлених сигналах через марну спробу розгорнути шум (а в чисто випадкових сигналах кількість хибних сусідів суттєво не зменшується зі збільшенням \\(m\\)). На рисунку нижче показано, як проекції на простори більшої розмірності можна використовувати для виявлення хибних найближчих сусідів. Наприклад, червона та жовта точки є сусідами в одновимірному просторі, але не в двовимірному.\n\n\n\n\n\n\n\nСередні хибні сусіди (Average False Neighbors, AFN): Ця модифікація методу FNN, розроблена Сао (1997), усуває один з його основних недоліків — необхідність евристичного вибору порогових значень \\(r\\). Метод використовує максимальну евклідову відстань для представлення найближчих сусідів і усереднює всі відношення відстані в \\(m+1\\) розмірності до розмірності \\(m\\) і визначає E1 та E2 як параметри. Оптимальна розмірність відповідає досягається тоді, коли E1 перестає змінюватися (досягає плато). E1 досягає плато при розмірності d0, якщо сигнал надходить від атрактора. Тоді d0+1* є оптимальною мінімальною розмірністю вкладення. E2 є корисною величиною для того, щоб відрізнити детерміновані сигнали від стохастичних. Константа E2, що близька до 1 для будь-якої розмірності вкладень \\(d\\), вказує на випадковість даних, оскільки майбутні значення не залежать від минулих значень.\n\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка у відліках. Для вибору оптимального значення цього параметра ми ще скористаємось методом complexity_delay().\ndimension_max (int) — максимальний розмір вкладення для тестування.\nmethod (str) — Може бути \"afn\" (середні хибні сусіди), \"fnn\" (найближчий хибний сусід) або \"cd\" (кореляційна розмірність).\nshow (bool) — Візуалізувати результат.\n****kwargs** — інші аргументи, такі як \\(R=10.0\\) або \\(A=2.0\\) (відносне та абсолютне граничне значення, тільки для методу \"fnn\").\n\nПовертає\n\ndimension (int) — оптимальна розмірність вкладень.\nparameters (dict) — словник python, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимальної розмірності.\n\nСпробуємо отримати оптимальне значення розмірності згідно зазначених процедур. В якості часової затримки можна взять \\(\\tau=100\\). Приблизно таке значення спостерігалося для кожної процедури.\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=100,\n                                                  dimension_max=10,\n                                                  method='cd',\n                                                  show=True)\n\n\n\n\nРисунок 3.20: Оптимальне значення розмірності на основі кореляційної розмірності для часового ряду Біткоїна\n\n\n\n\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=100,\n                                                  dimension_max=10,\n                                                  method='fnn',\n                                                  show=True)\n\n\n\n\nРисунок 3.21: Оптимальне значення розмірності на основі найближчих хибних сусідів для часового ряду Біткоїна\n\n\n\n\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=20,\n                                                  dimension_max=20,\n                                                  method='afnn',\n                                                  show=True)\n\n\n\n\nРисунок 3.22: Оптимальне значення розмірності на основі середніх найближчих хибних сусідів для часового ряду Біткоїна\n\n\n\n\nУ даному випадку розмірність вкладень можна обирати в діапазоні значень від 3 до 7. Тепер на основі отриманих результатів приступимо до побудови рекурентної діаграми.\n\n\n3.2.4 Побудова рекурентної матриці\nЯк вже зазначалося, рекурентний аналіз кількісно визначає кількість і тривалість рекурентних станів динамічної системи, що визначаються на основі реконструйованих траєкторій фазового простору.\nМи маємо змогу побудувати рекурентну матрицю, використовуючи метод recurrence_matrix().\nЙого синтаксис виглядає наступним чином:\nrecurrence_matrix(signal, delay=1, dimension=3, tolerance='default', show=False)\nПараметри\n\nsignal (Union[list, np.ndarray, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі.\ndimension (int) — розмірність вкладень, \\(m\\).\ntolerance (float) — радіус \\(\\varepsilon\\) багатовимірного околу в межах якого шукаються рекурентні траєкторії (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються схожими. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\). Емпіричним правилом є встановлення \\(r\\) таким чином, щоб відсоток точок, класифікованих як рекурентні, становив приблизно 2-5%.\nshow (bool) — візуалізувати рекурентну матрицю.\n\nПовертає\n\nnp.ndarray — рекурентну матрицю.\nnp.ndarray — матрицю відстаней.\n\nПобудуємо рекурентну матрицю для вихідних значень Біткоїна, його прибутковостей та стандартизованого вихідного ряду. Розмірність \\(m=4\\), часова затримка \\(\\tau=1\\), радіус \\(\\varepsilon=0.3\\).\n\nrc, _ = nk.recurrence_matrix(signal, \n                            delay=1, \n                            dimension=4, \n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРисунок 3.23: Рекурентна матриця для вихідних значень Біткоїна\n\n\n\n\nЯк можна бачити з представленого рисунку всі траєкторії залишаються доволі віддаленими один від одного, ніякої рекурентності тут не передбачається.\nТепер спробуємо подивитися на стандартизовані прибутковості.\n\nrc, _ = nk.recurrence_matrix(ret, \n                            delay=1, \n                            dimension=4,\n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРисунок 3.24: Рекурентна матриця для стандартизованих прибутковостей Біткоїна\n\n\n\n\nТепер можемо бачити, що Біткоїн став характризуватися чорними смугами, що відображають динаміку певних детермінованих процесів. У той же час білі смуги характеризують періоди абсолютно аномальної (непередбачуваної поведінки на даному ринку). Видно, що прибутковості залишаються доволі некорельованими, про що і свідчить переважне домінування саме білих областей.\nСпробуємо тепер подивитись на стандартизований вихідний ряд.\n\nrc, _ = nk.recurrence_matrix(for_rec, \n                            delay=1, \n                            dimension=4,\n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРисунок 3.25: Рекурентна матриця для стандартизованого вихідного ряду Біткоїна\n\n\n\n\nНа початку свого існування Біткоїн характеризувався доволі високим ступенем передбачуваності, меншої волатильності власних коливань. Надалі почали предомінувати білі області, але видно, що тепер Біткоїну властива динаміка подібна до броунівсього руху."
  },
  {
    "objectID": "lab_2.html#завдання-для-самостійної-роботи",
    "href": "lab_2.html#завдання-для-самостійної-роботи",
    "title": "3  Лабораторна робота № 2",
    "section": "3.3 Завдання для самостійної роботи",
    "text": "3.3 Завдання для самостійної роботи\n\nОтримати індекс часового ряду у викладача\nПровести дослідження його рекурентних властивостей згідно інструкції\nПорівняти фазові портрети і рекурентні діаграми для стандартизованого вихідного ряду та прибутковостей. Що спільного між ними і чим вони відрізняються?\nЗробити висновки"
  },
  {
    "objectID": "lab_3.html#теоретичні-відомості",
    "href": "lab_3.html#теоретичні-відомості",
    "title": "4  Лабораторна робота № 3",
    "section": "4.1 Теоретичні відомості",
    "text": "4.1 Теоретичні відомості\nДля якісного опису системи графічне представлення системи підходить якнайкраще. Однак головним недоліком графічного представлення є те, що воно змушує користувачів суб’єктивно інтуїтивно інтерпретувати закономірності та структури, представлені на рекурентній діаграмі.\nКрім того, зі збільшенням розміру даних, проблематичним представляється аналіз усіх \\(N^2\\) значень. Як наслідок, доводиться працювати з окремими ділянками вихідних даних. Аналіз у такий спосіб може створювати нові дефекти, які спотворюють об’єктивність спостережуваних закономірностей і призводять до неправильних інтерпретацій. Щоб подолати це обмеження і поширити об’єктивну оцінку серед дослідників, на початку 1990-х років Веббером та Збілутом були введені визначення та процедури для кількісної оцінки складності рекурентних діаграм, а згодом вони були розширені Марваном та ін.\nДрібномасштабні кластери можуть являти собою комбінацію ізольованих точок (випадкових рекурентностей). Подібна еволюція в різні періоди часу або в зворотному часовому порядку представлятиме діагональні лінії (детерміновані структури), а також вертикальні/горизонтальні лінії для позначення ламінарних станів (переривчастість) або станів, що предсталяють сингулярності. Для кількісного опису системи системи такі дрібномасштабні кластери слугують основою кількісного рекурентного аналізу (recurrence quantification analysis, RQA)."
  },
  {
    "objectID": "lab_3.html#хід-роботи",
    "href": "lab_3.html#хід-роботи",
    "title": "4  Лабораторна робота № 3",
    "section": "4.2 Хід роботи",
    "text": "4.2 Хід роботи\nПерш ніж переходити до опису кожної з мір та розрахунків, визначемось з інструментарієм для виконання RQA. Як і до цього, ми використовуватимемо бібліотеку neuralkit2.\nТепер імпортуємо бібліотеки для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виводу:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд фондового індексу Доу-Джонса за період з 1 грудня 1993 по 1 грудня 2022, використовуючи yfinance:\n\nsymbol = '^DJI'          # Символ індексу\nstart = \"1993-01-01\"     # Дата початку зчитування даних\nend = \"2022-01-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиведемо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 4.1: Динаміка щоденних змін індексу Доу-Джонса\n\n\n\n\nКористуючись тими методами, що ми розглянули в попередній лабораторній роботі, побудуємо атрактор даного ряда та його рекурентну діаграму. Але перш за все, треба стандартизувати наш ряд. Для цього оголосимо функцію transformation(), що прийматиме на вхід часовий сигнал, тип ряду, і повертатиме його перетворення:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nДалі приводимо ряд до стандартизованого вигляду.\n\nsignal = time_ser.copy()\nret_type = 6    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rec = transformation(signal, ret_type) \n\nДля всього ряду і для віконної процедру визначимо наступні параметри:\n\nрозмірність вкладень \\(m=3\\);\nчасова затримка \\(\\tau=1\\);\nрадіус багатовимірного околу \\(\\varepsilon = 0.3\\).\n\nЗадамо необхідні параметри для обчислення та виводу:\n\nm = 3                         # розмірність вкладень\ntau = 1                       # часові затримка\neps = 0.3                     # радіус\n\nІ тепер подивимось на фазові траєкторії досліджуваної системи у дво- та тривимірному просторах:\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=2, delay=tau), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРисунок 4.2: Двовимірний фазовий портрет стандартизованих вихідних значень досліджуваного ряду Доу-Джонса\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=3, delay=tau), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРисунок 4.3: Тривимірний фазовий портрет стандартизованих вихідних значень досліджуваного ряду Доу-Джонса\n\n\n\n\nЯк можна бачити по візуальному огляду траєкторій у фазовому просторі важко робити висновки стосовно передбачуванності або хаотичності системи. Спробуємо ще раз, але тепер послуговуючись рекурентною діаграмою:\n\nrc, _ = nk.recurrence_matrix(for_rec, \n                            delay=1, \n                            dimension=m,\n                            tolerance=eps,\n                            show=True)\n\n\n\n\nРисунок 4.4: Рекурентна матриця для стандартизованого вихідного ряду Доу-Джонса\n\n\n\n\nЯк можна бачити, на основі рекурентної діаграми в перспективі ми можемо отримати куди більше інформації стосовно еволюції системи. Видно, що 2000-2008 рік характеризувалися найвищим ступенем самоорганізації (рекурентності) про що свідчать доволі велика щільність чорних областей. У той же час можна бачити, що останні роки характеризуються найменшим ступенем рекурентності. Можливо, прогнозованість подій у межах 2022 року варто було б охарактеризувати за допомогою інших індикаторів, але але рекурентна матриця говорить, що події минулих років мало корелюють з теперішнім.\nМи вже зазначали, що якісна репрезентація рекурентності станів не є достатньо об’єктивною. Найращим варіантом у даному випадку буде використання рекурентного аналізу наряду с алгоритмом рухомого вікна, що використовувався нами у першій лабораторній роботі, і буде використовуватись і надалі.\n\n4.2.1 Віконна процедура\nДля подальшої роботи створюємо віконну процедуру, в якій знов визначаємо вид ряду та ще декілька параметрів. Потім ми ініціалізуємо масиви для кожної рекурентної міри.\n\nret_type = 6            # вид ряду\nwindow = 250            # ширина вікна\ntstep = 1               # часовий крок вікна \nlength = len(time_ser)  # довжина самого ряду\n\nm = 1                   # розмірність вкладень\ntau = 1                 # часові затримка\neps = 0.3               # радіус\n\n                        # Ініціалізуємо масиви для збереження віконних значень \n                        # рекурентних мір\n\nRR = []                 # Частота повторення\nDET = []                # Детермінізм\nDIV = []                # Розбіжність\nAVG_DIAG_LINE = []      # Усереднена довжина діагональних ліній\nENT_DIAG = []           # Ентропія діагональних ліній\nLAM = []                # Ламінарність\nTT = []                 # Час затримки\nENT_VERT = []           # Ентропія вертикальних ліній\nENT_WHITE_VERT = []     # Ентропія білих вертикальних ліній\nAVG_WVERT_LINE = []     # Усереднена довжина білих вертикальних ліній\nVERT_DIV = []           # Розбіжність вертикальних ліній\nRATIO_DET_REC = []      # Відношення детермінізму до частоти повторень\nRATIO_LAM_DET = []      # Відношення ламінарності до детермінізму\nWHITE_VERT_DIV = []     # Розбіжність білих вертикальних ліній\nDIAG_RR = []            # Діагональна частота рекурентних значень\n\nДля подальших розрахунків ми використовуватимемо метод complexity_rqa() бібліотеки neuralkit2. Синтаксис даного методу виглядає наступним чином:\ncomplexity_rqa(signal, dimension=3, delay=1, tolerance='sd', min_linelength=2, method='python', show=False)\nПараметри\n\nsignal (Union[list, np.ndarray, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі.\ndimension (int) — розмірність вкладень, \\(m\\).\ntolerance (float) — радіус \\(\\varepsilon\\) багатовимірного околу в межах якого шукаються рекурентні траєкторії (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються схожими. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\).\nmin_linelength (int) — мінімальна довжина діагональних та вертикальних ліній За замовчування дорівнює 2.\nmethod (str) — Може бути \"pyrqa\" для виконання рекурентного аналізу, але із використанням бібліотеки PyRQA (потребує додаткового встановлення).\nshow (bool) — візуалізувати рекурентну матрицю.\n\nПовертає\n\nrqa (DataFrame) — результати процедури RQA.\ninfo (dict) — словник, що містить інформацію відносно параметрів, що використовувались для виконання RQA.\n\nТепер можемо приступити до віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n    \n    resultRQA, _ = nk.complexity_rqa(fragm,\n                                     delay=tau,\n                                     dimension=m,\n                                     tolerance=eps)\n    \n    # Обчислення відношення ламінарності до детермінізму\n    resultRQA['LamiDet'] = resultRQA['Laminarity']/resultRQA['Determinism']\n\n    # Обчислення дивергенції чорних вертикальних ліній\n    resultRQA['VDiv'] = 1./resultRQA['VMax']\n\n    # Обчислення дивергенції білих вертикальних ліній\n    resultRQA['WVDiv'] = 1./resultRQA['WMax']\n\n    RR.append(resultRQA['RecurrenceRate'])\n    DET.append(resultRQA['Determinism'])\n    DIV.append(resultRQA['Divergence']) \n    AVG_DIAG_LINE.append(resultRQA['L'])\n    ENT_DIAG.append(resultRQA['LEn'])\n    LAM.append(resultRQA['Laminarity']) \n    TT.append(resultRQA['TrappingTime']) \n    ENT_VERT.append(resultRQA['VEn'])\n    ENT_WHITE_VERT.append(resultRQA['WEn'])\n    AVG_WVERT_LINE.append(resultRQA['W']) \n    VERT_DIV.append(resultRQA['VDiv'])\n    WHITE_VERT_DIV.append(resultRQA['WVDiv'])\n    RATIO_DET_REC.append(resultRQA['DeteRec']) \n    RATIO_LAM_DET.append(resultRQA['LamiDet'])\n    DIAG_RR.append(resultRQA['DiagRec'])\n\n100%|██████████| 7054/7054 [02:39&lt;00:00, 44.16it/s]\n\n\nЗберігаємо отримані результати в текстових файлах:\n\nname = f\"RQA_classic_name={symbol}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_m={m}_ \\\n    tau={tau}_eps={eps}.txt\"\n\nnp.savetxt(\"RR\" + name, RR)\nnp.savetxt(\"DIAG_RR\" + name, DIAG_RR)\nnp.savetxt(\"DET\" + name, DET)\nnp.savetxt(\"DIV\" + name, DIV)\nnp.savetxt(\"VERT_DIV\" + name, VERT_DIV)\nnp.savetxt(\"WHITE_VERT_DIV\" + name, WHITE_VERT_DIV)\nnp.savetxt(\"LAM\" + name, LAM)\nnp.savetxt(\"TT\" + name, TT)\nnp.savetxt(\"AVG_DIAG_LINE\" + name, AVG_DIAG_LINE)\nnp.savetxt(\"AVG_WRITE_VERT_LINE\" + name, AVG_WVERT_LINE)\nnp.savetxt(\"ENT_DIAG\" + name, ENT_DIAG)\nnp.savetxt(\"ENT_VERT\" + name, ENT_VERT)\nnp.savetxt(\"ENT_WHITE_VERT\" + name, ENT_WHITE_VERT)\nnp.savetxt(\"RATIO_DET_REC\" + name, RATIO_DET_REC)\nnp.savetxt(\"RATIO_LAM_DET\" + name, RATIO_LAM_DET)\n\n\n\n4.2.2 Рекурентні міри\nТепер займемося побудовою та інтерпретацією отриманих результатів. Для візуалізації графіків визначимо наступну функцію:\n\ndef plot_recurrence_measure(measure, label, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(time_ser.index[window:length:tstep], \n                  time_ser.values[window:length:tstep], \n                  \"b-\", label=fr\"{ylabel}\")\n    p2, = ax2.plot(time_ser.index[window:length:tstep],\n                   measure, \n                   color=clr, \n                   label=fr'${label}$')\n\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(f\"{ylabel}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(label +\n        f\" RQA_classic_name={symbol}_window={window}_step={tstep}_ \\\n        rettype={ret_type}_m={m}_tau={tau}_eps={eps}.jpg\")\n        \n    plt.show();\n\n\n4.2.2.1 Частота рекурентності (Recurrence rate)\nНайпростішим показником є частота рекурентності, яка визначає щільністю рекурентних точок на рекурентній діаграмі, ігноруючи лінію ідентичності:\n\\[\nRR = \\frac{1}{N^2}\\displaystyle\\sum_{i,j=1}^{N}R(i,j)\n\\]\nде \\(N\\) — кількість точок на траєкторії фазового простору.\nЧастота рекурентності відповідає ймовірності того, що певний стан повториться.\n\nplot_recurrence_measure(measure=RR, label='RR')\n\n\n\n\nРисунок 4.5: Динаміка індексу Доу-Джонса та частоти рекурентності\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, міра рекурентності зростає при крахових подіях, що вказує на зростання ступеня самоорганізації та злагодженості торгівельної активності трейдерів на цьому ринку.\n\n\n4.2.2.2 Діагональна частота рекурентності (Diagonal recurrence rate)\nДаний підхід базується на діагональних рекурентних профілях часового ряду. Діагональний рекурентний профіль кількісно оцінює кількість рекурентних точок на різних лагах, подібно до функції автокореляцій. Для отримання діагонального профілю рекурентностей просто підраховується частка рекурентних точок на діагоналях, розташованих в нижньому правому або нижньому лівому куті рекурентної діаграми, і будується графік як функція відстані від головної діагоналі, тобто лагу.\nПо іншому можна сказати, що діагональна частота рекурентності фіксує величину автокореляції на різних лагах.\n\nplot_recurrence_measure(measure=DIAG_RR, label='DRR')\n\n\n\n\nРисунок 4.6: Динаміка індексу Доу-Джонса та діагональної частоти рекурентності\n\n\n\n\nЗ представленого рисунку видно, що діагональна частота рекурентності зростає у передкризові та кризові стани, що вказує на зростання величини автокореляції, що в свою чергу демонструє зростання ступеню самоорганізації у кризові та передкризові стани.\n\n\n4.2.2.3 Детермінізм (Determinism)\nНаступним показником можна визначити частку рекурентних траєкторій, які формують діагональні лінії мінімальної довжини \\({\\displaystyle \\ell _{\\min }}\\). Ця міра називається детермінізмом і пов’язана з передбачуваністю динамічної системи:\n\\[\nDET={\\frac {\\sum _{\\ell =\\ell _{\\min }}^{N}\\ell \\,P(\\ell )}{\\sum _{\\ell =1}^{N}\\ell P(\\ell )}},\n\\]\nде \\(P(\\ell )\\) — частотний розподіл довжин \\(\\ell\\) діагональних ліній (тобто підраховує кількість діагональних профілів довжини \\(\\ell\\) ).\n\n\n\n\n\n\nДодаткова інформація по детермінізму\n\n\n\nДетерміновані системи характеризуються значною варіацією діагональних ліній різної довжинию. Періодичні сигнали будуть характеризуватися довгими діагональними лініями, в той час як для хаотичних сигналів діагональні лінії будуть короткими. Для стохастичним систем діагональні лінії взагалі будуть відсутніми, за винятком випадкових закономріностей, що утворюватимуть дуже короткі діагональні лінії.\nБілий шум, наприклад, мав би рекурентну діаграму з майже ізольованими рекурентними точками та дуже малих відсотком діагональних ліній, тоді як детермінований процес демонстрував би дуже малу кількість одиночних рекурентностей, але велику щільність довгих діагональних ліній.\n\n\n\nplot_recurrence_measure(measure=DET, label='DET')\n\n\n\n\nРисунок 4.7: Динаміка індексу Доу-Джонса та детермінізму\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, як правило, у передкризові та кризові стани показник детермінізму починає зростати, що свідчить і про зростання ступеня передбачуваності (впорядкованості) флуктуацій системи.\n\n\n4.2.2.4 Ламінарність (Laminarity)\nКількість рекурентних станів, які утворюють вертикальні лінії, можна кількісно визначити таким же чином. Ця міра називається ламінарністю і пов’язана з кількістю ламінарних фаз (незмінностей) у системі:\n\\[\nLAM={\\frac {\\sum _{v=v_{\\min }}^{N}vP(v)}{\\sum _{v=1}^{N}vP(v)}},\n\\]\nде \\(P(v)\\) — частотний розподіл довжин \\(v\\) вертикальних ліній, які мають довжину принаймні \\(v_{\\min}\\).\n\n\n\n\n\n\nДодаткова інформація по ламінарності\n\n\n\nЛамінарність характеризує ймовірність системи перебувати в ламінарному (незмінному) стані. Зі збільшенням ізольованих рекурентних точок у системі, міра ламінарності спадатиме.\n\n\n\nplot_recurrence_measure(measure=LAM, label='LAM')\n\n\n\n\nРисунок 4.8: Динаміка індексу Доу-Джонса та ламінарності\n\n\n\n\nМожна бачити, що при кризових станах ступінь ламінарності зростає. Як ми могли бачити, зростає і щільність діагональних точок, і загалом зростає кількість рекурентних траєкторій у фазовому просторі. Це вказує на те, що фінасовий індекс Доу-Джонса “застрягає” у стані кризи. Кризи характеризуються трендостійкістю, персистентністю та детермінованістю своєї поведінки.\n\n\n4.2.2.5 Середня довжина діагональних ліній (The average diagonal lines length)\nТакож можна виміряти середню довжину діагональних ліній. Cередня довжина діагональних лінії визначається як\n\\[\nL={\\frac  {\\sum _{{\\ell =\\ell _{\\min }}}^{N}\\ell \\,P(\\ell )}{\\sum _{{\\ell =\\ell _{\\min }}}^{N}P(\\ell )}}.\n\\]\nЗагалом цей показник характеризує середній період часу при якому дві траєкторії фазового простору знаходяться в достатній близькості один до одного.\n\n\n\n\n\n\nДодаткова інформація по середній довжині діагональних ліній\n\n\n\nСередня довжина діагональних ліній визначає середній час при якому система залишається передбачуваною.\n\n\n\nplot_recurrence_measure(measure=AVG_DIAG_LINE, label='AVG L')\n\n\n\n\nРисунок 4.9: Динаміка індексу Доу-Джонса та середньої довжини діагональних ліній\n\n\n\n\nЯк і до цього, ми можемо бачити, що середній час перебування Доу-Джонса у детермінованому стані зростає під час кризових явищ, що говорить зростання ступеня колективізації трейдерів на ринку.\n\n\n4.2.2.6 Час захоплення/затримки (Trapping time)\nУсереднена довжина діагональної лінії пов’язана із часом передбачуваності динамічної системи та часом затримки. У даному випадку ми середню довжину вертикальних ліній:\n\\[\nTT={\\frac {\\sum _{{v=v_{\\min }}}^{{N}}vP(v)}{\\sum _{{v=v_{\\min }}} ^{{N}}P(v)}}.\n\\]\n\n\n\n\n\n\nДодаткова інформація по середній довжині вертикальних ліній\n\n\n\nСередня довжина вертикальних ліній визначає середній час перебування системи в ламінарному стані. Тобто, вона відповідає середньому періоду часу при якому система “завмирає” у певному стані. Очевидно, що зростання цiєї величини характеризує дедалi бiльший час затримки дослiджуваної системи в певному станi.\n\n\n\nplot_recurrence_measure(measure=TT, label='TT')\n\n\n\n\nРисунок 4.10: Динаміка індексу Доу-Джонса та час затримки\n\n\n\n\nНа представленому рисунку видно, що \\(TT\\) зростає в (перед-)кризові стани, що вказує на потребу системи перебувати ще більший час у стані кризи.\n\n\n4.2.2.7 Середня довжина білих вертикальних лінії (Average white vertical lines length)\nСередня довжина білих вертикальних ліній може бути визначена як\n\\[\nWVL_{mean} = \\sum_{w=w_{min}}^{N} w \\cdot P(w) \\Big / \\sum_{w=w_{min}}^{N} P(w),\n\\]\nде \\(P(w)\\) — це частотний розподіл білих вертикальних ліній довжиною \\(w\\), а \\(w_{min}\\) відповідає найменшій довжині білих вертикальних ліній (найменшому періоду повернення до стану рекурентності).\n\n\n\n\n\n\nДодаткова інформація по середній довжині білих вертикальних ліній\n\n\n\nПредставлену міру можна охарактеризувати як середній горизонт непередбачуваності системи.\n\n\n\nplot_recurrence_measure(measure=AVG_WVERT_LINE, label='WVL_{mean}')\n\n\n\n\nРисунок 4.11: Динаміка індексу Доу-Джонса та середньої довжини білих вертикальних ліній\n\n\n\n\nЗростання середньої довжини бiлих вертикальних лiнiй демонструє, що кризовi подiї характеризуються не лише детермiнiзмом динамiки фондового ринку, але i несхожiстю даних подiй у порiвняннi з попереднiми станами.\n\n\n4.2.2.8 Ентропія діагональних ліній (Diagonal lines entropy)\nДля відповідних діагональних сегментів можна розрахувати необхідну кількість інформації для опису всього розподілу цього типу ліній. Імовірність \\(p(\\ell )\\) того, що діагональна лінія має точну довжину \\(\\ell\\), можна оцінити за частотним розподілом \\(P(\\ell )\\) із \\(p( \\ell )={\\frac {P(\\ell )}{\\sum _{{\\ell = \\ell_{\\min }}}^{N}P(\\ell )}}\\). Ентропія Шеннона цієї ймовірності виглядає наступним чином:\n\\[\nDLEn = -\\sum_{{\\ell =\\ell _{\\min }}}^{N}p(\\ell )\\ln p(\\ell ).\n\\]\nДаний показник відображає складність досліджуваної структури.\n\n\n\n\n\n\nДодаткова інформація по ентропії діагональних ліній\n\n\n\nДля некорельованого шуму чи осциляцiй ми тримали б мале значення цієї ентропiї. Мале значення даної ентропії вказувало би на те, що розподіл діагональних ліній представляється асиметричним: існувала б невеличка частка діагональних ліній конкретної довжини, що характеризувала би всю рекурентність досліджуваної системи. Зростання даної ентропії характеризувало би зростання симетричності розподілу довжин діагональних ліній.\n\n\n\nplot_recurrence_measure(measure=ENT_DIAG, label='DLEn')\n\n\n\n\nРисунок 4.12: Динаміка індексу Доу-Джонса та ентропії діагональних ліній\n\n\n\n\nВидно, що ентропія діагональних ліній зростає під час кризових явищ, що вказує на зростання впливу детермінованих процесів із різним ступенем передбачуваності.\n\n\n4.2.2.9 Ентропія вертикальних ліній (Vertical lines entropy)\nМи можемо визначити Шеннонівську ентропію для розподілу вертикальних структур рекурентної діаграми. Імовірність \\(p( v )\\) того, що вертикальна лінія має точну довжину $ v $, можна оцінити за частотним розподілом \\(P( v )\\) із \\(p( v )= P( v ) \\Big / \\sum _{{ v = v_{\\min }}}^{N}P( v )\\). Ентропія Шеннона цієї ймовірності визначається як\n\\[\nVLEn =-\\sum_{{ v = v_{\\min }}}^{N}p( v )\\ln p( v ).\n\\]\nЦя міра, по аналогії до попередньої ентропії, також є мірою складності системи.\n\n\n\n\n\n\nДодаткова інформація по ентропії вертикальних ліній\n\n\n\nДля синусоїдального процесу ми би очікували мале значення даної ентропії, оскільки це простий періодичний процес. Для складного процесу з пам’ятю ми би очiкуємо високе значення цього типу рекурентної ентропiї. Це означати- ме, що ламiнарнiсть процесу характеризуються рiзноманiтними перiодами довгостроковості пам’яті системи.\n\n\n\nplot_recurrence_measure(measure=ENT_VERT, label='VLEn')\n\n\n\n\nРисунок 4.13: Динаміка індексу Доу-Джонса та ентропії вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що ентропія вертикальних ліній починає зростати під час крахових явищ, що вказує на зростання ступеню ламінарності, тобто зростання рівномірності розподілу вертикальних ліній різноманітних довжин.\n\n\n4.2.2.10 Дивергенція (Divergence)\nПоказник \\(L_{\\max }\\) може надати нам інформацію про максимальний ступінь передбачуваності досліджуваного періоду. Зворотнє значення максимальної довжини діагональних ліній \\(L_{\\max }\\) або дивергенція (розбіжність) може вказати нам на швидкість та тривалість розбіжності досліджуваних траєкторій. Даний показник можна визначити як\n\\[\n\\text{DIV} = {\\frac {1}{L_{\\max }}}.\n\\]\nДана міра схожа на старший показник Ляпунова. Однак взаємозв’язок між цією мірою та позитивним максимальним показником Ляпунова набагато складніший (щоб обчислити показник Ляпунова з RP, необхідно враховувати весь розподіл частот діагональних ліній). Дивергенція може мати тенденцію позитивного максимального показника Ляпунова, але не більше.\n\n\n\n\n\n\nДодаткова інформація по дивергенції\n\n\n\nЧим вище значення дивергенції, тим швидше розбігаються траєкторії фазового простору. І навпаки, чим нижче значення дивергенції, тим ближче досліджувані траєкторії прилягають один до одного.\n\n\n\nplot_recurrence_measure(measure=DIV, label='DIV')\n\n\n\n\nРисунок 4.14: Динаміка індексу Доу-Джонса та дивергенції\n\n\n\n\nДаний рисунок показує, що дивергенція діагональних ліній починає спадати в кризові та передкризові періоди, що також вказує на зростання ступеня впорядкованості динаміки системи в дані періоди часу.\n\n\n4.2.2.11 Дивергенція вертикальних ліній (Vertical line divergence)\nЗворотнє значення максимальної довжини вертикальних ліній \\(V_{max}\\) або розбіжність вертикальних ліній можна визначити як:\n\\[\nVDIV = {\\frac {1}{V_{\\max }}}.\n\\]\n\n\n\n\n\n\nДодаткова інформація по дивергенції вертикальних ліній\n\n\n\nМаксимальна довижна вертикальних ліній надавала нам інформацію про максимальний ступінь незмінюваності системи. Вертикальна дивергенція дозволяє нам охарактеризувати швидкість настання або спаду ламінарності у системі. Чи вище значення \\(VDIV\\), тим швидше система виходить із ламінарного стану. І навпаки, чим нижчий даний показник, тим ближче траєкторії фазового простору один до одного, і тим вищий ступінь ламінарності системи в конкретний момент часу.\n\n\n\nplot_recurrence_measure(measure=VERT_DIV, label='VDIV')\n\n\n\n\nРисунок 4.15: Динаміка індексу Доу-Джонса та дивергенції вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що періоди криз характеризуються спадом вертикальної дивергенції, тобто зростанням кількості вертикальних структур, що характеризують ще більший ступінь ламінарності станів.\n\n\n4.2.2.12 Дивергенція білих вертикальних ліній\nЗворотнє значення максимальної довжини білих вертикальних ліній (\\(WVL_{max}\\)) можна охарактеризувати як дивергенцію білих вертикальних ліній. Її можна визначити наступним чином:\n\\[\nWVDIV = \\frac{1}{WVL_{max}}.\n\\]\nЗростання даного показника має вказувати на зростання ступеня рекурентності системи, а його спад має демонструвати зростання непередбачуваності.\n\nplot_recurrence_measure(measure=WHITE_VERT_DIV, label='WVDIV')\n\n\n\n\nРисунок 4.16: Динаміка індексу Доу-Джонса та дивергенції білих вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що дивергенція білих вертикальних ліній представляє доволі зашумлену динаміку, а тому не може бути використана в якості ефективного індикатора кризових явищ.\n\n\n4.2.2.13 Ентропія білих вертикальних ліній (White vertical lines entropy)\nІмовірність \\(p( \\omega )\\) того, що біла вертикальна лінія має точну довжину \\(\\omega\\), можна оцінити за частотним розподілом \\(P(\\omega )\\) із \\(p( \\omega )={\\frac {P( \\omega )}{\\sum _{{\\omega = \\omega_{\\min }}}^{N}P(\\omega )}}\\). Ентропія Шеннона цієї ймовірності,\n\\[\n{\\text{WVertEn}}=-\\sum_{{\\omega =\\omega _{\\min }}}^{N}p(\\omega )\\ln p(\\omega ),\n\\]\nде \\(\\omega_{min}\\) — мінімальна довжина білої вертикальної лінії.\n\nplot_recurrence_measure(measure=ENT_WHITE_VERT, label='WVLEN')\n\n\n\n\nРисунок 4.17: Динаміка індексу Доу-Джонса та ентропії білих вертикальних ліній\n\n\n\n\nВидно, що ентропія білих вертикальних ліній спадає у кризові та передкризові періоди фондового ринку, що вказує на зростання загальної передбачуваності системи та зміщення розподілу білих вертикальних ліній до конкретних довжин. Тобто, їх розподіл у періоди криз стає менш симетричним, що вказує на поступове заміщення білих вертикальних ліній чорними.\n\n\n4.2.2.14 Співвідношення частоти рекурентності до детермінізму \\(DET/RR\\)\nСпіввідношення між \\(DET\\) і \\(RR\\) (\\(RATIO\\)) можна використовувати для виявлення прихованих фазових переходів у системи:\n\\[\nRATIO_1=\\frac{DET}{RR}=N^2\\frac{\\displaystyle\\sum_{l=l_{min}}^{N}lP(l)}{\\left(\\displaystyle\\sum_{l=1}^{N}lP(l)\\right)^2}\n\\]\n\nplot_recurrence_measure(measure=RATIO_DET_REC, label='RATIO_1')\n\n\n\n\nРисунок 4.18: Динаміка індексу Доу-Джонса та співвідношення між мірою передбачуваності та рекурентності\n\n\n\n\nДаний показник спадає під час кризових явищ фондового ринку. Це говорить про те, що має зростати загальна щільність рекурентних точок, як ізольованих, так і просто розподілу вертикальних структур. Тобто, у кризові періоди \\(RR\\) представляється вищою за \\(DET\\).\n\n\n4.2.2.15 Співвідношення ламінарності до детермінізму (LAM/DET)\nТак само як і попередня міра, відношення ламінарності до детермінізму може дозволити нам виокремити приховані переходи в досліджуваному сигналі:\n\\[\nRATIO_2=\\frac{LAM}{DET}.\n\\]\n\nplot_recurrence_measure(measure=RATIO_LAM_DET, label='RATIO_2')\n\n\n\n\nРисунок 4.19: Динаміка індексу Доу-Джонса та співвідношення між мірою ламінарності та детермінізмом\n\n\n\n\nЯкщо виходити з динаміки показника \\(RATIO_2\\), можна сказати, що загальний ступінь детермінізму починає переважати над ламінарністю під час кризових явищ.\nАле по результатам представлених показників ми можемо сказати, що досліджувані крахові та передкрахові події характеризуються зростанням рекурентності, і подібного роду поведінка може бути використана в якості передвісника подальших криз."
  },
  {
    "objectID": "lab_4.html#теоретичні-відомості",
    "href": "lab_4.html#теоретичні-відомості",
    "title": "5  Лабораторна робота № 4",
    "section": "5.1 Теоретичні відомості",
    "text": "5.1 Теоретичні відомості\n\n5.1.1 Складність. Кількісні міри складності. Інформаційні методи оцінки складності.\nДане століття називають століттям складності. Сьогодні питання “що таке складність?” вивчають фізики, біологи, математики і інформатики, хоча при теперішніх досягненнях у розумінні оточуючого світу, однозначної відповіді на це питання немає.\nЗ цієї причини, відповідно до ідеї І. Пригожина, будемо досліджувати прояви складності системи, застосовуючи при цьому сучасні методи кількісного аналізу складності.\nСеред таких методів на увагу заслуговують: - інформаційно-ентропійні; - засновані на теорії хаосу; - скейлінгово-мультифрактальні.\nЗрозуміло, виходячи з різної природи методів, покладених в основу формування міри складності, вони приділяють певні вимоги до часових рядів, що слугують вхідними даними. Наприклад, перші дві групи методів вимагають стаціонарності вхідних даних. При цьому мають різну чутливість до таких характеристик, як детермінованність, стохастичність, причинність та кореляції. Тому у подальшому, порівнюючи комплексно ефективність різних показників складності, на вказані обставини ми будемо звертати увагу, підкреслюючи спеціально застосовність того чи іншого показника для характеристики різних сторін складності досліджуваних систем.\nРозгляд першої групи методів почнемо з добре відомої міри складності, запропонованої А. Колмогоровим.\nКолмогорівська складність. Поняття колмогорівської складності (або, як ще говорять, алгоритмічної ентропії) з’явилося в 1960-і роки на стику теорії алгоритмів, теорії інформації і теорії ймовірностей.\nІдея А. Колмогорова полягала в тому, щоб вимірювати кількість інформації, що міститься в індивідуальних скінчених об’єктах (а не у випадкових величинах, як у шеннонівській теорії інформації). Виявилось, що це можливо (хоча лише з точністю до обмеженого доданку). А. Колмогоров запропонував вимірювати кількість інформації в скінчених об’єктах за допомогою теорії алгоритмів, визначивши складність об’єкту як мінімальну довжину програми, що породжує цей об’єкт. Дане визначення стало базисом алгоритмічної теорії інформації, а також алгоритмічної теорії ймовірностей: об’єкт вважається випадковим, якщо його складність наближена до максимальної.\nЩо ж собою являє колмогорівська складність і як її виміряти? На практиці ми часто стикаємося з програмами, які стискують файли (для економії місця в архіві). Найбільш поширені називаються zip, gzip, compress, rar, arj та інші. Застосувавши таку програму до деякого файлу (з текстом, даними, програмою), ми отримуємо його стислу версію (яка, як правило, коротше початкового файлу). За нею можна відновити початковий файл з допомогою парної програми-“декомпресора”. Отже, у першому наближенні колмогорівську складність файлу можна описати як довжину його стислої версії. Тим самим файл, що має регулярну структуру і добре стискуваний, має малу колмогорівську складність (порівняно з його довжиною). Навпаки, погано стискуваний файл має складність, близьку до довжини.\nПрипустимо, що ми маємо фіксований спосіб опису (декомпресор) \\(D\\). Для даного слова \\(x\\) розглянемо всі його описи, тобто всі слова \\(y\\), для яких \\(D(y)\\) визначене \\(і\\) рівне \\(x\\). Довжину найкоротшого з них \\(l(y)\\) і називають колмогорівською складністю слова \\(x\\) при даному способі опису \\(D\\):\n\\[\nKS_{D}(x) = \\min\\{l(y)\\,|\\,D(y)=x\\},\n\\]\nде \\(l(y)\\) позначає довжину слова \\(y\\). Індекс \\(D\\) підкреслює, що визначення залежить від вибору способу \\(D\\).\nМожна показати, що існують оптимальні способи опису. Спосіб опису тим краще, чим він коротше. Тому природно дати таке визначення: спосіб \\(D_1\\) не гірше за спосіб \\(D_2\\), якщо \\(KS_{D_1}(x) \\leq KS_{D_2}(x)+c\\) при деякому \\(c\\) і при всіх \\(x\\).\nОтже, за Колмогоровим, складність об’єкту (наприклад, тексту — послідовності символів) — це довжина мінімальної програми яка виводить даний текст, а ентропія — це складність, що ділиться на довжину тексту. На жаль, це визначення чисто умоглядне. Надійного способу однозначно визначити цю програму не існує. Але є алгоритми, які фактично якраз і намагаються обчислити колмогорівські складність тексту і ентропію.\n\n\n5.1.2 Оцінка складності Колмогорова за схемою Лемпела-Зіва\nУніверсальна (в сенсі застосовності до різних мовних систем) міра складності кінцевої символьної послідовності була запропонована Лемпелем і Зівом. У рамках їх підходу складність послідовності оцінюється числом кроків процесу, що її породжує. Припустимими (редакційними) операціями при цьому є:\n\nгенерація символу (необхідна, як мінімум, для синтезу елементів алфавіту) і\nкопіювання “готового” фрагмента з передісторії (тобто з уже синтезованої частини тексту).\n\nНехай \\(\\Sigma\\) — скінчений алфавіт, \\(S\\) — текст (послідовність символів), складений з елементів \\(\\Sigma\\); \\(S[i]\\) — \\(i\\)-й символ тексту; \\(S[i:j]\\) — фрагмент тексту з \\(i\\)-го по \\(j\\)-й символ включно \\((i&lt;j)\\); \\(N=|S|\\) — довжина тексту \\(S\\). Тоді схему синтезу послідовності можна представити у вигляді конкатенації\n\\[\nH(S)=S[1:i_1]S[i_1+1:i_2]...S[i_{k-1}+1:i_k]...S[i_{m−1}+1:N], \\tag{1}\n\\]\nде \\(S[i_{k−1}+1:i_k]\\) — фрагмент \\(S\\), породжуваний на \\(k\\)-му кроці, а $m=m_{H}(S) — число кроків процесу. З усіляких схем породження \\(S\\) обирається мінімальна за числом кроків. Таким чином, складність послідовності \\(S\\) за Лемпелем-Зівом\n\\[\nc_{LZ}(S) = \\min_{H}\\{ m_{H}(S) \\}.\n\\]\nМінімальність числа кроків забезпечується вибором для копіювання на кожному кроці максимально довгого прототипу з передісторії. Якщо позначити через \\(j(k)\\) номер позиції, з якої починається копіювання на \\(k\\)-му кроці, то довжина фрагмента копіювання\n\\[\nl_{j(k)} = i_k - i_{k-1} - 1 = \\max_{j \\leq i_{k-1}}\\{ l_{j} : S[i_{k-1}+1:i_{k-1}+l_j]=S[j:j+l_{j}-1] \\}, \\tag{2}\n\\]\nа сам \\(k\\)-й компонент складнісного розкладання (1) можна записати у вигляді\n\\[\nS[i_{k-1}+1:i_{k}] =\n\\begin{cases}\n    S[j(k):j(k)+l_{j(k)}-1] & \\textrm{if} \\; j(k) \\neq 0, \\\\\n    S[i_{k-1}+1] & \\textrm{if} \\; j(k) = 0.\n\\end{cases} \\tag{3}\n\\]\nВипадок \\(j(k) = 0\\) відповідає ситуації, коли в позиції \\(i_{k−1}+1\\) стоїть символ, який раніше не зустрічався. При цьому ми застосовуємо операцію генерації символу.\nБудемо знаходити складність за Лемпелем-Зівом (LZ) для часового ряду, який являє собою, наприклад, щоденні значення індексу фондового ринку. Для дослідження динаміки LZ та порівняння з іншими фондовими ринками будемо знаходити дану міру складності для підряду фіксованої довжини (вікна). Для цього обчислимо логарифмічні прибутковості та перетворимо їх у послідовність бітів. При цьому можна задавати кількість станів, які диференційовані (система числення). Так, для двох різних станів маємо 0, 1, для трьох — 0, 1, 2 і т.д. Для двійкової системи кодування буде задаватися поріг по середньому значенню і стани, наприклад, прибутковостей (ret) кодуватимуться наступним чином:\n\\[\nret =\n\\begin{cases}\n0, & ret_t &lt; \\langle ret \\rangle \\\\\n1, & ret_t &gt; \\langle ret \\rangle.\n\\end{cases} \\tag{4}\n\\]\nТакож можна визначити так звану пермутаційну складність Лемпеля-Зіва (PLZС). У даному випадку би будемо опиратись на процедуру реконструкції фазового простору, що згадувалась в лабораторних 2 і 3. Згідно пермутаційній процедурі ми будемо брати фрагмент ряду довжини \\(m\\), що слугує розмірностю реконструйованого атрактора, та замінюємо кожне значення ряду його порядковим індексом. На подальшому ресунку представлено часовий ряд та його можливі порядкові шаблони:\n\n\n\nРисунок 5.1: Фрагмент часового ряду (а) та 6 можливих порядкових шаблонів, що можуть бути в цьому сигналі (b)\n\n\nАлгоритм Лемпеля-Зіва виконує дві операції: (1) додає новий біт в уже існуючу послідовність; (2) копіює вже сформовану послідовність. Алгоритмічна складність представляє собою кількість таких операцій, необхідних для формування заданої послідовності.\nДля випадкової послідовності довжини \\(n\\) алгоритмічна складність обчислюється за виразом \\(LZC_r = n / \\log(n)\\). Тоді відносна алгоритмічна складність знаходиться як відношення отриманої складності до складності випадкової послідовності: \\(LZC = LZC / LZC_{r}\\).\nОднак навіть цього підходу може бути недостатньо. Справа в тому, що складні сигнали проявляють притаманну їм складність на різних просторових і часових масштабах, тобто мають масштабно інваріантні властивості. Вони, зокрема проявляються через степеневі закони розподілу. Тому розрахунки алгоритмічної складності на “поверховому” масштабі сигналу можуть бути неприйнятними і призводити до помилкових висновків.\nДля подолання таких труднощів використовуються мультимасштабні методи, до розгляду яких ми і переходимо.\n\n\n5.1.3 Процедура грануляції для мультискейлінгового дослідження часових рядів. Мультимасштабні міри складності\nІдея цієї групи методів включає дві послідовно виконувані процедури:\n\nпроцес “грубого дроблення” (coarse graining — “грануляції”) початкового часового ряду — усереднення даних на сегментах, що не перетинаються, розмір яких (вікно усереднення) збільшуватиметься на одиницю при переході на наступний за величиною масштаб;\nобчислення на кожному з масштабів певного (до сих пір мономасштабного) показника складності.\n\nПроцес “грубого дроблення” (“грануляція”) полягає в усереднені послідовних відліків ряду в межах вікон, що не перетинаються, а розмір яких \\(\\tau\\) — збільшується при переході від масштабу до масштабу. Кожен елемент “гранульованого” часового ряду \\(y_{j}^{\\tau}\\) знаходиться у відповідності до виразу:\n\\[\ny_{j}^{\\tau} = \\frac{1}{\\tau}\\sum_{i=(j-1)\\tau+1}^{j\\tau}x_i, \\; 1 \\leq j \\leq N/\\tau,\n\\]\nде \\(\\tau\\) характеризує фактор масштабування. Довжина кожного “гранульованого” ряду залежить від розміру вікна \\(і\\) рівна \\(N/\\tau\\). Для масштабу рівного 1 “гранульований” ряд просто тотожний оригінальному.\n\n\n\nРисунок 5.2: Схематична ілюстрація процесу грубого дроблення (“грануляції”) початкового часового ряду для масштабів 2 і 3\n\n\nБібліотека neurokit2 представляє метод для обчислення як мономасштабного показника складності Лемпеля-Зіва, так і його мультимасштабного аналогу.\nСинтаксис мономасштабної процедури виглядає наступним чином:\ncomplexity_lempelziv(signal, delay=1, dimension=2, permutation=False, symbolize='mean', **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка, \\(\\tau\\). Використовується лише тоді, коли permutation=True.\ndimension (int) — розмірність вкладень, \\(m\\). Використовується лише коли permutation=True.\npermutation (bool) — якщо значення True, поверне складність Лемпеля-Зіва на основі порядкових патернів.\nsymbolize (str) — використовується тільки коли permutation=False. Метод перетворення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення None, щоб пропустити процес (якщо вхідний сигнал вже є дискретним). Можна скористатися методом complexity_symbolize() для використання іншої процедури символізації ряду.\n****kwargs** — інші аргументи, які передаються до complexity_ordinalpatterns() (якщо permutation=True) або complexity_symbolize().\n\nПовертає\n\nlzc (float) — складність Лемпеля-Зіва (LZC).\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення LZC.\n\nСинтаксис мультимасштабної процедури вже інший:\nentropy_multiscale(signal, scale='default', dimension=3, tolerance='sd', method='MSEn', show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму.\nscale (str або int або list) — список масштабних коефіцієнтів, що використовуються для процедури крос-грануляції часового ряду. Якщо значення \"default\", буде використано range(len(signal) / (dimension + 10)). Якщо \"max\", використовуватиме всі масштаби до половини довжини сигналу. Якщо ціле число, створить діапазон до вказаного цілого числа.\ndimension (int) — розмірність вкладення, \\(m\\).\ntolerance (float) — поріг пропускання \\(\\varepsilon\\) (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються подібними. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\).\nmethod (str) — яку версію мультимасштабного показника обчислювати. Переважна кількість показників за цим методом відповідають саме ентропійним підходам. Нас цікавитиме саме \"LZC\".\nshow (bool) — візуалізувати залежність показника від масштабу.\n****kwargs** — необов’язкові аргументи.\n\nПовертає\n\nfloat — точкова оцінка мультимасштабного показника окремого часового ряду, що відповідає площі під кривою значень цього показника, яка, по суті, є сумою вибіркових значень, наприклад, \"LZC\" в діапазоні масштабних коефіцієнтів.\ndict — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення мультимасштабного показника. Значення показника, що відповідають кожному фактору \"Scale\", зберігаються під ключем \"Value\".\n\n\n\n5.1.4 Шеннонівська складність\nЕнтропійний аналіз часових рядів за допомогою ентропійних показників різного роду буде проведено у наступних роботах. Зараз же ми розглянемо найпростішу з ентропій — ентропію Шеннона та порівняємо її можливості кількісно оцінювати складність часових послідовностей у порівнянні з мірою Лемпеля-Зіва.\nЕнтропія Шеннона — це статистичний квантифікатор, який широко використовується для характеристики складних процесів. Він здатний виявляти аспекти нелінійності в досліджуваних сигналах, сприяючи більш надійному поясненню нелінійної динаміки різних точок аналізу, що, в свою чергу, покращує розуміння природи складних систем, які характеризуються складністю та нерівноважністю. Окрім складності та нерівноважності, більшість, але не всі, складні системи також характеризуються неоднорідним розподілом зв’язків. Поняття ентропії було використано Шенноном в теорії інформації для передачі даних.\nЕнтропія - це міра невизначеності та випадковості в системі. Якщо припустити, що всі наявні дані належать до одного класу, то неважко передбачити клас нових даних. У цьому випадку ентропія дорівнює 0. Будучи величиною між 0 і 1, коли всі ймовірності рівні, ентропія набуває найбільшого значення. Невизначеність, що виникає, коли подія \\(E\\) відбувається з ймовірністю \\(p\\), можна позначити як \\(S(p)\\). Якщо ймовірність появи класу дорівнює 1, тоді ентропія мінімальна, \\(S(1) = 0\\). Відповідно до концепції Шеннона, якщо у нас наявні ймовірності реалізації певної події \\(p_1, p_2, p_3, ..., p_n\\), на виході отримується кількість інформації, що необхідна для опису цієї події. Тоді, Шеннонівська ентропія може бути визначена як\n\\[\nS = -\\sum_{i=1}^{n}p_i \\ln p_{i}.  \n\\]\nСинтаксис методу для розрахунку Шеннонівської ентропії виглядає наступним чином:\nentropy_shannon(signal=None, base=2, symbolize=None, show=False, freq=None, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\nbase (float) — основа логарифму, що за замовчуванням дорівнює 2, що дає одиницю в бітах. Зауважте, що scipy.stats.entropy() за замовчуванням використовує число Ейлера (np.e) (натуральний логарифм), що дає міру інформації, виражену в натах.\nsymbolize (str) — метод приведення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням дорівнює нулю, що пропускає процес (і припускає, що вхідні дані вже є дискретними).\nshow (bool) — якщо значення True, виводить дискретність сигналу.\nfreq (np.array) — замість сигналу можна надати вектор ймовірностей.\n****kwargs** — необов’язкові аргументи. Наразі не використовуються.\n\nПовертає\n\nshanen (float) — Шеннонівську ентропію.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення Шеннонівської ентопії.\n\n\n\n5.1.5 Інформація Фішера\nІнформацію Фішера було введено Р. А. Фішером у 1925 році як міру “внутрішньої точності” в теорії статистичних оцінок. Вона є центральною для багатьох статистичних галузей, що виходять далеко за межі теорії складності. Даний показник вимірює кількість інформації, яку спостережувана випадкова величина несе про невідомий параметр. В аналізі складності вимірюється кількість інформації, яку система несе “про себе”. Він базується на розкладанні за сингулярним значенням реконструйованого фазового простору. Значення показника Фішера зазвичай антикорельоване з іншими показниками складності (чим більше інформації система приховує про себе, тим більш передбачуваною і, відповідно, менш складною вона є).\nІнформацію Фішера можна визначити, використовуючи метод fisher_information() бібліотеки neurokit2. Її синтаксис виглядає наступним чином:\nfisher_information(signal, delay=1, dimension=2)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі, \\(\\tau\\).\ndimension (int) — розмірність векторів фазового простору, \\(m\\).\n\nПовертає\n\nfi (float) — обчислена міра інформації Фішера.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення інформації Фішера.\n\n\n\n5.1.6 Складність та параметри Хьорта\nПараметри Хьорта — це показники статистичних властивостей, які спочатку були введені Хьортом (Hjorth, 1970) для опису загальних характеристик сигналів електроенцифалограми у кількох кількісних термінах, але які можуть бути застосовані до будь-якого часового ряду. Параметрами є активність, рухливість і складність:\n\nПараметр активності (\\(Activity\\)) — це просто дисперсія сигналу, яка відповідає середній потужності сигналу (якщо його середнє значення дорівнює 0).\n\n\\[\nActivity = \\sigma^{2}_{signal}.\n\\]\n\nПараметр рухливості (\\(Mobility\\)) являє собою середню частоту або частку середньоквадратичного відхилення спектра потужності. Він визначається як квадратний корінь з дисперсії першої похідної сигналу, поділений на дисперсію сигналу.\n\n\\[\nMobility = \\frac{\\sigma_{dd}/\\sigma_{d}}{Complexity}.\n\\]\n\nПараметр складності (\\(Complexity\\)) дає оцінку смуги пропускання сигналу, яка вказує на схожість форми сигналу з чистою синусоїдою (для якої значення сходиться до 1). Іншими словами, це міра “надмірної деталізації” по відношенню до “найм’якшої” можливої форми кривої. Параметр “Складність” визначається як відношення рухливості першої похідної сигналу до рухливості самого сигналу.\n\n\\[\nComplexity = \\frac{\\sigma_d}{\\sigma_{signal}},\n\\]\nде \\(d\\) та \\(dd\\) представляють перші та другі похідні сигналу, відповідно.\n\n\n\nРисунок 5.3: Характеристичні зміни форми кривої, що ілюструє залежність кожного параметра\n\n\nБібліотека neurokit2 представляє метод для отримання відповідних показників. Її синтаксис виглядає наступним чином:\ncomplexity_hjorth(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nhjorth (float) — показник складності Хьорта.\ninfo (dict) — словник, що містить додаткові показники Хьорта, такі як \"Mobility\" та \"Activity\".\n\n\n\n5.1.7 Час декореляції\nЧас декореляції (decorrelation time, DT) визначається як час (у відліках) першого перетину нуля функції автокореляції. Коротший час декореляції відповідає менш корельованому сигналу. Наприклад, зменшення часу декореляції в сигналах електроенцифалограми спостерігається перед нападами, що пов’язано зі зменшенням потужності низьких частот.\nБібліотека neurokit2 представляє функціонал для визначення часу декореляції, а саме метод complexity_decorrelation(). Її синтаксис є наступним:\ncomplexity_decorrelation(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nfloat — час декореляції.\ndict — словник, що містить додаткову інформацію про додаткові показники.\n\n\n\n5.1.8 Відносна грубість (нерівність, шорсткість)\nВідносна шорсткість — це відношення локальної дисперсії (автоковаріації з лагом 1) до глобальної дисперсії (автоковаріації з лагом 0), яке можна використовувати для класифікації різних “шумів”. Його також можна використовувати як індекс для перевірки застосовності фрактального аналізу (показники фрактальності будуть використовуватись у наступних роботах).\nСинтаксис даного методу в бібліотеці neurokit2 виглядає наступним чином:\ncomplexity_relativeroughness(signal, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (часовий ряд) у вигляді вектора значень.\n****kwargs** (optional) — інші аргументи, що потребуються методу nk.signal_autocor().\n\nПовертає\n\nrr (float) — значення відносної грубості.\ninfo (dict) — словник, що містить інформацію відносно параметрів, що використовувались для обчислення показника грубості.\n\n\n\n5.1.9 Взаємна інформація\nКоли йдеться про виявлення зв’язків між змінними, ми часто використовуємо кореляцію Пірсона. Проблема полягає в тому, що цей показник знаходить лише лінійні зв’язки, що іноді може призвести до неправильної інтерпретації зв’язку між двома змінними. Тим не менш, інші статистичні методи вимірюють нелінійні зв’язки, такі як взаємна інформація (mutual information, MI).\nВзаємна інформація між двома випадковими величинами вимірює нелінійний зв’язок між ними. Крім того, вона показує, скільки інформації можна отримати з випадкової величини, спостерігаючи за іншою випадковою величиною.\nВона тісно пов’язана з поняттям ентропії. Тобто, зменшення невизначеності випадкової величини пов’язане з отриманням інформації з іншої випадкової величини. Отже, високе значення взаємної інформації вказує на велике зменшення невизначеності, тоді як низьке значення вказує на мале зменшення. Якщо взаємна інформація дорівнює нулю, це означає, що дві випадкові величини є незалежними.\nВзаємну інформацію можна розрахувати наступним чином:\n\\[\nI(X; Y) = \\sum_{y \\in Y}\\sum_{x \\in X}p(x, y) \\cdot \\log{\\left( \\frac{p(x,y)}{p(x)p(y)} \\right)},\n\\]\nде \\(p(x)\\) та \\(p(y)\\) ймовірності спостереження окремо \\(x\\) або \\(y\\), а \\(p(x,y)\\) ймовірність спостереження одночасно \\(x\\) та \\(y\\).\nОсновна відмінність між кореляцією та взаємною інформацією полягає в тому, що кореляція є мірою лінійної залежності, тоді як взаємна інформація вимірює загальну залежність (включаючи нелінійні зв’язки). Тому взаємна інформація виявляє залежності, які не залежать тільки від коваріації. Таким чином, взаємна інформація дорівнює нулю, коли дві випадкові величини є строго незалежними.\nБібліотека neurokit2 представляє інструментарій для знаходження взаємної інформації між двома сигналами \\(x\\) та \\(y\\). У даній роботі ми спробуємо віднайти взаємну інформацію як між двома часовими рядами, так і авто-взаємну інформацію, подібно до автокореляції.\nСинтаксис потрібної нам процедури виглядає наступним чином:\nmutual_information(x, y, method='varoquaux', bins='default', **kwargs)\nПараметри\n\nx (Union[list, np.array, pd.Series]) — масив значень.\ny (Union[list, np.array, pd.Series]) — масив значень.\nmethod (str) — метод для обчислення взаємної інформації: \"nolitsa\", \"varoquaux\", \"knn\", \"max\".\nbins (int) — кількість бінів гістограми. Використовується лише для \"nolitsa\" та \"varoquaux\". Якщо \"default\", кількість бінів оцінюється згідно методики Hacine-Gharbi (2018).\n****kwargs** — додаткові ключові аргументи для обраного методу.\n\nПовертає\n\nfloat — розрахована взаємна інформація.\n\nІснують різноманітні підходи до розрахунку взаємної інформації:\n\nnolitsa: Класична взаємна інформація (трохи швидше, ніж метод \"sklearn\").\nvaroquaux: Застосовує фільтр Гауса до об’єднаної гістограми. Величину згладжування можна налаштовувати за допомогою аргументу sigma (за замовчуванням sigma=1).\nknn: Непараметрична (тобто не заснована на біннінгу) оцінка за найближчими сусідами. Додаткові параметри включають k (за замовчуванням, k=3), кількість найближчих сусідів для використання.\nmax: Максимальний коефіцієнт взаємної інформації, тобто \\(MI\\) є максимальним при певній комбінації кількості бінів.\n\nІснує безліч різноманітних показників складності, що базуються на теорії інформації та інших парадигах, які ми ще представлятимемо в подальшому. Розглянемо ефективність використання зазначених показників у якості індикаторів або індикаторів-передвісників крахових подій."
  },
  {
    "objectID": "lab_4.html#хід-роботи",
    "href": "lab_4.html#хід-роботи",
    "title": "5  Лабораторна робота № 4",
    "section": "5.2 Хід роботи",
    "text": "5.2 Хід роботи\nСпочатку імпортуємо необхідні модулі для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виведення:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nЦього разу розглянему можливість побудови індикаторів-передвісників на прикладі фондового індексу S&P 500, але, окрім цього, додамо ще Біткоїн для розрахунку взаємної інформації між фондовим ринком та криптовалютним. Очевидно, що фондовий індекс S&P 500 мав би проіснувати довше за Біткоїн. До того ж, криптовалютний ринок працює безперервно на відміну від фондового, а тому треба буде об’єднати значення двох активів за тими датами що співпадають.\nВиконуємо зчитування фондового індексу:\n\nsymbol_1 = '^GSPC'         # Символ першого індексу\nstart_1 = \"2014-01-01\"     # Дата початку зчитування даних\nend_1 = \"2023-08-24\"       # Дата закінчення зчитування даних\n\ndata_1 = yf.download(symbol_1, start_1, end_1)  # вивантажуємо дані\ntime_ser_1 = data_1['Adj Close'].copy()         # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'      # підпис по вісі Ох \nylabel_1 = symbol_1        # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиконуємо зчитування криптовалютного індексу:\n\nsymbol_2 = 'BTC-USD'       # Символ другого індексу\nstart_2 = \"2014-01-01\"     # Дата початку зчитування даних\nend_2 = \"2023-08-24\"       # Дата закінчення зчитування даних\n\ndata_2 = yf.download(symbol_2, start_2, end_2)  # вивантажуємо дані\ntime_ser_2 = data_2['Adj Close'].copy()         # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'      # підпис по вісі Ох \nylabel_2 = symbol_2        # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nТепер створимо новий масив даних, що об’єднуватиме в собі значення S&P 500 та BTC по їх спільним датам:\n\n# приводимо значення індексів до типу DataFrame, щоб мати змогу їх об'єднати \n# за допомогою бібліотеки pandas\ndf_time_ser_1 = pd.DataFrame(time_ser_1) \ndf_time_ser_2 = pd.DataFrame(time_ser_2)\n\n\njoined = df_time_ser_1.merge(df_time_ser_2, # об'єднуємо по датам тієї бази, що містить \n                             on='Date',     # більше дат\n                             how='left')  \n\njoined = joined.rename(columns={joined.columns[0]: symbol_1,  # переіменовуємо колонки по \n                                joined.columns[1]: symbol_2}) # змінним symbol_1 та symbol_2\n\njoined = joined.dropna()  # видаляємо рядки, що містять нульові значення\n\nВиводимо отриману базу:\n\njoined\n\n\n\n\n\n\n\n\n^GSPC\nBTC-USD\n\n\nDate\n\n\n\n\n\n\n2014-09-17\n2001.569946\n457.334015\n\n\n2014-09-18\n2011.359985\n424.440002\n\n\n2014-09-19\n2010.400024\n394.795990\n\n\n2014-09-22\n1994.290039\n402.152008\n\n\n2014-09-23\n1982.770020\n435.790985\n\n\n...\n...\n...\n\n\n2023-08-17\n4370.359863\n26664.550781\n\n\n2023-08-18\n4369.709961\n26049.556641\n\n\n2023-08-21\n4399.770020\n26124.140625\n\n\n2023-08-22\n4387.549805\n26031.656250\n\n\n2023-08-23\n4436.009766\n26431.640625\n\n\n\n\n2249 rows × 2 columns\n\n\n\nІ візуалізуємо сам графік. Спочатку оголосимо функцію для попарної візуалізації рядів зі збереженням їх абсолютних значень:\n\ndef plot_pair(x_values, y_values, x_label, y_label, file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y_values[0], \n                  \"b-\", label=fr\"{y_label[0]}\")\n    p2, = ax2.plot(x_values,\n                   y_values[1], \n                   color=clr, \n                   label=fr'${y_label[1]}$')\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y_label[0]}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nі тепер візуалізуємо отримані ряди:\n\nvalues_plot = joined.iloc[:,0].values, joined.iloc[:,1].values\nylabels = ylabel_1, ylabel_2\nfile_name = f'joined {symbol_1}_{symbol_2}'\n\n\nplot_pair(joined.index, values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.4: Динаміка індексу S&P 500 та Біткоїна за досліджуваний період\n\n\n\n\n\n5.2.1 Розрахунок взаємної інформації\nРозглянемо взаємну інформацію як індикатор нелінійної кореляції між двома фінансовими активами, і спробуємо сказати, чи є між ними “істинний” взаємозв’язок. Виконуватимемо розрахунки із використанням алгоритму руховому вікна. Також визначимо функцію transform() для нормалізації ряду.\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\nret_type = 6                           # вид ряду\nwindow = 100                           # ширина вікна\ntstep = 1                              # часовий крок вікна \nlength = len(joined.iloc[:,0].values)  # довжина самого ряду\n\nMI = []                                # масив для віконної взаємної інформації\n\nТепер приступимо до розрахунків:\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагменти\n    fragm_1 = joined[symbol_1][i:i+window]  \n    fragm_2 = joined[symbol_2][i:i+window]\n\n    # виконуємо процедуру трансформації ряду \n    fragm_1 = transformation(fragm_1, ret_type)    \n    fragm_2 = transformation(fragm_2, ret_type)\n\n    # розраховуємо взаємну інформацію \n    mut_inf = nk.mutual_information(fragm_1, fragm_2)\n    \n    # та додаємо результат до масиву значень\n    MI.append(mut_inf)\n\n100%|██████████| 2149/2149 [00:03&lt;00:00, 706.05it/s]\n\n\nЗберігаємо отриманий результат у текстовому файлі:\n\nnp.savetxt(f\"mutual_inf_name1={symbol_1}_name2={symbol_2}_ \\\n    window={window}_step={tstep}_rettype={ret_type}.txt\" , MI)\n\nВізуалізуємо результат між відповідними показниками:\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(joined.index[window:length:tstep], \n                joined[symbol_1][window:length:tstep].values, \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(joined.index[window:length:tstep],\n                joined[symbol_2][window:length:tstep].values,\n                'red', \n                label=fr\"{symbol_2}\")\np3, = ax3.plot(joined.index[window:length:tstep],\n                MI,\n                'magenta', \n                label=r\"$MI$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"mutual_inf_name1={symbol_1}_name2={symbol_2}_ \\\n    window={window}_step={tstep}_rettype={ret_type}.jpg\")\n\nplt.show();\n\n\n\n\nРисунок 5.5: Динаміка індексу S&P 500, Біткоїна та взаємної інформації\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, на фондовому та криптовалютному ринках дійсно спостерігалися фази зростання взаємної інформації між ними. Найкраще це видно напередодні кризи 2018-го року, під час 2019, після коронавірусної пандемії та напередодні 2023 року. Для даного індикатора залишається простір для експериментів, що можуть вивести його на рівень достатньо потужного передвісника криз на фондовому ринку чи криптовалютному.\nЯк вже зазначалося, окрім обчислення взаємної інформації для двох пар часових сигналів, ми можемо обчислити автовзаємну інформація, тобто взаємну інформацію ряду самого із собою по різним часовим лагам, як це було пророблено для автокореляції. Недолік автокореляції полягає в тому, що вони визначає саме лінійний зв’язок теперішніх значень з попередніми. Автовзаємна інформація в свою чергу є показником нелінійного зв’язку теперішніх значень із попередніми.\nДля обчислення автовзаємної інформації визначимо наступну функцію:\n\ndef automut(x, maxlag):\n    n = len(x)                               # визначаємо довжину сигналу\n    lags = np.arange(0, maxlag, dtype=\"int\") # оголошуємо масив лагів від 0 до maxlag\n    mi = np.zeros(len(lags))                 # оголошуємо масив під значення взаємної інформації\n    for i, lag in enumerate(lags):           # проходимось по кожному лагу\n        \n        # виконуємо зміщення на lag значень \n        y1 = x[:n-lag].copy()\n        y2 = x[lag:].copy()\n\n        # і розраховуємо взаємну інформацію між часовим рядом y1\n        # та його зміщенною на lag кроків копією \n        mi[i] = nk.mutual_information(y1, y2, bins=100)\n\n    return mi\n\nВиведемо залежність автовзаємної інформації від лагу для всього ряду S&P 500 та Біткоїна. Спочатку розрахуємо вихідні значення ряду, далі прибутковості і потім волатильності. Для кожного з відповідних сигналів виведемо взаємну інформацію.\nВиконуємо перетворення S&P 500 та Біткоїна\n\nsp_init = transformation(time_ser_1, ret_type=1)\nsp_ret = transformation(time_ser_1, ret_type=4)\nsp_vol = np.abs(sp_ret.copy())\n\nbtc_init = transformation(time_ser_2, ret_type=1)\nbtc_ret = transformation(time_ser_2, ret_type=4)\nbtc_vol = np.abs(btc_ret.copy())\n\nРозраховуємо автовзаємну інформацію S&P 500 та Біткоїна\n\nmax_lag = 100\n\nmu_sp_init = automut(sp_init, max_lag)\nmu_sp_ret = automut(sp_ret, max_lag)\nmu_sp_vol = automut(sp_vol, max_lag)\n\nmu_btc_init = automut(btc_init, max_lag)\nmu_btc_ret = automut(btc_ret, max_lag)\nmu_btc_vol = automut(btc_vol, max_lag)\n\nlags = np.arange(0, max_lag, dtype=\"int\") # оголошуємо масив лагів від 0 до maxlag\n\n\nfig, ax = plt.subplots()                     # Створюємо порожній графік\n\nax.plot(lags, mu_sp_init, label=r'$MI $ ' + f'{symbol_1}')  # Додаємо дані до графіку\nax.plot(lags, mu_sp_ret, label=r'$MI$ ' + r'$g(t)$')                          \nax.plot(lags, mu_sp_vol, label=r'$MI$ ' +  r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Automutual information\")     # Додаємо підпис для вісі Оу\n\nplt.savefig(f'Automutual information {symbol_1}.jpg')  # Зберігаємо графік \nplt.show();                                            # Виводимо графік\n\n\n\n\nРисунок 5.6: Зміна з часом автовзаємної інформації для вихідного ряду x, нормалізованих прибутковостей g та модулів mod(g) фондового індексу S&P 500\n\n\n\n\n\nfig, ax = plt.subplots()                     # Створюємо порожній графік\n\nax.plot(lags, mu_btc_init, label=r'$MI $ ' + f'{symbol_2}')  # Додаємо дані до графіку\nax.plot(lags, mu_btc_ret, label=r'$MI$ ' + r'$g(t)$')                          \nax.plot(lags, mu_btc_vol, label=r'$MI$ ' +  r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Automutual information\")     # Додаємо підпис для вісі Оу\n\nplt.savefig(f'Automutual information {symbol_2}.jpg')  # Зберігаємо графік \nplt.show();                                            # Виводимо графік\n\n\n\n\nРисунок 5.7: Зміна з часом автовзаємної інформації для вихідного ряду x, нормалізованих прибутковостей g та модулів mod(g) криптовалютного індексу BTC\n\n\n\n\nЯк ми можемо бачити з представлених графіків, ступінь взаємної інформації це показник, що найкращим чином працює саме для вихідних значень часових сигналів. Для вихідного ряду ступінь взаємної інформації залишається доволі високим. Для прибутковостей і волатильностей взаємна інформація спадає одразу на першому лагу, що свідчить про незалежність значень на подальших часових затримках.\n\n\n5.2.2 Розрахунок мономасштабної складності Лемпеля-Зіва\nПродовжимо розраховувати й інші показники складності. Розглянемо можливість використання показника складності Лемпеля-Зіва в якості індикатора катастрофічних подій.\n\nret_type = 4                           # вид ряду\nwindow = 250                           # ширина вікна\ntstep = 1                              # часовий крок вікна \nlength = len(time_ser_1.values)        # довжина самого ряду\nm = 4                                  # розмірність вкладень \ntau = 1                                # часова затримка         \n\nLZC = []                               # класична складність Лемпеля-Зіва\nPLZC = []                              # пермутаційна складність Лемпеля-Зіва\n\n\nfor i in tqdm(range(0,length-window,tstep)):    # фрагменти довжиною window  \n                                                # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо класичну складність Лемпеля-Зіва \n    lzc, _ = nk.complexity_lempelziv(fragm)\n\n    # та пермутаційну складність Лемпеля-Зіва\n    plzc, _ = nk.complexity_lempelziv(fragm, \n                                      delay=tau, \n                                      dimension=m, \n                                      permutation=True)\n\n\n    # та додаємо результати до масиву значень\n    LZC.append(lzc)\n    PLZC.append(plzc)\n\n100%|██████████| 2177/2177 [00:18&lt;00:00, 116.70it/s]\n\n\nЗберігаємо результати в текстових файлах:\n\nnp.savetxt(f\"lzc_name={symbol_1}_window={window}_step={tstep}_rettype={ret_type}.txt\" , LZC)\nnp.savetxt(f\"plzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.txt\" , PLZC)\n\nТа візуалізуємо їх:\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n                time_ser_1.values[window:length:tstep], \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep],\n                LZC,\n                'gold', \n                label=fr\"$LZC$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep],\n                PLZC,\n                'red', \n                label=fr\"$PLZC$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"plzc_lzc_name={symbol_1}_ \\\n    window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.jpg\")\n\nplt.show();\n\n\n\n\nРисунок 5.8: Динаміка індексу S&P 500, класичної мономасштабної складності Лемпеля-Зіва та її пермутаційного аналогу\n\n\n\n\nНа даному рисунку видно, що 2 міри поводять себе асиметрично по відношенню один до одного: \\(LCZ\\) вказує на зростання складності, наприклад, події 2019 року. У той же час \\(PLCZ\\) вказує на спад складності системи в цей період. Варто дослідити мультимасштабну динаміку міри Лемпеля-Зіва для більш змістовних висновків.\n\n\n5.2.3 Обчислення мультимасштабної складності Лемпеля-Зіва\n\nret_type = 4\nret_sp = transformation(time_ser_1, ret_type)\n\n\nmslzc, info = nk.entropy_multiscale(ret_sp, method=\"LZC\", \n                                    scale=200, show=True)\n\n\n\n\nРисунок 5.9: Залежність від масштабу класичної складності Лемпеля-Зіва для S&P 500\n\n\n\n\nМультимасштабна динаміка пермутаційного показника складності Лемпеля-Зіва\n\nmsplzc, info = nk.entropy_multiscale(ret_sp, \n                                        method=\"LZC\",  \n                                        permutation=True,\n                                        dimension=m,\n                                        delay=tau, \n                                        scale=200, \n                                        show=True)\n\n\n\n\nРисунок 5.10: Залежність від масштабу пермутаційної складності Лемпеля-Зіва для S&P 500\n\n\n\n\nТепер розрахуємо віконну динаміку мультимасштабних показників Лемпеля-Зіва. Ми повертатимемо сумарну складність Лемпеля-Зіва за всіма масштабам.\n\nret_type = 4                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду\nm = 4                             # розмірність вкладень \ntau = 1                           # часова затримка         \n\nMSLZC = []                        # мультимасштабна складність Лемпеля-Зіва\nMSPLZC = []                       # мультимасштабна пермутаційна складність Лемпеля-Зіва\n\n\nfor i in tqdm(range(0,length-window,tstep)):    # фрагменти довжиною window  \n                                                # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо мультимасштабну складність Лемпеля-Зіва \n    mslzc, _ = nk.entropy_multiscale(fragm)\n\n    # та мультимасштабну пермутаційну складність Лемпеля-Зіва\n    msplzc, _ = nk.entropy_multiscale(fragm, \n                                      delay=tau, \n                                      dimension=m, \n                                      permutation=True)\n\n\n    # та додаємо результати до масиву значень\n    MSLZC.append(mslzc)\n    MSPLZC.append(msplzc)\n\n100%|██████████| 2177/2177 [00:49&lt;00:00, 43.90it/s]\n\n\n\nnp.savetxt(f\"mslzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}.txt\" , MSLZC)\nnp.savetxt(f\"msplzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.txt\" , MSPLZC)\n\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n                time_ser_1.values[window:length:tstep], \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep],\n                MSLZC,\n                'gold', \n                label=fr\"$MSLZC$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep],\n                MSPLZC,\n                'red', \n                label=fr\"$MSPLZC$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"msplzc_mslzc_name={symbol_1}_ \\\n    window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.jpg\")\n\nplt.show();\n\n\n\n\nРисунок 5.11: Динаміка індексу S&P 500, класичної мультимасштабної складності Лемпеля-Зіва та її пермутаційного аналогу\n\n\n\n\nТепер бачимо однозначну картину: обидві міри поводять себе синхронно, та спадають у кризові та передкризові періоди, що вказує на зростання ступеня детермінованості та самоорганізації ринку.\n\n\n5.2.4 Обчислення Шеннонівської ентропії\nЯк уже зазначалося, Шеннонівська ентропія — це міра непередбачуваності стану, або, еквівалентно, його середнього інформаційного вмісту. Ентропія Шеннона є однією з перших і найбільш базових мір ентропії та фундаментальним поняттям теорії інформації.\nРозраховуватимемо її в ковзному вікні.\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду  \nlog_base = np.exp(1)      \n\nshannon = []                      # ентропія Шеннона\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо ентропію Шеннона\n    p, be = np.histogram(fragm,         # розраховуємо щільність ймовірностей\n                        bins='auto', \n                        density=True)  \n    r = be[1:] - be[:-1]                # знаходимо dx\n    P = p * r                           # представляємо ймовірність як f(x)*dx\n    P = P[P!=0]                         # фільтруємо по всім ненульовим ймовірностям\n\n    sh_ent, _ = nk.entropy_shannon(freq=P, base=log_base) # розраховуємо ентропію \n    sh_ent /= np.log(len(P))                              # та нормалізуємо\n\n    # та додаємо результат до масиву значень\n    shannon.append(sh_ent)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 2135.92it/s]\n\n\n\nnp.savetxt(f\"shannon_ent_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\" , shannon)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], shannon\nylabels = ylabel_1, \"ShEn\"\nfile_name = f\"shannon_ent_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], \n            values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.12: Динаміка індексу S&P 500 та ентропії Шеннона\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, ентропія Шеннона реагує спадом на кризові періоди індексу S&P 500, що вказує на приріст ступеня періодизації системи, її детермінованості.\n\n\n5.2.5 Розрахунок інформаційного показника Фішера\nПерш за все задаємо параметри для розрахунків:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду  \nm = 3                             # розмірність вкладень\ntau = 1                           # часова затримка\n\nfisher = []                       # інформація Фішера\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    fish_inf, _ = nk.fisher_information(signal=fragm,\n                                        dimension=m, \n                                        delay=tau) \n\n    # та додаємо результат до масиву значень\n    fisher.append(fish_inf)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 2937.25it/s]\n\n\n\nnp.savetxt(f\"fisher_inf_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_dimension={m}_delay={tau}.txt\", fisher)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], fisher\nylabels = ylabel_1, \"FI\"\nfile_name = f\"fisher_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_dimension={m}_delay={tau}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.13: Динаміка індексу S&P 500 та інформаційного показника Фішера\n\n\n\n\nНа даному рисунку видно, що показник Фішера спадає у кризові та передкризові періоди, що говорить про спад кількості інформації, що необхідна для опису самоорганізованої динаміки фінансових криз, тобто зростання корельованості між діями трейдерів на ринку.\n\n\n5.2.6 Обчислення часу декореляції\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\ndecorrelation_time = []           # час декореляції\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    dec_time, _ = nk.complexity_decorrelation(fragm) \n\n    # та додаємо результат до масиву значень\n    decorrelation_time.append(dec_time)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 1857.09it/s]\n\n\n\nnp.savetxt(f\"dec_time_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", decorrelation_time)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], decorrelation_time\nylabels = ylabel_1, \"DT\"\nfile_name = f\"dec_time_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, \n            xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.14: Динаміка індексу S&P 500 та часу декореляції\n\n\n\n\nНа представленому рисунку видно, що час декореляції зростає у період краху, що вказує на зростання кореляції системи в цей період.\n\n\n5.2.7 Обчислення відносної шорсткості\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\nrelative_roughness = []           # відносна шорсткість\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    rr, _ = nk.complexity_relativeroughness(fragm) \n\n    # та додаємо результат до масиву значень\n    relative_roughness.append(rr)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 2094.81it/s]\n\n\n\nnp.savetxt(f\"rel_rough_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", relative_roughness)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], relative_roughness\nylabels = ylabel_1, \"RR\"\nfile_name = f\"rel_rough={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, \n            xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.15: Динаміка індексу S&P 500 та показника відносної шорсткості\n\n\n\n\nПоказник відносної шорсткості демонструє, що крахові події як, наприклад, у 2015, 2016, 2019, 2020 та 2023 роках характеризуються зростанням шорсткості своєї динаміка. Подібного роду поведінка є індикатором зростання шумової активності ринку: кореляційних характеристик та загальної варіації ринку в цілому. Зростання цього показника в періоди криз є індикатором зростання фрактальності ринку в дані періоди часу.\n\n\n5.2.8 Розрахунок показників складності Хьорта\nЗавершуємо хід роботи показниками складності Хьорта:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\nactivity = []                     # параметр активності\nmobility = []                     # параметр рухливості\ncomplexity = []                   # параметр складності\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо показники складності Хьорта\n    cmpl, info = nk.complexity_hjorth(fragm) \n\n    # та додаємо результат до масиву значень\n    activity.append(info['Activity'])\n    mobility.append(info['Mobility'])\n    complexity.append(cmpl)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 3338.20it/s]\n\n\n\nnp.savetxt(f\"activity_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", activity)\nnp.savetxt(f\"mobility_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", mobility)\nnp.savetxt(f\"complexity_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", complexity)    \n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\nax4.spines.right.set_position((\"axes\", 1.19))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n              time_ser_1.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep], \n               activity, \"r--\", label=r\"$Act$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep], \n               mobility, \"g-\", label=r\"$Mob$\")\np4, = ax4.plot(time_ser_1.index[window:length:tstep],\n               complexity, \"m-\", label=r\"$Comp$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax4.legend(handles=[p1, p2, p3, p4])\n\nplt.savefig(f\"hjorth_name={symbol_1}_ret={ret_type}_wind={window}_step={tstep}.jpg\")\nplt.show();\n\n\n\n\nРисунок 5.16: Динаміка індексу S&P500 наряду з показниками активності, мобільності та складності Хьорта\n\n\n\n\nНа даному рисунку видно, що параметр активності (\\(Act\\)) представляється найменш інформативним, оскільки він вказує тільки на зростання сукупної дисперсії сигналу. Видно тільки те, що активність значно почала зростати напередодні 2022 року, але для попередніх кризових станів ми не бачимо передвісницької поведінки цього індикатора, тому він ще вимагатиме додактових досліджень та експериментів, що виходять за рамки даного посібника.\nПитання передчасної ідентифікації наростання кризового явища найкраще вирішує показник мобільності (\\(Mob\\)). Ми бачимо, що даний показник зростає під час 2015-2016 років, напередодні 2019, при настанні коронавірусної пандемії, перед 2023 роком та 2024.\nПоказник складності Хьорта (\\(Comp\\)) реагує асиметричним чином: у той час коли мобільність зростає, показник складності спадає, вказуючи на те, що динаміки системи прагне до вищого ступеня періодичності або корельованості."
  },
  {
    "objectID": "lab_5.html#теоретичні-відомості",
    "href": "lab_5.html#теоретичні-відомості",
    "title": "6  Лабораторна робота № 5",
    "section": "6.1 Теоретичні відомості",
    "text": "6.1 Теоретичні відомості\nПитання динаміки розвитку і функціонування складних систем може розглядатись у двох варіантах:\n\nяк дослідження шумової активності;\nяк детерміністичного випадку з певним ступенем порядку.\n\nОстанніми роками було використано кілька підходів для ідентифікації механізмів, що лежать в основі розвитку та функціонування складних систем. Особливо корисні результати було отримано при їх дослідженні методами теорії випадкових матриць, моно- та мультифрактального аналізу, теорії хаосу з реконструкцією траєкторії системи у фазовому просторі та визначення її параметрів, рекурентного аналузу. Ми розглянули ці методи у попередніх роботах. Однак, застосування деяких із методів висуває вимоги до стаціонарності досліджуваних даних, потребує довгих часових рядів та комплексного обчислення кількох параметрів.\nІншим підходом до розгляду питання вивчення особливостей складних систем є обчислення характеристик ентропії. Для практичного застосування у якості міри невизначеності, а значить і складності сигналу, використовують десятки різновидів ентропії.\nКонцепція термодинамічної ентропії як міри хаосу системи добре відома у фізиці, однак, останніми роками поняття ентропії було застосоване до складних систем інших об’єктів (біологічних, економічних, соціальних тощо). Так, один із найбільш часто використовуваних методів визначення ентропії базується на обчисленні спектру потужності Фур’є та застосовується для вивчення сигналів (часових рядів) різної природи. Проте, використання дискретного перетворення Фур’є для аналізу часових рядів має свої недоліки, зокрема, на результати впливає нестаціонарність рядів, варіювання їх довжини від сотень до сотень тисяч, та обмеження самого методу (незмінність частотно-часових характеристик протягом всього часу функціонування системи). Тому виникає питання про розрахунок значень ентропії за допомогою інших методів.\nВведемо поняття ентропії, скориставшись інформацією, яку можна знайти у Вікіпедії.\nТермодинамічна ентропія \\(S\\), часто просто іменована ентропія, в хімії і термодинаміці є мірою кількості енергії у фізичній системі, яка не може бути використана для виконання роботи. Вона також є мірою безладдя, присутнього в системі.\nПоняття ентропії була вперше введено у 1865 році Рудольфом Клаузіусом. Він визначив зміну ентропії термодинамічної системи при оборотному процесі як відношення зміни загальної кількості тепла \\(\\Delta Q\\) до величини абсолютної температури \\(T\\):\n\\[\n\\Delta S = \\Delta Q / T.\n\\]\nРудольф Клаузіус дав величині \\(S\\) ім’я “ентропія”, що походить від грецького слова τρoπή, “зміна” (зміна, перетворення). Зверніть увагу на те, що рівність відноситься до зміни ентропії.\nУ 1877 році, Людвіг Больцман зрозумів, що ентропія системи може відноситися до кількості можливих “мікростанів” (мікроскопічних станів) що узгоджуються з їх термодинамічними властивостями. Розглянемо, наприклад, ідеальний газ у посудині. Мікростан визначений як позиції і імпульси кожного атома, що становить систему. Зв’язність пред’являє до нас вимоги розглядати тільки ті мікростани, для яких: (i) місцерозташування всіх частин розташовані в рамках судини, (ii) для отримання загальної енергії газу кінетичні енергії атомів підсумовуються. Больцман постулював що\n\\[\nS = k_{B}\\ln{\\Omega},\n\\]\nде константу \\(k_{B} = 1,38 \\cdot 10^{-23} Дж/К\\) ми знаємо тепер як сталу Больцмана, a \\(\\Omega\\) є числом мікростанів, які можливі в наявному макроскопічному стані. Цей постулат, відомий як принцип Больцмана, може бути оцінений як початок статистичної механіки, яка описує термодинамічні системи використовуючи статистичну поведінку компонентів, із яких вони складаються. Принцип Больцмана зв’язує мікроскопічні властивості системи (\\(\\Omega\\)) з однією з її термодинамічних властивостей (\\(S\\)).\nЗгідно визначенню Больцмана, ентропія є просто функцією стану. Більш того, оскільки (\\(\\Omega\\)) може бути тільки натуральним числом (1, 2, 3), ентропія повинна бути додатною — виходячи з властивостей логарифма.\nУ випадку дискретних станів квантової механіки кількість станів підраховується звичайним чином. В рамках класичної механіки мікроскопічний стан системи описується координатами \\(q_{i}\\) й імпульсами \\(p_{i}\\) окремих частинок, які пробігають неперервні значення. Для підрахунку станів у класичних системах фазовий простір розбивають на невеликі комірки із об’ємом, який відповідає сталій Планка. У такому випадку\n\\[\nS = k_{B}\\ln\\frac{1}{( 2\\pi\\hbar )^{s}} \\int \\prod_{i=1}^{s} dq_{i}dp_{i},\n\\]\nде \\(s\\) — число незалежних координат, \\(\\hbar\\) — приведена стала Планка, а інтегрування проводиться по області фазового простору, який відповідає певному макроскопічному стану.\nКлод Шеннон (Shannon, 1948) запропонував формулу для оцінки невизначеності кодової інформації в каналах зв’язку, звану ентропією Шеннона:\n\\[\nS = -k\\sum_{i=1}^{n}p_{i}\\ln{p_{i}},\n\\]\nде \\(p_{i}\\) — вірогідність того, що символ \\(i\\) зустрічається в коді, який містить \\(N\\) символів, \\(k\\) — розмірний множник.\nЗв’язок між ентропією і інформацією можна прослідкувати на наступному прикладі. Розглянемо тіло при абсолютному нулі температури, і хай ми маємо повну інформацію про координати і імпульси кожної частинки. Для простоти покладемо, що імпульси всіх частинок рівні нулю. В цьому випадку термодинамічна ймовірність рівна одиниці, а ентропія — нулю. При кінцевих температурах ентропія в рівновазі досягає максимуму. Можна зміряти всі макропараметри, що характеризують даний макростан. Проте ми практично нічого не знаємо про мікростан системи. Точніше кажучи, ми знаємо, що даний макростан можна реалізувати за допомогою дуже великого числа мікростанів. Таким чином, нульовій ентропії відповідає повна інформація (ступінь незнання рівний нулю), а максимальної ентропії — повне незнання мікростанів (ступінь незнання максимальний).\nУ теорії інформації ентропія (інформаційна ентропія) визначається як кількість інформації. Нехай \\(P\\) — апріорна вірогідність деякої події (ймовірність до проведення досвіду), а \\(P_{1}\\) – ймовірність цієї події після проведення досвіду. Для простоти вважатимемо, що \\(P_{1} = 1\\). За Шенноном, кількість інформації \\(I\\), яка дає точну відповідь (після проведення експерименту)\n\\[\nI = K \\log{P}.\n\\]\nЦя кількість інформації, за визначенням, дорівнює одному біту.\nФізичний сенс \\(I\\) — це міра нашого незнання. Іншими словами, \\(I\\) — це та інформація, яку ми можемо одержати, вирішивши завдання. У прикладі (тіло при абсолютному нулі температури), що розглядається вище, міра нашого незнання рівна нулю, оскільки \\(P = 1\\). Після проведення досвіду ми одержуємо нульову інформацію \\(I = 0\\), оскільки все було відомо до досвіду. Якщо розглядати тіло при кінцевих температурах, то до проведення досвіду число мікростанів, а отже, і \\(P\\) дуже велике. Після проведення досвіду ми одержуємо велику інформацію, оскільки нам стають відомими координати і імпульси всіх частинок.\nАналогія між кількістю інформації і ентропією \\(S\\), визначуваною з принципу Больцмана, очевидна. Досить покласти множник \\(K\\) рівним постійній Больцмана \\(k_{B}\\) і використовувати натуральний логарифм. Саме з цієї причини величину $ I $ називають інформаційною ентропією. Інформаційна ентропія (кількість інформації) була визначена по аналогії із звичайною ентропією, і вона має властивості, характерні для звичайній ентропії: адитивність, екстремальні властивості і т.д. Проте ототожнювати звичайну ентропію з інформаційною не можна, оскільки неясно, яке відношення має друге начало до інформації. Нагадаємо, що екстенсивна величина — ця така характеристика системи, яка росте із збільшенням розмірів системи, тобто, якщо наша система складається з двох незалежних підсистем \\(А\\) і \\(В\\), то ентропію всієї системи можна одержати складанням ентропій підсистем:\n\\[\nS(\\,A+B)\\, = S(\\,A)\\, + S(\\,B)\\,.\n\\]\nСаме ця властивість і означає екстенсивність, або адитивність, ентропії."
  },
  {
    "objectID": "lab_5.html#хід-роботи",
    "href": "lab_5.html#хід-роботи",
    "title": "6  Лабораторна робота № 5",
    "section": "6.2 Хід роботи",
    "text": "6.2 Хід роботи\n\nimport EntropyHub as eh\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport yfinance as yf\n#import antropy as ant\nimport neurokit2\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nplt.style.use('classic')\n\nparams = {\n    'axes.labelsize': 26,\n    'axes.titlesize':26, \n    \"axes.grid\" : False,\n    'font.size': 26, \n    'legend.fontsize': 26, \n    'xtick.labelsize': 26, \n    'ytick.labelsize': 26, \n    'lines.linewidth': 2,\n    'axes.facecolor': 'white',\n    'figure.facecolor': 'white',\n    'axes.titlesize': 'small',\n    'font.family': 'Times New Roman',\n    'savefig.dpi': 300\n}\n\nplt.rcParams.update(params)\n\nxlabel = 'time, days'\n\n\nsymbol = \"BTC-USD\"\nsymbol_for_graph = \"BTC-USD\"\n\nstart = \"2019-01-01\"\nend = \"2022-09-28\"\n\nsymbol_plot = symbol.split('.')[0]\nsymbol_for_graph_plot = symbol_for_graph.split('.')[0]\n\ndata = yf.download(symbol, start, end)\nclose = data['Adj Close'].copy()\nfor_graph = data['Adj Close'].copy()\n\nnp.savetxt(f'{symbol}_initial_time_series.txt', close.values)\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n6.2.1 Виведення графіку досліджуваного ряду\n\nfig, ax = plt.subplots()\nclose.plot(figsize=(15,8), xlabel=xlabel, ylabel=symbol_plot)\nax.legend([symbol_plot]);\n\nplt.savefig(f'{symbol}.jpg')\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\nfor_graph.plot(figsize=(15,8), xlabel=xlabel, ylabel=symbol_for_graph_plot)\nax.legend([symbol_for_graph]);\n\nplt.savefig(f'{symbol_for_graph_plot}.jpg')\nplt.show()\n\n\n\n\n\n\n6.2.2 Задання ширини вікна та кроку\n\nwindow = 100 # розмір вікна\ntstep = 1 # крок вікна \n\n\n\n6.2.3 Approximate entropy (Апроксимаційна ентропія)\n\nSteven M. Pincus, Approximate entropy as a measure of system complexity, Proceedings of the National Academy of Sciences, 88.6 (1991): 2297-2301.\n\nТеоретичний опис методики розрахунку\nЕнтропія подібності (Approximate Entropy, ApEn) є “статистикою регулярності”, що визначає можливість передбачувати флуктуації в часових рядах. Інтуїтивно вона означає, що наявність повторюваних шаблонів (послідовностей певної довжини, побудованих із чисел ряду, що слідують одне за іншим) флуктуацій у часовому ряді призводить до більшої передбачуваності часового ряду порівняно із рядами, де повторюваності шаблонів немає. Порівняно велике значення ApEn показує ймовірність того, що подібні між собою шаблони спостережень не будуть слідувати один за одним. Іншими словами, часовий ряд, що містить велику кількість повторюваних шаблонів, має порівняно мале значення ApEn, а значення ApEn для менш передбачуваного (більш складного) процесу є більшим.\nПри розрахунку ApEn для даного часового ряду \\(S_{N}\\), що складається із \\(N\\) значень \\(t(\\,1)\\,, t(\\,2)\\,, t(\\,3)\\,, ... , t(\\,N)\\,\\) вибираються два параметри, \\(m\\) та \\(r\\). Перший з цих параметрів, \\(m\\), вказує довжину шаблона, а другий — \\(r\\) — визначає критерій подібності. Досліджуються підпослідовності елементів часового ряду \\(S_{N}\\), що складаються з \\(m\\) чисел, взятих, починаючи з номера \\(i\\), і називаються векторами \\(p_{m} (\\,i)\\,\\). Два вектори (шаблони), \\(p_{m}(\\,i)\\,\\) та $ p_{m}(,j),$, будуть подібними, якщо всі різниці пар їх відповідних координат є меншими за значення \\(r\\), тобто якщо\n\\[\n| t(\\,i+k)\\, - t(\\,j+k)\\, | &lt; r \\quad \\textrm{для} \\quad 0 \\leq k &lt; m.\n\\]\nДля розглядуваної множини \\(P_{m}\\) всіх векторів довжини \\(m\\) часового ряду \\(S_{N}\\) можна обраховуються значення\n\\[\nC_{im}(\\,r)\\, = \\frac{n_{im}(\\,r)\\,}{N-m+1},\n\\]\nде \\(n_{im}(\\,r)\\,\\) — кількість векторів у \\(P_{m}\\), що подібні вектору \\(p_{m}(\\,i)\\,\\) (враховуючи вибраний критерій подібності \\(r\\)). Значення \\(C_{im}(\\,r)\\,\\) є часткою векторів довжини \\(m\\), що мають схожість із вектором такої ж довжини, елементи якого починаються з номера \\(i\\). Для даного часового ряду обраховуються значення \\(C_{im}(\\,r)\\,\\) для кожного вектора у \\(P_{m}\\), після чого знаходиться середнє значення \\(C_{m}(\\,r)\\,\\), яке виражає розповсюдженість подібних векторів довжини \\(m\\) у ряду \\(S_{N}\\). Безпосередньо ентропія подібності для часового ряду \\(S_{N}\\) з використанням векторів довжини \\(m\\) та критерію подібності \\(r\\) визначається за формулою:\n\\[\nApEn(\\,S_{N}, m, r)\\, = \\ln(\\,\\frac{C_{m}(\\,r)\\,}{C_{m+1}(\\,r)\\,})\\,,\n\\]\nтобто, як натуральний логарифм відношення повторюваності векторів довжиною \\(m\\) до повторюваності векторів довжиною \\(m+1\\).\nТаким чином, якщо знайдуться подібні вектори у часовому ряді, ApEn оцінить логарифмічну ймовірність того, що наступні інтервали після кожного із векторів будуть відрізнятись. Менші значення ApEn відповідають більшій ймовірності того, що за векторами слідують подібні їм. Якщо часовий ряд дуже нерегулярний — наявність подібних векторів не може бути передбачуваною і значення ApEn є порівняно великим.\nЗауважимо, що ApEn є нестійкою до вхідних даних характеристикою, оскільки досить сильно залежить від параметрів \\(m\\) та \\(r\\).\n\nm = 3 #розмірність вкладень\ntau = 1 #часова затримка\nr = 0.45 #параметр подібності\nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб. 3-відносні приб. 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nApEn = [] #масив для зберігання значень ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду:\n    \n    # 1 - вихідний ряд, 2 - детрендований, 3 - прибутковості, 4 - стандартизовані прибутковості, 5 - логарифмічні\n    \n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:\n        fragm = np.log(fragm) - np.log(fragm.shift(1))\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Ap, _ = eh.ApEn(Sig = fragm, m = m, tau = tau, r = r)\n    ApEn.append(Ap[-1])\n\n\nname_for_save = f\"ApEn_{symbol}_{window}_{tstep}_{m}_{tau}_{r}_{ret_type}.txt\" #ім'я файлу для зберігання\nwith open(name_for_save, 'w') as f: #відкриваємо на запис \n    ApEn_file = [str(line) + '\\n' for line in ApEn]\n    f.writelines(ApEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8)) # візуалізуємо результати розрахунку\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f'{symbol_for_graph_plot}', color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, ApEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], ApEn, label='ApEn', color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"ApEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, embedding dimension={m}, delay={tau}, r={r}.jpg\")\nplt.show()\n\n\n\n\n\n\n6.2.4 Fuzzy entropy (Нечітка ентропія)\n\nWeiting Chen, et al. Characterization of surface EMG signal based on fuzzy entropy, IEEE Transactions on neural systems and rehabilitation engineering, 15.2 (2007): 266-272.\nHong-Bo Xie, Wei-Xing He, and Hui Liu, Measuring time series regularity using nonlinear similarity-based sample entropy, Physics Letters A, 372.48 (2008): 7140-7146.\n\n\nm = 3 #розмірність вкладень\ntau = 1 #часова затримка\ncharacteristic_func = \"default\" #вид функції приналежності: default, sigmoid, gudermannian, linear\nr = (0.4, 2.0) #параметри, що подаються до функції приналежності. для default та sigmoid 2 значення r, \n                  #для gudermannian та linear 1 значення r  \nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб. 3-відносні приб. 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nFuzzEn = [] #масив для зберігання значень ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1: \n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:\n        fragm = np.log(fragm) - np.log(fragm.shift(1))\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Fuzz, _, _ = eh.FuzzEn(Sig = fragm, m = m, tau = tau, Fx = characteristic_func, r = r) #Рахуємо нечітку ентропію \n    FuzzEn.append(Fuzz[-1]) #дожаємо розрахованє значення до масиву значень \n\n\nname_for_save = f\"FuzzEn_{symbol}_{window}_{tstep}_{m}_{tau}_{characteristic_func}_{r}_{ret_type}.txt\" #ім'я файлу для зберігання\nwith open(name_for_save, 'w') as f: #відкриваємо на запис \n    FuzzEn_file = [str(line) + '\\n' for line in FuzzEn]\n    f.writelines(FuzzEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], close.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, FuzzEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], FuzzEn, label=\"FuzzEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"FuzzEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, membership function={characteristic_func}, embedding dimension={m}, delay={tau}, r={r}.jpg\")\n\nplt.show()\n\n\n\n\n\n\n6.2.5 Sample entropy (Ентропія шаблонів)\n\nJoshua S Richman and J. Randall Moorman, Physiological time-series analysis using approximate entropy and sample entropy, American Journal of Physiology-Heart and Circulatory Physiology (2000).\n\n\nm = 3 #розмірність вкладень\ntau = 1 #часова затримка\nr = 0.4 #параметр подібності\nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб. 3-відносні приб. 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nSampEn = [] #масив для зберігання значень ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:\n        fragm = np.log(fragm) - np.log(fragm.shift(1))\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Samp, _, _ = eh.SampEn(Sig = fragm, m = m, tau = tau, r = r, Logx=np.exp(1))\n    SampEn.append(Samp[-1])\n\n\nname_for_save = f\"SampEn_{symbol}_{window}_{tstep}_{m}_{tau}_{r}_{ret_type}.txt\" #ім'я файлу для зберігання\nwith open(name_for_save, 'w') as f: #відкриваємо на запис \n    SampEn_file = [str(line) + '\\n' for line in SampEn]\n    f.writelines(SampEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], close.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, SampEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], SampEn, label=\"SampEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"SampEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, embedding dimension={m}, delay={tau}, r={r}.jpg\")\n\nplt.show()\n\n\n\n\n\n\n6.2.6 Permutation entropy (Ентропія перестановок)\n\nChristoph Bandt and Bernd Pompe, Permutation entropy: A natural complexity measure for time series, Physical Review Letters, 88.17 (2002): 174102.\nXiao-Feng Liu, and Wang Yue, Fine-grained permutation entropy as a measure of natural complexity for time series, Chinese Physics B, 18.7 (2009): 2690.\nChunhua Bian, et al., Modiﬁed permutation-entropy analysis of heartbeat dynamics, Physical Review E, 85.2 (2012) : 021906\nBilal Fadlallah, et al., Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information, Physical Review E, 87.2 (2013): 022911.\nHamed Azami and Javier Escudero, Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation, Computer methods and programs in biomedicine, 128 (2016): 40-51.\nZhiqiang Huo, et al., Edge Permutation Entropy: An Improved Entropy Measure for Time-Series Analysis, 45th Annual Conference of the IEEE Industrial Electronics Soc, (2019), 5998-6003.\nZhe Chen, et al., Improved permutation entropy for measuring complexity of time series under noisy condition, Complexity, 1403829 (2019).\nMaik Riedl, Andreas MuЁller, and Niels Wessel, Practical considerations of permutation entropy, The European Physical Journal Special Topics, 222.2 (2013): 249-262.\n\n\nm = 4             # розмірність вкладень\ntau = 3           # часова затримка\nType = 'weighted' # none - класична; \n                  # finegrain - Fine-grained permutation entropy; \n                  # modified - Modiﬁed permutation entropy; \n                  # weighted - Weighted permutation entropy; \n                  # ampaware - Amplitude-aware permutation entropy; \n                  # edge - Edge permutation entropy; \n                  # uniquant - Uniform quantization-based permutation entropy; \n            \ntpx = -1          # finegrain tpx is the α parameter, a positive scalar (default: 1)\n                  # ampaware tpx is the A parameter, a value in range [0 1] (default: 0.5)\n                  # edge tpx is the r sensitivity parameter, a scalar &gt; 0 (default: 1)\n                  # uniquant tpx is the L parameter, an integer &gt; 1 (default: 4).\n\nlog = np.exp(1)    # основа логарифма\nnorm = True\nret_type = 4       # вид ряду: 1-вихідний, 2-абсолютні приб. 3-відносні приб. 4-нормалізовані приб. \nn = close.shape[0] # задаємо кількість значень\n\nPEn = []           # масив для зберігання значень нормалізованої перм. ентропії\nCPEn = []          # масив для зберігання значень умовної перм. ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window].copy() # відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    _, Pnorm, cPE = eh.PermEn(fragm, m = m, tau = tau, Typex = Type, tpx = tpx, Logx = log, Norm = norm)\n    PEn.append(Pnorm[-1])\n    CPEn.append(cPE[-1])\n\n\nPEn_for_save = f\"PEn_symbol={symbol}_window={window}_step={tstep}_d={m}_tau={tau}_ret={ret_type}_type={Type}_param={tpx}.txt\" #ім'я файлу для зберігання пермутаційної ентропії\nCPEn_for_save = f\"CPEn_symbol={symbol}_window={window}_step={tstep}_d={m}_tau={tau}_ret={ret_type}_type={Type}_param={tpx}.txt\" #ім'я файлу для зберігання умовної пермутаційної ентропії\nwith open(PEn_for_save, 'w') as f: #відкриваємо на запис \n    PEn_file = [str(line) + '\\n' for line in PEn]\n    f.writelines(PEn_file)\n    \nwith open(CPEn_for_save, 'w') as f: #відкриваємо на запис \n    CPEn_file = [str(line) + '\\n' for line in CPEn]\n    f.writelines(CPEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, PEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], PEn, label=\"PEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"PEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, type={Type}, tpx={tpx}, embedding dimension={m}, delay={tau}.jpg\")\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, CPEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], CPEn, label=\"CPEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"CPEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, type={Type}, tpx={tpx}, embedding dimension={m}, delay={tau}.jpg\")\n\nplt.show()\n\n\n\n\n\n\n6.2.7 Distribution entropy (Розподільна ентропія)\n\nLi, Peng, et al., Assessing the complexity of short-term heartbeat interval series by distribution entropy, Medical & biological engineering & computing 53.1 (2015): 77-87.\n\n\nm = 3 #розмірність вкладень\ntau = 1 #часова затримка\nbins = 'sturges' # Метод визначення бінів гістограми. Окрім цього можна обрати sqrt, rice, doanes\nnorm = True\nlog = np.exp(1)\nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб., 3-відносні приб., 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nDistEn = [] #масив значень для зберігання розподіленої ентропії \n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Dist = eh.DistEn(fragm, m = m, tau = tau, Bins = bins, Logx = log, Norm = norm)\n    DistEn.append(Dist[0])\n\nNote: 3/14 bins were empty\nNote: 3/14 bins were empty\nNote: 3/14 bins were empty\nNote: 12/14 bins were empty\nNote: 12/14 bins were empty\nNote: 12/14 bins were empty\nNote: 9/14 bins were empty\nNote: 7/14 bins were empty\nNote: 4/14 bins were empty\nNote: 2/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\n\n\n\nDistEn_for_save = f\"DistEn_symbol={symbol}_window={window}_step={tstep}_d={m}_tau={tau}_series_type={ret_type}_bins={bins}.txt\" #ім'я файлу для зберігання розподіленої ентропії\nwith open(DistEn_for_save, 'w') as f: #відкриваємо на запис \n    DistEn_file = [str(line) + '\\n' for line in DistEn]\n    f.writelines(DistEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, DistEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], DistEn, label=\"DistEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"DistEn_symbol={symbol}_window={window}_step={tstep}_d={m}_tau={tau}_series_type={ret_type}_bins={bins}.jpg\")\n\nplt.show()\n\n\n\n\n\n\n6.2.8 Dispersion entropy (Дисперсійна ентропія)\n\nMostafa Rostaghi and Hamed Azami, Dispersion entropy: A measure for time-series analysis IEEE Signal Processing Letters 23.5 (2016): 610-614.\nHamed Azami and Javier Escudero, Amplitude-and ﬂuctuation-based dispersion entropy, Entropy 20.3 (2018): 210.\nLi Yuxing, Xiang Gao and Long Wang, Reverse dispersion entropy: A new complexity measure for sensor signal, Sensors 19.23 (2019): 5203.\nWenlong Fu, et al., Fault diagnosis for rolling bearings based on ﬁne-sorted dispersion entropy and SVM optimized with mutation SCA-PSO, Entropy 21.4 (2019): 404.\n\nТеоретичний опис методики розрахунку\n\nnorm = True\nfluct = False # Якщо True повертаємо флуктуаційно-дисперсійну ентропію\nm = 3 \ntau = 1\nrho = 1 # *If Typex = \"finesort\", rho is the tuning parameter, a positive scalar (default:1)\n\nclasses = 3 # кількість символів, що задіяні при перетворені\nType = 'ncdf' # тип символьного перетворення. \"ncdf\" Normalised cumulative distribution function [19]\n                                            # \"kmeans\" K-means clustering algorithm. Note: The ”kmeans” algorithm uses random initialization conditions. This causes results to vary slightly each time it is called.\n                                            # \"linear\" Linear segmentation of signal range\n                                            # \"finesort\" Fine-sorted dispersion entropy\n                                            # \"equal\" Approx. equal number of symbols.\n\nlog = np.exp(1)\nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб., 3-відносні приб., 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nDispEn = [] # масив значень для зберігання дисперсійної ентропії \nRevDispEn = [] # для зберігання оборотної дисперсійної ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Disp, RevDisp = eh.DispEn(fragm, m = m, tau = tau, c = classes, Typex = Type, \n                              Logx = log, Fluct = fluct, Norm = norm, rho = rho)\n    DispEn.append(Disp)\n    RevDispEn.append(RevDisp)\n\n\nDispEn_for_save = f\"DispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_series_type={ret_type}_fluct={fluct}_rho={rho}_classes={classes}_type={Type}.txt\" #ім'я файлу для зберігання DispEn\nRevDispEn_for_save = f\"RevDispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_series_type={ret_type}_fluct={fluct}_rho={rho}_classes={classes}_type={Type}.txt\" #ім'я файлу для зберігання RevDispEn\nwith open(DispEn_for_save, 'w') as f: #відкриваємо на запис \n    DispEn_file = [str(line) + '\\n' for line in DispEn]\n    f.writelines(DispEn_file)\n    \nwith open(RevDispEn_for_save, 'w') as f: #відкриваємо на запис \n    RevDispEn_file = [str(line) + '\\n' for line in RevDispEn]\n    f.writelines(RevDispEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, DispEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], DispEn, label=\"DispEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"DispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_series_type={ret_type}_fluct={fluct}_rho={rho}_classes={classes}_type={Type}.jpg\")\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, RevDispEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], RevDispEn, label=\"RevDispEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"RevDispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_series_type={ret_type}_fluct={fluct}_rho={rho}_classes={classes}_type={Type}.jpg\")\n\nplt.show()"
  },
  {
    "objectID": "lab_6.html#теоретичні-відомості",
    "href": "lab_6.html#теоретичні-відомості",
    "title": "7  Лабораторна робота № 6",
    "section": "7.1 Теоретичні відомості",
    "text": "7.1 Теоретичні відомості\n\n7.1.1 Означення фрактала\nФракталами називають геометричні об’єкти: лінії, поверхні, просторові тіла, що мають сильно шорстку поверхню або форму і характеризуються властивістю самоподібності. Слово фрактал походить від латинського слова fractus і перекладається як дробовий, ламаний. Самоподібність як основна характеристика фрактала означає, що він більш-менш однорідно змінюється при широкому діапазоні масштабів. Так, при збільшенні маленькі фрагменти фрактала виходять дуже схожими на великі. В ідеальному випадку така самопподібність призводить до того, що фрактальний об’єкт виявляється інваріантним щодо розтягувань, тобто йому, як кажуть, притаманна дилатаційна симетрія. Вона передбачає незмінність основних геометричних особливостей фрактала при зміні масштабу.\nОчевидно, що фрактальні об’єкти реального світу не є нескінченно самоподібними й існує мінімальний масштаб \\(l_{min}\\), такий, що на масштабі \\(l \\approx l_{min}\\) властивість самоподібності зникає. Окрім цього, на достатньо великих масштабах довжин \\(l &gt; l_{max}\\), де \\(l_{max}\\) — характерний геометричний розмір об’єктів, ця властивість самоподібності також порушується. Тому властивості природніх фракталів розглядаються лише на масштабах \\(l\\), що задовільняють відношення \\(l_{min} \\ll l \\ll l_{max}\\). Такі обмеження природні, оскільки, коли ми приводимо в якості прикладу фракталу — ламану, негладку траєкторію броунівської частинки, то ми розуміємо, що цей образ представляє очевидну ідеалізацію. Справа в тому, що на малих масштабах приховується граничність маси і розмірів броунівської частнки, а також кінцевість часу зіткнення. При врахуванні цих обставин траєкторія броунівської частинки починає представляти гладку криву.\nВарто зазначити, що властивість точної самоподібності характерна лише для регулярних фракталів. Якщо замість детермінованого способу побудови включити в алгоритм їхнього створення деякий елемент випадковості (як це буває, наприклад, у багатьох процесах диференційованого зростання кластерів, електричному пробої тощо), то виникають так звані випадкові фрактали. Основна їхня відмінність від регулярних полягає в тому, що властивості самоподібності є справедливими тільки після відповідного усереднення за всіма статистично незалежними реалізаціями об’єкта. При цьому збільшена частина фрактала не точно ідентична вихідному фрагменту, проте їхні статистичні характеристики збігаються.\n\n\n7.1.2 Довжина берегової лінії\n\n\n\nРисунок 7.1: Визначення довжини берегової лінії між точками А та В\n\n\nПершочергово поняття фрактала у фізиці виникло у зв’язку із завданням про визначення довжини берегової лінії. Під час її вимірювання за наявною картою місцевості з’ясувалася цікава деталь — чим більш великомасштабна карта береться, тим довшою виявляється ця берегова лінія. Нехай, наприклад, відстань по прямій між розсташованими на береговій лінії точками А та В дорівнює R (див. Рисунок 7.1). Тоді, щоб виміряти довжину берегової лінії між цима точками, ми розташуємо по берегу жорстко пов’язані один з одним вузол так, що відстань між сусідніми вузлами дорівнювала б, наприклад, \\(l=10\\) км. Довжину берегової лінії в кілометрах між точками А і В ми приймемо тоді рівною числу вузлів мінус 1, помноженому на 10. Наступне вимірювання цієї довжини ми зробимо так само, але відстань між сусідніми вузлами зробимо вже рівною \\(l=1\\) км.\nВиявляється, що результат цих вимірювань буде різним. При зменшенні масштабу \\(l\\) ми отримуватимемо все більші й більші значення довжини. На відміну від гладкої кривої, лінія морського узбережжя виявляється найчастіше настільки порізаною (аж до найменших масштабів), що зі зменшенням довжини ланки \\(l\\) величина \\(L\\) — довжина берегової лінії — не прагне до кінцевого межі, а збільшується за степеневим законом\n\\[\nL \\approx l\\left( \\frac{R}{l} \\right)^{D}, \\tag{1}\n\\]\nде \\(D &gt; 1\\) — деякий степеневий показник, котрий іменується фрактальною розмірністю берегової лінії. Чим більше значення \\(D\\), тим більш ломаною або деталізованою представляється ця берегова лінія. Походження залежності (1) має бути інтуїтивно зрозумілим: чим менший масштаб ми використовуємо, тим меньші деталі узбережжя будуть враховані і тим менший вклад вони внесуть у вимірювану довжину. Навпаки, збільшуючи масштаб, ми “розгортаємо” узбережжя, зменшуючи довжину \\(L\\).\nТаким чином, ми бачимо, що для визначення довжини берегової лінії \\(L\\) за допомогою жорсткого масштабу \\(l\\), необхідно зробити \\(N=L/l\\) кроків, причому величина \\(L\\) змінюється з \\(l\\) так, що \\(N\\) залежить від \\(l\\) за законом \\(N \\approx (R/I)^{D}\\). У результаті зі зменшенням масштабу довжина берегової лінії необмежено зростає. Ця обставина різко відрізняє фрактальну криву від звичайної гладкої кривої (типу кола, еліпса), для якої межа довжини апроксимованої ламаної \\(L\\), яка апроксимує, за наближення до нуля довжини її ланки \\(l\\) є скінченною. У результаті для гладкої кривої її фрактальна розмірність \\(D = 1\\), тобто збігається з топологічною.\n\n\n7.1.3 Фрактальна розмірність множин\nВище було введено поняття про фрактальну розмірність берегової лінії. Дамо тепер загальне визначення цієї величини. Нехай \\(d\\) — звичайна Евклідова розмірність простору, в якому розташований наш фрактальний об’єкт (\\(d=1\\) — лінія, \\(d=2\\) — площина, \\(d=3\\) — звичайний тривимірний простір). Покриємо тепер цей об’єкт цілком \\(d\\)-мірними “кулями” радіуса 1. Припустимо, що нам потребувалося для цього не менше, ніж \\(N(l)\\) куль. Тоді, якщо за досить малих \\(l\\) величина \\(N(l)\\) змінюється з \\(l\\) за степеневим законом\n\\[\nN(l) \\sim \\frac{1}{l^D}, \\tag{2}\n\\]\nтоді \\(D\\) — називається хаусдорфовою або фрактальною розмірністю цього об’єкта. Очевидно, що ця формула еквівалентна відношеню \\(N \\approx \\left( R/l \\right)^{D}\\), що використовувалось вище для визначення довжини берегової лінії.\nФормулу (2) можна переписати у вигляді\n\\[\nD = -\\lim_{l \\to 0} \\frac{\\ln{N(l)}}{\\ln{l}}. \\tag{3}\n\\]\nЦе відношення й слугує загальним визначенням фрактальної розмірності \\(D\\). У відповідності до нього величина \\(D\\) представляє локальну характеристику досліджуваного об’єкта.\n\n\n7.1.4 Процедури обчислення монофрактальних розмірностей\nНаразі існує багато визначень та методів вимірювання фрактальної розмірності. Найпоширенішими одновимірними фрактальними розмірностями є розмірність Хаусдорфа, розмірність Хігучі, розмірність Петросяна та Коробчаста розмірність. Розмірність Хаусдорфа є найпростішою фрактальною розмірністю. Але її обчислювальна складність є високою, що ускладнює її практичне застосування. Коробкова розмірність є відносно простою, і фрактальну розмірність сигналу можна отримати, регулюючи розмір довжини сторони коробки. Тому вона є широко впізнаваємою та застосовуваною. Який показник фрактальної розмірності найточніше описує складність сигналу та здатний ідентифікувати кризові явища і представляє ключовий момент цієї лабораторної роботи.\n\n7.1.4.1 R/S-аналіз\nМетод R/S-аналізу, розроблений Мандельбротом та Уоллесом, базується на попередньо створеному методі гідрологічного аналізу Херста, і дозволяє обчислювати параметр самоподібності \\(H\\), який вимірює інтенсивність довготривалих залежностей у часовому ряді. Коефіцієнт \\(H\\), який називають коефіцієнтом Херста, містить мінімальні прогнози стосовно природи системи, що вивчається, і може класифікувати часові ряди. За допомогою цього показника розрізняють випадкові (гаусові) та невипадкові ряди; окрім того, він пов’язаний із фрактальною розмірністю, що, у свою чергу, характеризує ступінь згладженості графіка, побудованого на основі часового ряду. Методом R/S-аналізу можливо також виявити максимальну довжину інтервалу (цикл), на якому значення зберігають інформацію про початкові дані системи (довготривала пам’ять).\nАналіз починається з побудови ряду логарифмічних прибутковостей, \\(G(t) \\equiv \\ln{x(t + \\Delta t)} - \\ln{x(t)}\\), де \\(x(t)\\) — значення вихідного часового ряду в момент \\(t\\), \\(\\Delta t\\) — часовий крок. Отримана послідовність \\(G(t)\\) розбивається на \\(d\\) підпослідовностей довжини \\(n\\).\nДля кожної підпослідовності \\(m=1,...,d\\):\n\nшукається середнє значення \\(\\mu_m\\) та стандартне відхилення \\(S_m\\);\nдані нормалізуються шляхом віднімання середнього значення послідовності \\(X_{i,m}=G_{i,m}-\\mu_m\\), \\(i=1,...,n\\);\nзнаходиться кумулятивна сума послідовності \\(X\\)ів: \\(Y_{i,m}=\\sum_{j=1}^{i}X_{j,m}\\), \\(i=1,...,n\\);\nу межах кожної підпослідовності знаходиться розмах між максимальним та мінімальним значеннями: \\(R_m = \\max\\{Y_{1,m},...,Y_{n,m}\\}-\\min\\{Y_{1,m},...,Y_{n,m}\\}\\), який стандартизується середнім квадратичним відхиленням \\(R_{m}/S_{m}\\);\nобчислюється середнє \\((R/S)_n\\) нормованих значень розмаху для всіх підпослідовностей довжини \\(n\\).\n\nR/S-статистика, обрахована таким чином, відповідає співвідношенню \\((R/S)_{n} \\cong cn^{H}\\), де значення \\(H\\) може бути отримане шляхом обчислення \\((R/S)_n\\) для послідовностей інтервалів зі збільшенням часового горизонту:\n\\[\n\\log{(R/S)}_{n} = \\log{c} + H\\log{n}. \\tag{4}\n\\]\nЗнайти коефіцієнт Херста можна, побудувавши залежність \\((R/S)_n\\) vs. \\(n\\) у подвійному логарифмічному масштабі і взявши коефіцієнт нахилу прямої, яка інтерполює точки отриманого графіка. Якщо значення \\(H=0.5\\), говорять про послідовність, що представляє собою білий шум; \\(0.5 &lt; H \\leq 1\\) свідчить про персистентний ряд, коли існує тенденція слідування великих значень ряду за великими і навпаки; \\(H&lt;0.5\\) вказує на антиперсистентний ряд.\nПри збільшенні часового горизонту коефіцієнт нахилу інтерполюючої прямої повинен прямувати до значення \\(H=0.5\\); сам процес переходу свідчить про втрату впливу початкових умов на поточні значення, і, таким чином, можна говорити про горизонт довгої пам’яті — це точка, до якої коефіцієнт нахилу інтерполюючої прямої відмінний від 0.5, а після — близько 0.5.\n\n\n\n\n\n\nПримітка до R/S-аналізу\n\n\n\nМіж фрактальною розмірністю та показником Херста також існує зв’язок\n\\[\nD_f = 2-H.\n\\]\nЯкщо для берегової лінії ми визначали масштабування її довжини \\(L\\) в залежності від зміни \\(l\\), то у випадку з R/S-аналізом ми визначаємо зміну нормованого розмаху значень ряду в межах масштабу \\(n\\).\n\n\n\n\n7.1.4.2 Аналіз детрендованих флуктуацій\nАналіз детрендований флуктуацій (Detrended fluctuation analysis, DFA) базується на гіпотезі про те, що корельований часовий ряд може бути відображений на самоподібний процес шляхом інтегрування. Таким чином, вимірювання властивостей самоподібності може непрямо свідчити про кореляційні властивості ряду. Переваги АДФ порівняно з іншими методами (спектральний аналіз, R/S-аналіз) полягають в тому, що він виявляє довгочасові кореляції нестаціонарних часових рядів, а також дозволяє ігнорувати очевидні випадкові кореляції, що є наслідком нестаціонарності.\nІснують DFA різних порядків, що відрізняються трендами, які вилучаються з даних.\nРозглянемо DFA найнижчого порядку.\n\nДля часового ряду довжини \\(N\\) знаходиться кумулятивна сума, \\(y(k)=\\sum_{i=1}^{k}\\left( x_i - \\bar{x} \\right)\\), де \\(x_i\\) — це \\(i\\)-те значення часового ряду, \\(\\bar{x}\\) — його середнє значення, \\(k=1,...,N\\).\nОтриманий ряд \\(y(k)\\) розбивається на \\(m\\) підпослідовностей (вікон) однакової ширини \\(n\\) і для кожної підпослідовності (у кожному вікні) виконується наступне:\n\nза допомогою методу найменших квадратів знаходиться локальний лінійний тренд \\(y_{t}(k)\\);\n\nпідпослідовність детрендується шляхом віднімання значення локального тренду \\(y_{t}(k)\\) від значень ряду \\(y(k)\\), що належать послідовності \\(t\\);\nзнаходиться середнє \\(\\bar{y_t}\\) детрендований значень.\n\n\nДля отриманих таким чином значень на всіх підпослідовностях знаходиться:\n\\[\nF_n = \\sqrt{\\frac{1}{m}\\bar{y_t}},\n\\]\nде \\(n\\) — кількість точок у підпослідовності (ширина вікна), \\(m\\) — кількість підпослідовностей, \\(\\bar{y_t}\\) — середнє детрендованих значень для підпослідовності \\(t\\).\nВказана процедура повторюється для різних значень \\(n\\), внаслідок чого ми отримує набір залежностей \\(F_n\\) від \\(n\\). Побудова залежності \\(\\log{F(n)}\\) від \\(\\log{n}\\) та інтерполяція отриманих значень лінією регресії дає змогу обчислити показник скейлінга \\(\\alpha\\), що є коефіцієнтом кута нахилу інтерполяційної прямої і характеризує зміну кореляцій флуктуацій часового ряду \\(F_n\\) при збільшенні часового інтервалу \\(n\\).\nПорівняно із R/S-аналізом, DFA дає більші можливості інтерпретації скейлінгового показника \\(\\alpha\\):\n\nдля випадкового ряду (перемішаного чи “сурогатного”) \\(\\alpha = 0.5\\);\nпри наявності лише короткочасових кореляцій \\(\\alpha\\) може відрізнятись від 0.5, проте має тенденцію прямувати до 0.5 при збільшенні розміру вікна;\nЗначення \\(0.5 &lt; \\alpha \\leq 1.0\\) показує персистентні довгочасові кореляції, що відповідають степеневому закону;\n\\(0 &lt; \\alpha &lt; 0.5\\) означає антиперсистентний ряд;\ncпеціальний випадок, коли \\(\\alpha = 1\\), означає наявність \\(1/f\\) шуму.\nдля випадків, коли \\(\\alpha \\geq 1\\), кореляції існують, проте перестають відображувати степеневу залежність;\nвипадок \\(\\alpha = 1.5\\) свідчить про Броунівський шум, інтегрований білий шум.\n\nУ випадку степеневої залежності функції автокореляцій спостерігається спад автокореляції з показником \\(\\gamma\\):\n\\[\nC(L) \\sim L^{-\\gamma}.\n\\]\nНа додачу до цього, спектральна густина також спадає за степеневим законом:\n\\[\nP(f) \\sim f^{-\\beta}.\n\\]\nВідповідні показники виражаються через наступні відношення:\n\n\\(\\gamma=2-2\\alpha\\);\n\\(\\beta=2\\alpha-1\\).\n\nУ DFA другого порядку (DFA2) обчислюються відхилення \\(F^2(v,s)\\) профілю від інтерполяційного многочлена другого порядку. Таким чином, вилучаються впливи можливих лінійних та параболічних трендів для масштабів, більших за розглядувані. Взагалі, у DFA порядку \\(n\\) обчислюються відхилення профілю від інтерполяційного многочлена \\(n\\)-го порядку, що вилучає вплив всіх можливих трендів порядків до (\\(n-1\\)) для масштабів, більших від розміру вікна.\nПотім обчислюється найближчий поліном \\(y_{ν}(s)\\) для профілю на кожному із \\(2N_s\\) сегментів \\(v\\) і визначається відхилення\n\\[\nF^2(v,s) \\equiv \\frac{1}{s}\\sum_{i=1}^{s}\\left( x_{(v-1)s+i} - y_{i}(i) \\right)^{2}. \\tag{5}\n\\]\nДалі знаходиться середнє значення флуктуацій всіх детрендованих профілів:\n\\[\nF_2(s) \\equiv \\sqrt{\\left( \\frac{1}{2N_s} \\sum_{v=1}^{2N_s}F^{2}(v,s) \\right)}. \\tag{6}\n\\]\nЗначення формули (6) можна трактувати як середньоквадратичний зсув (переміщення) точки випадкових блукань у ланцюжку після \\(s\\) кроків.\n\n\n7.1.4.3 Фрактальна розмірність Хігучі\nФрактальна розмірність Хігучі — це один з різновидів монофрактальної розмірності, яка визначається наступним чином:\nПрипустимо, що у нас є часовий ряд\n\\[\nx(1), x(2),...,x(N)\n\\]\nі реконструйований часовий ряд \\(x_{m}^{k}\\):\n\\[\\begin{align*}\nx_{m}^{k} = \\{ x(m), x(m+k), x(m+2k), ..., \\\\\nx\\left( m+\\left[ \\frac{N-m}{k} \\right] \\cdot k \\right) \\},\n\\end{align*}\\]\nдля \\(m=1,2,...,k\\); де \\(m\\) представляє початковий час; \\(k=2,...,k_{max}\\) представляють ступінь часового зміщення. Позначення \\([\\cdot]\\) представляє цілу частину \\(x\\). Для кожного реконструйованого часового ряду \\(x_{m}^{k}\\) розраховується середня довжина часової послідовності \\(L_{m}(k)\\):\n\\[\nL_{m}(k) = \\frac{\\sum_{i=1}^{[\\frac{(N-m)}{k}]} | x(m+ik) - x(m+(i-1)\\cdot k) | \\cdot (N-1)}{[\\frac{N-m}{k}] \\cdot k}\n\\]\nДалі, для всіх середніх довжин \\(L_{m}(k)\\), знаходиться загальне середнє:\n\\[\nL(k) = \\frac{1}{k}\\sum_{m=1}^{k}L_{m}(k).\n\\]\nЗгідно методу Хігучі узагальне середнє значення \\(L(k)\\) пропорційне масштабу \\(k\\), тобто\n\\[\nL(k) \\propto k^{-D}.\n\\]\nДалі логарифмуємо обидві сторони й отримуємо наступну рівність:\n\\[\n\\ln{L(k)} \\propto D \\cdot \\ln{\\left( \\frac{1}{k} \\right)}.\n\\]\nІнтерполювавши лінію регресії через залежність \\(\\ln{L(k)}\\) від \\(\\ln{\\left( \\frac{1}{k} \\right)}\\), ми можемо отримати показник фрактальності \\(D\\), отримавши кут нахилу цієї лінії. Показник \\(D\\) і представлятиме фрактальну розмірність Хігучі.\n\n\n7.1.4.4 Фрактальна розмірність Петросяна\nСпочатку, для часового ряду \\(\\{ x_1, x_2,...,y_{N} \\}\\), створюємо його дискретизовану (бінарну) версію, \\(z_i\\):\n\\[\nz_i =\n\\begin{cases}\n    1, & x_i &gt; \\langle x \\rangle, \\\\\n    -1, & x_i \\leq \\langle x \\rangle.\n\\end{cases}\n\\]\nФрактальну розмірність Петросяна може бути визначена як\n\\[\nD = \\frac{\\log_{10}{N}}{\\log_{10} + \\log_{10}{\\left( \\frac{N}{N+0.4N_{\\Delta}} \\right)}},\n\\]\nде \\(N_{\\Delta}\\) — кількість загальних змін знаку змінної \\(z_i\\):\n\\[\nN_{\\Delta} = \\sum_{i=1}^{N-2} \\left|\\frac{z_{i+1}-z_i}{2}\\right|.\n\\]\n\n\n7.1.4.5 Фрактальна розмірність Каца\nПредставимо, що сигнал складається з пари точок \\(\\left( x_i, y_i \\right)\\). Тоді, фрактальна розмірність Каца визначається як\n\\[\nD = \\frac{\\log{N}}{\\log{N} + \\log{\\frac{d}{L}}},\n\\]\nде \\(L\\) визначається наступним чином:\n\\[\nL = \\sum_{i=0}^{N-2}\\sqrt{\\left( y_{i+1}-y_{i} \\right)^{2} + \\left( x_{i+1}-x_{i} \\right)^{2}}.\n\\]\nЗначення \\(d\\) визначається як максимальна відстань від початкової точки \\(\\left( x_1, y_1 \\right)\\) до всіх інших точок, а саме \\(d\\) може бути розраховане наступним чином:\n\\[\nd = \\max{\\left( \\sqrt{\\left( x_i - x_1 \\right)^{2} - \\left( y_i - y_1 \\right)^{2}} \\right)}.\n\\]\n\n\n7.1.4.6 Фрактальна розмірність Севчика\nСпочатку, для множини значень \\(\\left( x_i, y_i \\right)\\) виконується нормалізація:\n\\[\nx_{i}^{*} = \\frac{x_i-x_{min}}{x_{max}-x_{min}}, \\; y_{i}^{*} = \\frac{y_i-y_{min}}{y_{max}-y_{min}}.\n\\]\nФрактальна розмірність Севчика може бути визначена як\n\\[\nD = 1 + \\frac{\\ln{L}}{\\ln{[2 \\cdot \\left( N-1 \\right)]}},\n\\]\nде \\(L\\) — це довжина сигналу, що може бути визначена як\n\\[\nL = \\sum_{i=0}^{N-2}\\sqrt{\\left( y_{i+1}^{*}-y_{i}^{*} \\right)^{2} + \\left( x_{i+1}^{*}-x_{i}^{*} \\right)^{2}}.\n\\]\n\n\n7.1.4.7 Фрактальна розмірність через нормалізовану щільність довжини\nДаний показник розраховується в наступний спосіб:\n\nДля часового ряду \\(\\{ x_1, x_2,...,x_n \\}\\) виконується стандартизація: \\(y_i = \\frac{x_i - \\mu}{\\sigma}\\), де \\(\\mu\\) — це середнє значення ряду, \\(\\sigma\\) — це стандартне відхилення.\nРозраховується нормалізована щільність довжини:\n\n\\[\nNLD = \\frac{1}{N}\\sum_{i=2}^{N}\\left| y_i - y_{i-1} \\right|\n\\]\nФактичний розрахунок фрактальної розмірності сигналу базується на побудові монотонної калібрувальної кривої, \\(D = f(NLD)\\), за набором функцій Вейєрштрасса, для яких значення \\(D\\) задаються теоретично.\n\nДля обчислювальних цілей необхідно створити математичну модель цієї залежності. Автори даного підходу тестували дві моделі:\n\nлогарифмічну модель: \\(D = a \\cdot \\log{\\left(NLD - NLD_{0} \\right)} + C\\)\nстепеневу модель: \\(D = a \\cdot \\left(NLD - NLD_{0} \\right)^{k}\\). Бібліотека neurokit2 використовує саме степеневу модель. Параметр \\(a=1.9079\\), \\(k=0.18383\\) і \\(NLD_{0}=0.097178\\), згідно статті Kalauzi et al. 2009.\n\n\n\n\n7.1.4.8 Фрактальна розмірність через нахил спектральної щільності потужності\nФрактальну розмірність можна обчислити на основі аналізу нахилу спектральної щільності потужності (power spectral density slope, PSD) в сигналах, що характеризуються частотною степеневою залежністю.\nСпочатку виконується перетворення часового ряду до частотної області і далі сигнал розбивається на синусоїдальні та косинусоїдальні хвилі певної амплітуди, які разом “складаються”, щоб представити вихідний сигнал. Якщо існує систематичний зв’язок між частотами в сигналі і потужністю цих частот, то в логарифмічних координатах це проявляється в лінійній залежності. Кут нахилу лінії регресії приймається як оцінка фрактальної розмірності.\nНахил 0 відповідає білому шуму, а нахил менше 0, але більше -1, відповідає рожевому шуму, тобто шуму \\(1/f\\). Спектральні нахили крутіші за -2 вказують на дробовий броунівський рух, що є втіленням процесів випадкового блукання.\n\n\n7.1.4.9 Кореляційна розмірність\nКореляційна розмірність (\\(D_2\\)) — це похідна величина від кореляційного інтеграла Кореляційний інтеграл (кореляційна сума) може бути поданий в такому вигляді:\n\\[\nC(\\varepsilon) = \\frac{1}{N^{2}}\\sum_{\\substack{i,j=1 \\\\ i\\neq j}}^{N}\\Theta \\left( \\varepsilon - \\| \\vec{x}(i) - \\vec{x}(j) \\| \\right), \\; \\vec{x}(i) \\in \\Re^{m}.\n\\]\nСама кореляційна розмірність може бути виведена з наступної степеневої залежності:\n\\[\nC(\\varepsilon) \\sim \\varepsilon^{\\nu},\n\\]\nабо слідуючим чином:\n\\[\nD_2 = \\lim_{M\\to\\infty}\\lim_{\\varepsilon\\to 0}\\frac{\\log{\\left( g_{\\varepsilon}/N^2 \\right)}}{\\log{\\varepsilon}},\n\\]\nде \\(g_{\\varepsilon}\\) — це сумарна кількість пар точок, відстань між якими менша за радіус \\(\\varepsilon\\).\nЗа формулою \\(C(\\varepsilon)\\) ми відбираємо \\(i\\)-ту траєкторію та всі інші \\(j\\)-ті траєкторії, і дивимося, чи потрапляють \\(j\\)-ті траєкторії в округ \\(i\\)-ої траєкторії з радіусом \\(\\varepsilon\\). Якщо відстань між ними не перевищує округ зі згаданим радіусом, ми ставимо 1. Але якщо відстань між траєкторіями більша за \\(\\varepsilon\\), тоді ставимо 0. Далі все це підсумовується, ділиться на загальну кількість траєкторій. По суті кореляційний інтеграл це середня ймовірність того, що дві розглянуті траєкторії фазового простору, що розглядаються, будуть знаходитися досить близько одна до одної. Чим тісніше розташовані точки фазового простору одна до одної, тим більше значення кореляційного інтеграла. Чим більш рівновіддаленими видаються траєкторії одна від одної, тим ближче значення кореляційного інтеграла до нуля.\nЗначення кореляційної розмірності ми можемо відшукати аналогічно попередним фрактальним показникам: ми шукаємо залежність кореляційного інтеграла від значення \\(\\varepsilon\\). Ця залежність будується в логарифмічному масштабі.\n\n\n\n\n\n\nДодаткова інформація по кореляційні розмірності\n\n\n\nКореляційна розмірність за аналогією з попередніми показниками — це теж тангенс кута нахилу лінії регресії, побудованої в логарифмічному масштабі, але для залежності кореляційного інтеграла від \\(\\varepsilon\\). За аналогією з іншими показниками, кореляційна розмірність визначає швидкість зміни значення кореляційного інтеграла (крутість нахилу лінії регресії).\n\n\n\n\n\nРисунок 7.2: Зміна значення кореляційного інтеграла в залежності від ступеня розкиданості точок по фазовому простору системи\n\n\nЗа фазовим простором цієї хмари точок ( Рисунок 7.2 ) можна бачити, що більша згуртованість точок одна до одної має асоціюватися з меншою кореляційною розмірністю. Рисунок (а) характеризується найвищою близькістю точок одна до одної, але при цьому найменшою кореляційною розмірністю. Це можна пояснити так: якщо ми будуємо сітку з кіл радіусом \\(\\varepsilon\\) і поступово її збільшуємо, ми вже перестаємо бачити нові прилеглі траєкторії до тих \\(i\\)-их траєкторій, які ми розглядали із самого спочатку, на початкових значеннях \\(\\varepsilon\\). На рисунках (b) і (c) видно, що хмара траєкторій фазового простору є більш рівномірно розподіленою. За поступового збільшення радіуса кіл із центрами в кожній \\(i\\)-ій траєкторії ми повинні спостерігати пропорційне збільшення значення кореляційного інтеграла. У даному випадку при поступовому збільшенні \\(\\varepsilon\\) ми спостерігаємо появу все більшої і більшої кількості точок.\nУ періодичних системах кореляційна розмірність залишається постійною і дорівнює розмірності вкладення. Наприклад, для простої періодичної системи, такої як синусоїда, кореляційна розмірність дорівнюватиме 1 (оскільки вона лежить на одновимірній кривій), а якщо систему реконструюють у двовимірному фазовому просторі (за двома координатами), то кореляційна розмірність дорівнюватиме 2. У таких системах кореляційна розмірність не змінюється.\nДля хаотичних систем кореляційна розмірність має характерну поведінку, яка залежить від кількості змінних (розмірностей), необхідних для точного опису динаміки системи. На відміну від періодичних систем, кореляційна розмірність зростає в міру збільшення розмірності фазового простору, поки не досягне плато.\nЕлектрокардіограма (ЕКГ): ЕКГ-сигнали відображають електричну активність серця. Складність ЕКГ-сигналу може бути оцінена за допомогою кореляційної розмірності. Очікується, що кореляційна розмірність ЕКГ здорового серця буде вищою через наявність складних патернів і варіабельності. З іншого боку, аномальні ЕКГ-сигнали, наприклад, від пацієнтів з аритміями або серцевими захворюваннями, можуть мати нижчу кореляційну розмірність через втрату складності сигналу.\nЕлектроенцефалограма (ЕЕГ): Сигнали ЕЕГ реєструють електричну активність мозку. Кореляційна розмірність може використовуватися для аналізу складності мозкової активності, яка може змінюватися залежно від різних когнітивних станів, стадій сну або неврологічних розладів. У здорових людей сигнали ЕЕГ у стані бадьорості та уваги можуть мати вищу кореляційну розмірність порівняно з сигналами у стадії сну, коли активність мозку є більш регулярною і синхронізованою.\nДихальні сигнали: Дихальні сигнали, такі як частота дихання або повітряний потік, також можуть бути проаналізовані за допомогою кореляційної розмірності. Складність цих сигналів може змінюватися залежно від таких факторів, як стрес, фізичне навантаження або наявність респіраторних захворювань. За нормального дихання може спостерігатися вища кореляційна розмірність, тоді як порушення в дихальних сигналах, наприклад, за обструктивного апное уві сні або дихальних розладів, можуть призвести до зниження кореляційної розмірності.\nАналіз ходи: Кореляційна розмірність може бути використана для аналізу моделей ходи. Вона може допомогти в розумінні складності рухів людини під час ходьби або бігу. Зміни в кореляційній розмірності сигналів ходи можуть свідчити про зміну стабільності ходи або про наявність відхилень у ході, викликаних неврологічними або опорно-руховими захворюваннями.\nВаріативність динаміки серцевого ритму (ВСР): ВСР являє собою зміну часових інтервалів між послідовними ударами серця. Вона перебуває під впливом вегетативної нервової системи і відображає адаптивність і складність серцево-судинної системи. Вищий рівень ВСР, що відповідає вищій кореляційній розмірності, зазвичай асоціюється з кращим станом серцево-судинної системи та її адаптивністю до фізіологічних змін і змін навколишнього середовища. Її падіння може асоціюватися з аномальною динамікою серця.\nПослідовності ДНК: Кореляційна розмірність може бути використана і при аналізі послідовностей ДНК. Вона допомагає виявити самоподібні або фрактальні патерни всередині послідовностей, що може мати значення для розуміння генетичної складності, еволюційних зв’язків і регуляції генів. Висока кореляційна розмірність — висока складність ланцюжка ДНК. Мала кореляційна розмірність — спрощений ланцюжок ДНК.\nФінансові ринки Вища кореляційна розмірність у даних часових рядів фінансового ринку свідчить про більшу складність та існування в їхній основі самоподібних моделей або фрактальних структур. Хаотична поведінка цін на акції може бути пов’язана з періодами високої волатильності та непередбачуваності. З іншого боку, нижча величина кореляційної розмірності може свідчити про більш передбачувані та менш складні рухи цін, що відповідає періодам стабільності або менш волатильним ринковим умовам.\n\n\n\n\n\n\nПримітка по кореляційні розмірності\n\n\n\nКореляційна розмірність розглядає кількість інформації (кубиків, кіл), необхідну для опису тільки пари точок у фазовому просторі."
  },
  {
    "objectID": "lab_6.html#хід-роботи",
    "href": "lab_6.html#хід-роботи",
    "title": "7  Лабораторна робота № 6",
    "section": "7.2 Хід роботи",
    "text": "7.2 Хід роботи\nРозглянемо як можна застосовувати зазначені показники в якості індикаторів кризових станів.\nСпочатку імпортуємо необхідні бібліотеки для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nУ даній роботі скористаємось монофрактальними показниками для ідентифікації кризових явищ на ринку золота. Розглянемо значення золота за весь період, що представляє Yahoo! Finance. Для цього нам не треба буде вказувати початкову та кінцеву дати:\n\nsymbol = 'GC=F'                       # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 7.3: Динаміка щоденних змін індексу золота\n\n\n\n\nВизначимо функцію transformation() для стандартизації ряду:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n7.2.1 Обчислення показника Херста із використанням R/S-аналізу\nДля подальших розрахунків використовуватимемо бібліотеку neurokit2 та fathon. Другу можна встановити в наступний спосіб:\n\n!pip install fathon\n\nДалі імпортуємо саму бібліотеку та дотичні до неї модулі:\n\nimport fathon\nfrom fathon import fathonUtils as fu\n\nБібліотека neurokit містить необхідний метод для R/S-аналізу — fractal_hurst. Його синтаксис виглядає наступним чином:\nfractal_hurst(signal, scale='default', corrected=True, show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму бібліотеки pandas.\nscale (list) — список, що містить довжини вікон (кількість точок даних у кожній підмножині ряду), на які розбито сигнал.\ncorrected (bool) — якщо значення True, до вихідних даних буде застосовано поправочний коефіцієнт Аніса-Ллойда-Пітерса відповідно до очікуваного значення для окремих значень (R/S).\nshow (bool) — якщо значення True, виводить залежність \\((R/S)_n\\) від \\(n\\) (scale) у подвійному логарифмічному масштабі.\n\nПовертає\n\nh (float) — показник Херста.\n**kwargs — словник, що містить інформацію відносно використовуваних у процедурі параметрів.\n\nРозглянемо ступінь трендостійкості в динаміці фондового індексу золота, використовуючи весь часовий ряд. Далі знайдемо значення показника Херста в рамках віконної процедури.\n\n7.2.1.1 Увесь часовий ряд\nПершочергово знайдемо значення прибутковостей для нашого ряду та стандартизуємо їх. Після цього виконаємо обчислення.\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rs = transformation(signal, ret_type) \n\nВиконуємо R/S-аналіз:\n\nh, info = nk.fractal_hurst(for_rs, corrected=False, show=True)\n\n\n\n\nРисунок 7.4: Залежність значень R/S від скейлінгу побудованих в логарифмічному масштабі\n\n\n\n\nЯк ми можемо бачити з Рисунок 7.4, значення \\(h=0.53\\), що свідчить про подібність динаміки золота до випадкового блукання. Але оскільки закони, що регулюють ринок, змінюються з часом, мають змінюватись і кореляції всередині системи, а одже коефіцієнт Херста також має залежати від періоду в якому він розглядається.\n\n\n7.2.1.2 Віконна процедура\nВизначимо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nПриступимо до віконної процедури:\n\n# встановлюємо параметри\nret_type = 4                   # вид ряду\nwindow = 250                   # ширина вікна\ntstep = 1                      # часовий крок вікна \nlength = len(time_ser.values)  # довжина самого ряду\ncorr = False                   # поправочний коефіцієнт Аніса-Ллойда-Пітерса\n\nH = []                         # масив для віконного Херсту\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо взаємну інформацію \n    h, _ = nk.fractal_hurst(fragm, corrected=corr, show=False)\n    \n    # та додаємо результат до масиву значень\n    H.append(h)\n\n  1%|          | 41/5531 [00:00&lt;00:13, 404.03it/s]\n\n\n100%|██████████| 5531/5531 [00:13&lt;00:00, 419.90it/s]\n\n\n\nnp.savetxt(f\"rs_hurst_name={symbol}_window={window}_step={tstep}_ \\\n           rettype={ret_type}_corrected={corr}.txt\" , H)\n\nВізуалізуємо результат:\n\nmeasure_label = r'$H$'\nfile_name = f\"rs_hurst_name={symbol}_window={window}_step={tstep}_ \\\n           rettype={ret_type}_corrected={corr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          H, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 7.5: Динаміка індексу золота та показника Херста\n\n\n\n\nНа представленому рисунку ( Рисунок 7.5 ) можемо бачити, що показник Херста зростає в передкризовий період та спадає під час кризи. Перед кризою динаміка ринку характеризується зростанням трендостійкості (персистентності), що відзеркалює зростання скорельованості дій між трейдерами ринку.\n\n\n\n7.2.2 Обчислення на основі DFA\nБібліотека fathon представляє інструментарій як для виконання класичного аналізу детрендованих флуктуацій, так і для його мультифрактального аналогу, мова про який піде в наступній лабораторній.\n\n7.2.2.1 Для всього ряду\nСпочатку представимо значення \\(\\alpha\\) для всього ряду. Процедура розрахунків на основі бібліотеки fathon виглядатиме наступним чином:\n\nзнаходимо стандартизовані прибутковості ряду\n\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_dfa = transformation(signal, ret_type) \n\n\ncumulat = fu.toAggregated(for_dfa) # знаходимо кумулятивні накопичення\n\nrev = True # чи повторювати розрахунок ф-ції флуктуацій з кінця\norder = 2  # порядок локального лінійного тренду \n\npydfa = fathon.DFA(cumulat) # ініціалізація об'єкту DFA\n                            # для виконання подальших обчислень\n\nwin_beg = 100               # початкова ширина сегментів\nwin_end = 2000              # кінцева ширина сегментів\n\nwins = fu.linRangeByStep(win_beg, win_end) # генеруємо масив \n                                           # лінійно розділених \n                                           # елементів.\n\nn, F = pydfa.computeFlucVec(wins, \n                            polOrd=order, \n                            revSeg=rev)    # знаходимо функцію флуктуацій\n\nH, H_intercept = pydfa.fitFlucVec()        # знаходимо показник альфа\n\nВиводимо залежність функції флуктуацій від характеристичного масштабу:\n\npolyfit = np.polyfit(np.log(n), np.log(F), 1)\nfluctfit = np.exp(1) ** np.polyval(polyfit, np.log(n))\n\nБудуємо залежність функції флуктуацій від масштабу в подвійному логарифмічному масштабі:\n\nfig, ax = plt.subplots()\nfig.suptitle(\"Показник Херста на основі DFA\")\n\nax.scatter(\n        np.log(n),\n        np.log(F),\n        marker=\"o\",\n        zorder=1,\n        label=\"_no_legend_\",\n    )\n\nlabel = fr\"$\\alpha$ = {H:.2f}\"\nax.plot(np.log(n), np.log(fluctfit), \n        color=\"#E91E63\", zorder=2, \n        linewidth=3, label=label)\n\nax.set_ylabel(r'$\\ln{F_{2}(n)}$')\nax.set_xlabel(r'$\\ln{n}$')\n\nax.legend(loc=\"lower right\")\n\nplt.show()\n\n\n\n\nРисунок 7.6: Логарифмічна залежність значень функції флуктуацій від скейлінгу\n\n\n\n\nПроцедура DFA показує, що значення індексу золота представляються скоріше антиперсистентними, але представлений результат доволі близький до того, що був отриманий за допомогою R/S-аналізу. Розглянемо значення \\(\\alpha\\) в рамках алгоритму ковзного вікна.\n\n\n7.2.2.2 Віконна процедура\nВизначимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nrev = True      # чи повторювати розрахунок ф-ції флуктуацій з кінця\norder = 2       # порядок поліноміального тренду\n\nperiods = 1\n\nwin_beg = 10             # початковий масштаб сегментів\nwin_end = window-1       # кінцевий масштаб сегментів\n\n\n\nlength = len(time_ser.values) # довжина ряду\n\nalpha = []               # масив показників альфа (Херста)\nD_f = []                 # фрактальна розмірність\nbeta = []                # показник спектральної щільності\ngamma = []               # показник автокореляції\n\nЗнайдемо показник Херста (\\(\\alpha\\)), фрактальну розмірність (\\(D_f\\)), показник спектральної щільності (\\(\\beta\\)) та показник автокореляції (\\(\\gamma\\)):\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # знаходимо кумулятивні накопичення\n    cumulat_wind = fu.toAggregated(fragm) \n\n    # ініціалізація об'єкту DFA\n    pydfa = fathon.DFA(cumulat_wind) \n\n    # генеруємо масив лінійно розділених елементів\n    wins = fu.linRangeByStep(win_beg, win_end) \n\n    # знаходимо функцію флуктуацій\n    n, F_wind = pydfa.computeFlucVec(wins, polOrd=order, revSeg=rev)    \n\n    # знаходимо показник альфа\n    H_wind, _ = pydfa.fitFlucVec()\n\n    # знаходимо фрактальну розмірність        \n    D = 2. - H_wind\n\n    # показник спектральної щільності\n    bi = 2. * H_wind - 1 \n\n    # показник автокореляції\n    gi = 2. - 2. * H_wind\n\n    alpha.append(H_wind)\n    D_f.append(D)\n    beta.append(bi)\n    gamma.append(gi)\n\n100%|██████████| 5531/5531 [00:57&lt;00:00, 96.13it/s] \n\n\nЗберігаємо абсолютні значення показників до текстових файлів:\n\nnp.savetxt(f\"alpha_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", alpha)\nnp.savetxt(f\"D_f_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", D_f)\nnp.savetxt(f\"beta_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", beta)\nnp.savetxt(f\"gamma_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", gamma)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_alpha = fr'$\\alpha$'\nlabel_d = fr'$D_f$'\nlabel_beta = fr'$\\beta$'\nlabel_gamma = fr'$\\gamma$'\n\nfile_name_alpha = f\"alpha_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_d = f\"D_f_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_beta = f\"beta_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_gamma = f\"gamma_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\n\nВиводимо результати:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alpha, \n          ylabel, \n          label_alpha,\n          xlabel,\n          file_name_alpha)\n\n\n\n\nРисунок 7.7: Динаміка індексу золота та показника альфа\n\n\n\n\nЯкщо порівнювати з R/S-аналізом, Рисунок 7.7 демонструє, що динаміка узагальненого показника Херста отриманого за допомогою DFA є набагато стабільнішою. Тепер ми здатні диференціювати значну частку крахових подій, що мали місце на ринку золота. Узагальнений Херст показує, що передкризові явища характеризуються зростанням трендостійкості ринку або, іншими словами, підвищенням ступеня самоорганізації системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_f, \n          ylabel, \n          label_d,\n          xlabel,\n          file_name_d)\n\n\n\n\nРисунок 7.8: Динаміка індексу золота та фрактальної розмірності\n\n\n\n\nРисунок 7.8 показує, що \\(D_f\\) характеризується спадом при кризових станах. Це є індикатором того, що вищий ступень організованості ринку відзеркалюється в більш згладженій або менш шорстких флуктуаціях досліджуваного сигналу.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          beta, \n          ylabel, \n          label_beta,\n          xlabel,\n          file_name_beta)\n\n\n\n\nРисунок 7.9: Динаміка індексу золота та показника спектральної щільності\n\n\n\n\nНа даному рисунку ми бачимо динаміку показника \\(\\beta\\), що відноситься до спектральної густини потужності (\\(P(f)=1/f^{\\beta}\\)), зростає в кризові періоди, що говорить про спад потужності сигналу, що припадає на одиничний інтервал частоти. Це також є свідченням зростання кореляційних властивостей системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          gamma, \n          ylabel, \n          label_gamma,\n          xlabel,\n          file_name_gamma)\n\n\n\n\nРисунок 7.10: Динаміка індексу золота та показника автокореляції\n\n\n\n\nНа Рисунок 7.10 видно, що показник \\(\\gamma\\) спадає в кризові та передкризові періоди. Це є показником сповільнення спаду функції автокореляції, що в свою чергу також вказує на зростання корельованності динаміки системи.\n\n\n\n7.2.3 Обчислення фрактальної розмірності Хігучі\nЯк уже зазначалося, фрактальна розмірність Хігучі є одним із різновидів фрактальної розмірності для часових рядів. Вона обчислюється шляхом реконструкції \\(k\\)-максимальної кількості нових наборів даних. Для кожного відновленого набору даних обчислюється довжина кривої і відкладається проти відповідного k-значення в логарифмічній шкалі. HFD відповідає нахилу лінійного тренду за методом найменших квадратів.\nРозрахуємо оптимальне значення \\(k\\) для всього часового ряду. Бібліотека neurokit2 представляє готову процедуру для автоматизованого підбору даного параметру. Оптимальний \\(k_{max}\\) розраховується на основі точки, в якій значення фрактальної розмірності досягає плато для діапазону значень \\(k_{max}\\) (див. Vega, 2015).\nСинтаксис даної функції виглядає наступним чином:\ncomplexity_k(signal, k_max='max', show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\\(k_{max}\\) (Union[int, str, list], optional) — максимальна кількість інтервалів (має бути більше або дорівнювати 3), які потрібно перевірити. Якщо \\(k_{max}\\)=default, тоді вибирається максимально можливе значення, що відповідає половині довжини сигналу.\nshow (bool) — візуалізовується нахил кривої для обраного значення \\(k_{max}\\).\n\nПовертає\n\nk (float) — оптимальний \\(k_{max}\\) часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимального \\(k_{max}\\).\n\n\n7.2.3.1 Для всього ряду\nДля подальших розрахунків спочатку виконаємо перетворення ряду. Будемо використовувати вихідний часовий ряд для подальших розрахунків:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_higuchi = transformation(signal, ret_type) \n\nІ тепер отримаємо оптимальне значення \\(k_{max}\\) згідно зазначеної процедури:\n\nk_max, info = nk.complexity_k(for_higuchi, k_max=100, show=True)\n\n\n\n\nРисунок 7.11: Залежність розмірності Хігучі від діапазону значень kmax\n\n\n\n\nТепер побудуємо залежність довжини сигналу від часового зміщення в логарифмічному масштабі. Для фрактального сигналу має зберігатися лінійна залежність. Бібліотека neurokit2 містить метод для розрахунку даної фрактальної розмірності. Синтаксис цієї процедури виглядає наступним чином:\nfractal_higuchi(signal, k_max='default', show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\\(k_{max}\\) (Union[int, str, list], optional) — максимальна кількість інтервалів (має бути більше або дорівнювати 3), які потрібно перевірити.\nshow (bool) — візуалізовується нахил кривої для обраного значення \\(k_{max}\\).\n\nПовертає\n\nHFD (float) — фрактальна розмірність Хігучі для досліджуваного часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення фрактальної розмірності Хігучі.\n\n\nhfd, info = nk.fractal_higuchi(for_higuchi, k_max=k_max, show=True)\n\n\n\n\nРисунок 7.12: Залежність довжини сигналу від часового зміщення\n\n\n\n\nУ подальшому будемо послуговуватись отриманим оптимальним значенням для розрахунку розмірності Хігучі в рамках алгоритму ковзного вікна.\n\n\n7.2.3.2 Віконна процедура\nСкористаємось наступними параметрами:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nk_max_wind = 30                    # максимальне часове зміщення\n\nlength = len(time_ser.values)      # довжина ряду\n\nhfd_wind = []                      # масив показників Хігучі\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Хігучі\n    higuchi, _ = nk.fractal_higuchi(fragm, \n                                    k_max=k_max_wind, \n                                    show=False)\n\n    # зберігаємо результат до масиву значень\n    hfd_wind.append(higuchi)\n\n100%|██████████| 5531/5531 [00:15&lt;00:00, 362.90it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_higuchi_name={symbol}_kmax={k_max_wind}_\\\n           wind={window}_step={tstep}.txt\", hfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_higuchi = fr'$HFD$'\n\nfile_name_higuchi = f\"fd_higuchi_name={symbol}_kmax={k_max_wind}_\\\n           wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          hfd_wind, \n          ylabel, \n          label_higuchi,\n          xlabel,\n          file_name_higuchi)\n\n\n\n\nРисунок 7.13: Динаміка індексу золота та фрактальної розмірності Хігучі\n\n\n\n\nЯк можна бачити з представленого рисунку, фрактальна розмірність Хігучі може працювати як індикатор або індикатор-передвісник кризових явищ. Видно, що даний показник починає спадати у передкризові періоди чи у сам момент кризи, вказуючи на зростання згладженності динаміки системи, ступеня кореляцій та трендостійкості динаміки ринку.\n\n\n\n7.2.4 Обчислення фрактальної розмірності Петросяна\nПетросян (1995) запропонував швидкий метод оцінки фрактальної розмірності шляхом перетворення сигналу в двійкову послідовність, з якої оцінюється фрактальна розмірність. Існує кілька варіацій алгоритму (neurokit2, наприклад, пропонує \"A\", \"B\", \"C\" або \"D\"), що відрізняються насамперед способом створення дискретної (символьної) послідовності (див. complexity_symbolize() для деталей). Найпоширеніший метод (\"C\", за замовчуванням) бінаризує сигнал за знаком послідовних різниць.\nБільшість з цих методів дискретизації припускають, що сигнал є періодичним (без лінійного тренду). Для усунення лінійних трендів може бути корисним лінійне детрендування.\nСинтаксис даної процедури має наступний вигляд:\nfractal_petrosian(signal, symbolize='C', show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\nsymbolize (str) - метод перетворення неперервного вхідного сигналу в символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення None, що припускає, що вхідний сигнал вже є дискретним.\nshow (bool) — виводить дискретизацію сигналу.\n\nПовертає\n\nPFD (float) — фрактальна розмірність Петросяна для досліджуваного часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення фрактальної розмірності Петросяна.\n\nМи не розглядатимемо детально синтаксис функції complexity_symbolize(). Опишемо лише ті методи дискретизації, що дотичні до фрактальної розмірності Петросяна: - Метод A бінаризує сигнал за більшими та меншими значеннями порівняно із середнім значенням сигналу. Еквівалентом є method=\"mean\" (method=\"median\" також є допустимим). - Метод B використовує значення, що знаходяться в діапазоні \\(\\pm 1\\sigma\\), проти значень, що виходять за межі цього діапазону. - Метод C обчислює різницю між послідовними вибірками та бінаризує їх залежно від знаку. - Метод D відокремлює послідовні відліки, що перевищують \\(1\\sigma\\) сигналу, від інших менших змін.\nТепер розглянемо віконну динаміку даного показника.\n\n7.2.4.1 Віконна процедура\nОскільки більшість з даних методів дискретизації вимагаються детрендування ряду, тоді будемо виконувати розрахунки для прибутковостей індексу золота.Скористаємось наступними параметрами:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nsymb = \"B\"                    # тип дискретизації ряду\n\nlength = len(time_ser.values) # довжина ряду\n\npetr_wind = []                 # масив показників Петросяна\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Петросяна\n    petrocian, _ = nk.fractal_petrosian(fragm, \n                                        symbolize=symb, \n                                        show=False)\n\n    # зберігаємо результат до масиву значень\n    petr_wind.append(petrocian)\n\n100%|██████████| 5531/5531 [00:05&lt;00:00, 1034.18it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_petrosian_name={symbol}_method={symb}_\\\n           wind={window}_step={tstep}.txt\", petr_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_petrocian = fr'$PFD$'\n\nfile_name_petrocian = f\"fd_petrosian_name={symbol}_method={symb}_\\\n           wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          petr_wind, \n          ylabel, \n          label_petrocian,\n          xlabel,\n          file_name_petrocian)\n\n\n\n\nРисунок 7.14: Динаміка індексу золота та фрактальної розмірності Петросяна\n\n\n\n\nРисунок 7.14 показує, що показник Петросяна також спадає під час кризових подій, що вказує на зростання періодизації ринку та синхронізації активності трейдерів у відповідні моменти часу.\n\n\n\n7.2.5 Обчислення фрактальної розмірності Каца\nТепер обчислюємо фрактальну розмірність Каца. Евклідові відстані між послідовними точками сигналу підсумовуються і усереднюються, а також визначається максимальна відстань між початковою точкою і будь-якою іншою точкою у вибірці.\nФрактальна розмірність варіюється від 1.0 для прямих ліній, приблизно до 1.15 для випадкових блукань і наближається до 1.5 для найбільш “дивних” форм сигналу.\nСинтаксис процедури для розрахунку даної розмірності виглядає наступним чином:\nfractal_katz(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nKFD (float) — фрактальна розмірність Каца для досліджуваного часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію (наразі порожній, але повертається для узгодженості з іншими функціями).\n\n\n7.2.5.1 Віконна процедура\nОскільки даний показник є параметронезалежним, нам достатньо буде лише розміру часового вікна, кроку та типу ряду:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser.values)      # довжина ряду\n\nkz_wind = []                      # масив показників Каца\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Хігучі\n    katz, _ = nk.fractal_katz(fragm)\n\n    # зберігаємо результат до масиву значень\n    kz_wind.append(katz)\n\n100%|██████████| 5531/5531 [00:01&lt;00:00, 3412.20it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_katz_name={symbol}_wind={window}_step={tstep}.txt\", kz_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_katz = fr'$KFD$'\n\nfile_name_katz = f\"fd_katz_name={symbol}_wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          kz_wind, \n          ylabel, \n          label_katz,\n          xlabel,\n          file_name_katz)\n\n\n\n\nРисунок 7.15: Динаміка індексу золота та фрактальної розмірності Каца\n\n\n\n\nНа представленому рисунку видно, що фрактальна розмірність Каца також спадає у кризові та передкризові періоди. Це також є індикаторов зростання ступеня корельованості системи в дані періоди.\n\n\n\n7.2.6 Обчислення фрактальної розмірності Севчика\nАлгоритм цієї фрактальної розмірності був запропонований для обчислення фрактальної розмірності сигналів Севчиком (1998). Цей метод можна використовувати для швидкого вимірювання складності та випадковості сигналу.\nСинтаксис методу, що ми використовуватимемо в подальшому, має наступний вигляд:\nfractal_sevcik(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nSFD (float) — фрактальна розмірність Севчика для досліджуваного часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію (наразі порожній, але повертається для узгодженості з іншими функціями).\n\n\n7.2.6.1 Віконна процедура\nДля цього показника нам також не потребується нічого зайвого:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser.values)      # довжина ряду\n\nsfd_wind = []                      # масив показників Севчика\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Севчика\n    sevcik, _ = nk.fractal_sevcik(fragm)\n\n    # зберігаємо результат до масиву значень\n    sfd_wind.append(sevcik)\n\n100%|██████████| 5531/5531 [00:01&lt;00:00, 3440.38it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_cevcik_name={symbol}_wind={window}_step={tstep}.txt\", sfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_sevcik = fr'$SFD$'\n\nfile_name_sevcik = f\"fd_cevcik_name={symbol}_wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          sfd_wind, \n          ylabel, \n          label_sevcik,\n          xlabel,\n          file_name_sevcik)\n\n\n\n\nРисунок 7.16: Динаміка індексу золота та фрактальної розмірності Севчика\n\n\n\n\nБачимо, що і фрактальна розмірність Севчика реагує спадом на крахові події на ринку золота. Особливо характерними є спади під час кризи 2008, 2011, 2015 та 2020 років, що є ключовими та дослідженими багатьма вченими. Динаміка індексу золота під час зазначених кризових подій також характеризувалися зростанням персистентності (кореляцій).\n\n\n\n7.2.7 Обчислення фрактальної розмірності через нормалізовану щільність довжини\nЦе доволі простий показник, що відповідає середнім абсолютним послідовним різницям (стандартизованого) сигналу (np.mean(np.abs(np.diff(std_signal)))). Цей метод було розроблено для вимірювання складності сигналів дуже короткої тривалості (&lt; 30 відліків), і його можна використовувати, наприклад, коли цікавлять безперервні зміни фрактальної розмірності сигналу, обчислюючи його в межах ковзних вікон.\nДля таких методів, як для фрактальної розмірності Хігучі, стандартне відхилення віконної фрактальної розмірності різко зростає, коли довжина ряду стає коротшою. Цей метод дає менше стандартне відхилення, особливо для коротших фрагментів, хоча і за рахунок меншої точності середнього значення фрактальної розмірності вікна.\nСинтаксис процедури має наступний вигляд:\nfractal_nld(signal, corrected=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ncorrected (bool) — якщо значення True, змінює масштаб вихідного значення фрактальної розмірності відповідно до степеневої моделі, щоб зробити його більш порівнянним з “істинним” значенням: FD = 1.9079*((NLD-0.097178)^0.18383). Зауважте, що це може призвести до np.nan, якщо результат різниці буде від’ємним.\n\nПовертає\n\nNLDFD (float) — фрактальна розмірність через нормалізовану щільність довжини.\ninfo (dict) — словник, що містить додаткову інформацію (наразі порожній, але повертається для узгодженості з іншими функціями).\n\n\n7.2.7.1 Віконна процедура\nДля цього показника нам також не потребується нічого зайвого:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nnld_corrected = True               # нормалізація фрактальної розмірності\n\nlength = len(time_ser.values)      # довжина ряду\n\nnldfd_wind = []                    # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    nld, _ = nk.fractal_nld(fragm, \n                            corrected=nld_corrected)\n\n    # зберігаємо результат до масиву значень\n    nldfd_wind.append(nld)\n\n100%|██████████| 5531/5531 [00:05&lt;00:00, 994.20it/s] \n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_nld_name={symbol}_wind={window}_\\\n           step={tstep}_corrected={nld_corrected}.txt\", nldfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_nld = fr'$NLDFD$'\n\nfile_name_nld = f\"fd_nld_name={symbol}_wind={window}_\\\n                step={tstep}_corrected={nld_corrected}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          nldfd_wind, \n          ylabel, \n          label_nld,\n          xlabel,\n          file_name_nld)\n\n\n\n\nРисунок 7.17: Динаміка індексу золота та фрактальної розмірності через нормалізовану щільність довжини\n\n\n\n\nРисунок 7.17 показує, що \\(NLDFD\\) спадає під час кризових та передкризових подій, що вказує на зростання кореляцій у дані періоди.\n\n\n\n7.2.8 Обчислення фрактальної розмірності через нахил спектральної щільності потужності\nСкористаємось наступним методом бібліотеки neurokit2:\nfractal_psdslope(signal, method='voss1988', show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\nmethod (str) — метод оцінки фрактальної розмірності за нахилом, може бути \"voss1988\" (за замовчуванням) або \"hasselman2013\".\nshow (bool) — якщо значення True, повертає графік залежності спектральної щільності потужності від частоти в логарифмічному масштабі.\nkwargs — інші аргументи, які слід передати до signal_psd().\n\nПовертає\n\nslope (float) — оцінка фрактальної розмірності, отримана в результаті аналізу нахилу спектральної щільності потужності.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для аналізу нахилу спектральної щільності потужності.\n\n\n7.2.8.1 Для всього ряду\nДля подальших розрахунків спочатку виконаємо перетворення ряду. Будемо використовувати вихідний часовий ряд для подальших розрахунків:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_psd = transformation(signal, ret_type) \n\nІ тепер виведемо графік залежності спектральної щільності потужності від частоти в логарифмічному масштабі:\n\npsdslope, info = nk.fractal_psdslope(for_psd,\n                                     method=\"voss1988\",\n                                     show=True)\n\n\n\n\nРисунок 7.18: Залежність спектральної щільності потужності від частоти в логарифмічному масштабі\n\n\n\n\nЯк можна бачити з представленого графіку, спектральний нахил спектральної щільності потужності на різних частотах має лінійну залежність, а кут нахилу прямої, побудованої по спектру, близький до -2, що вказує на те, що динаміка індексу золота близька до дробового броунівського руху.\nТепер розглянемо варіацію кута нахилу спектра в рамках алгоритму ковзного вікна.\n\n\n7.2.8.2 Віконна процедура\nВстановимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nmethod_psd = \"voss1988\"        # метод для розрахунку\n                               # спектральної щільності\n\nlength = len(time_ser.values)  # довжина ряду\n\npsd_wind = []                  # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    psd, _ = nk.fractal_psdslope(fragm, method=method_psd)\n\n    # зберігаємо результат до масиву значень\n    psd_wind.append(psd)\n\n100%|██████████| 5531/5531 [00:16&lt;00:00, 337.75it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_psd_name={symbol}_method{method_psd}_\\\n           wind={window}_step={tstep}.txt\", psd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_psd = fr'$PSDFD$'\n\nfile_name_psd = f\"fd_psd_name={symbol}_method{method_psd}_\\\n                wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          psd_wind, \n          ylabel, \n          label_psd,\n          xlabel,\n          file_name_psd)\n\n\n\n\nРисунок 7.19: Динаміка індексу золота та фрактальної розмірності через нахил спектральної щільності потужності\n\n\n\n\nРисунок вище показує, що даний показник також реагує спадом у кризові та передкризові події, вказуючи на зростання автокореляції часового ряду під час даних подій. Також зрозуміло, що має місце варіації нахилу спектра щільності потужності. В одні моменти часу динаміка сигналу може бути подібна до броунівського руху, а в інші до білого шуму.\n\n\n\n7.2.9 Обчислення кореляційної розмірності\nКореляційна розмірність (також позначається як \\(D_2\\)) є нижньою границею оцінки фрактальної розмірності досліджуваного фазового простору.\nСпочатку здійснюється реконструкція фазового простору сигналі згідно методу часової затримки, і далі обчислюються відстані між усіма точками траєкторії. Потім обчислюється “кореляційна сума”, яка є часткою пар точок, відстань між якими менша за заданий радіус. Остаточна кореляційна розмірність апроксимується графіком залежності кореляційної суми від радіусу багатовимірного околу досліджуваних траєкторій, що будується в логарифмічному масштабі.\nЦю розмірність можна викликати через fractal_correlation(). Її синтаксис виглядає наступним чином:\nfractal_correlation(signal, delay=1, dimension=2, radius=64, show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка (\\(\\tau\\)).\ndimension (int) — розмірність вкладень (\\(m\\)).\nradius (Union[str, int, list]) — послідовність радіусів для перевірки. Якщо передано ціле число, буде отримано експоненціальну послідовність довжиною заданим скалярним значенням radius у межах від 2.5% до 50% від діапазону відстані. Методи, реалізовані в інших пакетах, можна використовувати, вказуючи \"nolds\", \"Corr_Dim\" або \"boon2008\".\nshow (bool) — графік кореляційної розмірності, якщо True. За замовчуванням — False.\nkwargs — інші аргументи для передачі (наразі не використовуються).\n\nПовертає\n\ncd (float) — кореляційна розмірність часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення кореляційної розмірності.\n\n\n7.2.9.1 Для всього часового ряду\nРозглянемо залежність кореляційної суми від радіусу для всього часового ряду. Перш за все виконаємо перетворення ряду:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_corr = transformation(signal, ret_type) \n\nТепер розрахуємо кореляційну розмірність, побудувавши залежність кореляційної суми від радіусу в логарифмічному масштабі:\n\ncd, info = nk.fractal_correlation(for_corr,\n                                  delay=1, \n                                  dimension=1,\n                                  radius=\"nolds\", \n                                  show=True)\n\n\n\n\nРисунок 7.20: Залежність кореляційної суми від радіусу багатовимірного околу досліджуваних траєкторій. Графік побудований у логарифмічному масштабі\n\n\n\n\nЯк ми можемо бачити, кореляційна сума дійсно має лінійну залежність для різних значень радіусу околу певної траєкторії, що вказує на фрактальність системи. Тепер подивимось як варіюється значення кореляційної розмірності в періоди турбулентності.\n\n\n7.2.9.2 Віконна процедура\nДля цього показника визначимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nd_wind = 2          # розмірність вкладень\ntau_wind = 1        # часова затримка\nrad_wind = \"nolds\"  # метод для визначення масиву радіусів\n\nlength = len(time_ser.values)      # довжина ряду\n\ncorr_wind = []                     # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    cd_wind, _ = nk.fractal_correlation(fragm,\n                                        delay=tau_wind, \n                                        dimension=d_wind,\n                                        radius=rad_wind)\n\n    # зберігаємо результат до масиву значень\n    corr_wind.append(cd_wind)\n\n100%|██████████| 5531/5531 [00:19&lt;00:00, 290.43it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_correlation_name={symbol}_wind={window}_\\\n                step={tstep}_dim={d_wind}_tau={tau_wind}_\\\n                radius={rad_wind}.txt\", corr_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_cd = fr'$CD$'\n\nfile_name_cd = f\"fd_correlation_name={symbol}_wind={window}_\\\n                step={tstep}_dim={d_wind}_tau={tau_wind}_\\\n                radius={rad_wind}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          corr_wind, \n          ylabel, \n          label_cd,\n          xlabel,\n          file_name_cd)\n\n\n\n\nРисунок 7.21: Динаміка індексу золота та кореляційної фрактальної розмірності\n\n\n\n\nРисунок 7.21 демонструє, що кореляційна розмірність для індексу золота також спадає у кризові та передкризові періоди, вказуючи на зростання корельованості теперішніх цін на золото з попередніми. Можна сказати і по іншому: у період криз трейдери починають самоорганізовуватись та колективно скупати або продавати відповідний актив; іншими словами, їх динаміка стає більш синхронною. Оскільки кореляційна розмірність вимірюється для траєкторій фазового простору, спад цього показника свідчить про зростання щільності досліджуваних траєкторій. Тобто, фазовий простір стає більш розрідженим, а всі його траєкторії концентрованими лише в одній конкретній області, що є індикатором згуртованості прихованих змінних досліджуваної системи."
  },
  {
    "objectID": "lab_7.html#теоретичні-відомості",
    "href": "lab_7.html#теоретичні-відомості",
    "title": "8  Лабораторна робота № 7",
    "section": "8.1 Теоретичні відомості",
    "text": "8.1 Теоретичні відомості\n\n8.1.1 Означення мультифракталів\nУ цій лабораторній ми викладемо основи теорії мультифракталів — неоднорідних фрактальних об’єктів, для повного опису яких, на відміну від регулярних фракталів, недостатньо введення лише однієї величини, його фрактальної розмірності \\(D\\), а потрібен цілий спектр таких розмірностей, кількість яких, взагалі кажучи, нескінченна. Причина цього полягає в тому, що поряд із суто геометричними характеристиками, які визначаються величиною \\(D\\), такі фрактали характеризуються й деякими специфічними статистичними властивостями.\n\n\n\nРисунок 8.1: Трикутник Серпинського, області якого згенеровані з нерівномірними ймовірностями\n\n\nПростіше всього пояснити, що розуміється під “неоднорідним фракталом” на прикладі трикутника Серпинського, отриманого за допомогою методу випадкових ітерацій.\nПрипустімо, однак, що в методі випадкових ітерацій ми тепер із якоїсь причини віддали перевагу одній із вершин трикутника, наприклад, вершині А, і стали вибирати її з імовірністю 90%. Дві ж інші вершини В і С для нас рівноцінні, але на їхню частку тепер припадає всього лише по 5%. Результат такої несиметричної гри” зображено нижче на рисунку вище.\nВидно, що точки всередині трикутника АВС розподілені тепер вкрай нерівномірно. Більша їх частина перебуває біля вершини А та її прообразів. Водночас у вершин В і С (і їхніх прообразів) їх є вкрай мало. Проте, за звичайною термінологією, ця множина точок (за умови прагнення числа ітерацій до нескінченності) є фракталом, тому що збереглася основна властивість фракта — самоподібність. Дійсно, трикутник DFC, хоча в ньому у 20 разів менше точок, за своїми статистичними і геометричними властивостями повністю подібний до великого трикутника АВС. Так само, як і у великому трикутнику, точки в ньому концентруються здебільшого поблизу вершини D — аналогу вершини А.\n\n\n\nРисунок 8.2: Розподіл точок по трикутнику Серпинського, представленого на попередньому рисунку\n\n\nРисунок 8.2 більш детально демонструє результуючий розподіл точок по трикутнику Серпинського. Цифри в кожному з маленьких трикутників показують його відносну заселеність точками множини.\nОднак, не дивлячись на нерівномірність розподілу точок фрактала, його фрактальна розмірність залишилась при цьому такою ж, \\(D=\\frac{\\ln{3}}{\\ln{2}}\\). Покриття цієї множини все меншими трикутниками можна здійснити по тому ж алгоритму, що й раніше. Таке співпадіння змушує замислитись над пошуком нових кількісних характеристик, котрі могли б відрізнити нерівномірний розподіл точок від рівномірного.\nІнший, складніший приклад неоднорідного фрактала, який ми б хотіли ще навести, показано на наступному рисунку. Ліворуч продемонстровано великий квадрат зі стороною, що дорівнює одиниці, який на цьому (нульовому) етапі повністю покриває собою деяку фрактальну множину точок \\(M\\). На наступному (першому) етапі, у центрі малюнка, показано, як ту саму множину можна покрити трьома меншими квадратами зі сторонами \\(l_1=1/2, \\ l_2=l_3=5/16\\), у яких, відповідно, міститься частка \\(p_1=1/2, \\ p_2=1/3\\) та \\(p_3=1/6\\) усіх точок.\n\n\n\nРисунок 8.3: Приклад мультифрактала, що підкоряється ренормалізаційній схемі\n\n\nНаступний етап покриття (зображений на рисунку праворуч) містить уже 9 квадратів зі сторонами \\(l_{1}^{2}=1/4, \\ l_{1}l_{2}=l_{1}l_{3}=5/32\\) (у нижньому правому куті) і \\(l_{2}l_{1}=5/32, \\ l_{2}^{2}=l_{2}l_{3}=25/256\\) (угорі праворуч і ліворуч). Відносна заселеність цих квадратів точками множини показана на рисунку. Вона відповідає добутку чинників заселеності (імовірностей): \\(p_{1}^{2}=1/4, \\ p_{1}p_{2}=1/6, \\ p_{1}p_{3}=1/12\\) — для нижньої правої групи, \\(p_{2}p_{1}=1/6, \\ p_{2}^{2}=1/9, \\ p_{2}p_{3} = 1/18\\) — для верхньої лівої та \\(p_{3}p_{1}=1/12, \\ p_{3}p_{2}=1/18, \\ p_{3}^{2}=1/36\\) — для верхньої правої групи. Зазначимо, що є чітка відповідність між заселеністю квадрата \\(p_{j}p_{i}\\) і його розмірами \\(l_{i}l_{i}\\).\nПодальший процес розбиття і покриття множини \\(M\\) здійснюється згідно із цією ренормалізаційною схемою. Кожен квадрат, що має на \\(n\\)-му кроці розмір \\(l\\) і заселеність \\(р\\), замінюється на \\(n+1\\) кроці трьома квадратами з розмірами \\(ll_{1}, \\ ll_{2}, \\ ll_{3}\\) і заселеностями \\(pp_{1}, \\ pp_{2}, \\ pp_{3}\\) відповідно, розміщеними таким самим чином відносно один одного, як показано на попередньому посередні рисунку.\nДвоє розглянутих вище випадки являють собою приклади неоднорідних фракталів. Під словом “неоднорідний” ми тут розуміємо нерівномірний розподіл точок множини по фракталу або нерівномірний розподіл малих та великих флуктуацій у часовому ряді. Причина неоднорідності в попередніх випадках одна й та сама — різні ймовірності заповнення геометрично однакових елементів фрактала, або в загальному випадку невідповідність імовірностей заповнення геометричним розмірам відповідних областей. Такі неоднорідні фрактальні об’єкти в літературі називають мультифракталами, і їх вивченням ми й займемося надалі.\n\n\n8.1.2 Узагальнені фрактальні розмірності \\(D_{q}\\)\nДамо загальне визначення мультифрактала. Розглянемо фрактальний об’єкт, що займає якусь обмежену ділянку \\(\\Omega\\) розміру \\(L\\) у Евклідовому просторі з розмірністю \\(d\\). Нехай на якомусь етапі його побудови він являє собою множину з \\(N \\gg 1\\) точок, якось розподілених у цій області. Ми будемо припускати, що врешті-решт \\(N \\to \\infty\\). Прикладом такої множини може слугувати трикутник Серпінського, побудований методом випадкових ітерацій. Кожен крок ітераційної процедури додає до цієї множини одну нову точку.\nРозіб’ємо всю область \\(\\Omega\\) на кубічні клітинки зі стороною \\(\\varepsilon \\ll L\\) та об’ємом \\(\\varepsilon^{d}\\). Далі нас будуть цікавити тільки зайняті клітинки, у яких міститься хоча б одна точка. Нехай номер зайнятих комірок \\(i\\) змінюється в межах \\(і=1, 2,..., N(\\varepsilon)\\), де \\(N(\\varepsilon)\\) — сумарна кількість зайнятих клітинок, яка, звісно, залежить від розміру клітинки \\(\\varepsilon\\).\nНехай \\(n_{i}(\\varepsilon)\\) представляє собою кількість точок у клітинці з номером \\(i\\), тоді величина\n\\[\np_{i}(\\varepsilon) = \\lim_{N\\to\\infty}\\frac{n_{i}(\\varepsilon)}{N}\n\\]\nпредставляє собою ймовірність того, що навмання взята точка з нашої множини знаходиться в комірці \\(i\\). Інакше кажучи, ймовірності \\(р_{i}\\) характеризують відносну заселеність комірок. З умови нормування ймовірності випливає, що\n\\[\n\\sum_{i=1}^{N(\\varepsilon)}p_{i}(\\varepsilon)=1.\n\\]\nУведемо тепер у розгляд узагальнену статистичну суму \\(Z(q,\\varepsilon)\\), що характеризується показником ступеня \\(q\\), який може набувати будь-яких значень в інтервалі \\(-\\infty&lt;q&lt;+\\infty\\)\n\\[\nZ(q,\\varepsilon)=\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon).\n\\]\nСпектр узагальнених фрактальних розмірностей \\(D_{q}\\), що характеризує даний розподіл точок в області \\(\\Omega\\), визначається за допомогою співвідношення\n\\[\nD_{q} = \\frac{\\tau(q)}{q-1},\n\\]\nде функція \\(\\tau(q)\\) має вид\n\\[\n\\tau(q)=\\lim_{\\varepsilon\\to 0}\\frac{\\ln{Z(q,\\varepsilon)}}{\\ln{\\varepsilon}}.\n\\]\nЯк ми покажемо нижче, якщо \\(D_{q}=D=\\text{const}\\), тобто не залежить від \\(q\\), то дана множина точок являє собою звичайний, регулярний фрактал, який характеризується лише однією величиною — фрактальною розмірністю \\(D\\). Навпаки, якщо функція \\(D_{q}\\) якось змінюється з \\(q\\), то розглянута множина точок представляє мультифрактал.\nТаким чином, мультифрактал у загальному випадку характеризується деякою нелінійною функцією \\(\\tau(q)\\), що визначає поведінку статистичної суми \\(Z(q,\\varepsilon)\\) при \\(\\varepsilon\\to 0\\)\n\\[\\tag{1}\nZ(q,\\varepsilon)=\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon) \\approx \\varepsilon^{\\tau(q)}.\n\\]\nСлід мати на увазі, що в реальній ситуації ми завжди маємо скінченне, хоча й дуже велике число дискретних точок \\(N\\), тому при комп’ютерному моделювані конкретної множини граничний перехід \\(\\varepsilon\\to 0\\) треба виконувати з обережністю, пам’ятаючи, що йому завжди передує ліміт \\(N \\to 0\\).\nПокажемо тепер, як поводиться узагальнена статистична сума у випадку звичайного регулярного фрактала з фрактальною розмірністю \\(D\\). У цьому випадку в усіх зайнятих комірках міститься однакова кількість точок\n\\[\nn_{i}(\\varepsilon)=\\frac{N}{N(\\varepsilon)},\n\\]\nтобто фрактал представляється однорідним. Тоді очевидно, що відносні населеності клітинок, \\(p_{i}(\\varepsilon)=1/N(\\varepsilon)\\), також однакові, і узагальнена статистична сума набуває вигляду\n\\[\\tag{2}\nZ(q,\\varepsilon) = N^{1-q}(\\varepsilon).\n\\]\nВрахуємо тепер, що, згідно визначеню фрактальної розмірності \\(D\\), кількість зайнятих клітинок при достатньо малому \\(\\varepsilon\\) поводить себе наступним чином:\n\\[\\tag{3}\nN(\\varepsilon) \\approx \\varepsilon^{-D}.\n\\]\nПідставляючи (3) у формулу (2), і порівнюючи з (1), отримуємо\n\\[\\tag{4}\n\\varepsilon^{\\tau(q)} = \\varepsilon^{-D(1-q)} \\to \\tau(q)=(q-1)D.\n\\]\nМи приходимо до висновку, що у випадку звичайного фрактала функція (4) є лінійною. Тоді всі \\(D_{q}\\) дійсно не залежать від \\(q\\). Фрактал у якого всі узагальнені фрактальні розмірності \\(D_{q}\\) співпадають називається монофракталом.\nЯкщо розподіл точок по клітинкам неоднаковий, тоді фрактал називається неоднорідним, тобто представляє із себе мультифрактал, і для його характеристики необхідний цілий спектр узагальнених фрактальних розмірностей \\(D_{q}\\), кількість котрих, у загальному випадку, нескінченна.\nТак, наприклад, при \\(q \\to +\\infty\\) основний внесок в узагальнену статистичну суму (1) вносять комірки, що містять найбільшу кількість частинок \\(n_{i}\\) у них і, відповідно, що характеризуються найбільшою ймовірністю їх заповнення \\(p_{i}\\). Навпаки, при \\(q \\to -\\infty\\) основний внесок в узагальнену статистичну суму вносять найбільш розрідженні комірки з найменшою ймовірністю їх заповнення \\(p_{i}\\). Таким чином, функція \\(D_{q}\\) показує, наскільки неоднорідним представляється досліджувана множина точок \\(\\Omega\\).\nУ подальшому для характеристики розподілу точок необхідно знати не тільки функцію \\(\\tau(q)\\), але і її похідну:\n\\[\n\\frac{d\\tau(q)}{dq} = \\lim_{\\varepsilon\\to 0}\\frac{\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}\\ln{p_{i}}}{\\left( \\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q} \\right)\\ln{\\varepsilon}}.\n\\]\nЦя похідна має важливий фізичний зміст, який буде продемонстровано пізніше. Зараз знову зазначимо, що для мультифрактальної системи вона не залишається константною і змінюється з \\(q\\).\n\n\n8.1.3 Функція мультифрактального спектра \\(f(\\alpha)\\)\n\n8.1.3.1 Спектр фрактальних розмірностей\nУ попередньому пункті ми ввели поняття мультифрактала — об’єкта, що представляє собою неоднорідний фрактал. Для його опису ми ввели множину узагальнених фрактальних розмірностей \\(D_{q}\\), де \\(q\\) приймає будь-які значення в інтервалі \\(-\\infty&lt;q&lt;+\\infty\\). Однак величини \\(D_{q}\\) не є, строго кажучи, фрактальними розмірностями в загальному розумінні цього слова.\nТому часто поряд із ними для характеристики мультифрактальної множини використовують так звану функцію мультифрактального спектра \\(f(\\alpha)\\) (спектр сингулярностей мультифрактала), до якої, як ми побачимо надалі, більше підходить термін фрактальна розмірність. Ми покажемо, що величина \\(f(\\alpha)\\) фактично дорівнює хаусдорфовій розмірності якоїсь однорідної фрактальної підмножини із вихідної множини \\(\\Omega\\), що дає домінантний внесок у статистичну суму при заданій величині \\(q\\).\nОднією з основних характеристик мультифрактала є набір імовірностей \\(р_{i}\\), що показують відносну заселеність клітинок \\(\\varepsilon\\), якими ми покриваємо цю множину. Чим менший розмір клітинки, тим менша величина її заселеності. Для самоподібних множин залежність \\(p_{i}\\) від розміру клітинки \\(\\varepsilon\\) має степеневий характер:\n\\[\np_{i}(\\varepsilon) \\approx \\varepsilon^{\\alpha_{i}},\n\\]\nде \\(\\alpha_{i}\\) являє собою деякий показник ступеня (різний для різнок клітинок \\(i\\)). Відомо, що для регулярного (однорідного) фрактала всі показники ступеня \\(\\alpha_{i}\\) однакові й рівні фрактальній розмірності \\(D\\):\n\\[\np_{i} = \\frac{1}{N(\\varepsilon)} \\approx \\varepsilon^{D}.\n\\]\nУ даному випадку статистична сума (1) приймає наступний вигляд:\n\\[\nZ(q,\\varepsilon) = \\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon) = N(\\varepsilon)\\varepsilon^{Dq}=\\varepsilon^{-D}\\varepsilon^{Dq} \\approx \\varepsilon^{D(q-1)}.\n\\]\nТому \\(\\tau(q)=D(q-1)\\) і всі узагальнені фрактальні розмірності \\(D_{q}=D\\) у цьому випадку співпадають та не залежать від \\(q\\).\nОднак, для такого складного об’єкта, як мультифрактал, унаслідок його неоднорідності, ймовірності заповнення клітинок \\(p_{i}\\) у загальному випадку різняться, і показник ступеня \\(\\alpha_{i}\\) для різних клітинок може приймати різні значення. Достатньо типовою є ситуація, коли ці значення неперервно заповнюють деякий закритий інтервал \\(\\left( \\alpha_{min}, \\alpha_{max} \\right)\\), причому\n\\[\np_{min} \\approx \\varepsilon^{\\alpha_{max}}, \\; \\text{a} \\; p_{max} \\approx \\varepsilon^{\\alpha_{min}}.\n\\]\nТепер перейдемо до питання о розподілі ймовірностей різних значень \\(\\alpha_{i}\\). Нехай \\(n(\\alpha)d\\alpha\\) є ймовірністю того, що \\(\\alpha_{i}\\) знаходиться в інтервалі від \\(\\alpha\\) до \\(\\alpha+d\\alpha\\). Іншими словами, \\(n(\\alpha)d\\alpha\\) представляє собою відносну кількість клітинок \\(i\\), що характеризуються однією і тією самою мірою \\(p_{i}\\) з \\(\\alpha_{i}\\), що лежать у цьому інтервалі. У випадку монофрактала, для котрого всі \\(\\alpha_{i}\\) однакові (і рівні фрактальній розмірності \\(D\\)), це число, очевидно, пропорційно повній кількості клітинок \\(N(\\varepsilon) \\approx \\varepsilon^{-D}\\), степеневим чином залежних від розміру клітинки \\(\\varepsilon\\). Показник ступеня в цьому співвідношені визначається фрактальною розмірністю множини \\(D\\).\nДля мультифрактала, однак, це не так, і різні значення \\(\\alpha_{i}\\) зустрічаються з ймовірністю, що характеризується не однією і тією ж величиною \\(D\\), а різними (в залежності від \\(\\alpha\\)) значеннями показниками ступеня \\(f(\\alpha)\\),\n\\[\\tag{5}\nn(\\alpha) \\approx \\varepsilon^{-f(\\alpha)}.\n\\]\nТаким чином, фізичний сенс функції \\(f(\\alpha)\\) полягає в тому, що вона представляє собою розмірність хаусдорфа деякої однорідної підмножини \\(\\Omega_{\\alpha}\\) із вихідної множини \\(\\Omega\\), що характеризується однаковими ймовірностями заповнення клітинок \\(p_{i} \\approx \\varepsilon^{\\alpha}\\). Оскільки фрактальна розмірність підмножини очевидно завжди менша або рівна фрактальній розмірності вихідної множини \\(D_{0}\\), має місце важлива нерівність для функції \\(f(\\alpha)\\):\n\\[\nf(\\alpha) \\leq D_{0}.\n\\]\nУ результаті можна зробити висновок, що множина різних значень функції \\(f(\\alpha)\\) (при різних \\(\\alpha\\)) представляє собою спектр фрактальних розмірностей однорідних підмножин \\(\\Omega_{\\alpha}\\), на які можна розбити вихідну множину \\(\\Omega\\), кожна з яких характеризується власним значенням фрактальної розмірності \\(f(\\alpha)\\).\nОскільки будь-якій підмножині належить лише частина загальної кількості клітинок \\(N(\\varepsilon)\\), на котрі ми розділили вихідну множину \\(\\Omega\\), умова нормування ймовірностей, очевидно, не виконується при підсумовуванні тільки по цій підмножині. Сума цих імовірностей стає менше одиниці. Тому й самі ймовірності \\(p_i\\) з одним і тим самим значенням \\(\\alpha_i\\) очевидно менше (або в крайньому випадку одного порядку), ніж величина \\(\\varepsilon^{f(\\alpha_i)}\\), яка обернено пропорційна кількості наявних клітинок, що покривають дану підмножину (нагадаємо, що у випадку монофрактала \\(p_i \\approx 1/N(\\varepsilon)\\)). У результаті ми приходимо до наступної важливої нерівності для функції \\(f(\\alpha)\\). А саме, при всіх значеннях \\(\\alpha\\)\n\\[\nf(\\alpha) \\leq \\alpha.\n\\]\nЗнак рівності має місце, наприклад, для повністю однорідного фрактала, де \\(f(\\alpha)=\\alpha=D\\).\n\n\n8.1.3.2 Перетворення Лежандра\nВстановимо зв’язок функції \\(f(\\alpha)\\) із введенною раніше функцією \\(\\tau(q)\\). Обчислимо для цього статистичну суму \\(Z(q,\\varepsilon)\\). Підставляє у статистичну суму ймовірності \\(p_i \\approx \\varepsilon^{\\alpha_i}\\), та переходячи від підсумовування по \\(i\\) до інтегрування по \\(\\alpha\\) з плотністю ймовірностей (5), ми отримаємо\n\\[\\tag{6}\nZ(q,\\varepsilon) = \\sum_{i=1}^{N(\\varepsilon)} p_{i}^{q}(\\varepsilon) \\approx \\int d\\alpha n(\\alpha)\\varepsilon^{q\\alpha} \\approx \\int d\\alpha\\varepsilon^{q\\alpha-f(\\alpha)}.\n\\]\nТак як величина \\(\\varepsilon\\) дуже мала, основний внесок у цей інтеграл вноситимуть ті значення \\(\\alpha(q)\\), при яких показник ступеня \\(q\\alpha-f(\\alpha)\\) виявляється мінімальним (а підінтегральна функція — максимальною). Цей вклад буде пропорційним значенню підінтегральної функції у точці максимума. Саме ж значення \\(\\alpha(q)\\) визначається при цьому з наступної умови:\n\\[\n\\left. \\frac{d}{d\\alpha}[ q\\alpha-f(\\alpha) ] \\right \\vert_{\\alpha=\\alpha(q)} = 0.\n\\]\nТакож, з умови мінімуму ми маємо\n\\[\n\\left. \\frac{d^{2}}{d\\alpha^{2}}[ q\\alpha-f(\\alpha) ] \\right \\vert_{\\alpha=\\alpha(q)} &gt; 0.\n\\]\nУ результаті отримуємо, що залежність \\(\\alpha(q)\\) неявним чином визначається з рівняння\n\\[\\tag{7}\nq = \\frac{df(\\alpha)}{d\\alpha}\n\\]\nі що функція \\(f(\\alpha)\\) усюди є випуклою\n\\[\nf^{''}(\\alpha)&gt;0.\n\\]\nЦе значить, що величина \\(f(\\alpha(q))\\) дійсно визначає фрактальну розмірність підмножини \\(\\Omega_{\\alpha(q)}\\), що має найбільший домінуючий внесок у статистичну суму (6) при заданій величині показника ступеня \\(q\\).\nОскільки \\(Z(q,\\varepsilon)=\\tau(q)\\), приходимо до висновку, що\n\\[\n\\tau(q) = q\\alpha(q) - f(\\alpha(q)).\n\\]\nПам’ятаючи, що \\(\\tau(q)=D(q-1)\\), можемо віднайти функцію \\(D_{q}\\):\n\\[\\tag{8}\nD_{q} = \\frac{1}{q-1}[ q\\alpha(q)-f(\\alpha(q)) ].\n\\]\nТаким чином, якщо ми знаємо функцію мультифрактального спектра \\(f(\\alpha)\\), то з домомогою співвідношень (7) та (8) ми можемо знайти функцію \\(D_{q}\\). Навпаки, знаючи \\(D_{q}\\), ми можемо знайти залежність \\(\\alpha(q)\\) з допомогою рівняння\n\\[\\tag{9}\n\\alpha(q) = \\frac{d}{dq}[(q-1)D_{q}]\n\\]\nі після цього знайти із (8) залежність \\(f(\\alpha(q))\\). Ці два рівняння і визначають функцію \\(f(\\alpha)\\).\n\\[\n\\frac{d\\tau}{dq}\\frac{dq}{d\\alpha} = q + \\alpha\\frac{dq}{d\\alpha} - \\frac{df}{d\\alpha}.\n\\]\nПриймаючи до уваги те, що \\(q=df/d\\alpha\\), і скорочуючи це рівняння на \\(dq/d\\alpha\\), приходимо до співвідношення\n\\[\n\\alpha = \\frac{d\\tau(q)}{dq},\n\\]\nщо еквівалентне виразу (9).\nВирази для \\(\\tau(q)\\) та \\(\\alpha(q)\\) задають перетворення Лежандра від змінних \\(\\{ q, \\tau(q) \\}\\) до змінних \\(\\{\\alpha, f(\\alpha)\\}\\):\n\\[\\tag{10}\n\\alpha = \\frac{d\\tau}{dq}\n\\]\nта\n\\[\\tag{11}\nf(\\alpha) = q\\frac{d\\tau}{dq} - \\tau.\n\\]\nЯк відомо, для однорідного фрактала \\(D_{q}=D=\\text{const}\\). Тому \\(\\alpha=d\\tau/dq=D\\) і \\(f(\\alpha)=q\\alpha-\\tau(q)=qD-D(q-1)=D\\). У цьому випадку “графік” функції \\(f(\\alpha)\\) на площині \\(\\left( \\alpha, f(\\alpha) \\right)\\) складається лише з однієї точки \\(\\left( D, D \\right)\\).\n\n\n\n8.1.4 Мультифрактальний аналіз детрендованих флуктуацій\nМонофрактальні та мультифрактальні структури фінансових сигналів є особливим різновидом масштабно-інваріантних структур. Найчастіше монофрактальна структура фінансових часових рядів визначається одним степеневим показником і передбачає, що масштабо-інваріантність не залежить від часу і простору. Однак, часто ми маємо змогу спостерігати просторово-часову варіацію масштабно-інваріантної структури досліджуваної складної системи. Ці просторово-часові варіації вказують на мультифрактальність фінансового сигналу, яка визначається мультифрактальним спектром. Ширина і форма мультифрактального спектра можуть також допомогти диференціювати варіативність серцевого ритму у пацієнтів із серцевими захворюваннями, такими як шлуночкова тахікардія, фібриляція шлуночків і застійна серцева недостатність. Мультифрактальний спектр може допомогти кількісно визначити асиметрію підйомів та спадів на фондовому чи криптовалютному ринках, передбачити фінансову кризу, що поступово наближується, і, таким чином, сприяти успішності подальших торгівельних рішень. Основна мета цього розділу — представити одну з найточніших процедур для визначення множини фрактальних показників — мультифрактальний аналіз детрендованих флуктуацій (multifractal detrended fluctuation analysis, MFDFA).\nПобудова MFDFA складається з восьми кроків: Розділ “Шум і випадкові блукання у часовому ряді” представляє метод приведення часового ряду до такого, що подібний до випадкового блукання, що є попереднім кроком для MFDFA. Розділ “Обчислення середньоквадратичної варіації часового ряду” представляє середньоквадратичну варіацію, яка є основною процедурою для подальших обчислень в MFDFA і типовим способом обчислення середньої варіації часових рядів різної природи. Розділ “Локальна середньоквадратична варіація часового ряду” представляє обчислення локальної варіації часового ряду як середньоквадратичного відхилення часового ряду в межах сегментів, що можуть як перекриватися, так і не перекриватися. У розділі “Локальне детрендування часового ряду” таке ж локальне середньоквадратичне відхилення обчислюється навколо трендів, які часто зустрічаються у фінансових часових рядах. У розділі “Монофрактальний аналіз детрендованих флуктуацій” амплітуди локальних середніх квадратичних відхилень підсумовуються в узагальнене середнє квадратичне відхилення. У сумарному середньоквадратичному відхиленні для сегментів з малими розмірами вибірки переважають швидкі флуктуації часового ряду. На противагу цьому, у сумарному середньоквадратичному відхиленні для сегментів з великими розмірами вибірки переважають повільні коливання. Степенева залежність між загальним середнім квадратичним відхиленням для декількох розмірів вибірки (тобто масштабів) визначається за допомогою монофрактального аналізу дентрендованих флуктуацій (monofractal detrended fluctuation analysis, DFA) і називається показником Херста (Hurst exponent, \\(H\\)). У розділі “Мультифрактальний аналіз детрендованих флуктуацій” MFDFA отримують шляхом розширення на \\(q\\)-й порядок узагальненого середньоквадратичного відхилення. Середньоквадратичне відхилення \\(q\\)-го порядку може розрізняти сегменти з малими та великими флуктуаціями. Степенева залежність між середньоквадратичним відхиленням \\(q\\)-го порядку чисельно визначається як показник Херста \\(q\\)-го порядку. У розділі “Мультифрактальний спектр часових рядів” на основі показника Херста \\(q\\)-го порядку обчислено декілька мультифрактальних спектрів."
  },
  {
    "objectID": "lab_11.html#теоретичні-відомості",
    "href": "lab_11.html#теоретичні-відомості",
    "title": "9  Лабораторна робота № 11",
    "section": "9.1 Теоретичні відомості",
    "text": "9.1 Теоретичні відомості\nНезворотність часу є фундаментальною властивістю нерівноважних дисипативних систем, і її втрата може свідчити про розвиток деструктивних процесів.\nЗ огляду на статистичні властивості досліджуваного сигналу, його еволюцію можна було б назвати незворотною, якби була відсутня інваріантність, тобто був би отриманий той же сигнал, якби ми виміряли його в протилежному напрямку. Функція \\(f\\) може бути застосована для знаходження характеристик, які відрізняються прямою і зворотною версіями, тобто часові ряди незворотні, якщо \\(f(X^d) \\neq f(X^r)\\). Основна ідея цього визначення полягає в тому, що немає ніяких обмежень на \\(f(\\cdot )\\).\nПередбачається, що стаціонарний процес \\(X\\) називається статистично зворотним у часі, якщо розподіл ймовірностей прямої та зворотної систем приблизно однаковий. Незворотність часових рядів вказує на наявність нелінійних залежностей (пам’яті) в динаміці системи, далекій від рівноваги, включаючи негауссові випадкові процеси та дисипативний хаос.\n\n9.1.1 Незворотність на основі діаграм Пуанкаре\nДіаграма Пуанкаре для часового ряду являє собою графік, на осі \\(x\\) якого розташовані значення для поточного часу \\(t\\), а на осі \\(y\\) — його наступні значення в часі \\(t+\\tau\\). Усі наступні значення, які рівні один одному (\\(x(t) = x(t+\\tau)\\)), розташовані на лінії ідентичності (line of identity, LI). Інтервали, що представляють зростаючу тендецію, відмічені вище LI (\\(x(t)&lt;x(t+\\tau)\\)), тоді як спадна тенденція характеризуватиметься скупченням точок нижче LI (\\(x(t)&gt;x(t+\\tau)\\)). Оцінюючи асиметрію точок на діаграмі, ми можемо вивести різні кількісні показники незворотності (асиметрії) досліджуваних систем.\nІндекс Гузіка (GIx)\nGIx можна визначити як відношення відстаней точок вище LI до відстаней усіх точок на діаграмі:\n\\[\nGIx = \\frac{\\sum_{i=1}^{a} \\left( D_{i}^{+} \\right)^{2}}{\\sum_{i=1}^{m} \\left( D_{i} \\right)^{2} },\n\\]\nде \\(a = C(P_{i}^{+})\\) позначає кількість точок над LI; \\(m = C(P_{i}^{+}) + C(P_{i}^{-})\\) позначає кількість точок на графіку Пуанкаре; \\(D_{i}^{+}\\) це відстань від точки над LI до самої LI. Відстань точки до LI можна визначити як\n\\[\nD_{i} = \\frac{|x(i+\\tau) - x(i)|}{\\sqrt{2}}.\n\\]\nІндекс Порти\nІндекс Порти (PIx) визначається як кількість точок нижче LI, поділена на загальну кількість точок на графіку Пуанкаре, за винятком тих, що знаходяться на LI:\n\\[\nPIx = \\frac{b}{m},\n\\]\nде \\(b = C(P_{i}^{-})\\) кількість точок нижче LI.\nІндекс Кошти\nІндекс Кошти бере до уваги кількість інкриментів (\\(x(i+1)-x(i) &gt; 0\\)) та декриментів (\\(x(i+1)-x(i) &lt; 0\\)). Вони представляються симетричними, якщо рівні один одному. Даний індекс розраховується для двовимірної мультимасштабної площини (\\(x(i), x(i+L)\\)), де новий крос-гранульований ряд \\(y_{\\tau}(i) = x(i+L)-x(i)\\) для \\(1 \\leq i \\leq N-\\tau\\) відображає асиметрію інкриментів та декриментів ряду, і індекс незворотності для діапазону масштабів \\(\\tau\\) визначається наступним виразом:\n\\[\nCIx_{\\tau} = \\frac{\\sum_{y_{\\tau}&lt;0} H[y_{\\tau}] - \\sum_{y_{\\tau}&gt;0} H[y_{\\tau}]}{N-\\tau}.\n\\]\nУзагальнений індекс Кошти для діапазону мастабів \\(\\tau\\) може бути визначений як\n\\[\nCIx = \\frac{1}{L} \\sum_{\\tau=1}^{L} |CIx_{\\tau}|,\n\\]\nде \\(L\\) — це максимальний масштаб.\nІндекс Ейлера\nОпираючись на асиметрію розподілу точок нижче та вище LI, Ейлер запропонував індекс асиметрії:\n\\[\nEIx = \\frac{\\sum_{i=1}^{N-1} \\left[ x(i)-x(i+\\tau) \\right]^{3}}{\\left[ \\sum_{i=1}^{N-1} \\left[ x(i)-x(i+\\tau) \\right]^{2} \\right]^{\\frac{3}{2}}}.\n\\]\nЗначне відхилення \\(EIx\\) від 0 вказує на асиметрію системи. Якщо \\(EIx&gt;0\\), розподіл точок на діаграмі Пуанкаре значно зміщений у сторону вище LI. Зворотня ситуація спостерігається для \\(EIx&lt;0\\). Для \\(EIx \\approx 0\\) досліджувані сегменти представляються зворотніми в часі.\nІндекс площі\nІндекс площі (AIx) визначається як сукупна площа секторів, що сформовані точками над LI поділена на сукупну площу секторів, що відповідають усім точкам на графіку Пуанкаре (крім тих, що розташовані точно на LI). Площа сектора, що відповідає певній точці \\(P_{i}\\) на графіку Пуанкаре, обчислюється як\n\\[\nS_{i} = \\frac{1}{2} \\times R\\theta_{i} \\times r^{2},\n\\]\nде \\(r\\) — це радіус сектора; \\(R\\theta_{i} = \\theta_{LI} - \\theta_{i}\\); \\(\\theta_{LI}\\) — це фазовий кут, і \\(\\theta_{i} = \\arctan{\\left[ \\frac{x(i+\\tau)}{x(i)} \\right]}\\), що визначає фазовий кут \\(i\\)-ої точки. Далі, \\(AIx\\) визначається за наступною формулою:\n\\[\nAIx = \\frac{\\sum_{i=1}^{a}|S_{i}|}{\\sum_{i=1}^{m}|S_{i}|}.\n\\]\nІндекс кута нахилу\nНа додачу до представлених вище мір, було запропоновано розраховувати незворотність сигналу з відношення кутів нахилу точок над LI до нахилу всіх точок на діаграмі:\n\\[\nSIx = \\frac{\\sum_{i=1}^{a}|R\\theta_{i}|}{\\sum_{i=1}^{m}|R\\theta_{i}|}.\n\\]\n\n\n9.1.2 Методи складних мереж\nГрафи видимості (VG) базуються на простому відображенні часових рядів у мережеву область, використовуючи локальну опуклість скалярно-позначених часових рядів, де кожне спостереження є вершиною в складній мережі. Дві вершини і пов’язані ребром, якщо для всіх вершин застосовується наступна умова:\n\\[\nx_{k} &lt; x_{j} + \\left( x_{i} - x_{j} \\right) \\frac{t_{j}-t_{k}}{t_{j}-t_{i}}.\n\\]\nМатрицю суміжності (\\(A_{ij}\\)) представленого ненаправленого та незваженого VG можна представити як:\n\\[\nA_{ij}^{VG} = A_{ji}^{VG} = \\prod_{k=i+1}^{j-1} H \\left( x_{k} &lt; x_{j} + \\left( x_{i} - x_{j} \\right) \\frac{t_{j}-t_{k}}{t_{j}-t_{i}} \\right),\n\\]\nде \\(H( \\cdot )\\) — це функція Гевісайда.\nГраф горизонтальної видимості (HVG) є спрощеною версією цього алгоритму. Для досліджуваного часового ряду набори вершин VG і HVG однакові, тоді як набір ребер HVG відображає взаємну горизонтальну видимість двох спостережень \\(x_{i}\\) та \\(x_{j}\\). Тобто можна побудувати ребро \\((i,j)\\), якщо \\(x_{k} &lt; \\min(x_{i}, x_{j})\\) для всіх \\(k\\) при \\(t_{i} &lt; t_{k} &lt; t_{j}\\) так що\n\\[\nA_{ij}^{VG} = A_{ji}^{VG} = \\prod_{k=i+1}^{j-1} H \\left( x_{i} - x_{k} \\right) H \\left( x_{j} - x_{k} \\right).\n\\]\nVG і HVG фіксують по суті одні й ті ж властивості досліджуваної системи, оскільки HVG є підграфом VG з тим же набором вершин, але володіє тільки підмножиною ребер VG. Зверніть увагу, що VG інваріантний щодо суперпозиції лінійних трендів, тоді як HVG — ні.\nОскільки визначення VGs та HVGs чітко враховує часовий порядок спостережень, напрямок часу нерозривно пов’язаний з отриманою структурою мережі. Щоб врахувати цей факт, ми визначаємо набір нових статистичних мережевих показників на основі двох простих характеристик вершин:\n\nОскільки кількість ребер інцидентних вершині \\(i\\) можна визначити як \\(k_{i}^{r} = \\sum_{j} A_{ij}\\), для (H)VG ми можемо переписати дану кількісну характеристику для вершини в час \\(t_{i}\\) відносно її минулих та майбутніх вершин:\n\n\\[\nk_{i}^{r} = \\sum_{j&lt;i} A_{ij} \\quad \\mathrm{and} \\quad k_{i}^{a} \\sum_{j&gt;i} A_{ij},\n\\]\nде \\(k_{i} = k_{i}^{r} + k_{i}^{a}\\), і \\(k_{i}^{r}\\) та \\(k_{i}^{a}\\) сприймаються як вхідні (минулі) та вихідні (майбутні) вершини.\n\nЛокальний коефіцієнт кластеризації \\(C_{i} = \\left( \\begin{matrix} k_{i}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j,k} A_{ij}A_{jk}A_{ki}\\) інша властивість старшного порядку структурного сусідства вершини \\(i\\). Для дослідження незворотності, ми можемо переписати дані характеристики наступним чином:\n\n\\[\nC_{i}^{r} = \\left( \\begin{matrix} k_{i}^{r}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j&lt;i,k&lt;i} A_{ij}A_{jk}A_{ki} \\quad \\textrm{and} \\quad C_{i}^{a} = \\left( \\begin{matrix} k_{i}^{a}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j&gt;i,k&gt;i} A_{ij}A_{jk}A_{ki}.\n\\]\nЯкщо уявити нашу систему зворотною в часі, ми припускаємо, що розподілу ймовірностей прямих і зворотних за часом характеристик повинні бути однаковими. Для незворотних процесів ми очікуємо виявити статистичну нееквівалентність. Ця нееквівалентність буде визначатися через дивергенцію Кульбака-Лейблера:\n\\[\nD_{KL}(p||q) = \\sum_{i=1}^{N} p(x_{i}) \\cdot \\log{\\left[ \\frac{p(x_{i})}{q(x_{i})} \\right]},\n\\]\nде \\(p(\\cdot)\\) відповідатиме розподілу вхідних характеристикам, а \\(q(\\cdot)\\) відповідатиме зворотнім.\n\n\n9.1.3 Незворотність на основі пермутаційних шаблонів\nІдея аналізу пермутаційних шаблонів (PP — permutation patterns) спочатку була запропонована Бандтом і Помпе, щоб надати дослідникам простий та ефективний інструмент для характеристики складності динаміки реальних систем. Він уникає порогу амплітуди і замість цього має справу з порядковими шаблонами перестановок. Їх частоти дозволяють відрізнити детерміновані процеси від абсолютно випадкових. Розрахунки PP припускають, що часовий ряд розбивається на пересічні підвектори довжини \\(d_{E}\\):\n\\[\n\\vec{X}(i) = \\left\\{ x(i), x(i+\\tau), ... , x(i+[d_{E}-1]\\tau) \\right\\},\n\\]\nде часова затримка \\(\\tau\\) відповідає часу розділення між елементами.\nПісля цього кожен вектор представляється у вигляді порядкового шаблону \\(\\pi = \\{ r_0, r_1, ... , r_{d_{E}-1} \\}\\), що має задовільняти наступній умові:\n\\[\nx(i+r_0) \\leq x(i+r_1) \\leq ... \\leq x(i+r_{d_{E}-1}).\n\\]\nЦікава для нас міра незворотності часу на основі PP може бути отримана шляхом врахування їх відносної частоти як для початкового, так і для оберненого часового ряду. Відповідно, якщо обидва типи мають приблизно однакові розподіли ймовірностей своїх патернів, часові ряди представляються зворотними, а для іншого випадку робиться протилежний висновок.\nРізницю між розподілами прямих часових рядів (\\(P^{d}\\)) та зворотних (\\(P^{r}\\)) можна оцінити за допомогою дивергенції Кульбака-Лейблера."
  },
  {
    "objectID": "lab_11.html#хід-роботи",
    "href": "lab_11.html#хід-роботи",
    "title": "9  Лабораторна робота № 11",
    "section": "9.2 Хід роботи",
    "text": "9.2 Хід роботи\n\n9.2.1 Підключення необхідних бібліотек\n\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport scienceplots\nimport pandas as pd\nimport yfinance as yf\nimport networkx as nx\nimport neurokit2 as nk\n\nfrom scipy.stats import entropy\nfrom ordpy import ordinal_distribution\nfrom tqdm import tqdm\nfrom scipy.integrate import quad\nfrom scipy.stats import gaussian_kde\nfrom scipy.spatial import distance\nfrom KDEpy import FFTKDE\nfrom ts2vg import NaturalVG, HorizontalVG\n\n%matplotlib inline\n\n\n\n9.2.2 Встановлення параметрів для побудови графіків\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\n\n\n9.2.3 Визначення функції для побудови рекурентного графа\n\ndef recurrence_net(time_ser, rec_thr, dim, tau, dist_type='euclidien'):\n    time_series = nk.complexity_embedding(time_ser, dimension=dim, delay=tau)\n    rp = (distance.cdist(time_series, time_series, dist_type) &lt;= rec_thr).astype(int)\n    adj_matrix_RN = rp\n    np.fill_diagonal(adj_matrix_RN, 0)\n\n    rec_nw = nx.from_numpy_matrix(adj_matrix_RN)\n    \n    return rec_nw\n\ndef node_positions_recurrence_net(ts, xs):\n    return {i: (xs[i], ts[i]) for i in range(len(ts))}\n\n\n\n9.2.4 Визначення функції для розрахунку індексу Кошти\n\ndef Costa_1(time_ser, taus):\n    Cst = []\n    for tau in taus:\n        fragm_Costa = np.array([time_ser[tau:], time_ser[:-tau]])\n        DiffCosta = np.diff(fragm_Costa,axis=0)\n        IncCosta = np.sum(DiffCosta&gt;0)\n        DecCosta = np.sum(DiffCosta&lt;0)\n        C = (IncCosta-DecCosta)/(len(time_ser)-tau)\n        Cst.append(C)\n    Costa = np.mean(np.abs(Cst))\n    return Costa\n\n\n\n9.2.5 Оголошення функцій для підрахунку показників незворотності\n\n9.2.5.1 Пермутаційна незворотність\n\ndef PermIrrever(time_ser, d_e, tau, delta=1e-10, distance_irr=\"kullback\"):\n    pattern, dist = ordinal_distribution(time_ser, dx=d_e, taux=tau, return_missing=True)\n    \n    m, n = pattern.shape\n    pf = []\n    pb = []\n\n    is_used = np.zeros((m))\n\n    for i in range(m):\n        if is_used[i] == 1:\n            continue\n\n        is_used[i] = 1\n        pf.append(i)\n        permb = pattern[i,::-1]\n        for j in range(m-1,-1,-1):\n            if np.sum(pattern[j,:] == permb) == n: \n                is_used[j] = 1\n                pb.append(j)\n                break\n    \n    if distance_irr == \"kullback\":\n        KLD_perm = dist[pf] * np.log((dist[pf] + delta) / (dist[pb] + delta))\n        return np.sum(KLD_perm)   \n    else:\n        return distance.jensenshannon(dist[pf] + delta, dist[pb] + delta)\n\n\n\n9.2.5.2 Графо-динамічна незворотність\n\ndef GraphIrrever(fragm_1, graph_type='classic', delta=1e-10, d_e_rec=3, tau_rec=1, eps_rec=0.1, dist_rec='chebyshev', distance_irr='kullback'):\n    \n    # будуємо граф \n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm_1)\n    elif graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm_1)\n    else:\n        g = recurrence_net(fragm_1, rec_thr=eps_rec*np.abs(np.std(fragm_1)), dim=d_e_rec, tau=tau_rec, dist_type=dist_rec)\n    \n    # розраховуємо вхідні та вихідні характеристики \n    \n    adjacency_mat = g.adjacency_matrix()\n    ret_deg, adv_deg = GetDegree(adjacency_mat)\n    ret_clust, adv_clust = GetLocalClusteringCoefficient(adjacency_mat, ret_deg, adv_deg)\n    \n    # використовуємо kde для знаходження функції щільності ймовірностей \n    \n    pdf_ret_deg = gaussian_kde(sorted(ret_deg), bw_method='scott')       \n    pdf_adv_deg = gaussian_kde(sorted(adv_deg), bw_method='scott')    \n    pdf_ret_clust = gaussian_kde(sorted(ret_clust), bw_method='scott')    \n    pdf_adv_clust = gaussian_kde(sorted(adv_clust), bw_method='scott')\n    \n    a_deg = min(min(ret_deg), min(adv_deg))  \n    b_deg = max(max(ret_deg), max(adv_deg))  \n    a_clust = min(min(ret_clust), min(adv_clust))\n    b_clust = max(max(ret_clust), max(adv_clust))\n                           \n    if distance_irr == 'kullback':\n        dkl_deg = lambda x: pdf_ret_deg.pdf(x) * np.log((pdf_ret_deg.pdf(x) + delta)/(pdf_adv_deg.pdf(x) + delta))        \n        dkl_clust = lambda x: pdf_ret_clust.pdf(x) * np.log((pdf_ret_clust.pdf(x) + delta)/(pdf_adv_clust.pdf(x) + delta))\n\n        distance_deg = quad(dkl_deg, a_deg, b_deg)[0]       \n        distance_clust = quad(dkl_clust, a_clust, b_clust)[0]\n    \n    if distance_irr == 'shannon':                                 \n        width_deg = (b_deg-a_deg)/len(ret_deg)\n        width_clust = (b_clust-a_clust)/len(ret_clust)\n\n        lin_deg = np.arange(a_deg, b_deg, width_deg) \n        lin_clust = np.arange(a_clust, b_clust, width_clust)\n\n        p_ret_deg = pdf_ret_deg.pdf(lin_deg) \n        p_adv_deg = pdf_adv_deg.pdf(lin_deg) \n        p_ret_clust = pdf_ret_clust.pdf(lin_clust) \n        p_adv_clust = pdf_adv_clust.pdf(lin_clust)\n    \n        distance_deg = distance.jensenshannon(p_ret_deg + delta, p_adv_deg + delta)\n        distance_clust = distance.jensenshannon(p_ret_clust + delta, p_adv_clust + delta)\n      \n    return distance_deg, distance_clust\n\n\n\n9.2.5.3 Функції для отримання ступеня вершини та локальної кластеризації\nПроцедура знаходження ступеню зв’язків та локальної кластеризації кожної вершини є доволі громіздкою. Для прискорення розрахунків відповідних процедур скористаємось бібліотекою numba. Numba — це швидкий компілятор для Python, який найкраще працює з кодом, що використовує масиви, функції та цикли NumPy. Найпоширеніший спосіб використання Numba — це колекція декораторів, які можна застосувати до ваших функцій, щоб доручити Numba їх компілювати. Коли здійснюється виклик функції, прикрашеної Numba, вона компілюється у машинний код “just-in-time” для виконання, і весь або частина вашого коду може згодом виконуватися зі швидкістю власного машинного коду!\nВстановити її можна в наступний спосіб:\n\n!pip install numba==0.56.4\n\nNumba надає декілька утиліт для генерації коду, але центральною функцією є декоратор numba.jit(). За допомогою цього декоратора ви можете позначити функцію для оптимізації JIT-компілятором Numba. Різні режими виклику викликають різні варіанти компіляції та поведінки. Імпортуємо відповідний декоратор з бібліотеки numba:\n\nfrom numba import jit\n\n\n@jit(nopython=True, nogil=True) \ndef GetDegree(AM):\n    numNodes = AM.shape[0]\n    retarded_degree = np.zeros((numNodes))\n    advanced_degree = np.zeros((numNodes))\n     \n    for i in range(numNodes):\n        retarded_degree[i] = AM[i, :i].sum()\n\n    for i in range(numNodes):\n        advanced_degree[i] = AM[i, i:].sum()\n        \n    return retarded_degree, advanced_degree\n\n\n@jit(nopython=True, nogil=True) \ndef GetLocalClusteringCoefficient(AM, ret_deg, adv_deg):\n    \n    numNodes = AM.shape[0]\n    retardedCC = np.zeros( (numNodes) )\n    advancedCC = np.zeros( (numNodes) )\n    ret_norm = ret_deg * (ret_deg - 1) / 2\n    adv_norm = adv_deg * (adv_deg - 1) / 2\n    \n    for i in range(numNodes):\n        if ret_norm[i] != 0: \n            counter = 0\n            \n            for j in range(i):\n                for k in range(j): \n                    if AM[i, j] == 1 and AM[j, k] == 1 and AM[k, i] == 1: \n                        counter += 1\n                        \n            retardedCC[i] = counter / ret_norm[i]\n    \n    for i in range(numNodes-2):\n        if adv_norm[i] != 0: \n            counter = 0\n            \n            for j in range(i+1, numNodes):\n                for k in range(i+1, j): \n                    if AM[i, j] == 1 and AM[j, k] == 1 and AM[k, i] == 1: \n                        counter += 1\n                        \n            advancedCC[i] = counter / adv_norm[i]\n                 \n                \n    return retardedCC, advancedCC\n    \n\n\n\n\n9.2.6 Завантажуємо дані з сайту Yahoo! Finance\n\nsymbol = '^RUT'                       # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\ndate_in_num = mdates.date2num(time_ser.index)\n\nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n9.2.7 Виводимо досліджувані ряди\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 9.1: Динаміка щоденних змін індексу Russell 2000\n\n\n\n\nКористуючись тими методами, що ми розглянули в попередній лабораторній роботі, побудуємо діаграму Пуанкаре та граф нашого часового ряду. Але перш за все, для діаграми Пункаре, треба знайти стандартизовані прибутковості. Для цього оголосимо функцію transformation(), що прийматиме на вхід часовий сигнал, тип ряду, і повертатиме його перетворення:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n\n9.2.8 Встановлення параметрів для розрахунків\n\nwindow = 500 # розмір ковзного вікна\ntstep = 1 # часовий крок\n\nret_type = 4 # тип ряду: 1 - вихідний, 2 - детрендований\n                        # 3 - стандартні прибутковості, \n                        # 4 - стандартизовані прибутковості, \n                        # 5 - абсолютні значення (волатильності)\n                        # 6 - стандартизований вихідний часовий ряд\n\n\n# параметри для рекурентного графу\nd_e_rec = 3 # розмірність вкладень\ntau_rec = 1 # часова затримка\neps_rec = 1.3 # радіус\ndist_rec = 'chebyshev' # відстань між траєкторіями: \n                       # canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, \n                       # ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, \n                       # ‘jensenshannon’, ‘kulsinski’, ‘kulczynski1’, ‘mahalanobis’, \n                       # ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, \n                       # ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n\n\n# параметри для мір незворотності\nd_e_perm = 3              # розмірність вкладень для пермутаційних патернів \ntau_perm  = 1             # часова затримка для пермутаційних патернів\ndistance_irr = 'kullback' # відстань між розподілами: kullback, shannon\ngraph_type = 'classic'    # тип графу: classic, horizontal, recurrent\n\n# параметри для мір асиметрії\ntau_assym = 1                 # часова затримка для діаграми Пуанкаре\ntau_Costa_begin = 1           # початковий часовий масштаб для індексу Кошти\ntau_Costa_end = 20            # кінцевий часовий масштаб для індексу Кошти\ntaus_Costa = np.arange(tau_Costa_begin, tau_Costa_end+1) # формуємо масив масштабів\n                \nlength = len(time_ser)\n\n\n\n9.2.9 Виводимо діаграму Пуанкаре та розраховуємо міри на її основі\n\nfor_puank = time_ser.copy()\n\nfor_puank = transformation(for_puank, ret_type)\n\n\nfig, ax1 = plt.subplots(1, 1)\n\nax1.scatter(for_puank[:-tau_assym],for_puank[tau_assym:], marker=\"X\", s=180, c=\"g\")\n\nlow_x, high_x = ax1.get_xlim()\nlow_y, high_y = ax1.get_ylim()\nax1.axline([low_x, low_y], [high_x, high_y])\n\nax1.set_aspect('equal', 'box')\nax1.set_xlabel(r'$g(t)$')\nax1.set_ylabel(r'$g(t+\\tau)$') \nax1.set_xlim(left=low_x, right=high_x)\nax1.set_ylim(bottom=low_y, top=high_y)\nplt.locator_params(axis='y', nbins=7)\n\nplt.savefig(f\"Poincare_plot_{symbol}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\nВиходячи з даної діаграми, можна зазначити, що для прибутковостей індексу Russell 2000 спостерігається асиметрія у сторону зростаючих флуктуацій ряду.\n\n\n9.2.10 Побудова показників незворотності із використанням ковзного вікна\nВизначаємо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nОголошуємо масиви для збереження результатів:\n\nPIx = []\nGIx = []\nSIx = []\nAIx = []\nEIx = []\nCIx = []\n\nРозраховуємо відповідні міри у віконній процедурі:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент ряду \n    \n    fragm = transformation(fragm, ret_type)\n    \n    Temp_fragm = np.array([fragm[:-tau_assym], fragm[tau_assym:]])\n    \n    T2   = np.transpose(np.arctan(Temp_fragm[1,:]/Temp_fragm[0,:])*180/np.pi)\n    Dup  = abs(np.diff(Temp_fragm[:,T2&gt;45],axis=0))\n    Dtot = abs(np.diff(Temp_fragm[:,T2!=45],axis=0))\n    Sup  = np.sum(abs(T2[T2&gt;45]-45))\n    Stot = np.sum(abs(T2[T2!=45]-45))\n    Aup  = np.sum(abs(np.transpose(((T2[T2&gt;45]-45))*np.sqrt(np.sum(Temp_fragm[:,T2&gt;45]**2,axis=0)))))\n    Atot = np.sum(abs(np.transpose(((T2[T2!=45]-45))*np.sqrt(np.sum(Temp_fragm[:,T2!=45]**2,axis=0)))))\n    Ethird = np.sum(np.transpose(Temp_fragm[0,:]-Temp_fragm[1,:])**3)\n    Etot = (np.sum(np.transpose(Temp_fragm[0,:]-Temp_fragm[1,:])**2))**(3/2)\n\n    \n    Porta = sum(T2&lt;45)/sum(T2!=45)\n    Gudzik = np.sum(Dup**2)/np.sum(Dtot**2)\n    Slope = Sup/Stot\n    Area = Aup/Atot\n    Eiler = Ethird/Etot\n    Costa = Costa_1(fragm, taus_Costa)\n    \n    PIx.append(Porta)\n    GIx.append(Gudzik)\n    SIx.append(Slope)\n    AIx.append(Area)\n    EIx.append(Eiler)\n    CIx.append(Costa)\n\n100%|██████████| 8569/8569 [00:14&lt;00:00, 600.95it/s]\n\n\nЗберігаємо значення до .txt файлів\n\nnp.savetxt(f\"Porta_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", PIx)\nnp.savetxt(f\"Gudzik_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", GIx)\nnp.savetxt(f\"Slope_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", SIx)\nnp.savetxt(f\"Area_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", AIx)\nnp.savetxt(f\"Eiler_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", EIx)\nnp.savetxt(f\"Costa_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", CIx)\n\n\n\n9.2.11 Візуалізація показників на основі діаграми Пуанкаре\n\n9.2.11.1 Індекс Порти\n\nmeasure_label = r\"$PIx$\"\nfile_name = f\"PIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          PIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРисунок 9.2: Динаміка індексу Russell 2000 та індексу Порти\n\n\n\n\n\n\n9.2.11.2 Індекс Гузіка\n\nmeasure_label = r\"$GIx$\"\nfile_name = f\"GIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          GIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРисунок 9.3: Динаміка індексу Russell 2000 та індексу Гузіка\n\n\n\n\n\n\n9.2.11.3 Індекс кута нахилу\n\nmeasure_label = r\"$SIx$\"\nfile_name = f\"SIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРисунок 9.4: Динаміка індексу Russell 2000 та індексу кута нахилу\n\n\n\n\n\n\n9.2.11.4 Індекс площі секторів\n\nmeasure_label = r\"$AIx$\"\nfile_name = f\"AIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          AIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРисунок 9.5: Динаміка індексу Russell 2000 та індексу площі секторів\n\n\n\n\n\n\n9.2.11.5 Індекс Ейлера\n\nmeasure_label = r\"$EIx$\"\nfile_name = f\"EIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          EIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРисунок 9.6: Динаміка індексу Russell 2000 та індексу Ейлера\n\n\n\n\n\n\n9.2.11.6 Індекс Кошти\n\nmeasure_label = r\"$CIx$\"\nfile_name = f\"CIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          CIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРисунок 9.7: Динаміка індексу Russell 2000 та індексу Кошти\n\n\n\n\n\n\n\n9.2.12 Побудова графу досліджуваного ряду\n\nindex_begin = 2000  # початковий індекс для графу\nindex_end = 4000    # кінцевий індекс для графу\n\nret_type = 1        # вид ряду\n\nfor_graph_plot = time_ser.copy()\n\nfor_graph_plot = transformation(for_graph_plot, ret_type)\n\ndate = date_in_num[index_begin:index_end] # вилучаємо необхідні по індексам дати\n\n# будуємо граф у залежності від типу графа\nif graph_type == 'classic':\n    g = NaturalVG(directed=None).build(for_graph_plot[index_begin:index_end], xs=date)\n    pos = g.node_positions()\n    nxg = g.as_networkx()\nelif graph_type == 'horizontal':\n    g = HorizontalVG(directed=None).build(for_graph_plot[index_begin:index_end], xs=date)\n    pos = g.node_positions()\n    nxg = g.as_networkx()\nelse:\n    g = recurrence_net(for_graph_plot[index_begin:index_end], \n                       rec_thr=eps_rec * np.abs(np.std(for_graph_plot[index_begin:index_end])), \n                       dim=d_e_rec, \n                       tau=tau_rec, \n                       dist_type=dist_rec)\n    \n    pos = node_positions_recurrence_net(for_graph_plot[index_begin:index_end], date)\n    nxg = g\n    \n    \n# встановлення параметрів для побудови графів\n\ngraph_plot_options = {\n    'with_labels': False,\n    'node_size': 2,\n    'node_color': [(0, 0, 0, 1)],\n    'edge_color': [(0, 0, 0, 0.15)],\n}\n\nВиводимо зв’язки видимості:\n\nfig, ax1 = plt.subplots(1, 1)\n\nnx.draw_networkx(nxg, ax=ax1, pos=pos, **graph_plot_options)\nax1.tick_params(bottom=True, labelbottom=True)\nax1.plot(time_ser.index[index_begin:index_end], \n         for_graph_plot[index_begin:index_end], \n         label=fr\"{symbol}\")\n\nax1.set_title('Visibility Connections', fontsize=22)\nax1.set_xlabel(xlabel)\nax1.set_ylabel(fr\"{symbol}\")\nax1.legend(loc='upper right')\n\nplt.savefig(f\"Time_ser_connections_symbol={symbol}_ \\\n    idx_beg={index_begin}_idx_end={index_end}_sertype={ret_type}_ \\\n    network_type={graph_type}.jpg\", bbox_inches=\"tight\")\n\nplt.show(); \n\n\n\n\nВиходячи з графу взятого нами фрагменту видно, що крах поблизу 2000-го року характеризується високою концентрацією вузлів. Це вказує на високий ступінь довготривалої пам’яті для кризових явищ фондового ринку, що в свою чергу впливає і на їх незворотність.\nТепер розглянемо сам граф:\n\npos = nx.spring_layout(nxg, k=0.15, iterations=100)\n# знаходимо вузол близький до центру графа (0.5,0.5)\ndmin = 1\nncenter = 0\nfor n in pos: \n    x, y = pos[n]\n    d = (x - 0.5)**2 + (y - 0.5)**2\n    if d &lt; dmin:\n        ncenter = n\n        dmin = d\n\n# розфарбовуємо в залежності від ступеня вершини\n\np = dict(nx.degree(nxg))\nfig, ax2 = plt.subplots(1, 1)\nax2.set_title('Graph representation')\nnx.draw_networkx_edges(nxg, ax=ax2, pos=pos, nodelist=[ncenter], alpha=0.4,width=0.1)\nnx.draw_networkx_nodes(nxg, ax=ax2, pos=pos, nodelist=list(p.keys()),\n                       node_size=10, edgecolors='r', linewidths=0.01,\n                       node_color=list(p.values()),\n                       cmap=plt.cm.Blues_r)\n        \nvmin = np.asarray(list(p.values())).min()\nvmax = np.asarray(list(p.values())).max()\n\nsm = plt.cm.ScalarMappable(cmap=plt.cm.Blues_r, norm=plt.Normalize(vmin=vmin, vmax=vmax))\ncb = plt.colorbar(sm, ax=ax2)\ncb.set_label('degree')\n\nplt.savefig(f\"Graph_representation_symbol={symbol}_ \\\n            idx_beg={index_begin}_idx_end={index_end} \\\n            _sertype={ret_type}_network_type={graph_type}.jpg\", bbox_inches=\"tight\")\nplt.show(); \n\n\n\n\n\n\n9.2.13 Побудова показників незворотності на основі пермутаційних шаблонів та графів\nІніціалізуємо масиви для збереження результатів розрахунків:\n\nDegree = []\nClust = []\nPerm = []\n\nРозпочинаємо процедуру рухомого вікна:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент ряду\n\n    fragm = transformation(fragm, ret_type)  # виконуємо перетворення\n        \n    deg, clust = GraphIrrever(fragm, \n                            graph_type=graph_type, \n                            delta=1e-10, \n                            d_e_rec=d_e_rec, \n                            tau_rec=tau_rec, \n                            eps_rec=eps_rec, \n                            dist_rec=dist_rec, \n                            distance_irr=distance_irr)  \n    \n    perm = PermIrrever(fragm, \n                        d_e=d_e_perm, \n                        tau=tau_perm, \n                        delta=1e-10, \n                        distance_irr=distance_irr)\n      \n    Degree.append(deg)\n    Clust.append(clust)\n    Perm.append(perm)\n\n100%|██████████| 8569/8569 [16:54&lt;00:00,  8.45it/s]\n\n\nЗберігаємо результати до .txt файлів\n\nnp.savetxt(f\"{distance_irr}_deg_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_graph_type={graph_type}.txt\", Degree)\nnp.savetxt(f\"{distance_irr}_clust_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_graph_type={graph_type}.txt\", Clust)\nnp.savetxt(f\"{distance_irr}_perm_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_d_e={d_e_perm}_tau={tau_perm}.txt\", Perm)\n\n\n\n9.2.14 Візуалізація показників на основі графів та пермутаційних шаблонів\n\n9.2.14.1 Ступінь незворотності на основі ступеня вершини\n\nmeasure_label = r\"$Dist_{deg}$\"\nfile_name = f\"Degree_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            _graph_type={graph_type}_dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Degree, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\nРисунок 9.8: Динаміка індексу Russell 2000 та показника незворотності на основі ступеня вершини\n\n\n\n\n\n\n9.2.14.2 Ступінь незворотності на основі показника локальної кластеризації\n\nmeasure_label = r\"$Dist_{clust}$\"\nfile_name = f\"Clust_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            _graph_type={graph_type}_dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Clust, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\nРисунок 9.9: Динаміка індексу Russell 2000 та показника незворотності на основі локальної кластеризації\n\n\n\n\n\n\n9.2.14.3 Ступінь незворотності на основі пермутаційних шаблонів\n\nmeasure_label = r\"$Dist_{clust}$\"\nfile_name = f\"Clust_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            d_e={d_e_perm}_tau={tau_perm} \\\n            _graph_type={graph_type}_dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Perm, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\nРисунок 9.10: Динаміка індексу Russell 2000 та показника незворотності на основі пермутаційних шаблонів"
  },
  {
    "objectID": "lab_11.html#висновок",
    "href": "lab_11.html#висновок",
    "title": "9  Лабораторна робота № 11",
    "section": "9.3 Висновок",
    "text": "9.3 Висновок\nУ даній роботі було розглянуто показники незворотності (асиметрії) системи на основі діаграм Пуанкаре, графу видимості та пермутаційних шаблонів. Було продемонстровано побудову діграми Пуанкаре та зв’язків видимості як для всього ряду, так і для деяких із його фрагментів. Видно, що значення на діаграмі Пуанкаре характеризуються розподілом точок, що виходять за межі нормального Гаусового розподілу. Для графу видимості видно, що кризові стани характеризуються значною концентрацією зв’язків, що є довгостроковими. Таким чином, поставало актуальним проводити розрахунок показників незворотності графового типу для вихідного ряду.\nПоказники на основі діаграми Пуанкаре демонструють зріст або спад у кризові періоди, що вказує на зростання асиметрії у даний період часу. Дані показники можуть слугувати в якості індикаторів кризових явищ.\nВиходячи з 3 показників незворотності, що представлені вище, видно, що дані показники починають зростати в передкризовий період, вказуючи на стартовий період хаосу. Найгірше серед них себе поводить \\(Dist_{deg}\\). Найращим чином \\(Dist_{clust}\\) та \\(Dist_{perm}\\). Їх і варто використовувати в якості індикаторів-передвісників крахів."
  },
  {
    "objectID": "lab_12.html#теоретичні-відомості",
    "href": "lab_12.html#теоретичні-відомості",
    "title": "10  Лабораторна робота № 12",
    "section": "10.1 Теоретичні відомості",
    "text": "10.1 Теоретичні відомості\n\n10.1.1 Імпортуємо необхідні бібліотеки\n\nimport numpy as np                 # бібліотека для роботи з масивами чисел\nimport matplotlib.pyplot as plt    # бібліотека для побудови графіків\nimport yfinance as yf              # бібліотека для зчитування фінансових даних з Yahoo Finance\nimport levy                        # бібліотека для роботи з альфа-стабільним розподілом Леві\nimport pandas as pd                # бібліотека для фільтрації даних та їх обробки\nimport scienceplots\nfrom scipy.stats import norm, laplace  # бібліотека для побудови теоретичного розподілу Гауса\n                                       # та Лапласа\nfrom tqdm import tqdm                  # бібліотека для виводу шкали завантаження\n\n%matplotlib inline\n\n\n\n10.1.2 Виконуємо налаштування рисунків\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 14,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\n\n\n10.1.3 Зчитування даних\n\ndf = pd.read_csv('databases\\^spx_d.csv')\ndf.set_index('Date', inplace=True)\ndf.index = pd.to_datetime(df.index)\ndf = df[df.index &gt;= '1950-01-01']\n\n\n\n10.1.4 Важкі хвости та Чорні понеділки\nСпочатку ми розглянемо розподіл щоденних прибутковостей індексу S&P 500, починаючи з 1950 року. У клітинці нижче буде показано Pandas dataframe df, що містить дані OHLC з файлу (ці дані були попередньо завантажені в клітинці вище).\n\ndf\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n1950-01-03\n16.66\n16.66\n16.66\n16.66\n7.000000e+05\n\n\n1950-01-04\n16.85\n16.85\n16.85\n16.85\n1.050000e+06\n\n\n1950-01-05\n16.93\n16.93\n16.93\n16.93\n1.416667e+06\n\n\n1950-01-06\n16.98\n16.98\n16.98\n16.98\n1.116667e+06\n\n\n1950-01-07\n17.09\n17.09\n17.09\n17.09\n1.116667e+06\n\n\n...\n...\n...\n...\n...\n...\n\n\n2023-04-10\n4085.20\n4109.50\n4072.55\n4109.11\n1.951642e+09\n\n\n2023-04-11\n4110.29\n4124.26\n4102.61\n4108.94\n2.000949e+09\n\n\n2023-04-12\n4121.72\n4134.37\n4086.94\n4091.95\n2.249009e+09\n\n\n2023-04-13\n4100.04\n4150.26\n4099.40\n4146.22\n2.198579e+09\n\n\n2023-04-14\n4140.11\n4163.19\n4113.20\n4137.64\n2.088609e+09\n\n\n\n\n18527 rows × 5 columns\n\n\n\nЦей історичний ряд щоденних цін містить ~ 18527 торгових днів. Замість того, щоб дивитися безпосередньо на ціни, ми розглянемо щоденні log-прибутковості індексу S&P 500. Пам’ятайте, що логарифмічна прибутковість \\(r_t\\) визначається як логарифм відношення між цінами закриття \\(p_t\\) у теперішній момент часу та попередній, \\(p_{t-1}\\):\n\\[\nr_t = \\log(\\frac{p_t}{p_{t-1}})\n\\]\nЛогарифмічні прибутковості характеризуються симетрією, тобто якщо в один день індекс втрачає \\(r_t = -0.1\\), а на наступний день індекс набирає \\(r_{t+1} = +0.1\\), індекс знову досягає початкової ціни. Зі звичайними прибутковостями, спочатку втративши \\(10\\%\\), а потім знову набравши \\(10\\%\\), ви не повернетесь до початкової ціни. Ми зберігаємо прибутковості в новому фреймі даних lr:\n\nlr = np.log(df['Close']).diff().dropna()\n\n\nlr.plot()\nplt.show();\n\n\n\n\nВикористовуючи вбудовані функції Pandas та Numpy, тепер ми можемо подивитись на найгірше щоденнє значення прибутковостей. Ми виявляємо, що найгірше значення становило \\(-0.229\\), що відповідає \\(-22.9\\%\\), і що це сталося в понеділок, 19 жовтня 1987 року — так званий Чорний понеділок.\n\n# найгірший щоденні прибутковості від закриття до закриття\nlr.min()\n\n-0.2289972265656708\n\n\n\n# відповідні арифметичні прибутковості\nnp.exp(lr.min()) - 1\n\n-0.20466926070038938\n\n\n\n# індекс найгіршого значення\nnp.argmin(lr)\n\n9583\n\n\n\ndf.iloc[9583:9585]\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n1987-10-16\n298.08\n298.92\n281.52\n282.70\n188055556.0\n\n\n1987-10-19\n282.70\n282.70\n224.83\n224.84\n335722222.0\n\n\n\n\n\n\n\nДалі ми будуємо графік вибіркового розподілу щоденних логарифмічних прибутковостей, що демонструє нам, як часто ми спостерігаємо щоденні прибутковості певного знака та величини. Зверніть увагу, що ми встановили логарифмічне масштабування для вісі Оy, щоб мати краще представлення про хвости розподілу, тобто дуже великі, але рідкісні негативні та позитивні прибутковості — крахи та підйоми:\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.yscale('log')\nplt.ylabel('Розп. ймовірностей')\nplt.xlabel('Щоденні лог-прибутковості')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.011, 99])\nplt.annotate('Чорний понеділок (1987)', [-0.226, 0.02], xytext=[-0.215, 0.15],\n             arrowprops=dict(facecolor='0.4', width=1, headwidth=6, headlength=6),\n             ha='left', va='bottom');\nplt.show();\n\n\n\n\nЯк ми можемо описати погіршення розподілу прибутковості? За допомогою логарифмічного масштабування осі Oy, щоб експоненціальний спад виглядав лінійно, тому ми могли би запідозрити, що при малих абсолютних значеннях прибутковості розподіл зменшується експоненціально, але в міру збільшення абсолютних значень прибутковостей ми все більше і більше відхиляємося від початкового нахилу. Насправді, ми можемо показати, що ні розповсюдженний розподіл Гауса (також званий нормальним розподілом), ні розподіл Лапласа з його експоненціальним спадом з обох сторін точно не можуть змоделювати розподіл прибутковостей S&P 500. Як розподіл Гауса, так і розподіл Лапласа недооцінюють ймовірність екстремальних подій — підйомів та крахів:\n\n# підганяємо розподіл Гауса та Лапласа під дані\nnorm_loc, norm_scale = norm.fit(lr)\nlaplace_loc, laplace_scale = laplace.fit(lr)\n\n# генеруємо значення (x,y) з використанням отриманих параметрів розподілів\nx_fit = np.linspace(-0.25, 0.125, 1000)\ny_fit_norm = norm(loc=norm_loc, scale=norm_scale).pdf(x_fit)\ny_fit_laplace = laplace(loc=laplace_loc, scale=laplace_scale).pdf(x_fit)\n\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.plot(x_fit, y_fit_laplace, lw=3, c='C3', alpha=0.8, label='Розподіл Лапласа')\nplt.plot(x_fit, y_fit_norm, lw=3, c='0.2', alpha=0.8, label='Розподіл Гауса')\nplt.yscale('log')\nplt.ylabel('Розп. ймовірностей')\nplt.xlabel('Щоденні лог-прибутковості')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.011, 99])\nplt.legend()\n\nplt.annotate('Чорний понеділок (1987)', [-0.226, 0.02], xytext=[-0.215, 0.15],\n             arrowprops=dict(facecolor='0.4', width=1, headwidth=6, headlength=6),\n             ha='left', va='bottom');\n\nplt.show();\n\n\n\n\n\n\n10.1.5 Відхилення в 23 сигма\nЯк ми можемо бачити вище, як розподіл Гауса, так і розподіл Лапласа різко недооцінюють ймовірність дуже волатильної динаміки! Часто зазначають, що трапилась “3-сигма подія на ринку” або щось подібне, тому що багатьом людям подобається оцінювати індивідуальні прибутковості по стандартному відхиленню (зване “сигмою”) підібраного гаусового розподілу. Однак, якщо Розподіл Гауса взагалі не відповідає розподілу вибірки, як у нашому випадку, оцінка краху в одиницях “сигма” може ввести в оману! Щоб показати це, ми спочатку перевіримо, на скільки сигм ринок змістився в Чорний понеділок 1987 року:\n\nnp.abs(np.min(lr))/np.std(lr, ddof=1)\n\n22.969393251617504\n\n\nВиходячи з розподілу по Гауса, Чорний понеділок був би подією з “23 сигмами”! Тепер Розподіл Гауса чітко говорить нам, як часто X-сигма подія має відбуватися:\n\n1-сигма: прибл. 1 з 3 днів\n2-сигма: прибл. 1 з 22 днів\n3-сигма: прибл. 1 з 370 днів\n\n…\nМи можемо розширити цю таблицю, попрацювавши з функцією кумулятивного розподілу, тісно пов’язана з функцією виживання, і перемістивши обчислення в логарифмічний простір для чисельної стабільності, як демонструють наступні комірки коду:\n\n# у середньому, як багато днів між 1-сигма подіями?\n1/(2*(1. - norm.cdf(1.)))\n\n3.1514871875343764\n\n\n\n# у середньому, як багато днів між 2-сигма подіями?\n1/(2*(1. - norm.cdf(2.)))\n\n21.97789450799283\n\n\n\n# у середньому, як багато днів між 3-сигма подіями?\n1/(2*(1. - norm.cdf(3.)))\n\n370.3983473449564\n\n\n\n# те саме, що й вище, але із використанням функції виживання\n1/(2*norm.sf(3.))\n\n370.3983473449592\n\n\n\n# те саме, що й вище, але із використанням логарифмічної функції функції виживання для чисельної стабільності\nnp.exp(-norm.logsf(3.) - np.log(2.))\n\n370.398347344959\n\n\n\n# те саме, що й вище, але із використанням логарифму з основою 10 для замість натурального логарифму для кращої інтерпретації\n10**(-norm.logsf(3.)/np.log(10) - np.log10(2.))\n\n370.3983473449588\n\n\n\n# як багато днів (log10 значення) між 23-сигма подіями, в середньому?\n(-norm.logsf(23.)/np.log(10) - np.log10(2.))\n\n116.33149536636726\n\n\nЩоб отримати додаткову інформацію про частоту екстремальних прибутковостей, ми можемо навіть створити невеликий інтерактивний віджет за допомогою повзунка, який вказує кількість “сигм”, а текст поруч із ним показує середню кількість днів між двома такими екстремальними подіями:\n\nfrom ipywidgets import IntSlider, interact\n\ndef years_between(X):\n    log10_days = (-norm.logsf(X)/np.log(10) - np.log10(2.))\n    days = int(10**log10_days)\n    return f'У сереньому, між двома X-сигма подіями, варто очікувати {days} дні(в), (день).'\n\ninteract(years_between, X=IntSlider(min=1, max=23, step=1, value=1));\n\n\n\n\n\nnp.log10(float(214533622638557983431869220329015643677794364342592664885632499326649056583365324378194575664549945880382591666749440))\n\n116.33149536636726\n\n\nВи бачите, що при переміщенні повзунка вправо кількість днів або років між двома наступними “X-сигма” подіями швидко наростає. Наша таблиця тепер виглядає наступним чином:\n\n1-сигма: прибл. 1 з 3 днів\n2-сигма: прибл. 1 з 22 днів\n3-сигма: прибл. 1 з 370 днів\n\n…\n\n23-сигма: приблизно 1 з \\(10^{116}\\) днів (!!!)\n\nВнесемо ясність: виходячи з розподілу Гауса, щоденні втрати Чорної п’ятниці 1987 року повинні бути подією, яку ми очікуємо раз на \\(10^{116}\\) днів. Це число в значній мірі незбагненно велике. Якби S&P 500 почав торгуватися відразу після народження Всесвіту, це становило б лише \\(10^{13}\\) торгових днів. Через $10^{116} $ днів усі зірки у Всесвіті давно згорять, навіть усі чорні діри випаруються і Всесвіт стане темним і порожнім місцем. Тож або ми повинні бути дуже щасливі, що єдина очікувана Чорна п’ятниця в історії та майбутньому Всесвіту позаду, або ви дійсно ніколи не повинні використовувати Розподіл Гауса для моделювання прибутковості акцій! Незалежно від ринку та деталей, якщо хтось говорить про події з 10 сигмами або 23 сигмами, він, безумовно, використовує неправильну модель, оскільки шанс спостерігати таку подію в нашому житті незначний.\nОтже, виникає питання: Як ми справляємось із такими екстремальними значеннями? Чи слід позначати їх як викиди або артефакти, щоб наші фінансові моделі краще описували більшість значень? Звичайно, ні, оскільки результат наших інвестицій може критично залежати не від більшості менших прибутковостей, а саме від таких екстремальних подій! Далі ми познайомимося з підходом, який може пояснити та екстраполювати за межі екстремальних подій, таких як Чорний понеділок. Але ми також побачимо, що не всі способи врахування важких хвостів розподілів працюють добре, оскільки деякі моделі, такі як GARCH може заколисати нас помилковим почуттям безпеки, коли ми завжди беремо до уваги минулі екстремальні події, але завжди дивуємося новим.\n\n\n10.1.6 Статистика степеневого розподілу\nТепер, коли ми знаємо, що розподіл Гауса не є хорошим вибором, який тип розподілу насправді може описати частоту екстремальних подій, які струшують S&P 500? Частота екстремально позитивні та негативні прибутковості асиметричні для більшості фінансових активів, оскільки крахи, як правило, більш сильні, ніж підйоми. Ось чому нижче ми зосередимося на лівому (негативному) хвості розподілу прибутковостей.\nОскільки нас цікавлять лише екстремальні події, ми розглянемо лише ті прибутковості, які менше \\(-0.03\\) (приблизно три стандартних відхилення від середнього значення). Нижче ми візуалізуємо ці екстремально негативні показники прибутковостей в логарифмічному масштабі (тут ми використовуємо абсолютні значення негативних прибутковостей). Як вісь Oy (яка показує частоту логарифмічних прибутковостей), так і вісь Ox (яка показує величину логарифмічних прибутковостей) масштабуються логарифмічно:\n\n# хвіст починається приблизно при трьох стандартних відхиленнях від середнього\nnp.mean(lr) - 3*np.std(lr)\n\n-0.029610514264246313\n\n\n\nneg_lr = -lr[lr &lt; 0]\n\nplt.figure()\nhist = plt.hist(neg_lr, bins=np.logspace(np.log10(0.03),np.log10(0.3), 20), density=True)\nplt.yscale(\"log\")\nplt.xscale(\"log\")\nplt.ylabel('Розп. ймов.')\nplt.xlabel('Щоденні негативні абсолютні прибутковості')\n\nplt.ylim([0.1, 110])\nplt.xlim([0.03, 0.3])\nplt.xticks([0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3], [0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3])\nplt.show();\n\n\n\n\nЯк ми бачимо, ця гістограма екстремальних абсолютних прибутків зменшується приблизно лінійно при логарифмічному масштабуванні. Щоразу, коли ви виявляєте пряму лінію на логарифмічному графіку, це вказує на так званий степеневий розподіл. Степеневе співвідношення ймовірності спостереження великих абсолютних логарифмічних прибутковостей задається у вигляді:\n\\[\np(|r_t|) = c \\cdot |r_t|^{-\\alpha}\n\\]\nКонстанта \\(c\\) на даний момент нас не надто турбує, вона просто гарантує, що ліва частина рівняння належним чином нормалізована, але показник $$ представляє для найбільший інтерес, оскільки він говорить нам, як працює степеневий закон. Якщо ви знаєте частоту даних абсолютних логарифмічних прибутковостей, то степеневий закон підкаже вам, у скільки разів менш імовірними були б логарифмічні прибутковості подвоєного розміру:\n\\[\np(2 \\cdot |r_t|) = c \\cdot (2 \\cdot |r_t|)^{-\\alpha} = 2^{-\\alpha} \\cdot c \\cdot |r_t|^{-\\alpha} = 2^{-\\alpha} \\cdot p(|r_t|)\n\\]\nабо простіше:\n\\[\n\\frac{p(2 \\cdot |r_t|)}{p(|r_t|)} = \\frac{1}{2^{\\alpha}}\n\\]\nДля $= 0 $ всі прибутковості однаково ймовірні, для $ = 2 $ подвоєння прибутковостей робить їх у 4 рази менш імовірними. Оскільки це правило подвоєння вгору або вниз не залежить від значення самого \\(\\left| r_t \\right|\\), але працює для всіх значень \\(\\left| r_t \\right|\\). Степеневе співвідношення також називаються “безмасштабним” або масштабо-інваріантним.\nЯк щодо показника \\(\\alpha\\) для наших надзвичайно негативних логарифмічних прибутковостей S&P 500? Ми можемо легко оцінити його за даними, але є деякі тонкощі у правильному розбитті вищезазначеної гістограми, які можуть вплинути на нашу оцінку (див., наприклад, White et al. (2008)). Щоб обійти ці проблеми з розбиттям, ми можемо замість цього оцінити степенний показник кумулятивної гістограми логарифмічних прибутковостей, а потім відняти одиницю від отриманого нахилу:\n\n# підганяємо лінію регресії до кумулятивної гістограми (log10 для ймовірності) vs. лог-прибутковості (log10)\nsorted_neg_lr = np.sort(neg_lr.values)\ncumulative_probability = np.linspace(1, 0, len(sorted_neg_lr)+1)[:-1]\n\nx_min = 0.03\nmask = sorted_neg_lr &gt;= x_min\nm, b = np.polyfit(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]), 1)\nalpha = -(m - 1)\n\nplt.figure()\nplt.plot(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]))\nplt.scatter(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]), label='sample data')\n\nx_fit = np.linspace(np.log10(x_min), np.log10(1.1*np.max(neg_lr)), 100)\ny_fit = m*x_fit + b\n\nplt.plot(x_fit, y_fit, label='степенева підгонка')\nplt.title(f'розрахований степеневий показник $\\\\alpha={alpha:.2f}$')\nplt.xlabel('лог-прибутковості (log10-scaled)')\nplt.ylabel('кумулятивна гістограма (log10-scaled)')\nplt.show();\n\n\n\n\nВиходячи з цієї оцінки, ми отримуємо степеневої показник, рівний \\(\\alpha \\approx 3.7\\). Якщо ми побудуємо гістограму наших надзвичайно негативних логарифмічних прибутковостей, то побачимо, що вона добре відображає зниження ймовірності, оскільки масштаб збоїв збільшується. Однак цей степеневий показник все ще недооцінює ймовірність настання чергового “Чорного понеділка”, оскільки лінія відповідності не ідеально підходить до цієї крайньої точки. Якби ми захотіли створити ще більш консервативну модель екстремальних подій, нам потрібно було б вручну ще більше зменшити значення $ $, щоб врахувати більшу ймовірність настання чорних понеділків за рахунок втрати точності підгонки для менших збоїв. Це перший раз, коли ми можемо побачити, як одна точка даних впливає на наші рішення щодо моделювання. На даний момент ми будемо довіряти оцінці параметра і погодимося з оцінкою \\(\\alpha=3.7\\).\n\nneg_lr = -lr[lr &lt; 0]\n\nplt.figure()\nhist = plt.hist(neg_lr, bins=np.logspace(np.log10(0.03),np.log10(0.3), 20), density=True)\nplt.yscale(\"log\")\nplt.xscale(\"log\")\nplt.ylabel('Розп. ймов.')\nplt.xlabel('Щоденні негативні абсолютні прибутковості')\nplt.ylim([0.1, 110]);\nplt.xlim([0.03, 0.3])\nplt.xticks([0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3], [0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3]);\n\nx_fit = np.logspace(np.log10(0.03),np.log10(0.3), 100)\ny_fit = 0.00022*(x_fit**-3.7)\n\nplt.plot(x_fit, y_fit, lw=3)\nplt.show();\n\n\n\n\nМи можемо використовувати цей степеневий закон, щоб екстраполювати, як часто ми очікуємо настання чорного понеділка (або “події з 23 сигмами”) у довгостроковій перспективі. Виходячи з наявних у нас даних, ми можемо оцінити, що ймовірність спостереження зниження на \\(-0.03\\) або гірше становить близько \\(1.6\\%\\). Слідуючи нашій степеневій кривій, зниження чорного понеділка на \\(-0.229\\), таким чином, відбувалося б із частотою \\(\\left(\\frac{0.229}{0.03}\\right)^{-3.7} \\approx \\frac{1}{1845}\\). Так само часто, як зниження на $ -0.03$, яке ми спостерігаємо приблизно раз на три місяці. Це означає, що, виходячи з нашої степеневої кривої, очікується, що чорні понеділки, подібні до 1987 року, траплятимуться приблизно раз на 450 років (див. код нижче).\n\n# як часто ринок знижується більш ніж на -0.03\n(neg_lr &gt; 0.03).mean()\n\n0.016091417910447763\n\n\n\n# у середньому, як багато днів між цима трьома подіями\n1./(neg_lr &gt; 0.03).mean()\n\n62.144927536231876\n\n\n\n# у скільки разів менше відбувається подій \"Чорного понеділка\" в порівнянні зі зниженням на -0.03?\n(np.max(neg_lr)/0.03)**3.7\n\n1845.1139014672306\n\n\n\nnp.max(neg_lr)\n\n0.2289972265656708\n\n\n\n# скільки років пройшло між двома подіями \"Чорного понеділка\",\n# виходячи з припущення, що подія -0.03 відбувається один раз у 63 дня?\n(1./((1/62)*1/1845))/252\n\n453.92857142857144\n\n\nЯкби ми включили більше даних, що містять інші приклади екстремальних подій, наприклад, з Великої депресії, ця оцінка може стати ще меншою. Аналогічно, якби ми скоригували степеневий показник так, щоб він був більш консервативним, ніж передбачає наш простий метод оцінки, ми також отримали б коротший період. Хоча 460 років все ще є дуже великим періодом, це набагато більш реалістична оцінка порівняно з \\(10^{114}\\) роками, які ми отримали, використовуючи розподіл Гауса. Однією з переваг степеневої моделі є її здатність до екстраполяції: використовуючи степеневу оцінку \\(\\alpha\\) та частоту менших прибутковостей, щодо яких ми маємо достовірні дані, ми можемо оцінити частоту майбутніх серйозних збоїв, щодо яких ми ще не отримали жодних даних. Ще про одну перевагу оцінки \\(\\alpha\\) ми розглянемо далі.\n\n\n\n\n\n\nПРИМІТКА\n\n\n\nНе існує такого поняття, як “подія з 10 сигмами”! Справжня подія з 10 сигмами настільки неймовірно рідкісна, що пережити її протягом нашого життя — мізерно мало. Екстремальні події, безумовно, трапляються, але вони не можуть бути описані за допомогою “сигм”, їх потрібно враховувати і екстраполювати з використанням степеневих законів. Завжди будьте обережні, коли аналітики виправдовують себе за те, що вони не були готові до “події з 10 сигмами”, оскільки це вказує на те, що їх моделі ризиків вкрай недосконалі.\n\n\n\n\n10.1.7 Статистичні моменти під впливом важких хвостів\nЧому все це так важливо? Степенневий показник \\(\\alpha\\) насправді багато говорить нам про стабільність і збіжність моментів розподілу ймовірностей, і це, в свою чергу, має значення для деяких найбільш часто використовуваних моделей у фінансах. Перші чотири центральні моменти — це:\n\nСереднє: очікуване значення розподілу\nДисперсія: Квадрат стандартного відхилення, який часто використовується для оцінки волатильності на основі розподілу прибутковості.\nАсиметрія: вимірює зміщення розподілу. Розподіл прибутковостей, як правило, має негативну асиметрію, оскільки збої є більш стрімкими, ніж підйоми.\nЕксцес: вимірює тяжкість хвостів розподілу. Розподіли прибутковостей зазвичай мають ексцес більший ніж у розподілі Гауса, тобто екстремальні події відбуваються частіше, ніж очікувалося в гаусовій моделі.\n\nЕксцес Гаусового розподілу дорівнює 3 (використовуючи визначення Пірсона, з визначенням Фішера надлишкового ексцесу це 0), але наша вибірка щоденних логарифмічних прибутковостей S&P 500 має ексцес \\(28.6\\), що знову вказує на те, що екстремальні події набагато більш вірогідні, що суперечить Гаусовому розподілу.\n\n# обчислюємо ексцес лог-прибутковостей\nfrom scipy.stats import kurtosis\n\nkurtosis(lr, fisher=False, bias=False)\n\n28.610931957080663\n\n\nТепер ми дійшли до того моменту, коли ми могли б запитати: наскільки сильно одна точка впливає на нашу оцінку ексцесу? Якщо ми перерахуємо ексцес всіх щоденних прибутків S&P 500 з 1950 року і виключимо тільки чорний понеділок 1987 року, ми отримаємо значення \\(14.3\\), майже половину від значення, яке ми отримуємо, коли використовуємо всі значення! Видалення подальших найгірших днів, звичайно, ще більше зменшує ексцес, але ефект значно менший.\n\n# обчислюємо ексцес лог-прибутковостей після видалення Чорного понеділка 1987\nkurtosis(lr[lr &gt; np.min(lr)], fisher=False, bias=False)\n\n14.3083633314701\n\n\n\nlr_sorted = lr.sort_values()\nkurt = [kurtosis(lr_sorted.iloc[i:], fisher=False, bias=False) for i in range(10)]\n\nplt.figure()\nplt.plot(np.arange(10), kurt, c='C3', lw=2, zorder=2)\nplt.scatter(np.arange(10), kurt, s=120, lw=0.75, edgecolor='k', facecolor='C3', zorder=3)\nplt.grid()\nplt.xlabel('кіль-ть виключених найгірших днів')\nplt.ylabel('Ексцес щоденних лог-прибутковостей')\nplt.ylim([0, 32])\nplt.show();\n\n\n\n\nБільш радикальний погляд на цю проблему досягається, якщо ми будуємо графік ексцесу в покроковому режимі, тобто для кожного дня ми будуємо розрахунковий ексцес, використовуючи всі минулі логарифмічні дані до цього моменту часу. Як ви можете бачити нижче, до “Чорного понеділка” ексцес ніколи не перевищував \\(15\\), проте навіть через 35 років після “Чорного понеділка” ексцес ще не “оговтався” від цієї події і залишається на позначці 30. Чи означає це, що ми повинні просто ігнорувати Чорний понеділок як викид і продовжувати використовувати нашу “чисту” оцінку в \\(14.6\\)? Ні, точно ні! Як ми побачимо незабаром, ми скоріше повинні запитати себе, чи є сенс у тому, щоб оцінювати ексцес!\n\nkurt = [kurtosis(lr.iloc[:i], fisher=False, bias=False) for i in range(1000, len(lr))]\n\n\nplt.figure()\nplt.plot(lr.index[1000:], kurt, c='C3', lw=3, zorder=2)\nplt.ylabel('Ексцес щоденних лог-прибутковостей')\nplt.show();\n\n\n\n\nТой факт, що одна точка даних може настільки сильно змінити нашу оцінку ексцесу, і, схоже, вона більше не сходиться, коли додаються нові точки даних, є чітким свідченням того, що ексцес базового розподілу ймовірностей насправді нескінченний! Проблема полягає в тому, що для кінцевої вибірки даних (а всі набори даних скінченні) ми завжди зможемо оцінити скінчене значення ексцесу. scipy.stats.kurtosis не може повернути \\(\\infty\\), він завжди повідомлятиме про скінченний ексцес вибірки. Але ця кінцева оцінка не допоможе нам нічого сказати про майбутню поведінку ринку, оскільки ексцес продовжуватиме переходити до ще більших значень до наступного чорного понеділка, іншого чорного вівторка … у більш-менш віддаленому майбутньому.\nОднак опис частоти екстремальних подій згідно степеневого закону, який ми представили вище, може допомогти нам вирішити, чи є ексцес кінцевим, так що, переглянувши достатню кількість точок даних, ми зможемо вирішити, чи слід прийняти той факт, що ексцес не піддається оцінці і справді нескінченний. Щоб побачити це, нам потрібно ввести ще один розподіл ймовірностей, t-розподіл Стьюдента. Це колоколообразний симетричний розподіл зі степеневими хвостами. Його додатковий параметр \\(\\nu\\) визначає показник степеневого закону \\(\\alpha=(\\nu+1)\\). Нижче ми підганяємо t-розподіл до наших логарифмічних прибутковостей S&P 500, зберігаючи при цьому степеневий показник \\(\\alpha=3.7\\) (відповідний \\(\\nu = 2.7\\)), який ми підганяли раніше:\n\nfrom scipy.stats import t\n\nfit_params = t.fit(lr, fix_df=2.7)\n\nx_fit = np.linspace(-0.25, 0.125, 1000)\ny_fit_t = t(*fit_params).pdf(x_fit)\n\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.plot(x_fit, y_fit_t, lw=3, c='C3', alpha=0.8, label=\"Підігнаний t-розподіл\")\nplt.yscale('log')\nplt.ylabel('Розп. ймов.')\nplt.xlabel('Щоденні лог-прибутковості')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.0011, 99])\nplt.legend()\n\nplt.title(f'Підігнаний степеневий показник: $\\\\alpha = {(fit_params[0]+1):.2f}$')\nplt.show();\n\n\n\n\nЗвичайно, можна поставити під сумнів, чи дійсно t-розподіл Стьюдента добре відповідає нашим логарифмічним прибутковостям, оскільки він переоцінює частоту абсолютних прибутковостей і все ще недооцінює ймовірність настання Чорного понеділка. На даний момент ми ігноруємо ці деталі та зосереджуємось на ексцесі t-розподілу Стьюдента та зв’язку зі степеневим показником \\(\\alpha\\):\n\n\\(\\text{Kurt} = \\frac{6}{\\nu-4} + 3~\\) для \\(~\\nu &gt; 4~\\) або \\(~\\alpha &gt; 5\\)\n\\(\\text{Kurt} = \\infty~~~~~~~~~~\\,\\) для \\(~2 &lt; \\nu \\leq 4~\\) або \\(~3 &lt; \\alpha \\leq 5\\)\nінакше невизначено\n\nЦе підтверджує наше початкове припущення про те, що при степеневому показнику, рівному \\(\\alpha \\approx 3.7\\), ексцес дійсно нескінченний, і немає сенсу оцінювати його по скінченій вибірці, оскільки оціночне значення буде зберігатися тільки до наступної екстремальної події, яка підштовхне його ще вище. Ми можемо легко змоделювати цей ефект, витягуючи випадкові вибірки з t-розподілів Стьюдента з різними степеневими показниками, які відповідають трьом режимам, зазначеним вище. Нижче ми витягуємо 100 000 випадкових значень з t-розподілів з \\(\\alpha = 2\\), \\(\\alpha = 3.7\\), \\(\\alpha = 6\\), а потім обчислюємо ексцес поетапно, як і раніше, щоб побачити, як еволюціонує передбачуваний ексцес вибірки з часом у трьох різних випадках:\n\nnp.random.seed(1)\nsamples_alpha_20 = t(1.0, 0., 1.).rvs(100000)\nsamples_alpha_37 = t(2.7, 0., 1.).rvs(100000)\nsamples_alpha_60 = t(5.0, 0., 1.).rvs(100000)\n\n\nm = np.arange(1000, 100000, 50)\nkurt_alpha_20 = [kurtosis(samples_alpha_20[:i], fisher=False, bias=False) for i in m]\nkurt_alpha_37 = [kurtosis(samples_alpha_37[:i], fisher=False, bias=False) for i in m]\nkurt_alpha_60 = [kurtosis(samples_alpha_60[:i], fisher=False, bias=False) for i in m]\n\n\nplt.figure()\n\nplt.subplot(311)\nplt.plot(m, kurt_alpha_60, c='C2', lw=3, zorder=2)\nplt.axhline(6/(5-4) + 3, lw=1, ls='--', c='k')\nplt.ylabel('Ексцес')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\\\alpha=6$')\n\nplt.subplot(312)\nplt.plot(m, kurt_alpha_37, c='C1', lw=3, zorder=2)\nplt.ylabel('Ексцес')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\\\alpha=3.7$')\n\nplt.subplot(313)\nplt.plot(m, kurt_alpha_20, c='C3', lw=3, zorder=2)\nplt.ylabel('Ексцес')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\\\alpha=2$')\n\nplt.tight_layout()\nplt.show();\n\n\n\n\nУ випадку \\(\\alpha=6\\) (\\(\\nu=5\\)) ексцес скінченний і повинен приймати значення \\(\\frac{6}{\\nu - 4}+3=9\\). Хоча ми бачимо деякі коливання, оціночне значення ексцесу повільно наближається до справжнього значення. Для прикладу, який відповідає степеневому показнику індексу S&P 500 з \\(\\alpha = 3.7\\), моделювання показує таку ж поведінку, що і для реальних даних: збіжність не може бути виявлена, скоріше кожна екстремальна подія збільшує ексцес; при нескінченних вибірках ми би досягли нескінченного ексцесу. У третьому випадку з \\(\\alpha = 2\\) ексцес t-розподілу Стьюдента навіть не визначений належним чином, і ми можемо побачити інший тип поведінки моделювання: зараз більшість точок даних збільшують оцінку ексцесу, а не лише кілька окремих екстремальних зразків. При такому повільно спадаючому статечному законі екстремальні значення зустрічаються повсюдно, що призводить до збільшення ексцесу при моделюванні.\n\n\n\n\n\n\nПримітка\n\n\n\nНавіть якщо певні властивості розподілу ймовірностей нескінченні, ми завжди знайдемо кінцеві вибіркові оцінки при перегляді даного набору даних просто тому, що набір даних містить лише кінцеву кількість точок. Однак, якщо теорія степеневого закону чітко говорить нам, що певна властивість нескінченна, ми не повинні ніде використовувати кінцеві оцінки і ми не повинні використовувати моделі, які вимагають, щоб ця властивість була кінцевою! Якщо ми все одно це зробимо, це в кращому випадку буде працювати лише до наступної екстремальної події, яка анулює наші минулі оцінки і — у випадку фінансових моделей — може збанкрутувати нас!\n\n\n\n\n10.1.8 Як ексцес дестабілізує розрахунки волатильності\nОскільки ексцес збільшується — або стає нескінченним — це також впливає на нижчі моменти, найголовніше, на дисперсію. Квадратний корінь дисперсії — це стандартне відхилення, або волатильність, як це називається у фінансах. Волатильність є розповсюдженним параметром при оптимізації портфеля і практично в кожній фінансовій моделі, оскільки в кінцевому підсумку вона стала найбільш часто використовуваним показником “ризику”. Щоб побачити, як ексцес впливає на оцінку волатильності, ми проводимо простий експеримент Монте-Карло: ми витягуємо щоденну прибутковість за 50 років з розподілу ймовірностей за нашим вибором (ми будемо використовувати Гаусовий і t-розподіл Стьюдента) і обчислюємо оцінку волатильності, використовуючи вибіркове стандартне відхилення. Ми робимо це не тільки один раз, але і 10000 разів, кожен раз, коли ми розраховуємо нову гіпотетичну щоденну прибутковість за 50 років і обчислюємо відповідну оцінку волатильності. Звичайно, не всі ці оцінки будуть однаковими, але коливатимуться навколо справжньої волатильності, оскільки ми маємо лише кінцеву кількість даних. Оскільки ми знаємо справжній розподіл ймовірностей, який ми використовуємо для моделювання, ми також знаємо справжню волатильність. Щоб побачити, наскільки точно ми можемо оцінити волатильність на основі даних за 50 років, ми обчислюємо відносну похибку між оцінками волатильності та справжньою волатильністю.\n\nn_sim = 10000\n\ntrue_var = 1.\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(norm(0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:08&lt;00:00, 1125.24it/s]\n\n\n0.5039568642168273\n\n\n\nn_sim = 10000\n\nalpha = 6.0\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:13&lt;00:00, 728.17it/s]\n\n\n0.9616583563963519\n\n\n\nn_sim = 10000\n\nalpha = 4.0\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:14&lt;00:00, 700.76it/s]\n\n\n3.8082666425995932\n\n\n\nn_sim = 10000\n\nalpha = 3.7\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:14&lt;00:00, 709.36it/s]\n\n\n6.115969986022528\n\n\n\nn_sim = 10000\n\nalpha = 3.1\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:13&lt;00:00, 718.23it/s]\n\n\n38.47707859009605\n\n\nМи отримуємо наступні відносні похибки для різних розподілів та їх параметрів: - Гаусовий розподіл; відн. похиб. \\(\\approx 0.5\\%\\) - t-розподіл Стьюдента з \\(\\alpha=6.0\\) (\\(\\nu=5.0\\)); відн. похиб. \\(\\approx 1.0\\%\\) - t-розподіл Стьюдента з \\(\\alpha=4.0\\) (\\(\\nu=3.0\\)); відн. похиб. \\(\\approx 3.8\\%\\) - t-розподіл Стьюдента з \\(\\alpha=3.7\\) (\\(\\nu=2.7\\)); відн. похиб. \\(\\approx 6.1\\%\\)\nПри степеневому показнику \\(\\alpha= 3.7\\) (що відповідає \\(\\nu = 2.7\\); як ми підрахували для S&P 500, щоб пояснити Чорний понеділок), щоденна прибутковість за 50 років все ще призводить до помилки оцінки волатильності більш ніж на \\(6%\\)! Зверніть увагу, що ця велика помилка оцінки зростатиме ще швидше, коли вона наближається до \\(\\alpha = 3.0\\), оскільки дисперсія стає нескінченною для t-розподілу Стьюдента при \\(\\alpha = 3.0\\) (\\(\\nu = 2.0\\)). Виходячи з t-розподілу прибутковостей S&P 500, похибка оцінки волатильності зросла в 12 разів (!) порівняно з тим, що ми очікували б, припускаючи, що прибутковості розподілені за Гаусом.\n\n\n\n\n\n\nПримітка\n\n\n\nЧим важчі хвости розподілу прибутковостей, тим більше точок даних необхідно для досягнення необхідної достовірності оцінок певних параметрів. Пам’ятайте про це щоразу, коли ви намагаєтеся оцінити ризик інвестування у відносно новий фінансовий актив, який має дані лише за кілька років.\n\n\n\n\n10.1.9 Наслідки для моделювання та прогнозування\nТой факт, що більш високі моменти розподілу прибутковості можуть бути нескінченними і що оцінка нижчих моментів, таких як дисперсія, дестабілізується за наявності “товстих хвостів”, має суттєві наслідки для деяких найбільш використовуваних моделей у фінансах. Ми коротко розглянемо дві з них, щоб проілюструвати проблему.\n\n\n10.1.10 Марковіц і стабільність волатильності\nУ 1954 році Гаррі Марковіц отримав ступінь доктора економічних наук за роботу з теорії фінансових портфелів. Його робота стала наріжним каменем сучасної портфельної теорії і тоді це було настільки новим, що під час захисту докторської дисертації Мілтон Фрідман стверджував, що ця робота насправді не стосується галузі економіки (як пояснив Марковіц у своїй Нобелівській лекції у 1990 році). Марковіц стверджував, що при заданому рівні волатильності існує набір вагових коефіцієнтів розподілу активів у портфелі, які дають максимальну надлишкову прибутковість портфеля (порівняно з безризиковим активом). Якщо ви переглянете діапазон значень волатильності та обчислите максимальну віддачу портфеля, отримавши оптимальні ваги розподілу, Ви отримаєте набір ефективних портфелів, який також називають межею ефективності. Одна точка на цій межі в багатьох випадках представляє особливий інтерес, — це точка максимуму коефіцієнта Шарпа.\nКоефіцієнт Шарпа — це часто критикуваний, але також часто використовуваний показник для оцінки ефективності портфеля або фінансових активів загалом, таких як гедж-фонди або ETF. Він визначається як:\n\\[\nS = \\frac{\\mu - \\mu_{\\text{rf}}}{\\sigma}.\n\\]\nТут \\(\\mu\\) позначає річну прибутковість портфеля, \\(\\mu_{\\text{rf}}\\) позначає річну безризикову прибутковість, а \\(\\sigma\\) позначає річну волатильність портфеля. Припускаючи нульову безризикову ставку (\\(\\mu_{\\text{rf}} = 0\\)), коефіцієнт Шарпа можна інтерпретувати як співвідношення сигнал/шум, відомий у фізиці та інженерії: максимізуючи коефіцієнт Шарпа, ми знаходимо найкращий компромісне значення, що максимізує наші прибутковості (сигнал) та мінімізує волатильності (шум).\nЯкщо наш портфель складається з \\(n\\) фінансових активів з очікуваною прибутковістю \\(\\vec{\\mu} = (\\mu_{1},...,\\mu_{n})\\), з коваріаційною матрицею \\(\\Sigma\\), яка містить інформацію про волатильність і кореляції між активами, і з вектором ваги розподілу \\(\\vec{w} = (w_1,...,w_n)\\) (нормалізовано до одиниці: \\(\\sum_{i=1}^n w_i = 1\\)), тоді коефіцієнт Шарпа у контексті сучасної теорії портфеля можна обчислити наступним чином:\n\\[\nS(\\vec{w}) = \\frac{\\vec{w}.\\vec{\\mu}}{\\sqrt{\\vec{w}\\Sigma\\vec{w}^T}}.\n\\]\nЗверніть увагу, що з цього моменту ми завжди припускаємо, що \\(\\mu_{\\text{rf}} = 0\\).\nЩоб максимізувати прибутковість відносно волатильності та отримати портфель з максимальним коефіцієнтом Шарпа, ми змінюємо ваги розподілу, поки не знайдемо набір ваг \\(\\vec{w}^*\\) з максимальним коефіцієнтом Шарпа:\n\\[\n\\vec{w}^* = \\text{argmax}_{\\vec{w}} ~ S(\\vec{w})\n\\]\nОсновне припущення, що лежить в основі сучасної теорії портфеля, полягає в тому, що об’єднаний розподіл логарифмічних прибутковостей всіх активів у портфелі може бути повністю охарактеризований мультиваріативним розподілом Гауса. Після попередніх частин цієї лабораторної ми дуже добре розуміємо цю помилку, і сучасна теорія портфеля по праву зазнала жорсткої критики за відсутність врахування “важких хвостів”, наприклад, з боку Нассіма Талеба:\n\n\n\n\n\n\nНассім Талеб у Чорний лебідь: Про (не)ймовірне в реальному житті (ст. 277)\n\n\n\nПісля краху фондового ринку (у 1987 році) вони нагородили двох теоретиків, Гаррі Марковіца та Вільяма Шарпа, які побудували прекрасні платонівські моделі на гаусовій основі, сприяючи тому, що називається сучасною теорією портфеля. Простіше кажучи, якщо ви видалите їх гаусові припущення і розглядаєте ціни як масштабо-інваріантні, у вас залишиться гаряче повітря. Нобелівський комітет міг би протестувати моделі Шарпа і Марковіца — вони працюють як шарлатанські ліки, що продаються в Інтернеті, — але ніхто в Стокгольмі, схоже, про це не подумав.\n\n\nТут ми просто хочемо проілюструвати наслідки припущення гаусовости, змоделювавши прибутковість двох гіпотетичних акцій і спробувавши знайти ваги розподілу по максимуму Шарпа. Як ми побачимо, великі хвости в модельованих розподілах прибутковостей вносять істотну невизначеність в розрахункові ваги розподілу — прямий наслідок нестабільності дисперсії.\nСпочатку ми генеруємо випадкові числа з Гаусового розподілу, достатнього для моделювання щоденної прибутковості двох гіпотетичних акцій за 10 років. Ми присвоюємо їм різну позитивну очікувану прибутковість і волатильність. Для простоти ми припускаємо, що кореляція між двома активами є нульовою:\n\nnp.random.seed(1234)\nn_years = 10\n\nlog_ret_1 = 0.0025*norm.rvs(size=n_years*252) + 0.0005\nlog_ret_2 = 0.0050*norm.rvs(size=n_years*252) + 0.0010\n\nplt.figure()\nplt.plot(np.exp(np.cumsum(log_ret_1)), label='Акція 1')\nplt.plot(np.exp(np.cumsum(log_ret_2)), label='Акція 2')\n\nplt.ylabel('Ціна акції')\nplt.xlabel('Торговий день')\nplt.legend()\nplt.yscale('log')\nplt.show();\n\n\n\n\nЗвичайно, насправді ми не знаємо справжньої очікуваної прибутковості або значень волатильності кореляцій між активами. Таким чином, ми повинні оцінити очікувану прибутковість за середнім значенням вибірки та матрицю коваріації за матрицею коваріації вибірки. Зверніть увагу, що існують більш складні способи оцінки цих параметрів на основі історичних даних, таких як оцінки усадки для коваріаційної матриці або моделі розподілу Блека-Літтермана. Ці методи, безумовно, покращують вибіркові оцінки, але загальний недолік сучасної теорії портфеля через наявність “важких хвостів” залишається. Щоб вивчити та протестувати різні методи оптимізації портфеля, рекомендуємо розглянути бібліотеку PyPortfolioOpt, що представляє собою добре задокументований пакет Python, який реалізує як основні, так і складні методи оптимізації портфеля та дозволяє легко тестувати ці методи на реальних даних. Поки ми дотримуємося вибіркових оцінок (і використовуємо коефіцієнт 252 — середню кількість торгових днів на рік для отримання очікуваної прибутковості в річному обчисленні і коваріації за щоденними оцінками):\n\nexp_returns = np.array([np.mean(log_ret_1), np.mean(log_ret_2)])*252\ncov_matrix = np.cov([log_ret_1, log_ret_2])*252\n\nДалі ми визначаємо функцію, яка приймає ваги розподілу кандидатів, розрахункову очікувану прибутковість і коваріаційну матрицю і виводить коефіцієнт Шарпа портфеля:\n\ndef sharpe(weights, exp_returns, cov_matrix):\n    return weights.dot(exp_returns) / np.sqrt(weights.dot(cov_matrix).dot(weights.T))\n\nsharpe(np.array([0.5, 0.5]), exp_returns, cov_matrix)\n\n4.810679451759804\n\n\nЗамість того, щоб використовувати реальний оптимізатор для пошуку ваг розподілу, які максимізують коефіцієнт Шарпа, ми просто переглядаємо всі можливі комбінації ваг з інтервалом \\(1\\%\\). Звичайно, це розумний підхід лише для двох активів, і він швидко стає нерозв’язним, використовуючи більше двох активів.\n\ncandidate_weights = np.array([np.linspace(0, 1, 101), 1. - np.linspace(0, 1, 101)]).T\ncandidate_weights[:5]\n\narray([[0.  , 1.  ],\n       [0.01, 0.99],\n       [0.02, 0.98],\n       [0.03, 0.97],\n       [0.04, 0.96]])\n\n\nТепер ми можемо перебрати всі наші ваги-кандидати, обчислити коефіцієнт Шарпа портфеля для всіх з них, а потім вибрати оптимальний набір ваг.\n\ncandidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\nopt_weights = candidate_weights[np.argmax(candidate_sharpe)]\nopt_weights\n\narray([0.7, 0.3])\n\n\nЩоб отримати уявлення про те, наскільки точно ми можемо оцінити ваги розподілу на основі гіпотетичної щоденної прибутковості за 10 років, ми зараз повторюємо цей експеримент, повторно генеруємо нові випадкові прибутковості, повторно оцінюємо очікувані прибутковості та матрицю коваріації та знаходимо оптимальні ваги. Ми робимо це 10000 разів. Зверніть увагу, що ми реєструємо лише першу вагу розподілу \\(w_1\\) для кожного запуску без втрати інформації, оскільки друга вага безпосередньо випливає з нормалізації \\(w_2 = 1 - w_1\\).\n\nnp.random.seed(1234)\nopt_weight_gaussian = []\n\nfor i in tqdm(range(10000)):\n    log_ret_1 = 0.0025*norm.rvs(size=n_years*252) + 0.0005\n    log_ret_2 = 0.0050*norm.rvs(size=n_years*252) + 0.0010\n\n    exp_returns = [np.mean(log_ret_1), np.mean(log_ret_2)]\n    cov_matrix = np.cov([log_ret_1, log_ret_2])\n\n    candidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\n    opt_weights = candidate_weights[np.argmax(candidate_sharpe)]\n\n    opt_weight_gaussian.append(opt_weights[0])\n\n100%|██████████| 10000/10000 [00:07&lt;00:00, 1358.21it/s]\n\n\nНарешті, ми можемо побудувати гістограму всіх 10000 значень, які ми отримуємо для \\(w_1\\)\n\nplt.figure()\nplt.hist(opt_weight_gaussian, 10, density=True);\nplt.xlim([0, 1])\nplt.xlabel('Вага $w_1$ для портфеля з максимальним коеф. Шарпа')\nplt.ylabel('Розп. ймов.')\nplt.show();\n\n\n\n\nЯк ми бачимо, розподіл для першої акції коливається приблизно від \\(60%\\) до \\(70%\\) на основі історичних даних за 10 років. Ми можемо додатково оцінити це кількісно, обчисливши 5-й і 95-й процентилі всіх значень \\(w_1\\). Таким чином, з упевненістю в \\(90%\\) ми отримаємо розподіл між \\(61%\\) і \\(72%\\):\n\nnp.percentile(opt_weight_gaussian, 5), np.percentile(opt_weight_gaussian, 95)\n\n(0.61, 0.72)\n\n\nТепер давайте повторимо цей експеримент, але цього разу ми замінимо розподіл Гауса на розподіл Стьюдента зі степеневим показником \\(\\alpha=3.7\\) (що відповідає кількості ступенів свободи \\(\\nu=2.7\\)). Ми залишаємо очікувану прибутковість і волатильність (виміряні стандартним відхиленням розподілу) точно такими ж. Зверніть увагу, що стандартне відхилення стандартного t-розподілу визначається як \\(\\sqrt{\\frac{\\nu}{\\nu-2}}\\) в нашому випадку, тому ми повинні внести поправку на цей коефіцієнт маштабування при вилученні випадкових значень зі стандартного t-розподілу. Нижче ви можете побачити пряме порівняння розподілів Гауса, використаних у наведеному вище моделюванні, та t-розподілів із відповідним середнім значенням та стандартним відхиленням.\n\nscale_correction = np.sqrt(2.7/(2.7-2.))\n\n\nx = np.linspace(-0.025, 0.025, 10000)\n\ny1_norm = norm(loc=0.0005, scale=0.0025).pdf(x)\ny1_t = t(loc=0.0005, scale=0.0025/scale_correction, df=2.7).pdf(x)\n\ny2_norm = norm(loc=0.001, scale=0.005).pdf(x)\ny2_t = t(loc=0.001, scale=0.005/scale_correction, df=2.7).pdf(x)\n\n\nplt.figure()\n\nplt.subplot(121)\n\nplt.fill_between(x, 0., y1_norm, facecolor='C0', alpha=0.7)\nplt.fill_between(x, 0., y1_t, facecolor='C3', alpha=0.5)\nplt.ylim([0, 300])\nplt.ylabel('Розп. ймов.')\nplt.title('Акція 1')\n\nplt.subplot(122)\nplt.fill_between(x, 0., y2_norm, facecolor='C0', alpha=0.7, label='Гаус')\nplt.fill_between(x, 0., y2_t, facecolor='C3', alpha=0.5, label=\"Стьюдент\")\nplt.ylim([0, 300])\nplt.ylabel('Розп. ймов.')\nplt.title('Акція 2')\nplt.legend()\n\nplt.tight_layout()\nplt.show(); \n\n\n\n\nЯк ми можемо легко бачити, узгоджені t-розподіли Стьюдента насправді дають менші значення прибутковостей. Однак вони також дають набагато більш екстремальні значення, але ми не можемо легко побачити це, оскільки значення щільності ймовірності в хвостах дуже малі! Якщо ми перейдемо до логарифмічного масштабування вісі Oy, картина стане більш чіткою:\n\nplt.figure()\n\nplt.subplot(121)\n\nplt.plot(x, y1_norm, c='C0', lw=3)\nplt.plot(x, y1_t, c='C3', lw=3)\nplt.ylabel('Розп. ймов.')\nplt.title('Акція 1')\nplt.yscale('log')\n\nplt.subplot(122)\nplt.plot(x, y2_norm, c='C0', lw=3, label='Гаус')\nplt.plot(x, y2_t, c='C3', lw=3, label=\"Стьюдент\")\nplt.ylabel('Розп. ймов.')\nplt.title('Акція 2')\nplt.yscale('log')\nplt.legend()\n\nplt.tight_layout()\nplt.show();\n\n\n\n\nТепер ми чітко бачимо, що, наприклад, ймовірність щоденних логарифмічних прибутковостей \\(-0.02\\) для акції 1 в \\(10^{10}\\) разів вища за умови t-розподілу порівняно з відповідним розподілом Гауса! Завжди дивіться на розподіли з логарифмічною шкалою щільності ймовірності, щоб отримати чітке уявлення про хвости. Використовуючи наші узгоджені t-розподіли, ми діємо, як і раніше, і моделюємо гіпотетичні 10-річні історичні показники для наших двох некорельованих акцій. Нижче ви можете побачити приклад. Тільки при уважному розгляді ви можете помітити, що ці криві сукупної прибутковості насправді трохи більш нерівні в порівнянні з тими, які були побудовані вище з гаусовими прибутковостями. Але різниця невловима, якщо дивитися на таку “наближену” діаграму десятирічного періоду:\n\nnp.random.seed(1234)\nn_years = 10\n\nlog_ret_1 = (0.0025/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0005\nlog_ret_2 = (0.0050/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0010\n\nplt.figure()\nplt.plot(np.exp(np.cumsum(log_ret_1)), label='Акція 1')\nplt.plot(np.exp(np.cumsum(log_ret_2)), label='Акція 2')\n\nplt.ylabel('Ціна акції')\nplt.xlabel('Торговий день')\nplt.legend()\nplt.yscale('log')\nplt.show();\n\n\n\n\nНарешті, щоб візуалізувати вплив t-розподілу на оцінку ваги розподілу, ми повторюємо наш експеримент Монте-Карло з моделювання 10000 різних 10-річних історичних значень та оцінюємо ваги розподілу в кожному з цих 10000 випадків. Потім ми будуємо графік розподілу \\(w_1\\) і порівнюємо цей розподіл з розподілом, отриманим в результаті моделювання на основі Гауса.\n\nnp.random.seed(1234)\nopt_weight_t = []\n\nfor i in tqdm(range(10000)):\n    log_ret_1 = (0.0025/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0005\n    log_ret_2 = (0.0050/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0010\n\n    exp_returns = [np.mean(log_ret_1), np.mean(log_ret_2)]\n    cov_matrix = np.cov([log_ret_1, log_ret_2])\n\n    candidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\n    opt_weights = candidate_weights[np.argmax(candidate_sharpe)]\n\n    opt_weight_t.append(opt_weights[0])\n\n100%|██████████| 10000/10000 [00:09&lt;00:00, 1037.43it/s]\n\n\n\nplt.figure()\nplt.hist(opt_weight_gaussian, 10, alpha=0.8, density=True, label='Гаусові прибутковості')\nplt.hist(opt_weight_t, 20, alpha=0.8, density=True, label=\"Прибутковості Стьюдента\")\nplt.xlim([0, 1])\nplt.legend()\nplt.xlabel('Вага $w_1$ для портфеля з максимальним коеф. Шарпа')\nplt.ylabel('Розп. ймов.')\nplt.show();\n\n\n\n\n\nnp.percentile(opt_weight_t, 5), np.percentile(opt_weight_t, 95)\n\n(0.54, 0.77)\n\n\nЯк ми можемо бачити, оптимальна вага розподілу \\(w_1\\) тепер коливається між \\(54\\%\\) і \\(77\\%\\), що приблизно в 2 рази перевищує ширину надійного інтервалу для оптимізації портфеля, яка дотримувалася припущення про гаусовість! Навіть при узгодженому стандартному відхиленні гаусового і t-розподілу, нескінченний ексцес t-розподілу дестабілізує вибіркову дисперсію наших змодельованих прибутковостей і, таким чином, знижує точність наших оціночних ваг розподілу. Хоча цей приклад, безумовно, не розкриває всіх слабкостей сучасної теорії портфеля, він допомагає нам побачити, як досить абстрактні ефекти, такі як нестабільність дисперсії через надмірний ексцес, в кінцевому результаті псують широко використовувані фінансові моделі. Використання модельованої прибутковості, заснованої на гаусовому розподілі порівняно з розподілом з великим хвостом, може допомогти виявити обмеження фінансових моделей або знайти мінімальний обсяг даних, необхідний для досягнення певної достовірності, перш ніж нова модель буде фактично запущена у виробництво!\n\n\n\n\n\n\nПримітка\n\n\n\nВажкі хвости розподілу прибутковостей можуть сильно вплинути на результати усталених математичних моделей у фінансах, або збільшуючи невизначеність параметрів (як у сучасній теорії портфеля), або приводячи до збоїв моделей поза вибірки через нескінченно великі моменти. У фінансах краще екстраполювати частоту екстремальних подій і готуватися до них за допомогою конкретних інструментів геджування, ніж намагатися передбачити майбутню прибутковість за допомогою моделей часових рядів!\n\n\n\n\n10.1.11 Розподіл Леві\nАльфа-стабільний розподіл є узагальненням розподілу Гауса, яке цінується за те, що воно тягне за собою жирні хвости. З цієї причини він широко використовується при обробці сигналів, наприклад, у медицині чи фінансах.\nЗагальний клас стабільних розподілів був введений і отримав цю назву французьким математиком Полем Леві на початку 1920-х років.\nРаніше ця тема привертала лише помірну увагу провідних експертів, хоча були і ентузіасти, з яких можна згадати російського математика Олександра Яковича Хінчіна. Натхненням для Леві стало бажання узагальнити відому центральну граничну теорему, згідно з якою будь-який розподіл ймовірностей з кінцевою дисперсією збігається до гауссового розподілу.\nСтабільні розподіли мають три виняткові властивості, які можна коротко підсумувати, заявляючи, що вони:\n\nінваріантні при додаванні;\nмають власну область збіжності;\nдозволяють канонічну форму характеристичної функції.\n\n\n10.1.11.1 Інваріантність при додавані\nВипадкова величина \\(X\\) підпорядковується стабільному розподілу \\(P(x) = \\text{Prob}\\{X \\leq x\\}\\) якщо для будь-якого \\(n \\geq 2\\) існують додатнє значення \\(c_{n}\\) та дійсне значення \\(d_{n}\\) такі, що\n\\[\nX_1 + X_2 + ... + X_n \\stackrel{d}{=} c_{n}X + d_{n},\n\\]\nде \\(X_1, X_2, ..., X_n\\) характеризуються як незалежні, ідентично розподілені випадкові величини. Також \\(\\stackrel{d}{=}\\) позначає рівність розподілів, тобто, випадкові величини з обох сторін мають однаковий розподіл ймовірностей.\nЗагалом, сума незалежних, ідентично розподілених випадкових величин результує у випадкову величину з іншим розподілом. Однак, для випадкових величин, що характеризуються стабільним розподілом, сума ідентично розподілених випадкових величин до величини такого самого розподілу. У цьому випадку результуюча випадкова величина (розподіл) може відрізнятися від попередніх величин характерним масштабом (\\(c_{n}\\)) та зміщенням (\\(d_{n}\\)). Якщо \\(d_{n} = 0\\), розподіл називається строго стабільним.\nВідомо, що нормована константа \\(c_{n}\\) має вид\n\\[\nc_{n} = n^{1/\\alpha} \\, \\text{with} \\, 0 &lt; \\alpha \\leq 2.\n\\]\nПараметр \\(\\alpha\\) має назву характеристична експонента або індекс стабільності стабільного розподілу.\nПопередня теорема має альтернативну версію, що включає в суму лише дві випадкові величини. Випадкова величина \\(X\\) підпорядковується стабільному розподілу, якщо для будь-яких позитивних значень \\(A\\) та \\(B\\) існує позитивне число \\(C\\) та дійсне число \\(D\\) такі, що\n\\[\nA X_1 + B X_2 \\stackrel{d}{=} C X + D,\n\\]\nде \\(X_1\\) та \\(X_2\\) незалежні копії \\(X\\). Тоді існує значення \\(\\alpha \\in (0, 2]\\) при яких значення \\(C\\) задовільняє рівність \\(C^{\\alpha} = A^{\\alpha} + B^{\\alpha}\\).\nДля строго стабільних розподілів \\(D = 0\\). Це означає, що всі лінійні комбінації випадкових незалежних, ідентично розподілених випадкових величин, що підкоряються строго стабільному розподілу, результують у випадкову величину з одним і тим же типом розподілу.\nСтабільний розподіл вважається симетричним, якщо випадкова величина \\(-X\\) має такий самий тип розподілу. Симетричний стабільний розподіл обов’язково строго стабільний.\nОскільки аналітичний вираз функції щільності ймовірностей для стабільного розподілу невідома, за винятком кількох членів стабільного сімейства, більшість традиційних методів математичної статистики не можуть бути використані. Відбовідними вийнятками є\n\nГаусовий розподіл \\(S_2(0,\\mu, \\sigma) = \\mathcal{N}(\\mu, 2\\sigma^2)\\). Гаусовий розподіл є спеціальним випадком стабільного розподілу при \\(\\alpha = 2\\) так що \\(\\mathcal{N}(\\mu, \\sigma) = S(2,0,\\mu, \\frac{\\sigma}{\\sqrt{2}})\\), де \\(\\mu\\) позначає середнє значення нормального розподілу, а \\(\\sigma\\) — це стандартне відхилення. Функція щільності ймовірностей має вид\n\n\\[\n\\frac{1}{\\sigma\\sqrt{2\\pi}}\\text{exp}^{-(x-\\mu)^{2}/2\\sigma^{2}}.\n\\]\n\nРозподіл Коші. Розподіл Коші — це ще одне представлення стабільного розподілу при \\(\\alpha = 1\\) та \\(\\beta = 0\\) такими, що \\(Cauchy(\\delta, \\gamma) = S_1(1,0,\\gamma,\\delta)\\), де \\(\\gamma\\) — це параметр масштабування, а \\(\\delta\\) — це параметр зсуву розподілу Коші. Функція щільності ймовірностей представлена як:\n\n\\[\n\\frac{\\gamma}{\\pi((x-\\delta)^2 + \\gamma^2)}, \\, -\\infty &lt; x &lt; \\infty.\n\\]\n\nРозподіл Леві також є вийнятком із класу стабільних розподілів, де \\(\\alpha = 0.5\\) і \\(\\beta = 1\\). Іншими словами, \\(Levy(\\delta, \\gamma) = S_{1/2}(0.5, 1, \\gamma, \\delta)\\). Функція щільності ймовірностей має вид\n\n\\[\n\\sqrt{\\frac{\\gamma}{2\\pi}}\\frac{1}{(x-\\delta)^{3/2}}\\exp{\\left[ \\frac{-\\gamma}{2(x-\\delta)} \\right]}, \\, \\delta &lt; x &lt; \\infty.\n\\]\nЯкщо \\(X \\sim S_{1/2}(0.5, 1, \\gamma, \\delta)\\), тоді для \\(x &gt; 0\\)\n\\[\nP(X \\leq x) = 2 \\left( 1 - \\phi \\left( \\sqrt{\\frac{\\gamma}{x}} \\right) \\right),\n\\]\nде \\(\\phi\\) позначає кумулятивну функцію нормального розподілу.\n\n\n10.1.11.2 Область збіжності\nІнше (еквівалентне) визначення стверджує, що стабільні розподіли — це єдині розподіли, які можна отримати при границі нормалізованих сум незалежних, ідентично розподілених випадкових величин. Кажуть, що випадкова величина \\(X\\) має область збіжності, тобто якщо існує послідовність незалежних, ідентично розподілених випадкових величин \\(Y_1, Y_2, ...\\) і послідовності позитивних чисел \\({\\gamma_n}\\) і дійсних чисел \\({\\delta_n}\\) таких, що\n\\[  \n\\frac{Y_1 + Y_2 + ... + Y_n}{\\gamma_n} + \\delta_n \\stackrel{d}{\\Rightarrow} X.  \n\\]\nКоли \\(X\\) це гаусова випадкова величина, а \\(Y_i's\\) є незалежними, ідентично розподіленими випадковими величинами з визначенною дисперсією, тоді рівняння вище є твердженням звичайної центральної граничної теореми. Область збіжності \\(X\\) вважається нормальною коли \\(\\gamma_n = n^{1/\\alpha}\\).\n\n\n10.1.11.3 Канонічні представлення характеристичної функції\nЧотири параметри використовуються для опису випадкової величини, що слідує за стабільним розподілом: \\(X \\sim S(\\alpha, \\beta, \\mu, \\gamma)\\). Параметр \\(\\alpha \\in (0, 2]\\) — це той, який нас найбільше зацікавить. Цей параметр визначає товщину хвостів. Параметр \\(\\beta \\in [-1, 1]\\) є параметром асиметрії. Останні два параметри позначають розташування \\((\\mu \\in \\Re)\\) і масштаб \\((\\gamma &gt; 0)\\) розподілу. Альфа-стабільний розподіл немає жодного аналітичного виразу для щільності ймовірності \\(X\\), але ми можемо охарактеризувати його характеристичною функцією:\n\\[\n    \\begin{split}\n    \\phi(t) &= E\\left[\\exp(itX)\\right] \\\\\n            &=\n        \\begin{cases}\n            \\exp\\left(i \\mu t - \\gamma^{\\alpha}|t|^{\\alpha} \\left[1-i\\beta\\text{sign}(t)\\tan{\\frac{\\pi\\alpha}{2}}\\right]\\right) & \\text{if} \\, \\alpha \\neq 1 \\\\ \\exp\\left(i \\mu t - \\gamma|t| \\left[1+i\\beta\\text{sign}(t)\\frac{2}{\\pi}\\log{|t|}\\right]\\right) & \\text{if} \\, \\alpha = 1.\n        \\end{cases}\n    \\end{split}\n\\]\nМи могли б використовувати перетворення Фур’є, щоб отримати функцію щільності розподілу ймовірностей з характеристичної функції:\n\\[\nf(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^{+\\infty} \\phi(t) \\cdot \\exp(-itX) dt.\n\\]\nАле наведена вище параметризація не є повністю задовільною, оскільки функція щільності розподілу ймовірностей не є безперервною, зокрема, при \\(\\alpha = 1\\). Дійсно, коли \\(\\beta &gt; 0\\), щільність розподілу зміщується вправо, коли \\(\\alpha &lt; 1\\) і вліво, коли \\(\\alpha &gt; 1\\), зі зсувом в сторону \\(+\\infty\\) (відповідно \\(-\\infty\\)), коли \\(\\alpha\\) прагне до 1. Таким чином, для прикладного аналізу даних та інтерпретації коефіцієнтів слід уникати такої параметризації.\nІснує багато параметризацій для стабільних законів, і ці різні параметризації викликали велику плутанину. Різноманітність параметризацій обумовлена поєднанням історичної еволюції плюс численними проблемами, які були проаналізовані за допомогою спеціалізованих форм стабільного розподілу. Є вагомі причини використовувати різні параметризації в різних ситуаціях. Якщо в пріорітеті чисельні розрахунки, робота із даними, то краще використовувати одну параметризацію. Якщо бажані прості алгебраїчні властивості розподілу або аналітичні властивості строго стійких законів, то краще розглянути декілька параметризацій. Нолан запропонував використовувати параметризацію Золотарьова (M), яку також часто позначають як \\(S^{0}\\). Характеристична функція, що відповідає \\(X \\sim S^{0}(\\alpha, \\beta, \\mu_{0}, \\gamma)\\), дорівнює:\n\\[\n    \\begin{split}\n    \\phi(t) &= E\\left[\\exp(itX)\\right] \\\\\n            &=\n        \\begin{cases}\n            \\exp\\left(i \\mu_{0} t - \\gamma^{\\alpha}|t|^{\\alpha} \\left[1+i\\beta\\text{sign}(t)\\tan{\\frac{\\pi\\alpha}{2}}\\left( \\gamma^{1-\\alpha}|t|^{1-\\alpha}-1 \\right)\\right]\\right) & \\text{if} \\, \\alpha \\neq 1 \\\\ \\exp\\left(i \\mu_{0} t - \\gamma|t| \\left[1+i\\beta\\text{sign}(t)\\frac{2}{\\pi}\\left(\\log{|t|} + \\log{\\gamma}\\right) \\right]\\right) & \\text{if} \\, \\alpha = 1.\n        \\end{cases}\n    \\end{split}\n\\]\nЦя альтернативна параметризація недалека від зазначеної напочатку. Єдина відмінність стосується параметра \\(\\mu\\), який у даній параметризації коригує зсув для значень \\(\\alpha\\) близьких до 1:\n\\[\n\\mu_{0} = \\begin{cases} \\mu + \\beta\\gamma\\tan{\\frac{\\pi\\alpha}{2}} & \\text{if} \\, \\alpha \\neq 1 \\\\ \\mu + \\beta\\frac{2}{\\pi}\\gamma\\log{\\gamma} & \\text{if} \\, \\alpha = 1. \\end{cases}\n\\]\n\n\n10.1.11.4 Метод розрахунку параметрів \\(\\alpha\\)-стабільного розподілу\nЧислені методи, такі як метод Маккаллоха, заснований на квантилях, і метод оцінки максимальної правдоподібності були розроблені в результаті відсутності аналітичних рішень. Припустимо, що \\(\\text{X} = (X_1, ... , X_T)\\) вектор, що складається з \\(T\\) незалежних ідентично розподілених випадкових величин із розподілу Парето, і також \\(x \\sim S_{\\alpha}(\\alpha, \\beta, \\delta, \\gamma)\\). Визначивши \\(\\theta = (\\alpha, \\beta, \\delta, \\gamma)\\), Митник, Доганоглу та Ченайо розробили алогритм максимальної правдоподібності і показали, що \\(\\theta\\) можна розрахувати, максимізуючи функцію логарифмічної правдоподібності\n\\[\nl(\\theta, x) = \\sum_{i=1}^{T}\\log{f(x_{i}, \\theta)}.\n\\]\nДюмушель застосував метод максимальної правдоподібності до стабільного розподілу і визначив функцію правдоподібності наступним чином:\n\\[\nL(\\theta) = \\prod_{k=1}^{n}S_{\\alpha,\\beta} \\left( \\frac{X_{k} - \\delta}{\\gamma} \\right) \\Big/ \\gamma,\n\\]\nде \\(\\theta = (\\alpha, \\beta, \\delta, \\gamma)\\) опираючись на \\(x = (x_1, ... , x_n)\\) для розміру вибірки \\(n\\)."
  },
  {
    "objectID": "lab_12.html#хід-роботи",
    "href": "lab_12.html#хід-роботи",
    "title": "10  Лабораторна робота № 12",
    "section": "10.2 Хід роботи",
    "text": "10.2 Хід роботи\n\n10.2.1 Зчитування з Yahoo Finance\n\n# встановлення назви індексу\nsymbol = \"^BSESN\" \n\n# встановлення діапазону з яким будемо працювати\nstart = \"1980-01-01\"\nend = \"2022-11-07\"\n\n# завантаження даних з Yahoo\ndata = yf.download(symbol, start, end)\ntime_ser = data['Adj Close'].copy()\n\n# підпис по вісі Ох \nxlabel = 'time, days'\n\n# підпис по вісі Оу\nylabel = symbol                       \n\n# збереження результату в текстовий документ \nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser_1.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n10.2.2 Виведення графіку досліджуваного ряду\n\ntime_ser.plot(figsize=(8,6), xlabel=xlabel, ylabel=fr\"{symbol}\")\nplt.savefig('ts1.jpg', bbox_inches=\"tight\")\nplt.show();\n\n\n\n\n\n\n10.2.3 Побудова розподілу Леві та розрахунок параметрів для всього ряду\n\nret_type = 4\nfor_levy = time_ser.copy()\n\nif ret_type == 1:\n    pass\nelif ret_type == 2:\n    for_levy = for_levy.diff()\nelif ret_type == 3:\n    for_levy = for_levy.pct_change()\nelif ret_type == 4:\n    for_levy = for_levy.pct_change()\n    for_levy -= for_levy.mean()\n    for_levy /= for_levy.std()\nelif ret_type == 5: \n    for_levy = for_levy.pct_change()\n    for_levy -= for_levy.mean()\n    for_levy /= for_levy.std()\n    for_levy = for_levy.abs()\nelif ret_type == 6:\n    for_levy -= for_levy.mean()\n    for_levy /= for_levy.std()\n\nfor_levy = for_levy.dropna().values\n\n\n\n10.2.4 Підганяємо розподіл Леві та Гаусовий для порівняння\n\nparams = levy.fit_levy(for_levy)\nmean, std = norm.fit(for_levy)\n\n\n\n10.2.5 Отримуємо параметри розподілу Леві у відповідності до однієї із параметризацій, що пропонує пакет \\(levy\\)\n\nalpha, beta, mu, sigma = params[0].get('1')\n\n\n\n10.2.6 Будуємо теоретичні та емпіричні розподіли\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10))\n\nax[0].hist(for_levy, bins=50, density=True, alpha=0.6, color='b')\n\nxmin = for_levy.min()\nxmax = for_levy.max()\n\nx = np.linspace(xmin, xmax, len(for_levy))\npdf = levy.levy(x, alpha, beta, mu, sigma)\npdf_norm = norm.pdf(x, mean, std)\n\nfig.suptitle(fr'Теоретичні та емпіричні $\\alpha$-стабільні розподіли для {symbol}', fontsize=20)\n\nax[0].plot(x, pdf, 'k', linewidth=2)\nax[0].plot(x, pdf_norm, 'r', linewidth=2)\nax[0].set_yscale('log')\nax[0].set_xlabel(r'$ x $')\nax[0].set_ylabel(r'$ f_{\\alpha}(x), \\, \\mathrm{ePDF}$')\n\nax[1].hist(for_levy, bins=50, density=True, alpha=0.6, color='g')\nax[1].plot(x, pdf, 'k', linewidth=2)\nax[1].plot(x, pdf_norm, 'r', linewidth=2)\nax[1].set_xlabel(r'$ x $')\nax[1].set_ylabel(r'$ f_{\\alpha}(x), \\, \\mathrm{ePDF}$')\n\n\nplt.show();\n\n\n\n\nРисунок 10.1: Теоретичні та емпіричні альфа-стабільні функції щільності ймовірностей\n\n\n\n\n\n\n10.2.7 Виводимо параметри розподілу Леві для заданого індексу\n\nprint(fr\"Параматери alpha = {alpha:.2f}, beta = {beta:.2f}, mu = {mu:.2f}, sigma = {sigma:.2f}\")\n\nПараматери alpha = 1.63, beta = -0.11, mu = -0.01, sigma = 0.53\n\n\nДля досліджуваного індексу бачимо, що параметр \\(\\alpha &lt; 2.0\\) та \\(\\beta &lt; 0\\), що вказує на відхилення розподілу даного індексу від нормального. Тобто, для даного ряду переважаючими є кризові явища, на що вказують важкі хвости розподілу. Із порівняльного аналізу Гауссового та Леві розподілів бачимо, що хвости нормального розподілу значно недооцінюють ймовірність появи кризових явищ чого, наприклад, не скажешь про альфа-стабільний розподіл. Взявши логарифм значень ймовірності по осі \\(Oy\\) ми можемо спостерігати, що, наприклад, недооцінка негативних прибутковостей Гауссовим розподілом, у порівнянні з альфа-стабільним, складає \\(\\approx 10^{15}\\) порядків. Для позитивних прибутковостей, що перевищують значення \\(+10\\sigma\\) недооцінка Гауссовим розподілом складає \\(\\approx 10^{27}\\) порядків. Теоретичне значення альфа-стабільного розподілу достатньо точно враховує важкі хвости емпіричного розподілу, що також виражається високим ексцесом розподілу. Також варто зазначити, що коефіцієнт асиметрії \\(\\beta\\) вказує на невеличке зміщення розподілу в ліву сторону, що також демонструє переважання кризових явищ.\n\n\n10.2.8 Дослідження поведінки альфа-стабільного розподілу Леві\n\nx = np.arange(-5, 5, .01)\nbeta_1 = 0\nmu = 0 \nsigm = 1 \n\nbeta_2 = 1.0\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 10))\n\nax[0][0].plot(x, levy.levy(x, 0.5, beta_1, mu, sigm), label = r\"$ \\alpha=0.5 $\")\nax[0][0].plot(x, levy.levy(x, 0.75, beta_1, mu, sigm), label = r\"$ \\alpha=0.75 $\")\nax[0][0].plot(x, levy.levy(x, 1.0, beta_1, mu, sigm), label = r\"$ \\alpha=1.0 $\")\nax[0][0].plot(x, levy.levy(x, 1.25, beta_1, mu, sigm), label = r\"$ \\alpha=1.25 $\")\nax[0][0].plot(x, levy.levy(x, 1.5, beta_1, mu, sigm), label = r\"$ \\alpha=1.5 $\")\n\nax[0][0].set_title(r\"Симетричні $\\alpha$-стабільні розподіли, $ \\beta = 0 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[0][0].legend(fontsize=20)\n\nax[0][0].set_xlabel(r\"$ x $\")\nax[0][0].set_ylabel(r\"$ f_{\\alpha}(x) $\")\n\n\nax[0][1].plot(x, levy.levy(x, 0.5, beta_2, mu, sigm), label = r\"$ \\alpha=0.5 $\")\nax[0][1].plot(x, levy.levy(x, 0.75, beta_2, mu, sigm), label = r\"$ \\alpha=0.75 $\")\nax[0][1].plot(x, levy.levy(x, 1.0, beta_2, mu, sigm), label = r\"$ \\alpha=1.0 $\")\nax[0][1].plot(x, levy.levy(x, 1.25, beta_2, mu, sigm), label = r\"$ \\alpha=1.25 $\")\nax[0][1].plot(x, levy.levy(x, 1.5, beta_2, mu, sigm), label = r\"$ \\alpha=1.5 $\")\n\nax[0][1].set_title(r\"Зміщенні $\\alpha$-стабільні розподіли, $ \\beta = 1 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[0][1].legend(fontsize=20)\n\nax[0][1].set_xlabel(r\"$ x $\")\nax[0][1].set_ylabel(r\"$ f_{\\alpha}(x) $\")\n\n\nax[1][0].plot(x, levy.levy(x, 0.5, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=0.5 $\")\nax[1][0].plot(x, levy.levy(x, 0.75, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=0.75 $\")\nax[1][0].plot(x, levy.levy(x, 1.0, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.0 $\")\nax[1][0].plot(x, levy.levy(x, 1.25, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.25 $\")\nax[1][0].plot(x, levy.levy(x, 1.5, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.5 $\")\n\nax[1][0].set_title(r\"Симетричні $\\alpha$-стабільні розподіли, $ \\beta = 0 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[1][0].legend(fontsize=20, loc=\"lower right\")\n\nax[1][0].set_xlabel(r\"$ x $\")\nax[1][0].set_ylabel(r\"$ F_{\\alpha}(x) $\")\n\nax[1][1].plot(x, levy.levy(x, 0.5, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=0.5 $\")\nax[1][1].plot(x, levy.levy(x, 0.75, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=0.75 $\")\nax[1][1].plot(x, levy.levy(x, 1.0, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.0 $\")\nax[1][1].plot(x, levy.levy(x, 1.25, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.25 $\")\nax[1][1].plot(x, levy.levy(x, 1.5, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.5 $\")\n\nax[1][1].set_title(r\"Зміщенні $\\alpha$-стабільні розподіли, $\\beta = 1$, $\\mu = 0$, $\\sigma = 1$\", y=1.03, fontsize=20)\n\nax[1][1].legend(fontsize=20, loc=\"lower right\")\n\nax[1][1].set_xlabel(r\"$ x $\")\nax[1][1].set_ylabel(r\"$ F_{\\alpha}(x) $\")\n\nfig.tight_layout()\nplt.show();\n\n\n\n\nРисунок 10.2: Залежність функції щільності ймовірностей альфа-стабільного розподілу Леві та кумулятивної функції щільності від різних значень параметрів розподілу\n\n\n\n\n\n\n10.2.9 Задання ширини вікна та кроку\nУ даному блоці оберемо тип ряду для якого і виконуватимуться розрахунки. Перед нами представлено 6 варіантів представлення часового ряду. Виконуватимемо подальші обчислення для стандартизованих прибутковостей, оскільки згідно багатьом роботам було показано, що розподіл прибутковостей тих же самих фінансових активів (фондових індексів, валютних, криптовалют, тощо) виходить за межі нормального Гауссового розподілу. Покажемо це у даному ноутбуці та застосуємо альфа-стабільний розподіл Леві для кращого моделювання складних систем та передчасної ідентифікації кризових явищ.\n\nwindow = 250 # ширина сковзного вікна\ntstep = 1 # крок\n\nret_type = 4 # тип ряду: 1 - вихідний, 2 - детрендований (різниця між теперішнім значенням та попереднім)\n                        # 3 - стандартизовані прибутковості, 4 - стандартизований ряд, 5 - абсолютні значення (волатильність)\n                        # 6 - стандартизований вихідний ряд\n        \nlength = len(time_ser)\n\n\n\n10.2.10 Розраховуємо параметри Леві, використовуючи алгоритм сковзного вікна\n\nalpha = []\nbeta = []\nmu = []\nsigma = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm.diff()\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5: \n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        fragm = fragm.abs()\n    elif ret_type == 6:\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values \n    \n    params = levy.fit_levy(fragm)\n    \n    a, b, m, s = params[0].get('0')\n    \n    alpha.append(a)\n    beta.append(b)\n    mu.append(m)\n    sigma.append(s)\n\n100%|██████████| 5995/5995 [24:13&lt;00:00,  4.12it/s]\n\n\n\n\n10.2.11 Зберігаємо значення у текстовому документі\n\nnp.savetxt(f\"alpha_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", alpha)\nnp.savetxt(f\"beta_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", beta)\nnp.savetxt(f\"mu_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", mu)\nnp.savetxt(f\"sigma_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", sigma)\n\n\n\n10.2.12 Визначимо функцію для побудови парних графіків\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n\n10.2.13 Виводимо динаміку показника стабільності \\(\\alpha\\)\n\nmeasure_label = r'$\\alpha$'\nfile_name = f\"alpha_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alpha, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 10.3: Динаміка фондового індексу BSESN та показника стабільності\n\n\n\n\nПараметр \\(\\alpha\\) (індекс стабільності хвостів розподілу) починає спадати у (перед)кризовий період, що робить його індикатором(-передвісником) кризових явищ. Під час криз у розподілі прибутковостей зростає ексцес, а самі хвости стають важчими, на що даний показник реагує передчасно.\nНаступним розглянемо параметр асиметрії \\(\\beta\\):\n\n\n10.2.14 Виводимо динаміку індексу асиметрії \\(\\beta\\)\n\nmeasure_label = r'$\\beta$'\nfile_name = f\"beta_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          beta, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 10.4: Динаміка фондового індексу BSESN та показника асиметрії\n\n\n\n\nДинаміка даної міри виглядає набагато хаотичніше у порівнянні з індексом стабільності. Для представлених результатів можна зробити наступний висновок: у передкризовий період даний показник має зростати, вказуючи на значну правосторонню асиметрію розподілу прибутковостей (переважання позитивних прибутковостей). Для кризового періоду цей показник має спадати, вказуючи на домінацію негативних прибутковостей (лівостороння асиметрія розподілу). Даний показник важко розглядати у якості надійного індикатора, оскільки його коливання представляються значними навіть при незначних падіннях представленого фондового індексу.\nНаступним розглянемо індекс розташування (зміщення) альфа-стабільного розподілу:\n\n\n10.2.15 Виводимо динаміку параметру зміщення \\(\\mu\\)\n\nmeasure_label = r'$\\mu$'\nfile_name = f\"mu_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          mu, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 10.5: Динаміка фондового індексу BSESN та показника зміщення\n\n\n\n\nПоказник розташування \\(\\mu\\) потроху спадає у кризовий період, демонструючи зміщення розподілу в сторону негативних прибутковостей. Тим не менш, цей розподіл представляєть настільки ж хаотичним як і показник асиметрії \\(\\beta\\).\nОстанні рисунки демонструють динаміку показника масштабування розподілу \\(\\sigma\\):\n\n\n10.2.16 Виводимо динаміку параметра масштабу \\(\\sigma\\)\n\nmeasure_label = r'$\\sigma$'\nfile_name = f\"sigma_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          sigma, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 10.6: Динаміка фондового індексу BSESN та показника масштабу\n\n\n\n\nІз представлених результатів видно, що даний показник спадає у (перед)кризовий період, вказуючи на зменшення масштабу (форми) альфа-стабільного розподілу Леві.\nОтже, з усіх 4-ох показників, показник стабільності \\(\\alpha\\) є найкращим для ідентифікації кризових явищ та побудови надійних стратегій ризик-менеджменту."
  },
  {
    "objectID": "lab_12.html#висновок",
    "href": "lab_12.html#висновок",
    "title": "10  Лабораторна робота № 12",
    "section": "10.3 Висновок",
    "text": "10.3 Висновок\nСтабільні розподіли — захоплюючий і плідний об’єкт досліджень в теорії ймовірностей; більше того, в даний час вони мають велику цінність при моделюванні складних процесів у фізиці, астрономії, економіці, теорії комунікацій, тощо.\nУ даній роботі було представлено теоретичні та чисельні обгрунтування в сторону альфа-стабільного розподілу Леві в якості практичної моделі для кращого розуміння та передбачення кризових явищ у складних системах.\nТут ми представили розрахунки як для усього ряду, так і для його підфрагментів, використовуючи алгоритм ковзного вікна. Виходячи з усього ряду прибутковостей, видно, що хвости їх розподілу далеко виходять за межі Гаусового. Найкраще емпіричний розподіл прибутковостей збігається саме з теоретичним альфа-стабільним розподілом Леві.\nВикористовуючи алгоритм сковзного вікна, ми побачили, що параметри альфа-стабільного розподілу змінюються з часом.\n\n\n\n\n\n\nЛітература для подальшого вивчення степеневих розподілів та важких хвостів\n\n\n\nNassim Taleb’s Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications (вільно доступна за посиланням)."
  },
  {
    "objectID": "appa.html",
    "href": "appa.html",
    "title": "Додаток A — Інструкція зі встановлення Anaconda Navigator",
    "section": "",
    "text": "Відвідайте сторінку Anaconda за наступним посиланням. Перейшовши, ви маєте побачити наступне зображення:\n\n\n\n\n\n\n\nНатискаємо на кнопку Download:\n\n\n\n\n\n\n\nЯкщо ви набиратимете назву anaconda у пошуковому рядку, тоді потрібно буде перейти за наступним посиланням, що має з’явитися найпершим:\n\n\n\n\n\n\nПерейшовши по виділеному посиланню, ви маєте побачити наступну сторінку:\n\n\n\n\n\nТак само як і в пункті 2, потрібно натиснути на кнопку Download, щоб розпочати встановлення.\n\nЗ’явиться вікно наступного виду, що запропонує зберегти встановлювальний файл там, де це потрібно:\n\n\n\n\n\n\n\nНатискаємо на клавішу Next:\n\n\n\n\n\n\n\nУважно читаємо ліцензійну угоду та натискаємо клавішу I Agree, якщо угода вас задовольняє або вам потрібно скласти залік/іспит:\n\n\n\n\n\n\n\nДалі, якщо ви не перебуваєте у команді розробників, і ви єдина людина, хто буде користуватися Anaconda — Just me ваш вибір:\n\n\n\n\n\n\nНатискаємо Next.\n\nНа наступному кроці нам пропонується обрати папку, до якої буде встановлено дистрибутив Anaconda. Бажано, щоб шлях до папки не містив кирилиці. Можна залишити шлях за замовчуванням. Ми натиснемо на Browse. Далі, обираємо зручний для нас диск та натискаємо Создать папку. Таким чином ми створимо в нашому диску папку з назвою anaconda до якої і буде завантажено Anaconda.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nНаступним кроком буде обрання середовища змінних (the environment variables).\nЯкщо ви встановлюєте Python вперше, відмітимо Add Anaconda to my PATH environment variable. Окрім всього, це дасть вам можливість використовувати Anaconda в командному рядку (або Git bash, cmder, powershell і т.д.).\nЯкщо ви вже маєте Python на своєму комп’ютері, тоді прапорець не варто відмічати. Ви матимете запускати Anaconda Navigator або Anaconda Command Prompt (розташований в меню Пуск, у розділі Anaconda) коли вам потрібно буде запускати Anaconda (ви завжди матимете можливість додати Anaconda до свого шляху пізніше, якщо не встановите цей прапорець).\nНа представленій локальній машині нам не треба відмічати прапорець.\n\n\n\n\n\n\n\nРекомендований підхід\n\n\n\n\n\n\n\nАльтернативний\n\n\n\n\n\n\nНатискаємо Install і чекаємо завершення:\n\n\n\n\n\n\n\n\n\n\n\n\nНатискаємо Next аж до вікна з подякою за встановлення Anaconda і після натискаємо Finish:\n\n\n\n\n\n\n\nНа наступному кроці потрібно перейти в меню Пуск і знайти папку під назвою Anaconda:\n\n\n\n\n\n\n\nРозгорнувши папку можна побачити цікаві для нас іконки:\n\n\n\n\n\n\nНа зазначеній вище фотографії можна бачити декілька ярликів Anaconda Navigator, Anaconda Powershell Promt, Anaconda Promt та Jupyter Notebook з різними найменуваннями anaconda та anaconda3. Це тому, що на представленій локальній машині попередньо було встановлено Anaconda, але на іншому локальному диску. Якщо ви встановлюєте Anaconda вперше чи перевстановлюєте, тоді ви матимете у 2 рази менше ярликів.\n\nНатиснувши на Anaconda Navigator, потрапляємо до середовища, що пропонує різноманітні інструменти для аналізу даних. Для подальшої роботи нам знадобиться лише Jupyter Notebook:\n\n\n\n\n\n\nНатискаємо на Launch та потрапляємо до середовища Jupyter (кореневої папки до якої було встановлено anaconda):\n\n\n\n\n\nУ верхньому лівому кутку буде знаходитись значок New. Спочатку натискаємо на нього і потім на Python 3. Має створитися відповідний Notebook у якому можна писати код:\n\n\nОкрім цього, в меню пуск можна натиснути на значок Jupyter Notebook та одразу перейти до роботи:\n\n\n\n\n\n\n\nНатиснувши на Anaconda Powershell Promt або Anaconda Promt можна перейти до командного рядка представленого дистрибутивом Anaconda. З його допомогою можна докачати необхідні модулі або запустити необхідний інструмент, що представляє Anaconda. Запустимо Jupyter Notebook за допомогою команди jupyter notebook:"
  },
  {
    "objectID": "appb.html#коментарі-коду",
    "href": "appb.html#коментарі-коду",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.1 Коментарі коду",
    "text": "B.1 Коментарі коду\nКоментар — це примітка, зроблена програмістом у вихідному коді програми. Його мета — прояснити вихідний код і полегшити людям відстеження того, що відбувається. Все, що міститься в коментарі, зазвичай ігнорується при фактичному запуску коду, що робить коментарі корисними для включення пояснень і міркувань, а також для видалення певних рядків коду, в яких ви можете бути не впевнені. Коментарі в Python створюються за допомогою символу решітуи (# вставити текст тут). Включення # у рядок коду коментує все, що слідує за ним.\n\n# print(\"Привіт усім\")\n# Це коментар\n# Ці рядки коду не змінять жодних значень\n# все, що слідує за першим #, не виконується як код\n\nВи можете побачити текст, укладений у потрійні лапки (\"\"\" вставте текст тут \"\"\"). Такий синтаксис представлятиме багаторядкове коментування, але це не зовсім точно. Це особливий тип string (тип даних, який ми розглянемо), який називаєтьсяdocstring, який використовується для пояснення призначення функції.\n\n\"\"\" This is a special string \"\"\"\n\n' This is a special string '\n\n\nПереконайтеся, що Ви прочитали коментарі в кожній комірці коду (якщо вони там є). Вони нададуть більш детальні пояснення того, що відбувається в режимі реального часу, коли ви переглядаєте кожен рядок коду."
  },
  {
    "objectID": "appb.html#змінна",
    "href": "appb.html#змінна",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.2 Змінна",
    "text": "B.2 Змінна\nЗмінні надають імена для значень. Якщо ви хочете зберегти значення для подальшого або повторного використання, ви присвоюєте значенню ім’я, зберігаючи вміст у змінній. Змінні в програмуванні працюють принципово аналогічно змінним в алгебрі, але в Python вони можуть приймати різні типи даних.\nОсновними типами змінних, які ми розглянемо в цьому розділі, є цілі числа, числа з плаваючою комою, логічні значення та рядки.\nЦіле число у програмуванні - це те саме, що і в математиці, кругле число без значень після десяткової коми. Ми використовуємо вбудовану функцію print() тут для відображення значень наших змінних, а також їх типів!\n\nmy_integer = 50\nprint(my_integer, type(my_integer))\n\n50 &lt;class 'int'&gt;\n\n\nЗмінні, незалежно від типу, призначаються за допомогою одного знака рівності (=). Змінні чутливі до регістру, тому будь-які зміни в зміні заголовних літер імені змінної будуть посилатися повністю на іншу змінну.\n\none = 1\nprint(one)\n\n1\n\n\nЧисло з плаваючою комою або float - це вигадлива назва дійсного числа (знову ж таки, як у математиці). Щоб визначити float, нам потрібно або включити десяткову крапку, або вказати, що значення є float.\n\nmy_float = 1.0\nprint(my_float, type(my_float))\nmy_float = float(1)\nprint(my_float, type(my_float))\n\n1.0 &lt;class 'float'&gt;\n1.0 &lt;class 'float'&gt;\n\n\nЗмінна типу float не округлятиме число, яке ви в ній зберігаєте, тоді як змінна типу integer округлятиме. Це робить floats більш придатними для математичних обчислень, де потрібно більше, ніж просто цілі числа.\nЗверніть увагу, що оскільки ми використовували функцію float(), щоб змусити число рахуватися float, ми можемо використовувати функцію int(), щоб змусити число представлятися в типі int.\n\nmy_int = int(3.14159)\nprint(my_int, type(my_int))\n\n3 &lt;class 'int'&gt;\n\n\nФункція int() також усіче будь-які цифри, які число може містити після десяткової коми!\nРядки дозволяють включати текст як змінну для роботи. Вони визначаються з використанням або одинарних лапок (’’), або подвійних лапок (““).\n\nmy_string = 'This is a string with single quotes'\nprint(my_string)\nmy_string = \"This is a string with double quotes\"\nprint(my_string)\n\nThis is a string with single quotes\nThis is a string with double quotes\n\n\nОбидва варіанти дозволені, так що ми можемо включити апострофи або лапки в рядок, якщо ми того побажаємо.\n\nmy_string = '\"Jabberwocky\", by Lewis Carroll'\nprint(my_string)\nmy_string = \"'Twas brillig, and the slithy toves / Did gyre and gimble in the wabe;\"\nprint(my_string)\n\n\"Jabberwocky\", by Lewis Carroll\n'Twas brillig, and the slithy toves / Did gyre and gimble in the wabe;\n\n\nЛогічні значення, або bools, - це двійкові типи змінних. bool може приймати лише одне з двох значень, це True або False. У цій ідеї істинних значень є набагато більше, коли мова заходить про програмування, про що ми розповімо пізніше в розділі Логічні оператори цього зошита.\n\nmy_bool = True\nprint(my_bool, type(my_bool))\n\nTrue &lt;class 'bool'&gt;\n\n\nІснує ще багато типів даних, які ви можете призначити змінними в Python, але це основні з них! Ми розглянемо ще трохи пізніше, коли будемо просуватися по цьому блокноту."
  },
  {
    "objectID": "appb.html#базова-математика",
    "href": "appb.html#базова-математика",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.3 Базова математика",
    "text": "B.3 Базова математика\nPython має ряд вбудованих математичних функцій. Їх можна ще більше розширити, імпортуючи пакет math або включивши будь-яку кількість інших обчислювальних пакетів.\nПідтримуються всі основні арифметичні операції: +, -, /, і *. Ви можете створювати експоненти за допомогою **, а модульна арифметика вводиться за допомогою оператора mod, %.\n\nprint('Addition: ', 2 + 2)\nprint('Subtraction: ', 7 - 4)\nprint('Multiplication: ', 2 * 5)\nprint('Division: ', 10 / 2)\nprint('Exponentiation: ', 3**2)\n\nAddition:  4\nSubtraction:  3\nMultiplication:  10\nDivision:  5.0\nExponentiation:  9\n\n\nЯкщо ви не знайомі з оператором mod, він працює як функція залишку. Якщо ми введемо \\(15 \\ \\% \\  4\\), він поверне залишок після ділення \\(15\\) на \\(4\\).\n\nprint('Частка: ', 15 % 4)\n\nЧастка:  3\n\n\nМатематичні функції також працюють зі змінними!\n\nfirst_integer = 4\nsecond_integer = 5\nprint(first_integer * second_integer)\n\n20\n\n\nПереконайтеся, що ваші змінні є плаваючими, якщо ви хочете, щоб у вашій відповіді були десяткові крапки. Якщо ви виконуєте математику виключно з цілими числами, Ви отримуєте ціле число. Включення будь-якого значення з плаваючою точкою в обчислення зробить результат плаваючим.\n\nfirst_integer = 11\nsecond_integer = 3\nprint(first_integer / second_integer)\n\n3.6666666666666665\n\n\n\nfirst_number = 11.0\nsecond_number = 3.0\nprint(first_number / second_number)\n\n3.6666666666666665\n\n\nPython має кілька вбудованих математичних функцій. Найбільш помітними з них є:\n\nabs()\nround()\nmax()\nmin()\nsum()\n\nУсі ці функції діють так, як ви очікували, враховуючи їх назви. Виклик abs() для числа поверне його абсолютне значення. Функція round() округлить число до вказаної кількості десяткових знаків (значення за замовчуванням дорівнює \\(0\\)). Виклик max() або min() для набору чисел поверне, відповідно, максимальне або мінімальне значення в наборі. Виклик sum() для набору чисел призведе до їх підсумовування. Якщо ви не знайомі з тим, як працюють колекції значень у Python, не хвилюйтеся! Ми детально розглянемо набір в наступному розділі.\nДодаткові математичні функції можуть бути додані разом з пакетом math.\n\nimport math\n\nМатематична бібліотека додає довгий список нових математичних функцій до Python. Не соромтеся ознайомитися з документацією для отримання повного списку та деталей. У ньому полягають деякі математичні константи\n\nprint('Pi: ', math.pi)\nprint(\"Euler's Constant: \", math.e)\n\nPi:  3.141592653589793\nEuler's Constant:  2.718281828459045\n\n\nА також деякі часто використовувані математичні функції\n\nprint('Cosine of pi: ', math.cos(math.pi))\n\nCosine of pi:  -1.0"
  },
  {
    "objectID": "appb.html#колекції",
    "href": "appb.html#колекції",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.4 Колекції",
    "text": "B.4 Колекції\n\nB.4.1 Списки (lists)\nСписок у Python - це впорядкована колекція об’єктів, яка може містити будь-який тип даних. Ми визначаємо список, використовуючи квадратні дужки ([]).\n\nmy_list = [1, 2, 3]\nprint(my_list)\n\n[1, 2, 3]\n\n\nМи також можемо отримати доступ до списку та проіндексувати його за допомогою дужок. Щоб вибрати окремий елемент, просто введіть назву списку, а потім індекс елемента, який ви шукаєте, у фігурних дужках.\n\nprint(my_list[0])\nprint(my_list[2])\n\n1\n3\n\n\nІндексація в Python починається з $ 0$. Якщо у вас є список довжиною \\(n\\), перший елемент списку знаходиться з індексом \\(0\\), другий елемент з індексом \\(1\\), і так далі, і тому подібне. Останній елемент списку матиме індекс \\(n-1\\). Будьте обережні! Спроба отримати доступ до неіснуючого індексу призведе до помилки.\n\nprint('The first, second, and third list elements: ', my_list[0], my_list[1], my_list[2])\nprint('Accessing outside the list bounds causes an error: ', my_list[3])\n\nThe first, second, and third list elements:  1 2 3\n\n\nIndexError: list index out of range\n\n\nМи можемо побачити кількість елементів у списку, викликавши функцію len().\n\nprint(len(my_list))\n\n3\n\n\nМи можемо оновлювати та змінювати список, отримуючи доступ до індексу та призначаючи нове значення.\n\nprint(my_list)\nmy_list[0] = 42\nprint(my_list)\n\n[1, 2, 3]\n[42, 2, 3]\n\n\nЦе принципово відрізняється від того, як обробляються рядки. Список є змінним, що означає, що ви можете змінювати елементи списку без зміни самого списку. Деякі типи даних, такі як рядки, є незмінними, що означає, що ви взагалі не можете їх змінити. Як тільки рядок або інший незмінний тип даних був створений, він не може бути безпосередньо змінений без створення абсолютно нового об’єкта.\n\nmy_string = \"Strings never change\"\nmy_string[0] = 'Z'\n\nTypeError: 'str' object does not support item assignment\n\n\nЯк ми вже говорили раніше, список може містити будь-який тип даних. Таким чином, списки також можуть містити рядки.\n\nmy_list_2 = ['one', 'two', 'three']\nprint(my_list_2)\n\n['one', 'two', 'three']\n\n\nСписки також можуть містити кілька різних типів даних одночасно!\n\nmy_list_3 = [True, 'False', 42]\n\nЯкщо ви хочете об’єднати два списки, їх можна об’єднати символом +.\n\nmy_list_4 = my_list + my_list_2 + my_list_3\nprint(my_list_4)\n\n[42, 2, 3, 'one', 'two', 'three', True, 'False', 42]\n\n\nОкрім доступу до окремих елементів списку, ми можемо отримати доступ до груп елементів за допомогою зрізу.\n\nmy_list = ['friends', 'romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nB.4.1.1 Зріз (slicing)\nМи використовуємо двокрапку (:) для нарізки списків.\n\nprint(my_list[2:4])\n\n['countrymen', 'lend']\n\n\nВикористовуючи :, ми можемо вибрати групу елементів у списку, починаючи з першого вказаного елемента і закінчуючи (але не включаючи) останнім зазначеним елементом.\nМи також можемо вибрати все після певного значення\n\nprint(my_list[1:])\n\n['romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nІ все перед конкретним значенням\n\nprint(my_list[:4])\n\n['friends', 'romans', 'countrymen', 'lend']\n\n\nВикористання негативних чисел буде відлічуватися з кінця індексів, а не з початку. Наприклад, індекс -1 вказує на останній елемент списку.\n\nprint(my_list[-1])\n\nears\n\n\nВи також можете додати третій компонент для нарізки. Замість того, щоб просто вказати першу та кінцеву частини вашого зрізу, ви можете вказати розмір кроку, який ви хочете зробити. Таким чином, замість того, щоб брати кожен окремий елемент, ви можете взяти будь-який інший елемент.\n\nprint(my_list[0:7:2])\n\n['friends', 'countrymen', 'me', 'ears']\n\n\nТут ми вибрали весь список (оскільки 0:7 дасть елементи від 0 до 6), і ми вибрали розмір кроку 2. Отже, це виведе елемент 0, елемент 2, елемент 4 тощо на вибраний елемент списку. Ми можемо пропустити вказаний початок і кінець нашого фрагмента, вказавши лише крок, якщо хочемо.\n\nprint(my_list[::2])\n\n['friends', 'countrymen', 'me', 'ears']\n\n\nСписки неявно вибирають початок і кінець списку, якщо не вказано інше.\n\nprint(my_list[:])\n\n['friends', 'romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nПри негативному розмірі кроку ми можемо навіть перевернути список!\n\nprint(my_list[::-1])\n\n['ears', 'your', 'me', 'lend', 'countrymen', 'romans', 'friends']\n\n\nPython не має власних матриць. Інші пакети, такі як numpy, додають матриці як окремий тип даних, але в базовому Python найкращим способом створення матриці є використання списку списків.\nМи також можемо використовувати вбудовані функції для створення списків. Зокрема, ми розглянемо range() (тому що ми будемо використовувати його пізніше!). Діапазон може приймати кілька різних вхідних даних і поверне список.\n\nb = 10\nmy_list = range(b)\nprint(my_list)\n\nrange(0, 10)\n\n\nПодібно до наших попередніх методів нарізки списків, ми можемо визначити як початок, так і кінець нашого діапазону. Це поверне список, який включає початок і виключає кінець, точно так само, як зріз.\n\na = 0\nb = 10\nmy_list = range(a, b)\nprint(my_list)\n\nrange(0, 10)\n\n\nМи також можемо вказати розмір кроку. Це знову має таку ж поведінку, як і зріз.\n\na = 0\nb = 10\nstep = 2\nmy_list = range(a, b, step)\nprint(my_list)\n\nrange(0, 10, 2)\n\n\n\n\n\nB.4.2 Кортежі (Tuples)\nКортеж - це тип даних, подібний до списку в тому сенсі, що він може містити різні типи даних. Ключова відмінність тут полягає в тому, що кортеж є незмінним. Ми визначаємо кортеж, розділяючи елементи, які ми хочемо включити комами. Зазвичай кортеж укладають в круглі дужки.\n\nmy_tuple = 'I', 'have', 30, 'cats'\nprint(my_tuple)\n\n('I', 'have', 30, 'cats')\n\n\n\nmy_tuple = ('I', 'have', 30, 'cats')\nprint(my_tuple)\n\n('I', 'have', 30, 'cats')\n\n\nЯк згадувалося раніше, кортежі незмінні. Ви не можете змінити будь-яку їх частину, не визначивши новий кортеж.\n\nmy_tuple[3] = 'dogs' # Намагається змінити значення 'cats', що зберігається в кортежі, на 'dogs'\n\nTypeError: 'tuple' object does not support item assignment\n\n\nВи можете нарізати кортежі так само, як ви нарізаєте списки!\n\nprint(my_tuple[1:3])\n\n('have', 30)\n\n\nІ об’єднайте їх так, як Ви б це зробили з рядками!\n\nmy_other_tuple = ('make', 'that', 50)\nprint(my_tuple + my_other_tuple)\n\n('I', 'have', 30, 'cats', 'make', 'that', 50)\n\n\nМи можемо упакувати значення разом, створивши кортеж (як зазначено вище), або ми можемо розпакувати значення з кортежу, витягуючи їх.\n\nstr_1, str_2, int_1 = my_other_tuple\nprint(str_1, str_2, int_1)\n\nmake that 50\n\n\nРозпакування присвоює кожне значення кортежу по порядку кожній змінній у лівій частині знака рівності. Деякі функції, включаючи спеціальні функції, можуть повертати кортежі, тому ми можемо використовувати це, щоб безпосередньо розпакувати їх і отримати доступ до потрібних нам значень.\n\n\nB.4.3 Множини (Sets)\nМножини - це набір невпорядкованих, унікальних елементів. Він працює майже точно так, як ви очікували б від звичайного набору математичних задач, і визначається за допомогою фігурних дужок ({}).\n\nthings_i_like = {'dogs', 7, 'the number 4', 4, 4, 4, 42, 'lizards', 'man I just LOVE the number 4'}\nprint(things_i_like, type(things_i_like))\n\n{'lizards', 'dogs', 'the number 4', 4, 'man I just LOVE the number 4', 7, 42} &lt;class 'set'&gt;\n\n\nЗверніть увагу, як будь-які додаткові екземпляри одного і того ж елемента видаляються в остаточному наборі. Ми також можемо створити множину зі списку, використовуючи функцію set().\n\nanimal_list = ['cats', 'dogs', 'dogs', 'dogs', 'lizards', 'sponges', 'cows', 'bats', 'sponges']\nanimal_set = set(animal_list)\nprint(animal_set) # видаляємо всі дублікати зі списку\n\n{'dogs', 'bats', 'lizards', 'cats', 'sponges', 'cows'}\n\n\nВиклик len() для множини повідомить вам, скільки в ньому елементів.\n\nprint(len(animal_set))\n\n6\n\n\nОскільки множина представляє невпорядковану структуру даних, ми не можемо отримати доступ до окремих елементів за допомогою індексу. Однак ми можемо легко перевірити приналежність (щоб побачити, чи міститься щось у наборі) та використовувати об’єднання та перетини множин за допомогою вбудованих функцій set.\n\n'cats' in animal_set # Тут ми перевіряємо наявність членства, використовуючи ключове слово 'in'.\n\nTrue\n\n\nТут ми перевірили, чи міститься рядок cats у нашому animal_set, і він повернув True, повідомивши нам, що він насправді знаходиться в нашому наборі.\nМи можемо з’єднати множини, використовуючи типові математичні оператори множин, а саме | для об’єднання та & для перетину. Використання | або & поверне саме те, що ви очікували б, якщо Ви знайомі з множинами в математиці.\n\nprint(animal_set | things_i_like) # Ви також можете написати things_i_like / animal_set без будь-якої різниці\n\n{'dogs', 'the number 4', 4, 'bats', 'man I just LOVE the number 4', 7, 42, 'lizards', 'cats', 'sponges', 'cows'}\n\n\nСполучення двох наборів за допомогою | об’єднує множини, видаляючи будь-які повторення, щоб зробити кожен елемент набору унікальним.\n\nprint(animal_set & things_i_like) # Ви також можете написати things_i_like & animal_set без будь-якої різниці\n\n{'lizards', 'dogs'}\n\n\nСполучення двох наборів за допомогою & обчислює перетин обох наборів, повертаючи набір, який містить лише те, що вони мають спільне.\nЯкщо вам цікаво дізнатися більше про вбудовані функції для наборів, не соромтеся ознайомитися з документацією.\n\n\nB.4.4 Словники (Dictionaries)\nЩе однією важливою структурою даних у Python є словник. Словники визначаються за допомогою комбінації фігурних дужок ({}) і двокрапок (:). Фігурні дужки визначають початок і кінець словника, а двокрапки вказують пари ключ-значення. Словник - це, по суті, набір пар ключ-значення. Ключ будь-якого запису повинен бути незмінним типом даних. Це робить кандидатами як рядки, так і кортежі. Ключі можуть бути як додані, так і видалені.\nУ наступному прикладі ми маємо словник, що складається з пар ключ-значення, де ключовим є жанр художньої літератури (рядок), а значенням є список книг (list) у цьому жанрі. Оскільки колекція все ще вважається єдиною сутністю, ми можемо використовувати її для збору декількох змінних або значень в одну пару ключ-значення.\n\nmy_dict = {\"High Fantasy\": [\"Wheel of Time\", \"Lord of the Rings\"],\n           \"Sci-fi\": [\"Book of the New Sun\", \"Neuromancer\", \"Snow Crash\"],\n           \"Weird Fiction\": [\"At the Mountains of Madness\", \"The House on the Borderland\"]}\n\nПісля визначення словника ми можемо отримати доступ до будь-якого окремого значення, вказавши його ключ у дужках.\n\nprint(my_dict[\"Sci-fi\"])\n\n['Book of the New Sun', 'Neuromancer', 'Snow Crash']\n\n\nМи також можемо змінити значення, пов’язане з даним ключем\n\nmy_dict[\"Sci-fi\"] = \"I can't read\"\nprint(my_dict)\n\n{'High Fantasy': ['Wheel of Time', 'Lord of the Rings'], 'Sci-fi': \"I can't read\", 'Weird Fiction': ['At the Mountains of Madness', 'The House on the Borderland']}\n\n\nДодати нову пару ключ-значення так само просто, як і визначити її.\n\nmy_dict[\"Historical Fiction\"] = [\"Pillars of the Earth\"]\nprint(my_dict[\"Historical Fiction\"])\n\n['Pillars of the Earth']\n\n\n\nprint(my_dict)\n\n{'High Fantasy': ['Wheel of Time', 'Lord of the Rings'], 'Sci-fi': \"I can't read\", 'Weird Fiction': ['At the Mountains of Madness', 'The House on the Borderland'], 'Historical Fiction': ['Pillars of the Earth']}"
  },
  {
    "objectID": "appb.html#рядки-strings",
    "href": "appb.html#рядки-strings",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.5 Рядки (Strings)",
    "text": "B.5 Рядки (Strings)\nМи вже знаємо, що рядки зазвичай використовуються для тексту. Ми можемо використовувати вбудовані операції для легкого об’єднання, розділення та форматування рядків, залежно від наших потреб.\nСимвол + вказує на конкатенацію мовою рядків. Це об’єднає два рядки в довший рядок.\n\nfirst_string = '\"Beware the Jabberwock, my son! /The jaws that bite, the claws that catch! /'\nsecond_string = 'Beware the Jubjub bird, and shun /The frumious Bandersnatch!\"/'\nthird_string = first_string + second_string\nprint(third_string)\n\n\"Beware the Jabberwock, my son! /The jaws that bite, the claws that catch! /Beware the Jubjub bird, and shun /The frumious Bandersnatch!\"/\n\n\nРядки також індексуються приблизно так само, як і списки.\n\nmy_string = 'Supercalifragilisticexpialidocious'\nprint('The first letter is: ', my_string[0]) # Uppercase S\nprint('The last letter is: ', my_string[-1]) # lowercase s\nprint('The second to last letter is: ', my_string[-2]) # lowercase u\nprint('The first five characters are: ', my_string[0:5]) # Remember: slicing doesn't include the final element!\nprint('Reverse it!: ', my_string[::-1])\n\nThe first letter is:  S\nThe last letter is:  s\nThe second to last letter is:  u\nThe first five characters are:  Super\nReverse it!:  suoicodilaipxecitsiligarfilacrepuS\n\n\nВбудовані об’єкти та класи часто мають пов’язані з ними спеціальні функції, які називаються методами. Ми отримуємо доступ до цих методів, використовуючи точку (‘.’). Ми детальніше розглянемо об’єкти та пов’язані з ними методи в іншій лекції!\nВикористовуючи рядкові методи, ми можемо підраховувати екземпляри символу або групи символів.\n\nprint('Count of the letter i in Supercalifragilisticexpialidocious: ', my_string.count('i'))\nprint('Count of \"li\" in the same word: ', my_string.count('li'))\n\nCount of the letter i in Supercalifragilisticexpialidocious:  7\nCount of \"li\" in the same word:  3\n\n\nМи також можемо знайти перший екземпляр символу або групи символів у рядку.\n\nprint('The first time i appears is at index: ', my_string.find('i'))\n\nThe first time i appears is at index:  8\n\n\nА також замінити символи в рядку.\n\nprint(\"All i's are now a's: \", my_string.replace('i', 'a'))\n\nAll i's are now a's:  Supercalafragalastacexpaaladocaous\n\n\n\nprint(\"It's raining cats and dogs\".replace('dogs', 'more cats'))\n\nIt's raining cats and more cats\n\n\nІснують також деякі методи, які є унікальними для рядків. Функція upper() перетворює всі символи в рядку в верхній регістр, в той час як lower() перетворює всі символи в рядку в нижній регістр!\n\nmy_string = \"I can't hear you\"\nprint(my_string.upper())\nmy_string = \"I said HELLO\"\nprint(my_string.lower())\n\nI CAN'T HEAR YOU\ni said hello\n\n\n\nB.5.1 Форматування рядків\nВикористовуючи метод format(), ми можемо додавати значення змінних і зазвичай форматувати наші рядки.\n\nmy_string = \"{0} {1}\".format('Marco', 'Polo')\nprint(my_string)\n\nMarco Polo\n\n\n\nmy_string = \"{1} {0}\".format('Marco', 'Polo')\nprint(my_string)\n\nPolo Marco\n\n\nМи використовуємо фігурні дужки ({}) для позначення частин рядка, які будуть заповнені пізніше, і ми використовуємо аргументи функції format() для надання значень для заміни. Цифри у фігурних дужках вказують індекс значення в аргументах format().\nДивіться format() документація для отримання додаткових прикладів.\nЯкщо вам потрібне швидке та брудне форматування, ви можете замість цього використовувати символ%, який називається оператором форматування рядка.\n\nprint('insert %s here' % 'value')\n\ninsert value here\n\n\nСимвол % в основному вказує Python на створення заповнювача. Будь-який символ, що слідує за % (у рядку), вказує, який тип матиме значення, введене в заповнювач. Цей символ називається типом перетворення. Після закриття рядка нам знадобиться ще один %, за яким слідують значення для вставки. У випадку одного значення ви можете просто помістити його туди. Якщо ви вставляєте більше одного значення, вони повинні бути укладені в кортеж.\n\nprint('There are %s cats in my %s' % (13, 'apartment'))\n\nThere are 13 cats in my apartment\n\n\nУ цих прикладах %s вказує, що Python повинен перетворити значення в рядки. Існує кілька типів перетворення, які ви можете використовувати, щоб уточнити форматування. Дивіться форматування рядка для отримання додаткових прикладів та більш повної інформації про використання."
  },
  {
    "objectID": "appb.html#логічні-оператори",
    "href": "appb.html#логічні-оператори",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.6 Логічні оператори",
    "text": "B.6 Логічні оператори\n\nB.6.1 Базова логіка\nЛогічні оператори мають справу з булевими значеннями, як ми коротко розглянули раніше. Якщо ви пам’ятаєте, bool приймає одне з двох значень: True або False (або \\(1\\) або \\(0\\)). Основні логічні твердження, які ми можемо зробити, визначаються за допомогою вбудованих компараторів. Це == (дорівнює), != (не дорівнює), &lt; (Менше), &gt; (Більше), &lt;= (менше або дорівнює) і &gt;= (більше або дорівнює).\n\nprint(5 == 5)\n\nTrue\n\n\n\nprint(5 &gt; 5)\n\nFalse\n\n\nЦі компаратори також працюють у поєднанні зі змінними.\n\nm = 2\nn = 23\nprint(m &lt; n)\n\nTrue\n\n\nМи можемо зв’язати ці компаратори разом, щоб створити більш складні логічні оператори, використовуючи логічні оператори or, and і not.\n\nstatement_1 = 10 &gt; 2\nstatement_2 = 4 &lt;= 6\nprint(\"Statement 1 truth value: {0}\".format(statement_1))\nprint(\"Statement 2 truth value: {0}\".format(statement_2))\nprint(\"Statement 1 and Statement 2: {0}\".format(statement_1 and statement_2))\n\nStatement 1 truth value: True\nStatement 2 truth value: True\nStatement 1 and Statement 2: True\n\n\nОператор or виконує логічне обчислення або. Будь-який компонент, об’єднаний за допомогою або, що є True, представлятиме все твердження як True. Оператор and виводить True, лише якщо всі компоненти разом є True. В іншому випадку він видасть False. Твердження not просто інвертує значення істинності будь-якого наступного за ним твердження. Таким чином, твердження True буде оцінено як False, коли перед ним буде поставлено not. Аналогічно, False твердження стане True, коли перед ним буде стояти not.\nПрипустимо, у нас є два логічні твердження, \\(P\\) і \\(Q\\). Таблиця істинності для основних логічних операторів виглядає наступним чином:\n\n\n\nP\nQ\nnot P\nP and Q\nP or Q\n\n\n\n\nTrue\nTrue\nFalse\nTrue\nTrue\n\n\nFalse\nTrue\nTrue\nFalse\nTrue\n\n\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n\nМи можемо зв’язати кілька логічних операторів разом, використовуючи логічні оператори.\n\nprint(((2 &lt; 3) and (3 &gt; 0)) or ((5 &gt; 6) and not (4 &lt; 2)))\n\nTrue\n\n\nЛогічні твердження можуть бути настільки простими або складними, наскільки нам подобається, залежно від того, що нам потрібно висловити. Оцінюючи наведене вище логічне твердження крок за кроком, ми бачимо, що ми оцінюємо (True and True) or (False and not False). Дана конструкція набуваж вигляду True or (False and True). Згодом стає True or False, і в кінцевому рахунку оцінюється як True.\n\nB.6.1.1 Істинність\nТипи даних у Python мають цікаву характеристику, яка називається істинністю. Це означає, що більшість вбудованих типів будуть оцінюватися як True або False, коли потрібне логічне значення (наприклад, за допомогою оператора if). Як правило, контейнери, такі як рядки, кортежі, словники, списки та множини, повертають True, якщо вони взагалі що-небудь містять, і False, якщо вони нічого не містять.\n\n# Cхоже до того, як працюють float() та int(), book () змушує значення вважатися логічним!\nprint(bool(''))\n\nFalse\n\n\n\nprint(bool('I have character!'))\n\nTrue\n\n\n\nprint(bool([]))\n\nFalse\n\n\n\nprint(bool([1, 2, 3]))\n\nTrue\n\n\nІ так далі, для інших колекцій та контейнерів. None також оцінюється як False. Число 1 еквівалентно True, а число 0 також еквівалентно False в логічному контексті.\n\n\n\nB.6.2 If-оператори\nМи можемо створювати сегменти коду, які виконуються тільки при виконанні набору умов. Ми використовуємо оператори if у поєднанні з логічними операторами для створення розгалужень у нашому коді.\nБлок if вводиться, коли умова вважається True. Якщо умова оцінюється як False, блок if буде просто пропущений, якщо тільки до нього не додається блок else. Умови створюються за допомогою логічних операторів або за допомогою істинності значень у Python. Оператор if визначається двокрапкою і блоком тексту з відступом.\n\nif \"Condition\":\n    print(True)\nelse:\n    print(False)\n\nTrue\n\n\n\ni = 4\nif i == 5:\n    print('The variable i has a value of 5')\n\nОскільки в цьому прикладі i = 4 і оператор if шукає лише те, чи i = 5, оператор print ніколи не буде виконаний. Ми можемо додати оператор else, щоб створити блок коду на випадок надзвичайних ситуацій на випадок, якщо умова в операторі if не буде оцінена як True.\n\ni = 5\nif i == 5:\n    print(\"Усі рядки в цьому блоці з відступом є частиною цього блоку\")\n    print('Змінна i має значення 5')\nelse:\n    print(\"Усі рядки в цьому блоці з відступом є частиною цього блоку\")\n    print('Змінна i не дорівнює 5')\n\nУсі рядки в цьому блоці з відступом є частиною цього блоку\nЗмінна i має значення 5\n\n\nМи можемо реалізувати інші гілки від того самого оператора if, використовуючи elif, скорочення від else if. Ми можемо включати стільки elifсів, скільки захочемо, поки не вичерпаємо всі логічні гілки умови.\n\ni = 1\nif i == 1:\n    print('Змінна i має значення 1')\nelif i == 2:\n    print('Змінна і має значення 2')\nelif i == 3:\n    print('Змінна і має значення 3')\nelse:\n    print(\"Мене не хвилює змінна і\")\n\nМене не хвилює змінна і\n\n\nВи також можете вкласти оператори if в інші оператори if, щоб перевірити наявність додаткових умов.\n\ni = 10\nif i % 2 == 0:\n    if i % 3 == 0:\n        print('і ділиться як на 2, так і на 3!')\n    elif i % 5 == 0:\n        print('і ділиться як на 2, так і на 5!')\n    else:\n        print('i ділиться на 2, але не на 3 або 5!')\nelse:\n    print('Я припускаю, що i - непарне число.')\n\nі ділиться як на 2, так і на 5!\n\n\nПам’ятайте, що ми можемо згрупувати кілька умов разом, використовуючи логічні оператори!\n\ni = 11\nj = 12\nif i &lt; 10 and j &gt; 11:\n    print('{0} менше 10 і {1} більше 11!'.format(i, j))\n\nВи можете використовувати логічні компаратори для порівняння рядків!\n\nmy_string = \"Farthago delenda est\"\nif my_string == \"Carthago delenda est\":\n    print('And so it was! For the glory of Rome!')\nelse:\n    print('War elephants are TERRIFYING. I am staying home.')\n\nWar elephants are TERRIFYING. I am staying home.\n\n\nЯк і у випадку з іншими типами даних, == перевірить, чи дві речі з обох сторін мають однакове значення.\nДеякі вбудовані функції повертають логічне значення, тому їх можна використовувати як умови в операторі if. Користувацькі функції також можуть бути сконструйовані таким чином, щоб вони повертали логічне значення. Це буде розглянуто пізніше в розділі визначення функції!\nКлючове слово in зазвичай використовується для перевірки приналежності значення до іншого значення. Ми можемо перевірити приналежність у контексті оператора if і використовувати його для виведення значення істини.\n\nif 'a' in my_string or 'e' in my_string:\n    print('Those are my favorite vowels!')\n\nThose are my favorite vowels!\n\n\nТут ми використовуємо in, щоб перевірити, чи містить змінна my_string містить якісь конкретні літери. Пізніше ми будемо використовувати in для перебору списків!"
  },
  {
    "objectID": "appb.html#циклічні-структури",
    "href": "appb.html#циклічні-структури",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.7 Циклічні структури",
    "text": "B.7 Циклічні структури\nЦиклічні структури є однією з найважливіших частин програмування. Цикл for і цикл while надають спосіб багаторазового запуску блоку коду повторно. Цикл while буде повторюватися, поки не буде виконана певна умова. Якщо в будь-який момент після ітерації ця умова більше не виконується, цикл завершується. Цикл for буде виконувати ітерацію по послідовності значень і завершиться, коли послідовність закінчиться. Натомість ви можете включити умови в цикл for, щоб вирішити, чи повинен він закінчуватися достроково, чи ви можете просто дозволити йому піти своїм шляхом.\n\ni = 10\nwhile i &gt; 0:\n    i -= 1\n    print('Я зациклений! {0} значень до завершення!'.format(i))\n\nЯ зациклений! 9 значень до завершення!\nЯ зациклений! 8 значень до завершення!\nЯ зациклений! 7 значень до завершення!\nЯ зациклений! 6 значень до завершення!\nЯ зациклений! 5 значень до завершення!\nЯ зациклений! 4 значень до завершення!\nЯ зациклений! 3 значень до завершення!\nЯ зациклений! 2 значень до завершення!\nЯ зациклений! 1 значень до завершення!\nЯ зациклений! 0 значень до завершення!\n\n\nЗа допомогою циклів while нам потрібно переконатися, що щось насправді змінюється від ітерації до ітерації, щоб цикл фактично закінчувався. У цьому випадку ми використовуємо скорочення i -= 1 (скорочення від i = i - 1), так що значення i стає меншим з кожною ітерацією. Врешті-решт i буде зменшено до 0, що призведе до виконання умови False та виходу з циклу.\nЦикл for повторюється задану кількість разів, що визначається при вказівці запису в цикл. У цьому випадку ми повторюємо список, повернутий з range(). Цикл for вибирає значення зі списку по порядку і тимчасово присвоює йому значення i, щоб із цим значенням можна було виконувати операції.\n\nfor i in range(5):\n    print('Я зациклений! Я вже на {0}-ій ітерації!'.format(i + 1))\n\nЯ зациклений! Я вже на 1-ій ітерації!\nЯ зациклений! Я вже на 2-ій ітерації!\nЯ зациклений! Я вже на 3-ій ітерації!\nЯ зациклений! Я вже на 4-ій ітерації!\nЯ зациклений! Я вже на 5-ій ітерації!\n\n\nЗверніть увагу, що в цьому циклі for ми використовуємо ключове слово in. Використання ключового слова in не обмежується перевіркою приналежності, як у прикладі if-конструкцій. Ви можете оброблювати будь-яку колекцію за допомогою циклу for, використовуючи ключове слово in.\nУ цьому наступному прикладі ми переглянемо множину, оскільки хочемо перевірити наявність вмісту та додати до нового набору.\n\nmy_list = {'cats', 'dogs', 'lizards', 'cows', 'bats', 'sponges', 'humans'}\nmammal_list = {'cats', 'dogs', 'cows', 'bats', 'humans'} # перераховані всі ссавці в у світі\nmy_new_list = set()\nfor animal in my_list:\n    if animal in mammal_list:\n        # додаємо будь-яку тварину, що знаходиться і в my_list, і в mammal_list\n        my_new_list.add(animal)\n\nprint(my_new_list)\n\n{'dogs', 'bats', 'cats', 'humans', 'cows'}\n\n\nЄ два твердження, які дуже корисні при роботі як з циклами for, так і з циклами while. Це break і continue. Якщо break трапляється в будь-який момент під час виконання циклу, цикл негайно завершується.\n\ni = 10\nwhile True:\n    if i == 14:\n        break\n    i += 1\n    print(i)\n\n11\n12\n13\n14\n\n\n\nfor i in range(5):\n    if i == 2:\n        break\n    print(i)\n\n0\n1\n\n\nОператор continue вкаже циклу негайно завершити цю ітерацію і перейти до наступної ітерації циклу.\n\ni = 0\nwhile i &lt; 5:\n    i += 1\n    if i == 3:\n        continue\n    print(i)\n\n1\n2\n4\n5\n\n\nЦей цикл пропускає друк числа \\(3\\) через інструкцію continue, яка виконується, коли ми вводимо оператор if. Код ніколи не бачить команди для друку числа \\(3\\), оскільки він уже перейшов до наступної ітерації.\nЗмінна, яку ми використовуємо для ітерації циклу, збереже своє значення при завершенні циклу. Аналогічно, будь-які змінні, визначені в контексті циклу, продовжуватимуть існувати поза ним.\n\nfor i in range(5):\n    loop_string = 'Я виходжу за межі циклу!'\n    print('Я вічний! Я {0} і я існую скрізь!'.format(i))\n\nprint('Моє значення {0}'.format(i))\nprint(loop_string)\n\nЯ вічний! Я 0 і я існую скрізь!\nЯ вічний! Я 1 і я існую скрізь!\nЯ вічний! Я 2 і я існую скрізь!\nЯ вічний! Я 3 і я існую скрізь!\nЯ вічний! Я 4 і я існую скрізь!\nМоє значення 4\nЯ виходжу за межі циклу!\n\n\nМи також можемо виконувати ітерації по словнику!\n\nmy_dict = {'firstname' : 'Inigo', 'lastname' : 'Montoya', 'nemesis' : 'Rugen'}\n\n\nfor key in my_dict:\n    print(key)\n\nfirstname\nlastname\nnemesis\n\n\nЯкщо ми просто перебираємо словник, не роблячи нічого іншого, ми отримуємо лише ключі. Ми можемо або використовувати ключі для отримання значень, як у прикладі:\n\nfor key in my_dict:\n    print(my_dict[key])\n\nInigo\nMontoya\nRugen\n\n\nАбо ми можемо використовувати функцію items(), щоб отримати і ключ, і значення одночасно\n\nfor key, value in my_dict.items():\n    print(key, ':', value)\n\nfirstname : Inigo\nlastname : Montoya\nnemesis : Rugen\n\n\nФункція items створює кортеж з кожної пари ключ-значення, а цикл for розпаковує цей кортеж в ключ, значення при кожному окремому виконанні циклу!"
  },
  {
    "objectID": "appb.html#функції",
    "href": "appb.html#функції",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.8 Функції",
    "text": "B.8 Функції\nФункція-це багаторазовий блок коду, який ви можете викликати повторно для виконання обчислень, виведення даних або дійсно робити все, що завгодно. Це один з ключових аспектів використання мови програмування. Щоб додати до вбудованих функцій у Python, ви можете визначити свої власні!\n\ndef hello_world():\n    \"\"\" Виводить Hello, world! \"\"\"\n    print('Hello, world!')\n\nhello_world()\n\nHello, world!\n\n\n\nfor i in range(5):\n    hello_world()\n\nHello, world!\nHello, world!\nHello, world!\nHello, world!\nHello, world!\n\n\nФункції визначаються за допомогою def, імені функції, списку параметрів та двокрапки. Все, що вказано з відступом нижче двокрапки, буде включено у визначення функції.\nМи можемо змусити наші функції робити все, що ви можете зробити зі звичайним блоком коду. Наприклад, наша функція hello_world() виводить рядок при кожному його виклику. Якщо ми хочемо зберегти значення, обчислене функцією, ми можемо визначити функцію так, щоб вона return потрібне нам значення. Це дуже важлива особливість функцій, оскільки будь-яка змінна, визначена виключно всередині функції, не буде існувати поза нею.\n\ndef see_the_scope():\n    return \"Я тут застряг!\"\n\nprint(see_the_scope())\n\nЯ тут застряг!\n\n\n\na = see_the_scope()\nprint(a)\n\nЯ тут застряг!\n\n\nОбласть змінної - це частина блоку коду, де ця змінна прив’язана до певного значення. Функції в Python мають закриту область дії, що робить можливим прямий доступ до змінних лише всередині цих областей. Якщо ми передамо ці значення оператору return, ми можемо отримати їх із функції.\n\ndef free_the_scope():\n    in_function_string = \"Anything you can do I can do better!\"\n    return in_function_string\nmy_string = free_the_scope()\nprint(my_string)\n\nAnything you can do I can do better!\n\n\nТак само, як ми можемо отримувати значення з функції, ми також можемо розміщувати значення у функції. Ми робимо це, визначаючи нашу функцію з параметрами.\n\ndef multiply_by_five(x):\n    \"\"\" Множимо вхідне значення на 5 \"\"\"\n    return x * 5\n\nn = 4\nprint(n)\nprint(multiply_by_five(n))\n\n4\n20\n\n\nУ цьому прикладі у нас був лише один параметр для нашої функції, x. Ми можемо легко додати додаткові параметри, розділивши всі комою.\n\ndef calculate_area(length, width):\n    \"\"\" Визначаємо площу прямокутника \"\"\"\n    return length * width\n\n\nl = 5\nw = 10\nprint('Area: ', calculate_area(l, w))\nprint('Length: ', l)\nprint('Width: ', w)\n\nArea:  50\nLength:  5\nWidth:  10\n\n\n\ndef calculate_volume(length, width, depth):\n    \"\"\" Визначаємо об'єм прямокутної призми \"\"\"\n    return length * width * depth\n\nЯкщо ми хочемо, ми можемо визначити функцію так, щоб вона приймала довільну кількість параметрів. Ми повідомляємо Python, що хочемо цього, використовуючи зірочку (*).\n\ndef sum_values(*args):\n    sum_val = 0\n    for i in args:\n        sum_val += i\n    return sum_val\n\n\nprint(sum_values(1, 2, 3))\nprint(sum_values(10, 20, 30, 40, 50))\nprint(sum_values(4, 2, 5, 1, 10, 249, 25, 24, 13, 6, 4))\n\n6\n150\n343\n\n\nВи використовуєте *args як параметр для вашої функції - це коли ви не знаєте, скільки значень можна передати в неї, як у випадку з нашою функцією sum. Зірочка в даному випадку - це синтаксис, який повідомляє Python, що ви збираєтеся передати довільну кількість параметрів у свою функцію. Ці параметри зберігаються у вигляді кортежу.\n\ndef test_args(*args):\n    print(type(args))\n\ntest_args(1, 2, 3, 4, 5, 6)\n\n&lt;class 'tuple'&gt;\n\n\nНаші функції можуть повертати будь-який тип даних. Це дозволяє нам легко створювати функції, які перевіряють умови, які ми можемо захотіти відстежувати.\nТут ми визначаємо функцію, яка повертає логічне значення. Ми можемо легко використовувати це в поєднанні з операторами if та іншими ситуаціями, які потребують логічного значення.\n\ndef has_a_vowel(word):\n    \"\"\"\n    Перевіряємо, чи містить слово голосну\n\n    \"\"\"\n    vowel_list = ['a', 'e', 'i', 'o', 'u']\n\n    for vowel in vowel_list:\n        if vowel in word:\n            return True\n\n    return False\n\n\nmy_word = 'catnapping'\nif has_a_vowel(my_word):\n    print('Містить.')\nelse:\n    print('Не містить.')\n\nМістить.\n\n\n\ndef point_maker(x, y):\n    \"\"\" Групує значення x і y в точку, технічно кортеж \"\"\"\n    return x, y\n\nЦя наведена вище функція повертає впорядковану пару вхідних параметрів, збережених як кортеж.\n\na = point_maker(0, 10)\nb = point_maker(5, 3)\ndef calculate_slope(point_a, point_b):\n    \"\"\" Обчислює лінійний нахил між двома точками\"\"\"\n    return (point_b[1] - point_a[1])/(point_b[0] - point_a[0])\nprint(\"Кут нахилу між a і b {0}\".format(calculate_slope(a, b)))\n\nКут нахилу між a і b -1.4"
  },
  {
    "objectID": "appb.html#подальші-кроки",
    "href": "appb.html#подальші-кроки",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.9 Подальші кроки",
    "text": "B.9 Подальші кроки\nЯкщо ви хочете глибше заглибитися в матеріал, тоді зверніться до документації по Python."
  },
  {
    "objectID": "appc.html#комірка-markdown",
    "href": "appc.html#комірка-markdown",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.1 Комірка Markdown",
    "text": "C.1 Комірка Markdown\nЯк можна здогадатися з назви в Markdown осередках можна створювати текст в markdown форматі. Підтримуються різні способи форматування, які можна подивитися за посиланням. Текст, який ви зараз читаєте, також знаходиться в markdown клітинці.\nКрім форматування тексту також підтримується можливість створення математичних формул за допомогою LaTex. Формулу можна вбудувати в текст (наприклад, \\(e^{i\\pi}=-1\\)) або створити в окремому рядку:\n\\[e^x=\\sum_{k=0}^\\infty \\frac{x^k}{k!}\\]\nДля редагування тексту в markdown осередку необхідно два рази клікнути по ній."
  },
  {
    "objectID": "appc.html#комірка-code",
    "href": "appc.html#комірка-code",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.2 Комірка Code",
    "text": "C.2 Комірка Code\nНаступна комірка є Сode осередком і в ній можна писати код і виконувати його. Для виконання коду необхідно натиснути Ctrl + Enter(виконати і залишитися в поточній комірці) або Shift + Enter (виконати і перейти в наступну комірку)\n\nimport numpy as np # імпортуємо бібліотеку\n\nЯкщо останній рядок коду повертає яке-небудь значення, то воно відображається відразу після комірки\n\nnp.random.rand(10) # генеруємо випадкові значення\n\narray([0.68528999, 0.70609617, 0.82579621, 0.38756313, 0.87001097,\n       0.91044051, 0.31743928, 0.27789269, 0.79610991, 0.58255746])"
  },
  {
    "objectID": "appc.html#автодоповнення-та-робота-з-документацією",
    "href": "appc.html#автодоповнення-та-робота-з-документацією",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.3 Автодоповнення та робота з документацією",
    "text": "C.3 Автодоповнення та робота з документацією\nДля автодоповнення можна використовувати клавішу &lt;TAB &gt; після точки або всередині дужки при виклику функції. При цьому вийде список доступних варіантів, які можна вибрати, щоб автоматично доповнити код. Можете спробувати автодоповнення поставивши курсор після np.random.&lt;TAB&gt;.\nВ Jupyter є кілька способів викликати документацію. Перший спосіб це використовувати поєднання клавіш Shift + Tab. Другий спосіб поставити знак ? після необхідного модуля\n\nnp?"
  },
  {
    "objectID": "appc.html#magic-команди",
    "href": "appc.html#magic-команди",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.4 Magic команди",
    "text": "C.4 Magic команди\nJupyter підтримує набір так званих “чарівних” (magic) команд. Це різні корисні команди, які не є частиною Python. Всі ці команди починаються з %.\nМожна безпосередньо завантажити вміст зовнішнього файлу в комірку за допомогою команди %load\n\n# %load code/magic_example.py\ndef square(x): # ініціалізуємо функцію знаходження квадрату вхідного значення\n    \"\"\"\n    Squares given number\n    \"\"\"\n    return x ** 2 # повертаємо значення\n\n\nprint(square(42)) # виводимо результат\n\n1764\n\n\nЗ корисних команд також можна відзначити команду %timeit, яка виконує код багато разів і виводить середній час виконання коду\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n377 µs ± 21.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nСписок усіх magic команд можна подивитися окремою командою %lsmagic."
  },
  {
    "objectID": "appc.html#робота-з-графікою",
    "href": "appc.html#робота-з-графікою",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.5 Робота з графікою",
    "text": "C.5 Робота з графікою\nУ Python є багато бібліотек для візуалізації даних. Багато з них інтегруються з Jupyter і відображають графіки\n\nimport matplotlib.pyplot as plt # імпортуємо бібліотеку\n\n# вбудовуємо вивдені рисунки в юпітеровський ноутбук\n%matplotlib inline\n\nplt.plot([1, 4], [1, 4]) # виводимо лінію по двум точкам\n\n\n\n\nабо декілька графіків\n\nall_data = [np.random.normal(0, std, size=100) for std in range(1, 4)] # генеруємо список значень із нормального розподілу\nlabels = ['x1', 'x2', 'x3'] # ініціалізуємо список міток\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4)) # ініціалізуємо об'єкт матриці рисунків\n\n# прямокутна коробчаста діаграма\nbplot1 = axes[0].boxplot(all_data,\n                         vert=True,  # вертикальне вирівнювання\n                         patch_artist=True,  # заповнити кольором\n                         labels=labels)  # використовується для позначення підписів по осі x\naxes[0].set_title('Прямокутна діаграма') # встановлюємо титулку для першого рисунку\n\n# побудова коробчастої діаграми з виїмкою\nbplot2 = axes[1].boxplot(all_data,\n                         notch=True,\n                         vert=True,  # вертикальне вирівнювання\n                         patch_artist=True,  # заповнити кольором\n                         labels=labels)  # використовується для позначення підписів по осі x\naxes[1].set_title('Прямокутна діаграма з виїмкою') # встановлюємо титулку для другого рисунку\n\n# заповнити кольорами\ncolors = ['pink', 'lightblue', 'lightgreen']\nfor bplot in (bplot1, bplot2):\n    for patch, color in zip(bplot['boxes'], colors):\n        patch.set_facecolor(color)\n\n# додати сітку з горизонтальних ліній\nfor ax in axes:\n    ax.yaxis.grid(True)\n    ax.set_xlabel('Три відокремлений зразки')\n    ax.set_ylabel('Спостережувані значення')\n\n\n\n\nабо тривимірну графіку\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\n\nfig = plt.figure(figsize=(10, 10)) # ініціалізуємо об'єкт рисунок\nax = fig.add_subplot(projection='3d') # додаємо до об'єкту тривимірне представлення\n\n# ініціалізуємо дані\nX = np.arange(-5, 5, 0.25)\nY = np.arange(-5, 5, 0.25)\nX, Y = np.meshgrid(X, Y) # заповнюємо поверхню значеннями по двом осям\nR = np.sqrt(X**2 + Y**2)\nZ = np.sin(R) # ініціалізуємо значення по вісі Oz\n\n# будуємо поверхню\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm) # будуємо тривимірну поверхню по заданим значенням\n\n\nax.set_zlim(-1.01, 1.01) # встановлюємо границі по вісі Oz\nax.zaxis.set_major_locator(LinearLocator(10)) # встановлюємо 10 граничних ліній по вісі Oz\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f')) # визначаємо формат виведення значень"
  },
  {
    "objectID": "appc.html#інші-можливості",
    "href": "appc.html#інші-можливості",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.6 Інші можливості",
    "text": "C.6 Інші можливості\nДля Jupyter Notebook було створено велику кількість плагінів. Наприклад, можна вбудовувати відео з youtube:\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('kjBOesZCoqc')\n\n\n        \n        \n\n\nАбо інтерактивні карти (дана комірка відобразиться тільки якщо у вас встановлений folium. Якщо у вас нічого не відображається, то можете пропустити даний приклад, він далі не знадобиться)\nВстановити необхідну бібліотеку можна через команду pip install назва бібліотеки, яку варто прописати в консолі, як представлено в прикладі нижче:\n\nАбо, як варіант, можна прописати команду прямо в комірці середовища Jupyter Notebook, як представлено в прикладі нижче:\n\n!pip install folium\n\nRequirement already satisfied: folium in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (0.14.0)\nRequirement already satisfied: requests in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from folium) (2.27.1)\nRequirement already satisfied: numpy in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from folium) (1.21.2)\nRequirement already satisfied: branca&gt;=0.6.0 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from folium) (0.6.0)\nRequirement already satisfied: jinja2&gt;=2.9 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from folium) (2.11.3)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from jinja2&gt;=2.9-&gt;folium) (2.0.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from requests-&gt;folium) (2.0.4)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from requests-&gt;folium) (1.26.9)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from requests-&gt;folium) (3.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from requests-&gt;folium) (2022.12.7)\n\n\n\nimport folium\nm = folium.Map(zoom_start=12, location=[47.89829743895897, 33.36626740165739])\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nАбо вбудувати будь-який інший шматок HTML за допомогою magic команди %%html. Нижче наведено приклад вбудовування посту з Твіттеру\n\n%%html\n&lt;blockquote class=\"twitter-tweet\" data-lang=\"en\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Replace &quot;AI&quot; with &quot;matrix multiplication &amp; gradient descent&quot; in the calls for &quot;government regulation of AI&quot; to see just how absurd they are&lt;/p&gt;&mdash; Ben Hamner (@benhamner) &lt;a href=\"https://twitter.com/benhamner/status/892136662171504640?ref_src=twsrc%5Etfw\"&gt;July 31, 2017&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\nReplace \"AI\" with \"matrix multiplication & gradient descent\" in the calls for \"government regulation of AI\" to see just how absurd they are— Ben Hamner (@benhamner) July 31, 2017"
  },
  {
    "objectID": "appc.html#гарячі-клавіші",
    "href": "appc.html#гарячі-клавіші",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.7 Гарячі клавіші",
    "text": "C.7 Гарячі клавіші\nБагато дій можна виконати за допомогою гарячих клавіш. Список гарячих клавіш можна знайти в меню Help - Keyboard shortcuts. Нижче наведено список найбільш корисних поєднань:\n\n\n\n\n\n\n\nКлюч\nОписання\n\n\n\n\nEsc\nвийти з режиму редагування та виділити поточну комірку\n\n\nEnter\nперейти в режим редагування комірки поточної комірки\n\n\nCtrl+S, S\nзберегти файл\n\n\nCtrl+Enter\nвиконати код і залишитися в поточній комірці\n\n\nShift + Enter\nвиконати код і перейти в наступну клітинку\n\n\nShift + Tab\nвиводить спливаюче вікно з документацією\n\n\na\nдодати комірку згори (above)\n\n\nb\nдодати комірку знизу (below)\n\n\nc\nскопіювати комірку\n\n\nv\nвставити скопійовану клітинку\n\n\ndd\nвидалити комірку\n\n\nz\nскасування останньої дії"
  }
]