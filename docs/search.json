[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Моделювання складних систем у Python",
    "section": "",
    "text": "Передмова\nЗадекларований перехід економіки України на інноваційний шлях розвитку в умовах обмеженості ресурсів, у першу чергу фінансових, вимагає зосередження їх на найбільш перспективних напрямках, де імовірність досягнення конкурентного успіху є найбільшою. Як свідчить практика, такого роду оцінки доцільно виконувати із застосуванням економіко-математичних моделей. Множинність шляхів вибору моделей і методів актуалізує необхідність розуміння основних тенденцій розвитку наукового знання і ключових досягнень.\nГоворячи мовою синергетики, необхідно виділити такі параметри порядку — провідні змінні, які з плином часу починають визначати динаміку і розвиток складної системи, і підпорядковувати собі її інші параметри, які призведуть до ефективного спрощення складного об’єкта.\nВсе більш очевидною є революція, що почалася в природничих та гуманітарних науках і пов’язана з вивченням феномена самоорганізації і дослідженням мережних структур. Мережна парадигма складності обумовлена великим значенням, яке придбали такі об’єкти, і тим, що на початку ХХІ століття очевидною стала разюча аналогія в топології мережних структур, що виникають завдяки активному використанню інформаційно-телекомунікаційних, гуманітарних, управлінських, військових технологій.\nМережі стали одним з двигунів економіки. У своїй історії людство долало різні бар’єри, створюючи нові матеріали, технології, об’єкти. Однак “мережна економіка” зіткнулася з обмеженнями самої людини — так званим “когнітивним бар’єром”. Як показали психологи, людина в змозі активно, творчо взаємодіяти з 5-7 людьми (з рештою опосередковано або стандартно, незалежно від того, скільки у нього друзів у соціальній мережі). Вона може одночасно стежити за 5-7 змінними (незалежно від того, наскільки великий обсяг інформації їй доступний). Приймаючи рішення, вона може зважити 5-7 факторів (скільки б даних у неї не було).\nЗліт нової економіки в США в 1990-х роках, пов’язаний багато в чому з інтернет- компаніями, породив ілюзії, що капіталізація мережних структур пропорційна числу зв’язків між вузлами, тобто квадрату числа вузлів \\(N^2\\). Однак, коли на початку 2000-х років міхур «нової економіки» лопнув (криза «доткомів - .com»), то виявилося, що реальна капіталізація мала зовсім іншу залежність від числа об’єктів, пов’язаних мережею — \\(N\\ln{N}\\). Інакше кажучи, не «всі зв’язуються з усіма», а майже всі взаємодіють з декількома дуже великими вузлами- хабами, які вже тісно пов’язуються між собою. Подібним же чином виявляється влаштована інфраструктура більшості складних систем, незалежно від їх природи.\nЧерговим викликом для нас стала нова індустріальна революція, актуалізована давоським форумом 2016 року. Згідно з опитуванням 800 лідерів технологічних компаній, проведеним спеціально для форуму в Давосі, ключовими драйверами змін стануть хмарні технології, розвиток способів збору і аналізу Big Data, краудсорсінг, шерінгова економіка і біотехнології.\nОчевидно, що революційні вимоги Індустрії 4.0 потребують нових парадигм моделювання соціально-економічних систем. На наш погляд, такою парадигмою може стати мережна парадигма складних систем.\nМережні технології змінили обчислювальну математику, системний аналіз, інформаційні технології. В останні роки було реалізовано кілька грандіозних мережних проектів, в яких поставлена задача вирішувалася завдяки спільним діям сотень тисяч або навіть мільйонів комп’ютерів. Це і криптографічні проблеми, і пошук ліків проти раку, заснований на математичному моделюванні взаємодії різних речовин з клітинами. Це розподілений аналіз даних космічних експериментів та обробка результатів, отриманих на Великому адронному колайдері.\nУ зв’язку з новою мережною парадигмою складності перед фахівцями з моделювання виникають принципово нові, актуальні задачі. Ось, на наш погляд, тільки деякі з них:\n\nдослідження надійності, робастності кіберфізичних мереж відносно випадкових помилок та направлених атак;\nаналіз когнітивних можливостей складних систем;\nмоделювання мультиплексних мереж;\nвплив нанотехнологій на формування наноекономіки та ін.\n\nСьогодні вже зрозуміло, що відповідних інновацій вимагає і система освіти. На початку комп’ютерної ери основну цінність і проблеми становили власне комп’ютери (hardware) і акцент робився на підготовку фахівців з обчислювальної техніки. Потім величезне значення набуло програмне забезпечення (software) і була розпочата підготовка дослідників у цій галузі (computer science) та інженерів-програмістів (computer engineering). У даний час на перший план виходять фахівці з мережних технологій (NetWare). Саме таких фахівців треба починати готувати у провідних національних університетах. За оцінками експертів з Індустрії 4.0 основними трендами освітньої сфери на двадцятирічному горизонті стануть: поширення цінностей мережної культури, прагматизація освіти, автоматизація рутинних інтелектуальних операцій, розвиток індустрії поліпшення когнітивних здібностей, боротьба за таланти, зростання значущості глобальних людських цінностей, уваги до природи і дбайливого поводження з ресурсами.\nМета курсу моделювання складних систем — формування системи теоретичних знань та практичних навичок щодо моделювання структурних і динамічних властивостей систем різної природи як засобу дослідження та управління складними явищами на макро-, мезо- й мікрорівнях. Протягом останніх десяти-п’ятнадцяти років відбулися відчутні зміни в розумінні фундаментальних закономірностей складних систем. Виявилось, що такі складні системи мають універсальні емерджентні властивості, які не знаходять адекватного розуміння у рамках традиційних парадигм. Тому для аналізу все активніше використовуються сучасні методи та моделі фундаментальних наук, які у поєднанні з новітніми досягненнями в галузі інформаційних технологій та досить ємними базами даних (мільйони записів навіть в базах некомерційного призначення) забезпечили значний прогрес у розумінні та квантифікації природи цих систем. Сюди відносяться методи фрактального і мультифрактального аналізу, дослідження рекурентних властивостей динамічних систем, нелінійної динаміки, теорії хаосу і біфуркацій тощо. З’явились нові «кількісні» напрямки економіки: математична економіка, фізична економіка, еконофізика та ін. Деякі з таких моделей, не знайшовши поки що відображення у навчальній літературі, включені до цієї роботи.\nНаукову основу курсу складають теоретичні моделі, математичний апарат, сучасні концепції та парадигми, які визначають підходи до вивчення характеристик складних систем. Курс базується на знаннях, одержаних при вивченні дисциплін математичного циклу, основ теорії систем та системного аналізу, моделювання, фінансового аналізу, макро- і мікроекономіки.\nЗавдання курсу — оволодіння теоретичними знаннями та інструментарієм моделювання складних систем, вивчення підходів до дослідження й аналізу, методів прогнозування їхнього розвитку, управління розвитком та функціонуванням складних систем у різних умовах функціонування.\nПредметом курсу є математичні моделі і методи дослідження складних систем різної природи.\nУ результаті вивчення дисципліни студент повинен:\n\nзнати структурні та динамічні характеристики складної системи; моделі прогнозування характеристик системи; основні методи оцінки якості функціонування; методи оцінки структурних змін; методи дослідження та моделювання складних природних та штучних систем;\nвміти здійснювати класифікацію характеристик складної системи, проводити порівняльний аналіз методів прогнозування; оцінити якість функціонування ієрархічної системи; визначити катастрофічні зміни в системі, які описуються рівняннями динаміки, визначити джерела структурних катастроф;\nдослідити та проаналізувати комплекс моделей складної системи;\nбути ознайомленим з сучасними напрямками розвитку сучасних теорій та парадигм, які використовуються для дослідження якісних характеристик динамічних систем.\n\nСистема контролю якості навчання студентів містить такі заходи:\n\nмодульний контроль;\nпроведення контрольних робіт;\nконтроль теоретичних знань у ході практичних занять;\nвиконання індивідуальних завдань на лабораторних заняттях.\n\nОрганізація самостійної роботи студентів передбачає підготовку до семінарських занять, проведення індивідуальних консультацій, виконання курсової роботи.\nВідсутність альтернативних підручників і методичних посібників з курсу моделювання складних систем спонукала авторів розробити методичні вказівки до виконання практичних робіт з даного курсу. Вони спираються на деяке авторське бачення, певні висновки мають дискусійний характер. Автори будуть вдячні за бачення, спрямоване на покращення змісту та методики викладання дисципліни."
  },
  {
    "objectID": "lab_1.html#теоретичні-відомості",
    "href": "lab_1.html#теоретичні-відомості",
    "title": "1  Лабораторна робота № 1",
    "section": "1.1 Теоретичні відомості",
    "text": "1.1 Теоретичні відомості\n\n1.1.1 Аналіз динаміки прибутків, модулів прибутків та волатильностей\nОстаннім часом вчені все більше цікавляться економічними часовими рядами, і відбувається це за кількох причин, зокрема: (1) економічні часові ряди, такі як індекси акцій, курсів валют, залежать від розвитку великої кількості взаємодіючих систем, і є прикладами складних систем, що широко вивчаються у науці; (2) з’явилась велика кількість доступних баз з даними про економічні системи, що містять інформацію з різними часовими шкалами (починаючи з 1 хвилини і закінчуючи 1 роком). Внаслідок цього вже на даний час існує також велика кількість розроблених методів (зокрема, у статистичній фізиці), спрямованих на отримання характеристик цін акцій чи курсів валют, що еволюціонують у часі.\nДослідження, проведені над часовими рядами, показують, що стохастичний процес, який лежить у основі зміни ціни, характеризується кількома ознаками. Розподіл зміни ціни має виділений хвіст порівняно із Гаусовим розподілом. Функція автокореляції зміни ціни спадає експоненційно з певним характерним часом. Однак, виявляється, що амплітуда зміни ціни, виміряна за абсолютними значеннями чи квадратами цін, показує степеневі кореляції з довго часовою персистентністю аж до кількох місяців, або навіть років. Такі довгочасові залежності краще моделюються з використанням «додаткового процесу», що в економічній літературі часто називається волатильністю. Волатильність змін ціни акції є мірою того, як сильно ринок схильний до флуктуацій, тобто відхилень ціни від попередніх значень.\nПершим кроком при проведенні аналізу є побудова оцінювача волатильності. Ми будемо отримувати волатильність як локальне середнє модуля зміни ціни.\nРозуміння статистичних властивостей волатильності має також важливе практичне застосування. Волатильність є інтересом торговців, оскільки визначає ризик і є ключовим входом практично до всіх моделей цін опціонів (вторинного цінного паперу), включаючи і класичну модель Блека-Шоулза. Без задовільних методів оцінювання волатильності трейдерам було б надзвичайно важко визначати ситуації, в яких опціони попадають в недооцінку чи переоцінку.\n\n\n1.1.2 Визначення волатильності\nТермін волатильність представляє узагальнену міру величини ринкових флуктуацій (відхилень). У літературі існує досить багато визначень волатильності, проте ми будемо використовувати наступне: волатильність є локальним середнім модуля зміни ціни на відповідному часовому інтервалі \\(T\\), що є рухомим параметром нашої оцінки. Для індексу \\(X(t)\\) визначимо зміну ціни \\(G(t)\\) як зміну логарифмів індексів,\n\\[\nG(t) = \\ln{X(t+\\Delta t) - \\ln{X(t)}} \\cong \\frac{X(t+\\Delta t) - X(t)}{X(t)},\n\\tag{1.1}\\]\nде \\(\\Delta t\\) є часовим інтервалом затримки. Величину (Рівняння 1.1) називають прибутковістю (return). Якщо використовувати границі, то малі зміни \\(X(t)\\) приблизно відповідають змінам, визначеним другою рівністю. Ми лише підраховуємо час роботи ринку, викидаємо ночі, вихідні та свята із набору даних, тобто, вважається, що ринок працює без перерв.\nМодуль \\(G(t)\\) описує амплітуду флуктуацій. У порівнянні із значеннями \\(G(t)\\) їх модуль не показує глобальних трендів, але великі значення \\(G(t)\\) відповідають крахам та великим миттєвим змінам на ринках.\nВизначимо волатильність як середнє від \\(G(t)\\) для часових вікон \\(T = n \\cdot \\Delta t\\), тобто\n\\[\nV_{T} = \\frac{1}{n}\\sum_{t^{'}=t}^{t+n-1}\\left| G(t^{'}) \\right|,\n\\tag{1.2}\\]\nде \\(n\\) є цілим числом. Таке визначення може бути ще узагальнене заміною \\(G(t)\\) на \\(\\left| G(t) \\right|^{\\gamma}\\), де \\(\\gamma &gt; 1\\) дає більш виражені великі значення \\(G(t)\\), в той час як \\(0 &lt; \\gamma &lt; 1\\) виділяє малі значення \\(G(t)\\).\nУ цьому визначенні волатильності використовується два параметри, \\(\\Delta t\\) та \\(n\\). Параметр \\(n\\) є шаблонним (чи модельним) часовим інтервалом для даних, а параметр \\(\\Delta t\\) є кроком переміщення часового вікна. Зауважимо, що вказане визначення волатильності має внутрішню помилку, а саме: вибір більшого часового інтервалу \\(T\\) веде до збільшення точності визначення волатильності. Однак, велике значення \\(T\\) також включає погане розбиття часу на інтервали, що веде, у свою чергу, до врахування не всієї прихованої у ряді інформації.\n\n\n1.1.3 Визначення кореляцій\nДля визначення кореляцій часового ряду використовується функція автокореляції. Саме поняття автокореляції означає кореляцію часового ряду самого з собою (між попередніми та наступними значеннями). Автокореляцію іноді називають послідовною кореляцією, що означає кореляцію між членами ряду чисел, розташованих у певному порядку. Також синонімами цього терміну є лагова кореляція та персистентність. Наприклад, часто зустрічається автокореляція геофізичних процесів, що означає перенесення залишкового процесу на наступні часові проміжки.\nПозитивно автокорельований часовий ряд часто називають персистентним, що значить існування тенденції слідування великих значень за великими та малих за малими, інакше позитивно корельований часовий ряд можна назвати інертним.\nВізьмемо \\(N\\) пар спостережень двох змінних \\(x\\) та \\(y\\). Коефіцієнт кореляції між парами \\(x\\) та \\(y\\) визначається як\n\\[\nr = \\frac{\\sum \\left( x_i - \\bar{x} \\right) \\left( y_i - \\bar{y} \\right)}{\\sqrt{\\sum \\left( x_i - \\bar{x} \\right)^{2}} \\sqrt{\\sum \\left( y_i - \\bar{y} \\right)^{2}}},\n\\tag{1.3}\\]\nде сума знаходиться по всім \\(N\\) спостереженням.\nТаким же чином можна визначати й автокореляцію, або ж кореляцію всередині досліджуваного часового ряду. Для автокореляції першого порядку береться лаг (часова затримка), рівний 1 часовій одиниці. Таким чином, автокореляція першого порядку використовує перші \\(N−1\\) спостережень \\(x_t, t = 1,..., N−1\\), та наступні \\(N−1\\) спостережень \\(x_t , t = 2,..., N\\).\nКореляція між \\(x_t\\) та \\(x_t + 1\\) визначається наступним чином:\n\\[\nr_1 = \\frac{\\sum_{t=1}^{N-1} \\left( x_t - \\bar{x} \\right) \\left( x_{t+1} - \\bar{x} \\right)}{\\sum_{t=1}^{N}\\left( x_t - \\bar{x} \\right)^2},\n\\tag{1.4}\\]\nде \\(x\\) — це середнє для досліджуваного періоду.\nРівняння (Рівняння 1.4) може бути узагальнене для отримання кореляції між спостереженнями, розділеними \\(k\\) часовими інтервалами:\n\\[\nr_k = \\frac{\\sum_{t=1}^{N-k} \\left( x_t - \\bar{x} \\right) \\left( x_{t+k} - \\bar{x} \\right)}{\\sum_{t=1}^{N}\\left( x_t - \\bar{x} \\right)^2}.\n\\tag{1.5}\\]\nЗначення \\(r_k\\) називається коефіцієнтом автокореляції з лагом \\(k\\). Графік функції автокореляції як залежності \\(r_k\\) від \\(k\\) також називають корелограмою."
  },
  {
    "objectID": "lab_1.html#хід-роботи",
    "href": "lab_1.html#хід-роботи",
    "title": "1  Лабораторна робота № 1",
    "section": "1.2 Хід роботи",
    "text": "1.2 Хід роботи\nДля подальшої роботи з моделювання складних систем візьмемо з основу бібліотеку yfinance, що дозволяє працювати з даними фінансових ринків засобами мови програмування Python.\n\n\n\n\n\n\nПримітка\n\n\n\nYahoo!, Y!Finance, and Yahoo! finance є зареєстрованими товарними знаками Yahoo, Inc.\nyfinance не є афілійованим, схваленим або перевіреним Yahoo, Inc. Це інструмент з відкритим вихідним кодом, який використовує загальнодоступні API Yahoo, і призначений для дослідницьких та освітніх цілей.\nВи повинні звернутися до умов використання Yahoo! (сюди, сюди і сюди) для отримання детальної інформації про ваші права на використання фактично завантажених даних. Пам’ятайте — фінансовий API Yahoo! призначений лише для особистого використання.\n\n\nДля встановлення бібліотеки yfinance можете скористатися наступною командою:\n\n!pip install yfinance --upgrade --no-cache-dir\n\nГітхаб репозиторій містить більше інформації по самій бібліотеці та помилкам, що можуть виникнути та їх потенційним рішенням.\n\n1.2.1 Вступ до модуля Ticker()\nПерш за все імпортуємо бібліотеку yfinance за допомогою наступної команди:\n\nimport yfinance as yf\n\nМодуль Ticker() дозволяє отримувати ринкові та метадані для цінного паперу, використовуючи Python:\n\nmsft = yf.Ticker(\"MSFT\")\nprint(msft)\n\nyfinance.Ticker object &lt;MSFT&gt;\n\n\nМожна вилучити всю інформацію по досліджуваному індексу:\n\n# отримати інформацію по індексу\nprint(msft.info)\n\n{'address1': 'One Microsoft Way', 'city': 'Redmond', 'state': 'WA', 'zip': '98052-6399', 'country': 'United States', 'phone': '425 882 8080', 'website': 'https://www.microsoft.com', 'industry': 'Software—Infrastructure', 'industryDisp': 'Software—Infrastructure', 'sector': 'Technology', 'sectorDisp': 'Technology', 'longBusinessSummary': 'Microsoft Corporation develops and supports software, services, devices and solutions worldwide. The Productivity and Business Processes segment offers office, exchange, SharePoint, Microsoft Teams, office 365 Security and Compliance, Microsoft viva, and Microsoft 365 copilot; and office consumer services, such as Microsoft 365 consumer subscriptions, Office licensed on-premises, and other office services. This segment also provides LinkedIn; and dynamics business solutions, including Dynamics 365, a set of intelligent, cloud-based applications across ERP, CRM, power apps, and power automate; and on-premises ERP and CRM applications. The Intelligent Cloud segment provides server products and cloud services, such as azure and other cloud services; SQL and windows server, visual studio, system center, and related client access licenses, as well as nuance and GitHub; and enterprise services including enterprise support services, industry solutions, and nuance professional services. The More Personal Computing segment offers Windows, including windows OEM licensing and other non-volume licensing of the Windows operating system; Windows commercial comprising volume licensing of the Windows operating system, windows cloud services, and other Windows commercial offerings; patent licensing; and windows Internet of Things; and devices, such as surface, HoloLens, and PC accessories. Additionally, this segment provides gaming, which includes Xbox hardware and content, and first- and third-party content; Xbox game pass and other subscriptions, cloud gaming, advertising, third-party disc royalties, and other cloud services; and search and news advertising, which includes Bing, Microsoft News and Edge, and third-party affiliates. The company sells its products through OEMs, distributors, and resellers; and directly through digital marketplaces, online, and retail stores. The company was founded in 1975 and is headquartered in Redmond, Washington.', 'fullTimeEmployees': 221000, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Satya  Nadella', 'age': 55, 'title': 'Chairman & CEO', 'yearBorn': 1967, 'fiscalYear': 2022, 'totalPay': 12676750, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Bradford L. Smith LCA', 'age': 63, 'title': 'Pres & Vice Chairman', 'yearBorn': 1959, 'fiscalYear': 2022, 'totalPay': 4655274, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Amy E. Hood', 'age': 50, 'title': 'Exec. VP & CFO', 'yearBorn': 1972, 'fiscalYear': 2022, 'totalPay': 4637915, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Judson  Althoff', 'age': 49, 'title': 'Exec. VP & Chief Commercial Officer', 'yearBorn': 1973, 'fiscalYear': 2022, 'totalPay': 4428268, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Christopher David Young', 'age': 50, 'title': 'Exec. VP of Bus. Devel., Strategy & Ventures', 'yearBorn': 1972, 'fiscalYear': 2022, 'totalPay': 4588876, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Alice L. Jolla', 'age': 55, 'title': 'Corp. VP & Chief Accounting Officer', 'yearBorn': 1967, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. James Kevin Scott', 'age': 50, 'title': 'Exec. VP of AI & CTO', 'yearBorn': 1972, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Brett  Iversen', 'title': 'VP of Investor Relations', 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Hossein  Nowbar', 'title': 'Chief Legal Officer', 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Frank X. Shaw', 'title': 'Chief Communications Officer', 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 6, 'boardRisk': 3, 'compensationRisk': 3, 'shareHolderRightsRisk': 2, 'overallRisk': 2, 'governanceEpochDate': 1693526400, 'compensationAsOfEpochDate': 1672444800, 'maxAge': 86400, 'priceHint': 2, 'previousClose': 328.65, 'open': 329.51, 'dayLow': 325.03, 'dayHigh': 329.59, 'regularMarketPreviousClose': 328.65, 'regularMarketOpen': 329.51, 'regularMarketDayLow': 325.03, 'regularMarketDayHigh': 329.59, 'dividendRate': 2.72, 'dividendYield': 0.0083, 'exDividendDate': 1692144000, 'payoutRatio': 0.2748, 'fiveYearAvgDividendYield': 1.05, 'beta': 0.904564, 'trailingPE': 33.579372, 'forwardPE': 25.809818, 'volume': 8304959, 'regularMarketVolume': 8304959, 'averageVolume': 24743406, 'averageVolume10days': 19741410, 'averageDailyVolume10Day': 19741410, 'bid': 325.98, 'ask': 326.04, 'bidSize': 1300, 'askSize': 1100, 'marketCap': 2420020871168, 'fiftyTwoWeekLow': 213.43, 'fiftyTwoWeekHigh': 366.78, 'priceToSalesTrailing12Months': 11.419771, 'fiftyDayAverage': 332.0454, 'twoHundredDayAverage': 293.46735, 'trailingAnnualDividendRate': 2.72, 'trailingAnnualDividendYield': 0.008276282, 'currency': 'USD', 'enterpriseValue': 2409976823808, 'profitMargins': 0.34146, 'floatShares': 7423522720, 'sharesOutstanding': 7429760000, 'sharesShort': 38317589, 'sharesShortPriorMonth': 35852374, 'sharesShortPreviousMonthDate': 1690761600, 'dateShortInterest': 1693440000, 'sharesPercentSharesOut': 0.0052, 'heldPercentInsiders': 0.00052, 'heldPercentInstitutions': 0.73212, 'shortRatio': 1.82, 'shortPercentOfFloat': 0.0052, 'impliedSharesOutstanding': 7429760000, 'bookValue': 27.748, 'priceToBook': 11.738501, 'lastFiscalYearEnd': 1688083200, 'nextFiscalYearEnd': 1719705600, 'mostRecentQuarter': 1688083200, 'earningsQuarterlyGrowth': 0.2, 'netIncomeToCommon': 72361000960, 'trailingEps': 9.7, 'forwardEps': 12.62, 'pegRatio': 2.08, 'lastSplitFactor': '2:1', 'lastSplitDate': 1045526400, 'enterpriseToRevenue': 11.372, 'enterpriseToEbitda': 23.622, '52WeekChange': 0.37539232, 'SandP52WeekChange': 0.17256784, 'lastDividendValue': 0.68, 'lastDividendDate': 1692144000, 'exchange': 'NMS', 'quoteType': 'EQUITY', 'symbol': 'MSFT', 'underlyingSymbol': 'MSFT', 'shortName': 'Microsoft Corporation', 'longName': 'Microsoft Corporation', 'firstTradeDateEpochUtc': 511108200, 'timeZoneFullName': 'America/New_York', 'timeZoneShortName': 'EDT', 'uuid': 'b004b3ec-de24-385e-b2c1-923f10d3fb62', 'messageBoardId': 'finmb_21835', 'gmtOffSetMilliseconds': -14400000, 'currentPrice': 325.7199, 'targetHighPrice': 440.0, 'targetLowPrice': 232.0, 'targetMeanPrice': 386.95, 'targetMedianPrice': 400.0, 'recommendationMean': 1.8, 'recommendationKey': 'buy', 'numberOfAnalystOpinions': 43, 'totalCash': 111256002560, 'totalCashPerShare': 14.974, 'ebitda': 102022995968, 'totalDebt': 79441002496, 'quickRatio': 1.536, 'currentRatio': 1.769, 'totalRevenue': 211914997760, 'debtToEquity': 38.522, 'revenuePerShare': 28.46, 'returnOnAssets': 0.14245, 'returnOnEquity': 0.38824, 'freeCashflow': 47268999168, 'operatingCashflow': 87581999104, 'earningsGrowth': 0.202, 'revenueGrowth': 0.083, 'grossMargins': 0.6892, 'ebitdaMargins': 0.48143002, 'operatingMargins': 0.41772997, 'financialCurrency': 'USD', 'trailingPegRatio': 2.4062}\n\n\nМожна вилучити ринкові значення за максимальний період часу:\n\n# отримати ринкові історичні значення індексу\nmsft.history(period=\"max\")\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n1986-03-13 00:00:00-05:00\n0.055004\n0.063093\n0.055004\n0.060396\n1031788800\n0.0\n0.0\n\n\n1986-03-14 00:00:00-05:00\n0.060396\n0.063632\n0.060396\n0.062553\n308160000\n0.0\n0.0\n\n\n1986-03-17 00:00:00-05:00\n0.062553\n0.064172\n0.062553\n0.063632\n133171200\n0.0\n0.0\n\n\n1986-03-18 00:00:00-05:00\n0.063632\n0.064172\n0.061475\n0.062014\n67766400\n0.0\n0.0\n\n\n1986-03-19 00:00:00-05:00\n0.062014\n0.062553\n0.060396\n0.060936\n47894400\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-09-14 00:00:00-04:00\n339.149994\n340.859985\n336.570007\n338.700012\n20267000\n0.0\n0.0\n\n\n2023-09-15 00:00:00-04:00\n336.920013\n337.399994\n329.649994\n330.220001\n37666900\n0.0\n0.0\n\n\n2023-09-18 00:00:00-04:00\n327.799988\n330.399994\n326.359985\n329.059998\n16834200\n0.0\n0.0\n\n\n2023-09-19 00:00:00-04:00\n326.170013\n329.390015\n324.510010\n328.649994\n16505900\n0.0\n0.0\n\n\n2023-09-20 00:00:00-04:00\n329.510010\n329.589996\n325.029999\n325.720001\n8307027\n0.0\n0.0\n\n\n\n\n9457 rows × 7 columns\n\n\n\nОкрім цього, yfinance дозволяє отримати інформацію по дивідентам та сплітам фінансового індексу:\n\n# показувати дії (дивіденди, спліти)\nmsft.actions\n\n\n\n\n\n\n\n\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n1987-09-21 00:00:00-04:00\n0.00\n2.0\n\n\n1990-04-16 00:00:00-04:00\n0.00\n2.0\n\n\n1991-06-27 00:00:00-04:00\n0.00\n1.5\n\n\n1992-06-15 00:00:00-04:00\n0.00\n1.5\n\n\n1994-05-23 00:00:00-04:00\n0.00\n2.0\n\n\n...\n...\n...\n\n\n2022-08-17 00:00:00-04:00\n0.62\n0.0\n\n\n2022-11-16 00:00:00-05:00\n0.68\n0.0\n\n\n2023-02-15 00:00:00-05:00\n0.68\n0.0\n\n\n2023-05-17 00:00:00-04:00\n0.68\n0.0\n\n\n2023-08-16 00:00:00-04:00\n0.68\n0.0\n\n\n\n\n88 rows × 2 columns\n\n\n\n\n# продемонструвати дивіденти\nmsft.dividends\n\nDate\n2003-02-19 00:00:00-05:00    0.08\n2003-10-15 00:00:00-04:00    0.16\n2004-08-23 00:00:00-04:00    0.08\n2004-11-15 00:00:00-05:00    3.08\n2005-02-15 00:00:00-05:00    0.08\n                             ... \n2022-08-17 00:00:00-04:00    0.62\n2022-11-16 00:00:00-05:00    0.68\n2023-02-15 00:00:00-05:00    0.68\n2023-05-17 00:00:00-04:00    0.68\n2023-08-16 00:00:00-04:00    0.68\nName: Dividends, Length: 79, dtype: float64\n\n\n\n# продемонструвати спліти\nmsft.splits\n\nDate\n1987-09-21 00:00:00-04:00    2.0\n1990-04-16 00:00:00-04:00    2.0\n1991-06-27 00:00:00-04:00    1.5\n1992-06-15 00:00:00-04:00    1.5\n1994-05-23 00:00:00-04:00    2.0\n1996-12-09 00:00:00-05:00    2.0\n1998-02-23 00:00:00-05:00    2.0\n1999-03-29 00:00:00-05:00    2.0\n2003-02-18 00:00:00-05:00    2.0\nName: Stock Splits, dtype: float64\n\n\nДля методу history() доступні наступні аргументи:\n\nperiod: період даних для завантаження (або використовуйте параметр period, або використовуйте start і end). Допустимі періоди: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max.\ninterval: інтервал даних (внутрішньоденні дані не можуть перевищувати 60 днів) Допустимі інтервали 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo.\nstart: Якщо не використовується період — завантажте рядок дати початку у форматі (YYYY-MM-DD) або datetime.\nend: Якщо не використовується період — завантажте рядок дати закінчення (YYYY-MM-DD) або datetime.\nprepost: Включати в результати попередні та пост ринкові дані (За замовчуванням False).\nauto_adjust: Автоматично налаштовувати всі OHLC (ціни відкриття, закриття, найбільшу та найменшу) (За замовчуванням True).\nactions: Завантажувати події дивідендів та дроблення акцій (За замовчуванням True).\n\n\n\n1.2.2 Одночасне вивантаження декількох ринкових активів\nЯк і до цього, ви також можете завантажувати дані для кількох тикерів одночасно.\n\ndata = yf.download(\"SPY AAPL\", \n                   start=\"2017-01-01\", \n                   end=\"2017-04-30\") # вивантажуємо дані, \n                                     # зберігаємо до змінної data\n\ndata.head() # виводимо перші 5 рядків нашого масиву даних \n\n[*********************100%%**********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-03\n27.059307\n200.629669\n29.037500\n225.240005\n29.082500\n225.830002\n28.690001\n223.880005\n28.950001\n225.039993\n115127600\n91366500\n\n\n2017-01-04\n27.029026\n201.823288\n29.004999\n226.580002\n29.127501\n226.750000\n28.937500\n225.610001\n28.962500\n225.619995\n84472400\n78744400\n\n\n2017-01-05\n27.166475\n201.662903\n29.152500\n226.399994\n29.215000\n226.580002\n28.952499\n225.479996\n28.980000\n226.270004\n88774400\n78379000\n\n\n2017-01-06\n27.469332\n202.384430\n29.477501\n227.210007\n29.540001\n227.750000\n29.117500\n225.899994\n29.195000\n226.529999\n127007600\n71559900\n\n\n2017-01-09\n27.720936\n201.716385\n29.747499\n226.460007\n29.857500\n227.070007\n29.485001\n226.419998\n29.487499\n226.910004\n134247600\n46939700\n\n\n\n\n\n\n\nДля отримання конкретно цін закриття індексу SPY, вам варто використовувати наступну команду: data['Close']['SPY'] Але, якщо вам потребується згрупувати дані по їх символу, можна скористатися наступним записом:\n\ndata = yf.download(\"SPY AAPL\", \n                   start=\"2017-01-01\", \n                   end=\"2017-04-30\",\n                   group_by=\"ticker\")\n\n[*********************100%%**********************]  2 of 2 completed\n\n\nТепер для звернення до цін закриття індексу SPY, вам треба використовувати наступний запис: data['SPY']['Close'].\nМетод download() приймає додатковий параметр — threads для швидшої обробки великої кількості фінансових індексів одночасно.\nДля подальшої роботи у даній лабораторній та подальших нас ще цікавитимуть наступні бібліотеки:\n\nmatplotlib: комплексна бібліотека для створення статичних, анімованих та інтерактивних візуалізацій на Python. Matplotlib робить прості речі простими, а складні — можливими.\npandas: програмна бібліотека, написана для мови програмування Python для маніпулювання та аналізу даних. Зокрема, вона пропонує структури даних та операції для маніпулювання числовими таблицями та часовими рядами.\nnumpy: бібліотека, що додає підтримку великих багатовимірних масивів і матриць, а також велику колекцію високорівневих математичних функцій для роботи з цими масивами.\nneurokit2: зручна бібліотека, що забезпечує легкий доступ до розширених процедур обробки біосигналів. Дослідники та клініцисти без глибоких знань програмування або біомедичної обробки сигналів можуть аналізувати фізіологічні дані за допомогою лише двох рядків коду. Перевага даної бібліотеки полягає в тому, що вона надає функціонал, який можна використовувати не лише для біомедичних сигналів, але й для фінансових, фізичних тощо.\n\nВстановити кожну з даних бібліотек можна в наступний спосіб: !pip install *назва бібліотеки*:\n\n!pip install matplotlib pandas numpy neurokit2\n\nДалі нам треба буде визначити стиль рисунків для виведення та збереження. Зробити це можна в наступний спосіб. Для використання подальшого стилю рисунків потребується встановити наступну бібліотеку:\n\n# для встановлення останньої версії (із PyPI)\n!pip install SciencePlots\n\nІмпортуємо кожну із зазначених бібліотек:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport numpy as np\nimport neurokit2 as nk\nimport scienceplots\n\n# магічна команда для вбудування рисунків у середовище jupyter блокнотів\n%matplotlib inline  \n\nВиконаємо налаштування стилю наших подальших рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків \n                                      # за замовчуванням\n        \n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nПредставлені налаштування є орієнтовними і можуть змінюватись у ході наступних лабораторних. Ви можете встановлювати власні налаштування. На сайті бібліотеки matplotlib можна ознайомитись з усіма можливими командами.\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи yfinance:\n\nsymbol = 'BTC-USD'       # Символ індексу\nstart = \"2015-09-01\"     # Дата початку зчитування даних\nend = \"2020-03-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nВажливо\n\n\n\nПредставлені в подальшому методи є універсальними. Іншими словами, ви можете їх використовувати не лише для фінансових часових рядів, але й для, біологічних, фізичних і для будь-яких інших систем, що існують в осяжній нами реальності та можуть бути репрезентовані у вигляду часового ряду. Може бути так, що наявні у вас дані, наприклад, представлені у форматі текстового документа (.txt). Нижче представлено приклад зчитування текстового файлу, що представляє залежність між напруженням і деформацією. Представлену далі залежність було отримано в результаті механічних випробувань певного металу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                 # Символ індексу\n\npath = \"databases\\sMpa11.txt\"     # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,          # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()    # копіюємо значення кривої \n                                  # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'    # підпис по вісі Ох \nylabel = symbol              # підпис по вісі Оу\n\nТепер виведемо значення, що були зчитані з текстового документа:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)\nax.set_ylabel(ylabel)\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 1.1: Діаграма деформації металевого матеріалу\n\n\n\n\nДля даного ряду, за потребою, можна виконувати подальші розрахунки.\n\n\n\n\n\n\n\nУвага\n\n\n\nЗнову повертаємось до Біткоїна. Для відтворення подальших розрахунків з Біткоїном, блок коду в якому зчитувалась та виводилась крива “напруга-видовження” треба проігнорувати.\n\n\nВиведемо значення Біткоїна:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)\nax.set_ylabel(ylabel)\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 1.2: Динаміка щоденних змін індексу Біткоїна\n\n\n\n\nВидно, що ряд нестаціонарний, що викликає певні ускладнення для подальшого аналізу. Тому перейдемо до прибутковостей, які вже є стаціонарними, а їх нормалізація стандартним відхиленням дозволяє легко порівнювати їх розподіл з розподілом Гауса.\nПрибутковості розраховуватимуться згідно (Рівняння 1.1). У Python ми використовуватимемо метод pct_change() для знаходження прибутковостей, що доступний нам завдяки бібліотеці pandas.\nСтандартизовані прибутковості визначаються наступним шляхом:\n\\[\ng(t) = \\frac{G(t) - \\mu}{\\sigma},  \n\\]\nде \\(\\mu\\) відповідає середньому значенню прибутковостей за досліджуваний часовий інтервал, а \\(\\sigma\\) представляє стандартне відхилення.\n\nret = time_ser.copy()      # копіюємо значення вихідного ряду для збереження \n                           # його від змін\n\nret = ret.pct_change()     # знаходимо прибутковості\nret -= ret.mean()          # вилучаємо середнє \nret /= ret.std()           # ділимо на стандартнє відхилення\n\nret = ret.dropna().values  # видаляємо всі можливі нульові значення \n\nВиводимо отриманий результат:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index[1:], ret)           # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Додаємо підпис для вісі Ох\nax.set_ylabel(ylabel + ' прибутковості')   # Додаємо підпис для вісі Оу\nax.axhline(y = 3.0, color = 'r', linestyle = '--')  # Додаємо горизонтальну лінію, що роз-\n                                                    # межує 3 сигма події\nax.axhline(y = -3.0, color = 'r', linestyle = '--') # Додаємо горизонтальну лінію, що роз-\n                                                    # межує -3 сигма події\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'Прибутковості{symbol}.jpg')  # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 1.3: Нормалізовані прибутковості досліджуваного часового ряду\n\n\n\n\nЗверніть увагу, що флуктуації нормалізованих прибутковостей досить часто перевищують величину \\(\\pm 3\\sigma\\), що, як відомо, надзвичайно рідко спостерігається для незалежних подій. Цей факт можна відобразити шляхом порівняння функції розподілу нормалізованих флуктуацій з розподілом Гауса (Рис. 1.4). Очевидно, що хвости розподілу вихідного ряду містять значні флуктуації, вони досить помітні (часто кажуть “важкі” у порівнянні з самою “головою” розподілу).\nДля побудови нормального розподілу скористаємось бібліотекою scipy. Встановити її можна по аналогії з попередніми бібліотеками.\n\n# Для встановлення останньої версії scipy\n!pip install scipy\n\n\nfrom scipy.stats import norm # імпорт модуля norm для побудови Гаусового розподілу\n\nФункція щільності ймовірності для norm має наступний вид:\n\\[\n    f(x) = \\frac{\\exp{(-x^2/2)}}{\\sqrt{2\\pi}}\n\\]\nдля дійсних значень \\(x\\).\n\nmu, sigma = norm.fit(ret)\n\nx = np.linspace(ret.min(), ret.max(), 10000) # Генеруємо 10000 значень для побудови \n                                             # Гаусового розподілу\np = norm.pdf(x, mu, sigma)                   # Отримання значень функції щільності\n\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(x, p, label='Гаус')                # Додаємо дані до графіку\nax.hist(ret, bins=50,                      # Побудова гістограми для прибутковостей\n        density=True, \n        alpha=0.6, \n        color='g',\n        label='прибутковості '+symbol)\n\nax.legend()                                # Додаємо легенду\nax.set_xlabel(\"g\")                         # Додаємо підпис для вісі Ох\nax.set_ylabel(r\"$f(g)$\")                   # Додаємо підпис для вісі Оу\nax.set_yscale('log')\n\n\nplt.savefig(f'Гаус + прибутковості {symbol}.jpg')       # Зберігаємо графік \nplt.show();                                             # Виводимо графік\n\n\n\n\nРис. 1.4: Порівняння функцій розподілу нормалізованих прибутковостей з нормальним розподілом\n\n\n\n\nЯк ми можемо бачити, підігнана крива Гауса відхиляється від істинної частоти настання подій, що перевищують \\(\\pm 3\\sigma\\). Отже, ми можемо стверджувати, що наші прибутковості не є незалежними. Підтвердження цьому факту будемо шукати шляхом вивчення кореляційних властивостей нашого часового ряду.\nДля простоти обчислень скористаємось функцією signal_autocor() бібліотеки neurokit2. Виглядає дана функція наступним чином:\nsignal_autocor(signal, lag=None, demean=True, method='auto', show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) - Вектор значень.\nlag (int) — Часовий лаг. Якщо вказано, буде повернуто одне значення автокореляції сигналу з його власним лагом.\ndemean (bool) — Якщо має значення True, від сигналу буде відніматися середнє значення сигналу перед обчисленням автокореляції.\nmethod (str) — Використання \"auto\" запускає scipy.signal.correlate для використання швидшого алгоритму. Інші методи зберігаються з причин застарілості, але не рекомендуються. Інші методи включають \"correlation\" (за допомогою np.correlate()) або \"fft\" (швидке перетворення Фур’є).\nshow (bool) — якщо значення True, побудувати графік автокореляції для всіх значень затримки.\n\nПовертає\n\nr (float) - крос-кореляція сигналу з самим собою на різних часових лагах. Мінімальний часовий лаг дорівнює 0, максимальний часовий лаг дорівнює довжині сигналу. Або значення кореляції на певному часовому лазі, якщо лаг не дорівнює None.\ninfo (dict) - Словник, що містить додаткову інформацію, наприклад, довірчий інтервал.\n\n\n# розрахунок автокореляції\n\nr_init, _ = nk.signal_autocor(time_ser.values, \n                              method='correlation')  # для вихідних значень ряду                                                                    \nr_ret, _ = nk.signal_autocor(ret, \n                             method='correlation')   # для прибутковостей\nr_vol, _ = nk.signal_autocor(np.abs(ret), \n                             method='correlation')   # для модулів прибутковостей\n\nr_range = np.arange(1, len(r_ret) + 1)               # генерація лагів\n\n\nfig, ax = plt.subplots()                    # Створюємо порожній графік\n\nax.plot(r_range, r_init[1:], label=symbol)  # Додаємо дані до графіку\nax.plot(r_range, r_ret, label=r'$g(t)$')                          \nax.plot(r_range, r_vol, label=r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Autocorrelation r\")          # Додаємо підпис для вісі Оу\nax.set_ylim(-1.1, 1.1)                      # Встановлюємо обмеження по вісі Oy\n\nplt.savefig(f'Автокореляції {symbol}.jpg')  # Зберігаємо графік \nplt.show();                                 # Виводимо графік\n\n\n\n\nРис. 1.5: Зміна з часом парних автокореляційних функцій для вихідного ряду x, нормалізованих прибутковостей g та їх модулів mod(g)\n\n\n\n\nАле, досліджуючи складні системи варто пам’ятати, що їх складність є варіативною. Тому і внутрішні кореляції системи на різних часових лагах також варіюються з плином часу. Із цього випливає, що подальши розрахунки варто виконувати не для всього ряду, а для його фрагментів.\nПодальші розрахунки здійснюватимуться в рамках алгоритму ковзного (рухомого) вікна. Для цього виділятиметься частина часового ряду (вікно), для якої розраховуватимуться міри складності, потім вікно зміщуватиметься разом з часовим рядом на заздалегідь визначену величину, і процедура повторюватиметься до тих пір, поки значення всього ряду не будуть вичерпані. Далі, порівнюючи динаміку фактичного часового ряду і відповідних мір складності, ми матимемо змогу судити про характерні зміни в динаміці міри складності зі зміною досліджуваної системи. Якщо та чи інша міра складності поводиться певним чином для всіх періодів крахів, наприклад, зменшується або збільшується під час передкризовий або передкритичний період, то вона може служити їх індикатором або провісником.\nРозглянемо як поводитиме себе функція автокореляцій та волатильність в рамках алгоритму ковзного вікна.\nСпочатку визначимо параметри\n\nret_type = 4 # вид ряду: \n             # 1 - вихідний, \n             # 2 - детрендований (різниця між теп. значенням та попереднім)\n             # 3 - прибутковості звичайні, \n             # 4 - стандартизовані прибутковості, \n             # 5 - абсолютні значення (волатильності),\n             # 6 - стандартизований вихідний ряд \n\nlength = len(time_ser) # довжина всього ряду\n\nwindow = 250    # довжина вікна - період у межах якого розраховуватимуться наші індикатори\ntstep = 1       # крок зміщення вікна\nvolatility = [] # масив значень волатильностей \nautocorr = []   # масив значень автокореляції при змінній lag\n\nДалі розпочнемо розрахунки. Для відслідковування прогресу зміщення ковзного вікна скористаємось бібліотекою tqdm. Її можна встановити аналогічно попереднім бібліотекам.\n\n!pip install tqdm\n\nІмпортуємо модуль для візуалізації прогресу\n\nfrom tqdm import tqdm\n\nі тепер приступимо до виконання віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # Фрагменти довжиною window  \n                                              # з кроком tstep\n                                              \n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент\n\n                                          # подальшому відбираємо потрібний тип ряду                                         \n    if ret_type == 1:                     # вихідні значення \n        pass\n    elif ret_type == 2:                   # різниці\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:                   # прибутковості\n        fragm = fragm.pct_change()\n    elif ret_type == 4:                   # стандартизовані прибутковості\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:                   # абсолютні значення прибутковостей\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        fragm = fragm.abs()\n    elif ret_type == 6:                   # стандартизований вихідний ряд\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values         # видаляємо зайві нульові значення, якщо є\n    \n    # розрахунок віконної автокореляції\n    r_window, _ = nk.signal_autocor(fragm, method='correlation') \n\n    # розрахунок волатильності по модулям прибутковостей                                     \n    vol_window = np.mean(np.abs(fragm))\n\n    # збереження результатів до масивів\n    volatility.append(vol_window)\n    autocorr.append(r_window[1])\n\n100%|██████████| 1393/1393 [00:01&lt;00:00, 884.25it/s]\n\n\nЗбережемо результати в окремих текстових файлах\n\n# збереження результатів ковзної автокореляції\nnp.savetxt(f\"autocorr_name={symbol}_ \\\n            window={window}_step={tstep}_ \\\n            rettype={ret_type}.txt\", autocorr)\n\n# збереження результатів ковзної волатильності\nnp.savetxt(f\"volatility_name={symbol}_ \\\n            window={window}_step={tstep}_ \\\n            rettype={ret_type}.txt\", volatility)\n\nНарешті порівняємо динаміку вихідного ряду і розрахованих похідних. Для цього врахуємо, що автокореляцію і волатильність ми рахували для рухомого вікна. Результати представлено на Рис. 1.6.\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.10))\nax4.spines.right.set_position((\"axes\", 1.19))\n\np1, = ax.plot(time_ser.index[window:length:tstep], \n              time_ser.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window+1:length:tstep], \n               ret[window:length:tstep], \"r--\", label=r\"$G(t)$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], \n               autocorr, \"g-\", label=r\"$A$\")\np4, = ax4.plot(time_ser.index[window:length:tstep],\n               volatility, \"m-\", label=r\"$V$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax4.legend(handles=[p1, p2, p3, p4])\n\nplt.savefig(f\"all_name={symbol}_ret={ret_type}_\\\n            wind={window}_step={tstep}.jpg\")\nplt.show();\n\n\n\n\nРис. 1.6: Динаміка індексу Біткоїна, нормалізованих прибутковостей, віконних автокореляції та волатильності\n\n\n\n\nАналізуючи графік, можна зробити висновок, що у певні моменти спостерігалися стрибки волатильності (як і автокореляції) із поступовим зменшенням її до попереднього рівня, що може бути внаслідок збурень у процесі роботи ринку. Аналіз таких збурень, їх частоти та сили, дозволяє виявляти приховані закономірності роботи ринку."
  },
  {
    "objectID": "lab_1.html#висновок",
    "href": "lab_1.html#висновок",
    "title": "1  Лабораторна робота № 1",
    "section": "1.3 Висновок",
    "text": "1.3 Висновок\nТаким чином, аналіз флуктуацій прибутковостей та волатильностей шляхом побудови функції автокореляції та розподілу ймовірності дозволяє отримати певні висновки, що можуть допомогти в роботі із аналізованими часовими рядами і ринком, з якого взято зазначені часові ряди. Зокрема, у даному випадку, можна давати рекомендації аналітикам, що працюють на фінансових ринках."
  },
  {
    "objectID": "lab_1.html#завдання-для-самостійної-роботи",
    "href": "lab_1.html#завдання-для-самостійної-роботи",
    "title": "1  Лабораторна робота № 1",
    "section": "1.4 Завдання для самостійної роботи",
    "text": "1.4 Завдання для самостійної роботи\n\nОтримати індекс часового ряду у викладача\nПровести дослідження згідно інструкції\nДослідити зміни розрахованих величин для вікон 100 і 500, з кроком 1. Порівняти результати\nЗробити висновки"
  },
  {
    "objectID": "lab_1.html#контрольні-питання",
    "href": "lab_1.html#контрольні-питання",
    "title": "1  Лабораторна робота № 1",
    "section": "1.5 Контрольні питання",
    "text": "1.5 Контрольні питання\n\nПорівняйте вид залежностей флуктуацій цін і прибутковостей. Чому при розрахунках користуються не цінами, а прибутковостями?\nЯку характеристику ряду визначає волатильність?\nУ чому причина різних залежностей для прибутковостей та їх модулів?"
  },
  {
    "objectID": "lab_1.html#додаток",
    "href": "lab_1.html#додаток",
    "title": "1  Лабораторна робота № 1",
    "section": "1.6 Додаток",
    "text": "1.6 Додаток\nДля обрання часового індексу часового ряду використаємо дані, що розміщенні на сайті Yahoo! Finance. Оскільки окремі фінансові показники не завжди є доступними, будемо використовувати список компаній, що входять до індексу DJIA. За вказаним посиланням та номером у списку групи оберіть компанію, що входить до індексу та проведіть відповідні розрахунки. Порівняйте отримані результати з Біткоїном."
  },
  {
    "objectID": "lab_2.html#теоретичні-відомості",
    "href": "lab_2.html#теоретичні-відомості",
    "title": "2  Лабораторна робота № 2",
    "section": "2.1 Теоретичні відомості",
    "text": "2.1 Теоретичні відомості\nДослідження складних систем, як природних, так і штучних, показали, що в їх основі лежать нелінійні процеси, ретельне вивчення яких необхідне для розуміння і моделювання складних систем. У останні десятиліття набір традиційних (лінійних) методик дослідження був істотно розширений нелінійними методами, одержаними з теорії нелінійної динаміки і хаосу; багато досліджень були присвячені оцінці нелінійних характеристик і властивостей процесів, що протікають в природі (скейлінг, фрактальна розмірність). Проте більшість методів нелінійного аналізу вимагає або достатньо довгих, або стаціонарних рядів даних, які досить важко одержати з природи. Більш того, було показано, що дані методи дають задовільні результати для моделей реальних систем, що ідеалізуються. Ці чинники вимагали розробки нових методик нелінійного аналізу даних.\nСтан природних або штучних систем, як правило, змінюється в часі. Вивчення цих, часто складних, процесів — важлива задача в багатьох дисциплінах, дозволяє зрозуміти і описати їх суть, наприклад, для прогнозування стану на деякий час в майбутнє. Метою таких досліджень є знаходження математичних моделей, які б достатньо відповідали реальним процесам і могли б бути використані для розв’язання поставлених задач.\nРозглянемо ідею і коротко теорію рекурентного аналізу, наведемо деякі приклади, розглянемо його можливі області застосування при аналізі і прогнозування складних фінансово-економічних систем.\n\n2.1.1 Фазовий простір та його реконструкція\nСтан системи описується її змінними стану\n\\[\nx^1(t),x^2(t),...,x^d(t)\n\\]\nде верхній індекс — номер змінної. Набір із \\(d\\) змінних стану у момент часу \\(t\\) складає вектор стану \\(\\vec x(t)\\) в \\(d\\)-вимірному фазовому просторі. Даний вектор переміщається в часі в напрямі,визначуваному його вектором швидкості:\n\\[\n\\dot{\\vec x}(t)=\\partial_t\\vec x(t)=\\vec F(t)\n\\]\nПослідовність векторів \\(\\vec x(t)\\) утворює траєкторію у фазовому просторі, причому поле швидкості \\(\\vec F\\) дотичне до цієї траєкторії. Еволюція траєкторії описує динаміку системи і її атрактор. Знаючи \\(\\vec F\\), можна одержати інформацію про стан системи в момент \\(t\\) шляхом інтегрування виразу. Оскільки форма траєкторії дозволяє судити про характер процесу (періодичні або хаотичні процеси мають характерні фазові портрети), то для визначення стану системи не обов’язково проводити інтегрування, достатньо побудувати графічне відображення траєкторії.\nПри дослідженні складних систем часто немає інформації про всі змінні стану, або не все з них можливо виміряти. Як правило, є єдине спостереження, проведене через дискретний часовий інтервал \\(\\Delta t\\). Таким чином, вимірювання записуються у вигляді ряду \\(u_i(t)\\) i , де \\(t=i\\cdot \\Delta t\\). Інтервал \\(\\Delta t\\) може бути постійним, проте це не завжди можливо і створює проблеми для застосування стандартних методів аналізу даних, що вимагають рівномірної шкали спостережень.\nВзаємодії і їх кількість в складних системах такі, що навіть по одній змінній стану можна судити про динаміку всієї системи в цілому (даний факт був встановлений групою американських учених при вивченні турбулентності). Таким чином, еквівалентна фазова траєкторія, що зберігає структури оригінальної фазової траєкторії, може бути відновлена з одного спостереження або часового ряду за теоремою Такенса (Takens) методом часових затримок:\n\\[\n\\widehat{\\vec x}(t)=(u_i,u_{i+\\tau},...,u_{i+(m-1)\\tau})\n\\]\nде \\(m\\) — розмірність вкладення, \\(\\tau\\) — часова затримка (реальна часова затримка визначається як \\(\\tau \\cdot \\Delta t\\)). Топологічні структури відновленої траєкторії зберігаються, якщо \\(m \\geq 2 \\cdot d+1\\), де \\(d\\) — розмірність атрактора. На практиці більшості випадків атрактор може бути відновлений і при \\(m \\leq 2d\\). Затримка, як правило, вибирається апріорно.\nІснує кілька підходів до вибору мінімально достатньої розмірності \\(m\\), крім аналітичного. Високу ефективність показали методи, засновані на концепції фальшивих найближчих точок (false nearest neighbours, FNN). Суть її заключається у тому, що при зменшенні розмірності вкладення відбувається збільшення кількості фальшивих точок, що потрапляють в околицю будь-якої точки фазового простору. Звідси витікає простий метод — визначення кількості FNN як функції від розмірності. Існують і інші методи, засновані на цій концепції — наприклад, визначення відносин відстаней між одними і тими ж сусідніми точками при різних \\(m\\). Розмірність атрактора також може бути визначена за допомогою крос-кореляційних сум.\n\n\n\n\n \n\n\nРис. 2.1: Відрізок траєкторії у фазовому просторі системи Рьослера \\(i\\) (a); відповідний рекурентний графік (b). Вектор фазового простору в точці \\(j\\), який потрапляє в околицю (сіре коло в (a)) заданого вектора фазового простору вектора в точці \\(i\\) вважається точкою рекурентності (чорна точка на траєкторії в (a)). Вона позначається чорною точкою на рекурентній діаграмі у позиції \\((i, j)\\). Вектор фазового простору за межами околу (порожнє коло в (a)) призводить до білої точки в рекурентній діаграмі\n\n\n\n\n2.1.2 Рекурентний аналіз\nПроцесам в природі властива яскраво виражена рекурентна поведінка, така, як періодичність або іррегулярна циклічність. Більш того, рекурентність (повторюваність) станів в значенні проходження подальшої траєкторії достатньо близько до попередньої є фундаментальною властивістю дисипативних динамічних систем. Ця властивість була відмічена ще в 80-х роках XIX століття французьким математиком Пуанкаре (Poincare) і згодом сформульовано у вигляді “теореми рекурентності”, опублікованої в 1890 р.:\n\n\n\n\n\n\nПримітка\n\n\n\nЯкщо система зводить свою динаміку до обмеженої підмножини фазового простору, то система майже напевно, тобто з вірогідністю, практично рівною 1, скільки завгодно близько повертається до якого-небудь спочатку заданого режиму.\n\n\nСуть цієї фундаментальної властивості у тому, що, не дивлячись на те, що навіть саме мале збурення в складній динамічній системі може привести систему до експоненціального відхилення від її стану, через деякий час система прагне повернутися до стану, деяким чином близького до попереднього, і проходить при цьому подібні етапи еволюції.\nПереконатися в цьому можна за допомогою графічного зображення траєкторії системи у фазовому просторі. Проте можливості такого аналізу сильно обмежені. Як правило, розмірність фазового простору складної динамічної системи більша трьох, що робить практично незручним його розгляд напряму; єдина можливість — проекції в дво- і тривимірні простори, що часто не дає вірного уявлення про фазовий портрет.\nУ 1987 р. Екман (Eckmann) і співавтори запропонували спосіб відображення \\(m\\)-вимірної фазової траєкторії станів системи \\(\\vec x(t)\\) завдовжки \\(N\\) на двовимірну квадратну двійкову матрицю розміром \\(N \\times N\\) , в якій 1 (чорна точка) відповідає повторенню стану при деякому часі \\(i\\) в деякий інший час \\(j\\), а обидві координатні осі є осями часу. Таке представлення було назване рекурентною картою або діаграмою (recurrence plot, RP), оскільки воно фіксує інформацію про рекурентну поведінку системи.\nМатематично вищесказане описується як\n\\[\nR_{i,j}^{m,\\varepsilon_i}=\\Theta(\\varepsilon_i-\\| \\vec x_i - \\vec x_j \\|), \\cdot \\vec x \\in \\Re^m, \\cdot i, j=1...N\n\\]\nде \\(N\\) — кількість даних станів, \\(x_i, \\varepsilon_i\\) — розмір околиці точки \\(\\vec x\\) у момент \\(i\\), \\(\\| \\cdot \\|\\) — норма і \\(\\Theta(\\cdot)\\) — функція Хевісайда.\nНепрактично і, як правило, неможливо знайти повну рекурентність у значенні \\(\\vec x_i \\equiv \\vec x_j\\) (стан динамічної, а особливо — хаотичної системи не повторюється повністю еквівалентно початковому стану, а підходить до нього скільки завгодно близько). Таким чином, рекурентність визначається як достатня близькість стану \\(\\vec x_j\\) до стану \\(\\vec x_i\\). Іншими словами, рекурентними є стани \\(\\vec x_j\\), які потрапляють в \\(m\\)-вимірну околицю з радіусом \\(\\varepsilon_i\\) і центром в \\(\\vec x_i\\). Ці точки \\(\\vec x_j\\) називаються рекурентними точками (recurrence points).\nОскільки \\(R_{i,i}=1\\), \\(i=1,...,N\\) за визначенням, то рекурентна діаграма завжди міститьчорну діагональну лінію — лінію ідентичності (line of identity, LOI) під кутом \\(\\pi/4\\) до осей координат. Довільно узята рекурентна точка не несе якої-небудь корисної інформації про стани в часи \\(i\\) і \\(j\\). Тільки вся сукупність рекурентних точок дозволяє відновити властивості системи.\nЗовнішній вигляд рекурентної діаграми дозволяє судити про характер процесів, які протікають в системі, наявності і впливі шуму, станів повторення і завмирання (ламінарності), здійсненні в ході еволюції системи різких змін стану (екстремальних подій).\n\n\n    \nРис. 2.2: Типові динамічні ряди і їх рекурентні карти\n\n\n\n\n2.1.3 Аналіз діаграм\nОчевидно, що процеси різної поведінки даватимуть рекурентні діаграми з різним рисунком. Таким чином, візуальна оцінка діаграм може дати уявлення про еволюцію досліджуваної траєкторії. Виділяють два основних класи структури зображення: топологія (typology), що представляється крупномасштабними структурами, і текстура (texture), що формується дрібномасштабними структурами.\nТопологія дає загальне уявлення про характер процесу. Виділяють чотири основні класи:\n\nоднорідні рекурентні діаграми типові для стаціонарних і автономних систем, в яких час релаксації малий у порівнянні з довжиною ряду;\nперіодичні структури, що повторюються (діагональні лінії, узори у шаховому порядку) відповідають різним осцилюючим системам з періодичністю в динаміці;\nдрейф відповідає системам з параметрами, що поволі змінюються, що робить білими лівий верхній і правий нижній кути рекурентної діаграми;\nрізкі зміни в динаміці системи, рівно як і екстремальні ситуації, обумовлюють появу білих областей або смуг.\n\nРекурентні діаграми спрощують виявлення екстремальних і рідкісних подій.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nРис. 2.3: Характернi топологiї рекурентних дiаграм: (а) — однорiдна (нормально розподiлений шум); (b) — перiодична (генератор Ван дер Поля); (c) — дрейф (вiдображення Iкеди з накладеною послiдовнiстю, що лiнiйно росте); (d) — контрастнi областi або смуги (узагальнений броунiвський рух)\n\n\nДокладний розгляд рекурентних діаграм дозволяє виявити дрібномасштабні структури — текстуру, яка складається з простих точок, діагональних, горизонтальних і вертикальних ліній. Комбінації вертикальних і горизонтальних ліній формують прямокутні кластери точок.\n\nсамотні, окремо розташовані рекурентні точки з’являються в тому разі, коли відповідні стани рідкісні, або нестійкі в часі, або викликані сильною флуктуацією. При цьому вони не є ознаками випадковості або шуму;\nдіагональні лінії \\(R_{i+k, j+k}=1\\) (при \\(k = 1...l\\) де \\(l\\) — довжина діагональної лінії) з’являються у разі, коли сегмент траєкторії у фазовому просторі пролягає паралельно іншому сегменту, тобто траєкторія повторює саму себе, повертаючись в одну і ту ж область фазового простору у різний час. Довжина таких ліній визначається часом, протягом якого сегменти траєкторії залишаються паралельними; напрям (кут нахилу) ліній характеризує внутрішній час підпроцесів, відповідних даним сегментам траєкторії. Проходження ліній паралельно лінії ідентичності (під кутом \\(\\pi/4\\) до осей координат) свідчить про однаковий напрям сегментів траєкторії, перпендикулярно — про протилежний («відображені» сегменти), що може також бути ознакою реконструкції фазового простору з невідповідною розмірністю вкладення. Нерегулярна поява діагональних ліній є ознакою хаотичного процесу;\nвертикальні (горизонтальні) лінії \\(R_{i, j+k}=1\\) (при \\(k = 1...\\upsilon\\), де \\(\\upsilon\\) — довжина вертикальної або горизонтальної лінії) виділяють проміжки часу, в котрі стан системи не змінюється або змінюється трохи (система як би «заморожена» на цей час), що є ознакою «ламінарних» станів.\n\n\n\n\nРис. 2.4: Основнi концепцiї рекурентного аналiзу. Вiдображена дiаграма рекурентностi базується на часовому ряду, що було реконструйовано до 11 реконструйованих векторiв, вiд \\(\\vec{X}(0)\\) до \\(\\vec{X}(10)\\). Видiлено дiагональну лiнiю довжиною \\(d = 3\\), вертикальна лiнiя довжиною \\(v = 3\\) i бiлу вертикальну лiнiю довжиною \\(w = 5\\)"
  },
  {
    "objectID": "lab_2.html#хід-роботи",
    "href": "lab_2.html#хід-роботи",
    "title": "2  Лабораторна робота № 2",
    "section": "2.2 Хід роботи",
    "text": "2.2 Хід роботи\nСпочатку побудуємо дво- та тривимірні фазові портрети як для модельних значень, так і для реальних. Використовуватимемо бібліотеки neurokit2 для побудови атракторів та рекурентного аналізу.\n\n2.2.1 Процедура реконструкції фазового простору\nДля побудови фазового портрету скористаємось методами complexity_attractor() та complexity_embedding() бібліотеки neuralkit2. Синтаксис complexity_attractor() виглядає наступним чином:\ncomplexity_attractor(embedded='lorenz', alpha='time', color='last_dim', shadows=True, linewidth=1, **kwargs)\nПараметри\n\nembedded (Union[str, np.ndarray]) — результат функції complexity_embedding(). Також може бути рядком, наприклад, \"lorenz\" (атрактор Лоренца) або \"rossler\" (атрактор Рьосслера).\nalpha (Union[str, float]) — прозорість ліній. Якщо \"time\", то лінії будуть прозорими як функція часу (повільно).\ncolor (str) — Колір графіку. Якщо \"last_dim\", буде використано останній вимір (максимум 4-й) вбудованих даних, коли розмірність більша за 2. Корисно для візуалізації глибини (для 3-вимірного вбудовування), або четвертого виміру, але працюватиме це повільно.\nshadows (bool) — якщо значення True, 2D-проекції буде додано до бокових сторін 3D-атрактора.\nlinewidth (float) — задає товщину лінії.\nkwargs — До палітри кольорів (наприклад, name=\"plasma\") або до симулятора системи Лоренца передаються додаткові аргументи ключових слів, такі як duration (за замовчуванням = 100), sampling_rate (за замовчуванням = 10), sigma (за замовчуванням = 10), beta (за замовчуванням = 8/3), rho (за замовчуванням = 28).\n\nЯк вже зазначалося, побудова фазового простору, на основі якого і проводитиметься рекурентний аналіз, вимагає реконструкції. Виконати реконструкції фазового простору із одновимірного часового ряду можна із використанням методу часових затримок.\nМетод часових затримок є однією з ключових концепцій науки про складність, що ми використовуватимемо і в подальших лабораторних. Він базується на ідеї, що динамічна система може бути описана вектором чисел, який називається її “станом”, що має на меті забезпечити повний опис системи в певний момент часу. Множина всіх можливих станів називається “простором станів”.\nТеорема Такенса (1981) припускає, що послідовність вимірювань динамічної системи містить у собі всю інформацію, необхідну для повної реконструкції простору станів. Метод часових затримок намагається визначити стан \\(s\\) системи в певний момент часу \\(t\\), шукаючи в минулій історії спостережень схожі стани, і, вивчаючи еволюцію схожих станів, виводити інформацію про майбутнє системи.\nЯк візуалізувати динаміку системи? Послідовність значень стану в часі називається траєкторією. Залежно від системи, різні траєкторії можуть еволюціонувати до спільної підмножини простору станів, яка називається атрактором. Наявність та поведінка атракторів дає інтуїтивне уявлення про досліджувану динамічну систему.\nОдже, згідно Такенсу, ідея полягає в тому, щоб на основі одиничних вимірювань системи, отримати \\(m\\)-розмірні реконструйовані часові вкладення\n\\[\n\\vec{y}_i = \\left( y_i, y_{i+\\tau}, ... , y_{i+(m-1)\\tau} \\right),\n\\tag{2.1}\\]\nде \\(i\\) проходить в діапазоні \\(1,..., N-(m-1)\\tau\\); значення \\(\\tau\\) представляє часову затримку, а \\(m\\) — це розмірність вкладень (кількість змінних, що включає кожна траєкторія).\nКод для реконструкції фазового простору може виглядати наступним чином:\n\ndef complexity_embedding(signal, dimension, delay):\n    N = len(signal)                                        # вимірюємо довжину сигналу\n    Y = np.zeros((dimension, N - (dimension - 1) * delay)) # ініціалізуємо масив нулів,\n                                                           # що будуть представляти траєкторії\n    for i in range(dimension):\n        Y[i] = signal[i * delay : i * delay + Y.shape[1]]  # заповнюємо кожну траєкторію \n\n    embedded = Y.T                                          \n    return embedded                                        # повертаємо результат \n\nДля реконструкції фазового простору використовуватимемо метод complexity_embedding(). Його синтаксис виглядає наступним чином:\ncomplexity_embedding(signal, delay=1, dimension=3, show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень. Також може бути рядком, наприклад, \"lorenz\" (атрактор Лоренца), \"rossler\" (атрактор Росслера) або \"clifford\" (атрактор Кліффорда) для отримання попередньо визначеного атрактора.\ndelay (int) — часова затримка (часто позначається \\(\\tau\\) іноді називають запізненням). Ще розглянемо метод complexity_delay() для оцінки оптимального значення цього параметра.\ndimension (int) — розмірність вкладень (\\(m\\), іноді позначається як \\(d\\) або порядок). Далі звернемось до методу complexity_dimension(), щоб оцінити оптимальне значення для цього параметра.\nshow (bool) — Побудувати графік реконструйованого атрактора.\nkwargs — інші аргументи, що передаються до complexity_attractor().\n\nПовертає\n\narray — реконструйований атрактор розміру length - (dimension - 1) * delay\n\nДалі імпортуємо необхідні для подальшої роботи модулі\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport scienceplots\nimport pandas as pd\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виводу\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nТепер розглянемо можливість використання методу часових затримок і отриманих в подальшому атракторів у якості індикатора складності. Як і в попередній роботі, для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи yfinance:\n\nsymbol = 'BTC-USD'       # Символ індексу\nstart = \"2015-09-01\"     # Дата початку зчитування даних\nend = \"2020-03-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nСпочатку оберемо вид ряду: 1. вихідний ряд 2. детермінований (різниця між теперішнім та попереднім значенням) 3. прибутковості звичайні 4. стандартизовані прибутковості 5. абсолютні значення (волатильності) 6. стандартизований ряд\nДля подальших розрахунків накращим варіантом буде вибір стандартизованого вихідного ряду або прибутковостей, оскільки значення вихідного часового ряду відрізняються на декілька порядків, і можуть сильно перевищувати встановлений параметр \\(\\varepsilon\\). Тобто, для вихідних значень, що сильно різняться між собою, увесь часовий діапазон буде розглядатися як нерекурентний.\nСпочатку визначимо функції для виконання перетворення ряду:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nі тепер виконаємо перетворення, використовуючи дану функцію:\n\nsignal = time_ser.copy()\nret_type = 6    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rec = transformation(signal, ret_type) \n\nОскільки ми не матимемо змоги візуалізувати багатовимірний фазовий простір (\\(m&gt;3\\)), ми послуговуватимемось значеннями \\(m=2\\) та \\(m=3\\). Значення \\(\\tau\\) будемо варіювати як із власних переконань, так і з опорою на функціонал бібліотеки neuralkit2.\nСкористаємось методом complexity_simulate() для генерації різних тестових сигналів.\n\nsignal_random_walk = nk.complexity_simulate(duration=30, \n                                            sampling_rate=100, \n                                            method=\"randomwalk\") # симуляція випадкового блукання\n\n\nnk.complexity_attractor(embedded=nk.complexity_embedding(signal_random_walk, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"orange\"); \n\n\n\n\nРис. 2.5: Двовимірний фазовий портрет випадкового блукання\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_random_walk, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"orange\");\n\n\n\n\nРис. 2.6: Тривимірний фазовий портрет випадкового блукання\n\n\n\n\n\nsignal_ornstein = nk.complexity_simulate(duration=30, \n                                        sampling_rate=100, \n                                        method=\"ornstein\") # симуляція системи Орнштайна\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_ornstein, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРис. 2.7: Двовимірний фазовий портрет системи Орнштайна\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_ornstein, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРис. 2.8: Двовимірний фазовий портрет системи Орнштайна\n\n\n\n\n\nnk.complexity_attractor(color = \"last_dim\", alpha=\"time\", duration=1);\n\n\n\n\nРис. 2.9: Тривимірний фазовий портрет атрактора Лоренца\n\n\n\n\n\nnk.complexity_attractor(\"rossler\", color = \"blue\", alpha=1, sampling_rate=5000);\n\n\n\n\nРис. 2.10: Тривимірний фазовий портрет атрактора Рьосслера\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"lime\"); \n\n\n\n\nРис. 2.11: Двовимірний фазовий портрет вихідних значень досліджуваного ряду Біткоїна\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"lime\"); \n\n\n\n\nРис. 2.12: Тривимірний фазовий портрет вихідних значень досліджуваного ряду Біткоїна\n\n\n\n\nУ зазначених вище прикладах прикладах ми обирали параметри \\(m\\) і \\(\\tau\\) згідно нашим власним міркуванням. Але, як правило, при виконанні серйозного дослідження, що матиме прикладне застосування, лише власних переконань буває недостатньо. У нашому випадку бажано було б, щоб зазначені параметри обирались автоматично, опираючись на конкретну статистичну процедуру. Бібліотека neurokit2 представляє функціонал для автоматичного підбору параметрів розмірності та часової затримки. Коротко їх опишемо.\n\n\n2.2.2 Автоматизований підбір параметра часової затримки, \\(\\tau\\)\nЧасова затримка (Tau \\(\\tau\\) також відома як Lag) є одним з двох критичних параметрів, що беруть участь у процедурі реконструкції фазового простору. Він відповідає затримці у відліках між вихідним сигналом і його затриманою версією (версіями). Іншими словами, скільки відліків ми розглядаємо між певним станом сигналу та його найближчим минулим станом.\nЯкщо \\(\\tau\\) менше оптимального теоретичного значення, послідовні координати стану системи корельовані і атрактор недостатньо розгорнутий. І навпаки, коли \\(\\tau\\) більше, ніж повинно бути, послідовні координати майже незалежні, що призводить до некорельованої та неструктурованої хмари точок.\nВибір параметрів затримки та розмірності представляє нетривіальну задачу. Один з підходів полягає у їх (напів)незалежному виборі (оскільки вибір розмірності часто вимагає затримки) за допомогою функцій complexity_delay() та complexity_dimension(). Однак, існують методи спільного оцінювання, які намагаються знайти оптимальну затримку та розмірність одночасно.\nЗауважте також, що деякі автори (наприклад, Розенштейн, 1994) пропонують спочатку визначити оптимальну розмірність вбудовування, а потім розглядати оптимальне значення затримки як оптимальну затримку між першою та останньою координатами затримки (іншими словами, фактична затримка має дорівнювати оптимальній затримці, поділеній на оптимальну розмірність вбудовування мінус 1).\nДекілька авторів запропонували різні методи для вибору затримки:\n\nФрейзер і Свінні (1986) пропонують використовувати перший локальний мінімум взаємної інформації між затриманим і незатриманим часовими рядами, ефективно визначаючи значення \\(\\tau\\), для якого вони діляться найменшою інформацією (і де атрактор є найменш надлишковим). На відміну від автокореляції, взаємна інформація враховує також нелінійні кореляції.\nТейлер (1990) запропонував вибирати таке значення \\(\\tau\\), при якому автокореляція між сигналом та його зміщенною версією при \\(\\tau\\) вперше перетинає значення \\(1/\\exp\\). Методи, що базуються на автокореляції, мають перевагу в короткому часі обчислень, коли вони обчислюються за допомогою алгоритму швидкого перетворення Фур’є (fast Fourier transform, FFT).\nКасдаглі (1991) пропонує замість цього брати перший нульовий перетин автокореляції.\nРозенштейн (1993) пропонує апроксимувати точку, де функція автокореляцій падає до \\(\\left( 1-1/\\exp \\right)\\) від свого максимального значення.\nРозенштейн (1994) пропонує наближатися до точки, близької до 40% нахилу середнього зміщення від діагоналі.\nКім (1999) пропонує оцінювати Tau за допомогою кореляційного інтегралу, який називається C-C методом, і який, як виявилося, узгоджується з результатами, отриманими за допомогою методу взаємної інформації. Цей метод використовує статистику в реконструйованому фазовому просторі, а не аналізує часову еволюцію ряду. Однак час обчислень для цього методу значно довший через необхідність порівнювати кожну унікальну пару парних векторів у реконструйованому сигналі на кожну затримку.\nЛайл (2021) описує “Реконструкцію симетричного проекційного атрактора” (Symmetric Projection Attractor Reconstruction, SPAR), де \\(1/3\\) від домінуючої частоти (тобто довжини середнього “циклу”) може бути підходящим значенням для приблизно періодичних даних, і робить атрактор чутливим до морфологічних змін. Див. також доповідь Астона. Цей метод також є найшвидшим, але може не підходити для аперіодичних сигналів. Аргумент algorithm (за замовчуванням \"fft\") передається до аргументу method методу signal_psd().\n\nМожна також зазначити наступний метод для об’єднаного підбору параметрів затримки та розмірності:\n\nГаутама (2003) зазначає, що на практиці часто використовують фіксовану часову затримку і відповідно регулюють розмірність вбудовування. Оскільки це може призвести до великих значень \\(m\\) (а отже, до вкладених даних великого розміру) і, відповідно, до повільної обробки, вони описують метод оптимізації для спільного визначення \\(m\\) і \\(\\tau\\) на основі показника entropy ratio.\n\nРозглянемо оптимальні значення розмірності та затримки для часового сигналу Біткоїна:\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"fraser1986\")\n\n\n\n\nРис. 2.13: Оптимальне значення часової затримки на основі методу Фрейзера і Свінні для часового ряду Біткоїна\n\n\n\n\nРис. 2.13 показує, що перший локальний мінімум взаємної інформації для стандартизованих вихідних значень Біткоїна знаходиться на 273 лагу. Для візуального огляду реконструйованого атрактора це значення, можливо, є найбільш адекватним. Але використовуючи настільки велику часову затримку, ми втрачаємо доволі багато проміжних значень, що також можуть містити досить важливу приховану інформацію для кількісних розрахунків.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"theiler1990\")\n\n\n\n\nРис. 2.14: Оптимальне значення часової затримки на основі методу Тейлера для часового ряду Біткоїна\n\n\n\n\nРис. 2.14 демонструє, що автокореляція між стандартизованих вихідним сигналом Біткоїна та його зміщенною версією при \\(\\tau=195\\) вперше перетинає значення \\(1/\\exp\\). Бачимо, що дане значення затримки є трохи меншим за те, що було отримано до цього, але суті це не змінює. Також бачимо, що між реконструйованими атракторами для \\(\\tau=195\\) та \\(\\tau=273\\) немає кардинальної візуальної різниці.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=500, show=True,\n                                        method=\"casdagli1991\")\n\ndelay\n\nЯк можна бачити по прикладу вище, не всі методи надають адекватну оцінку розмірності нашого сигналу. Спробуємо привести вихідні значення Біткоїна до прибутковостей та повторити процедуру Касдаглі ще раз.\n\nret_type = 4 \nret = transformation(signal, ret_type)\n\n\ndelay, parameters = nk.complexity_delay(ret, \n                                        delay_max=300, show=True,\n                                        method=\"casdagli1991\")\n\n\n\n\nРис. 2.15: Оптимальне значення часової затримки на основі методу Касдаглі для прибутковостей Біткоїна\n\n\n\n\nЦього разу нам вдалося досягти оптимального результату, але приклад вище демонструє, що кожна процедура має свої виключення. Рис. 2.15 показує, що значення прибутковостей Біткоїна характеризуються певними кореляціями лише на перших 4-ох лагах. Подальші часові зміщення роблять значення прибутковостей незалежними один від одного.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"rosenstein1993\")\n\n\n\n\nРис. 2.16: Оптимальне значення часової затримки на основі методу Розенштайна (1993) для часового ряду Біткоїна\n\n\n\n\nРис. 2.16 демонструє, що при \\(\\tau=101\\) функція автокореляцій перетинає значення \\(\\left( 1-1/\\exp \\right)\\). При цьому видно, що навіть для такого лагу зберігається значна частка кореляцій між стандартизованими вихідними значеннями Біткоїна.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"rosenstein1994\")\n\n\n\n\nРис. 2.17: Оптимальне значення часової затримки на основі методу Розенштайна (1994) для часового ряду Біткоїна\n\n\n\n\nРисунок вище показує, що при \\(\\tau=120\\) зміщення реконструйованих траєкторій від їх оригінального положення на лінії ідентичності зберігає найбільшу кількість інформації стосовно атрактора стандартизованих значень Біткоїна.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"lyle2021\")\n\n\n\n\nРис. 2.18: Оптимальне значення часової затримки на основі методу Лайла для часового ряду Біткоїна\n\n\n\n\nЗгідно представленого вище результату найбільш значущі частоти, отримані за допомогою перетворення Фур’є, зберігаються при \\(\\tau=109\\).\nТепер подивимось як це виглядатиме для об’єднаного підбору параметрів\n\ndelay, parameters = nk.complexity_delay(for_rec,\n    delay_max=np.arange(1, 10, 1), # діапазон значень затримки\n    dimension_max=10,              # максимальна розмірність вкладень\n    method=\"gautama2003\",\n    surrogate_n=5,                 # Кількість сурогатних сигналів \n                                   # для генерації\n    surrogate_method=\"random\",     # Спосіб генерації сигналів\n    show=True)\n \n\n\n\n\nРис. 2.19: Оптимальне значення розмірності та затримки на основі методу Гаутами для часового ряду Біткоїна\n\n\n\n\n\ndimension = parameters[\"Dimension\"]\ndimension\n\n10\n\n\nОскільки представлена вище процедура є доволі громіздкою в плані обчислювальних потужностей, ми обрали діапазон \\(\\tau\\) в межах від 1 до 10. Видно, що при \\(\\tau\\) близької до 3 оптимальне значення розмірності атрактора дорівнює 10. Можливо, при значеннях \\(\\tau\\) близьких до 100 або 200, ми могли б отримати зовсім інше значення розмірності, але це потребує додаткових експериментів.\n\n\n2.2.3 Автоматизований підбір параметра розмірності вкладень, \\(m\\)\nЗа дану процедуру відповідає метод complexity dimension(). Її синтаксис виглядає наступним чином:\ncomplexity_dimension(signal, delay=1, dimension_max=20, method='afnn', show=False, **kwargs)\nХоча зазвичай використовують \\(m=2\\) або \\(m=3\\), але різні автори пропонують наступні процедури підбору:\n\nКореляційна розмірність (Correlation Dimension, CD): Одним з перших методів оцінки оптимального \\(m\\) був розрахунок кореляційної розмірності для вкладень різного розміру і пошук насичення (тобто плато) в її значенні при збільшенні розміру векторів. Одне з обмежень полягає в тому, що насичення буде також мати місце, коли даних недостатньо для адекватного заповнення простору високої розмірності (зауважте, що в загальному випадку не рекомендується мати настільки великі вкладення, оскільки це значно скорочує довжину сигналу).\nНайближчі хибні сусіди (False Nearest Neighbour, FNN): Метод, запропонований Кеннелом та ін., базується на припущенні, що дві точки, які є близькими одна до одної в достатній розмірності вбудовування, повинні залишатися близькими при збільшенні розмірності. Алгоритм перевіряє сусідів при збільшенні розмірності вкладень, поки не знайде лише незначну кількість хибних сусідів при переході від розмірності \\(m\\) до \\(m+1\\). Це відповідає найнижчій розмірності вкладення, яка, як передбачається, дає розгорнуту реконструкцію просторово-часового стану. Цей метод може не спрацювати в зашумлених сигналах через марну спробу розгорнути шум (а в чисто випадкових сигналах кількість хибних сусідів суттєво не зменшується зі збільшенням \\(m\\)). На рисунку нижче показано, як проекції на простори більшої розмірності можна використовувати для виявлення хибних найближчих сусідів. Наприклад, червона та жовта точки є сусідами в одновимірному просторі, але не в двовимірному.\n\n\n\n\n\n\n\nСередні хибні сусіди (Average False Neighbors, AFN): Ця модифікація методу FNN, розроблена Сао (1997), усуває один з його основних недоліків — необхідність евристичного вибору порогових значень \\(r\\). Метод використовує максимальну евклідову відстань для представлення найближчих сусідів і усереднює всі відношення відстані в \\(m+1\\) розмірності до розмірності \\(m\\) і визначає E1 та E2 як параметри. Оптимальна розмірність відповідає досягається тоді, коли E1 перестає змінюватися (досягає плато). E1 досягає плато при розмірності d0, якщо сигнал надходить від атрактора. Тоді d0+1* є оптимальною мінімальною розмірністю вкладення. E2 є корисною величиною для того, щоб відрізнити детерміновані сигнали від стохастичних. Константа E2, що близька до 1 для будь-якої розмірності вкладень \\(d\\), вказує на випадковість даних, оскільки майбутні значення не залежать від минулих значень.\n\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка у відліках. Для вибору оптимального значення цього параметра ми ще скористаємось методом complexity_delay().\ndimension_max (int) — максимальний розмір вкладення для тестування.\nmethod (str) — Може бути \"afn\" (середні хибні сусіди), \"fnn\" (найближчий хибний сусід) або \"cd\" (кореляційна розмірність).\nshow (bool) — Візуалізувати результат.\nkwargs — інші аргументи, такі як \\(R=10.0\\) або \\(A=2.0\\) (відносне та абсолютне граничне значення, тільки для методу \"fnn\").\n\nПовертає\n\ndimension (int) — оптимальна розмірність вкладень.\nparameters (dict) — словник python, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимальної розмірності.\n\nСпробуємо отримати оптимальне значення розмірності згідно зазначених процедур. В якості часової затримки можна взять \\(\\tau=100\\). Приблизно таке значення спостерігалося для кожної процедури.\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=100,\n                                                  dimension_max=10,\n                                                  method='cd',\n                                                  show=True)\n\n\n\n\nРис. 2.20: Оптимальне значення розмірності на основі кореляційної розмірності для часового ряду Біткоїна\n\n\n\n\nРис. 2.20 представляє, що оптимальна розмірність вкладень при якій досягається найбільш інформативна репрезентація фазового простору дорівнює 7.\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=100,\n                                                  dimension_max=10,\n                                                  method='fnn',\n                                                  show=True)\n\n\n\n\nРис. 2.21: Оптимальне значення розмірності на основі найближчих хибних сусідів для часового ряду Біткоїна\n\n\n\n\nЗ представленого вище рисунку видно, що найнижча розмірності вкладення, яка, як передбачається, дає розгорнуту реконструкцію просторово-часового стану, дорівнює 3. Саме при переході від 3-ох вимірного фазового простору до 4-ох вимірного ми бачимо, що кількість хибних сусідів стає мінімальною і далі не наростає.\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=20,\n                                                  dimension_max=20,\n                                                  method='afnn',\n                                                  show=True)\n\n\n\n\nРис. 2.22: Оптимальне значення розмірності на основі середніх найближчих хибних сусідів для часового ряду Біткоїна\n\n\n\n\nАлгоритм середніх хибних сусідів показує, що тут розмірність вкладень \\(m=5\\) є найоптимальнішою. При подальшому наростанні розмірності, атрактор має походити на більш стохастичний, що вказує на втрату всіх кореляцій, що могли бути присутні в досліджуваному сигналі.\nЗгідно з представленими вище алгоритмами автоматичного підбору, розмірність вкладень можна обирати в діапазоні значень від 3 до 7. Тепер на основі отриманих результатів приступимо до побудови рекурентної діаграми.\n\n\n2.2.4 Побудова рекурентної матриці\nЯк вже зазначалося, рекурентний аналіз кількісно визначає кількість і тривалість рекурентних станів динамічної системи, що визначаються на основі реконструйованих траєкторій фазового простору.\nМи маємо змогу побудувати рекурентну матрицю, використовуючи метод recurrence_matrix().\nЙого синтаксис виглядає наступним чином:\nrecurrence_matrix(signal, delay=1, dimension=3, tolerance='default', show=False)\nПараметри\n\nsignal (Union[list, np.ndarray, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі.\ndimension (int) — розмірність вкладень, \\(m\\).\ntolerance (float) — радіус \\(\\varepsilon\\) багатовимірного околу в межах якого шукаються рекурентні траєкторії (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються схожими. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\). Емпіричним правилом є встановлення \\(r\\) таким чином, щоб відсоток точок, класифікованих як рекурентні, становив приблизно 2-5%.\nshow (bool) — візуалізувати рекурентну матрицю.\n\nПовертає\n\nnp.ndarray — рекурентну матрицю.\nnp.ndarray — матрицю відстаней.\n\nПобудуємо рекурентну матрицю для вихідних значень Біткоїна, його прибутковостей та стандартизованого вихідного ряду. Розмірність \\(m=4\\), часова затримка \\(\\tau=1\\), радіус \\(\\varepsilon=0.3\\).\n\nrc, _ = nk.recurrence_matrix(signal, \n                            delay=1, \n                            dimension=4, \n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРис. 2.23: Рекурентна матриця для вихідних значень Біткоїна\n\n\n\n\nЯк можна бачити з представленого рисунку всі траєкторії залишаються доволі віддаленими один від одного, ніякої рекурентності тут не передбачається.\nТепер спробуємо подивитися на стандартизовані прибутковості.\n\nrc, _ = nk.recurrence_matrix(ret, \n                            delay=1, \n                            dimension=4,\n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРис. 2.24: Рекурентна матриця для стандартизованих прибутковостей Біткоїна\n\n\n\n\nТепер можемо бачити, що Біткоїн став характризуватися чорними смугами, що відображають динаміку певних детермінованих процесів. У той же час білі смуги характеризують періоди абсолютно аномальної (непередбачуваної поведінки на даному ринку). Видно, що прибутковості залишаються доволі некорельованими, про що і свідчить переважне домінування саме білих областей.\nСпробуємо тепер подивитись на стандартизований вихідний ряд.\n\nrc, _ = nk.recurrence_matrix(for_rec, \n                            delay=1, \n                            dimension=4,\n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРис. 2.25: Рекурентна матриця для стандартизованого вихідного ряду Біткоїна\n\n\n\n\nНа початку свого існування Біткоїн характеризувався доволі високим ступенем передбачуваності, меншої волатильності власних коливань. Надалі почали предомінувати білі області, але видно, що тепер Біткоїну властива динаміка подібна до броунівсього руху."
  },
  {
    "objectID": "lab_2.html#завдання-для-самостійної-роботи",
    "href": "lab_2.html#завдання-для-самостійної-роботи",
    "title": "2  Лабораторна робота № 2",
    "section": "2.3 Завдання для самостійної роботи",
    "text": "2.3 Завдання для самостійної роботи\n\nОтримати індекс часового ряду у викладача\nПровести дослідження його рекурентних властивостей згідно інструкції\nПорівняти фазові портрети і рекурентні діаграми для стандартизованого вихідного ряду та прибутковостей. Що спільного між ними і чим вони відрізняються?\nЗробити висновки"
  },
  {
    "objectID": "lab_3.html#теоретичні-відомості",
    "href": "lab_3.html#теоретичні-відомості",
    "title": "3  Лабораторна робота № 3",
    "section": "3.1 Теоретичні відомості",
    "text": "3.1 Теоретичні відомості\nДля якісного опису системи графічне представлення системи підходить якнайкраще. Однак головним недоліком графічного представлення є те, що воно змушує користувачів суб’єктивно інтуїтивно інтерпретувати закономірності та структури, представлені на рекурентній діаграмі.\nКрім того, зі збільшенням розміру даних, проблематичним представляється аналіз усіх \\(N^2\\) значень. Як наслідок, доводиться працювати з окремими ділянками вихідних даних. Аналіз у такий спосіб може створювати нові дефекти, які спотворюють об’єктивність спостережуваних закономірностей і призводять до неправильних інтерпретацій. Щоб подолати це обмеження і поширити об’єктивну оцінку серед дослідників, на початку 1990-х років Веббером та Збілутом були введені визначення та процедури для кількісної оцінки складності рекурентних діаграм, а згодом вони були розширені Марваном та ін.\nДрібномасштабні кластери можуть являти собою комбінацію ізольованих точок (випадкових рекурентностей). Подібна еволюція в різні періоди часу або в зворотному часовому порядку представлятиме діагональні лінії (детерміновані структури), а також вертикальні/горизонтальні лінії для позначення ламінарних станів (переривчастість) або станів, що предсталяють сингулярності. Для кількісного опису системи системи такі дрібномасштабні кластери слугують основою кількісного рекурентного аналізу (recurrence quantification analysis, RQA)."
  },
  {
    "objectID": "lab_3.html#хід-роботи",
    "href": "lab_3.html#хід-роботи",
    "title": "3  Лабораторна робота № 3",
    "section": "3.2 Хід роботи",
    "text": "3.2 Хід роботи\nПерш ніж переходити до опису кожної з мір та розрахунків, визначемось з інструментарієм для виконання RQA. Як і до цього, ми використовуватимемо бібліотеку neuralkit2.\nТепер імпортуємо бібліотеки для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport scienceplots\nimport pandas as pd\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виводу:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд фондового індексу Доу-Джонса за період з 1 грудня 1993 по 1 грудня 2022, використовуючи yfinance:\n\nsymbol = '^DJI'          # Символ індексу\nstart = \"1993-01-01\"     # Дата початку зчитування даних\nend = \"2022-01-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиведемо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 3.1: Динаміка щоденних змін індексу Доу-Джонса\n\n\n\n\nКористуючись тими методами, що ми розглянули в попередній лабораторній роботі, побудуємо атрактор даного ряда та його рекурентну діаграму. Але перш за все, треба стандартизувати наш ряд. Для цього оголосимо функцію transformation(), що прийматиме на вхід часовий сигнал, тип ряду, і повертатиме його перетворення:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nДалі приводимо ряд до стандартизованого вигляду.\n\nsignal = time_ser.copy()\nret_type = 6    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rec = transformation(signal, ret_type) \n\nДля всього ряду і для віконної процедру визначимо наступні параметри:\n\nрозмірність вкладень \\(m=3\\);\nчасова затримка \\(\\tau=1\\);\nрадіус багатовимірного околу \\(\\varepsilon = 0.3\\).\n\nЗадамо необхідні параметри для обчислення та виводу:\n\nm = 3                         # розмірність вкладень\ntau = 1                       # часові затримка\neps = 0.3                     # радіус\n\nІ тепер подивимось на фазові траєкторії досліджуваної системи у дво- та тривимірному просторах:\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=2, delay=tau), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРис. 3.2: Двовимірний фазовий портрет стандартизованих вихідних значень досліджуваного ряду Доу-Джонса\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=3, delay=tau), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРис. 3.3: Тривимірний фазовий портрет стандартизованих вихідних значень досліджуваного ряду Доу-Джонса\n\n\n\n\nЯк можна бачити по візуальному огляду траєкторій у фазовому просторі важко робити висновки стосовно передбачуванності або хаотичності системи. Спробуємо ще раз, але тепер послуговуючись рекурентною діаграмою:\n\nrc, _ = nk.recurrence_matrix(for_rec, \n                            delay=1, \n                            dimension=m,\n                            tolerance=eps,\n                            show=True)\n\n\n\n\nРис. 3.4: Рекурентна матриця для стандартизованого вихідного ряду Доу-Джонса\n\n\n\n\nЯк можна бачити, на основі рекурентної діаграми в перспективі ми можемо отримати куди більше інформації стосовно еволюції системи. Видно, що 2000-2008 рік характеризувалися найвищим ступенем самоорганізації (рекурентності) про що свідчать доволі велика щільність чорних областей. У той же час можна бачити, що останні роки характеризуються найменшим ступенем рекурентності. Можливо, прогнозованість подій у межах 2022 року варто було б охарактеризувати за допомогою інших індикаторів, але але рекурентна матриця говорить, що події минулих років мало корелюють з теперішнім.\nМи вже зазначали, що якісна репрезентація рекурентності станів не є достатньо об’єктивною. Найращим варіантом у даному випадку буде використання рекурентного аналізу наряду с алгоритмом рухомого вікна, що використовувався нами у першій лабораторній роботі, і буде використовуватись і надалі.\n\n3.2.1 Віконна процедура\nДля подальшої роботи створюємо віконну процедуру, в якій знов визначаємо вид ряду та ще декілька параметрів. Потім ми ініціалізуємо масиви для кожної рекурентної міри.\n\nret_type = 6            # вид ряду\nwindow = 250            # ширина вікна\ntstep = 1               # часовий крок вікна \nlength = len(time_ser)  # довжина самого ряду\n\nm = 1                   # розмірність вкладень\ntau = 1                 # часові затримка\neps = 0.3               # радіус\n\n                        # Ініціалізуємо масиви для збереження віконних значень \n                        # рекурентних мір\n\nRR = []                 # Частота повторення\nDET = []                # Детермінізм\nDIV = []                # Розбіжність\nAVG_DIAG_LINE = []      # Усереднена довжина діагональних ліній\nENT_DIAG = []           # Ентропія діагональних ліній\nLAM = []                # Ламінарність\nTT = []                 # Час затримки\nENT_VERT = []           # Ентропія вертикальних ліній\nENT_WHITE_VERT = []     # Ентропія білих вертикальних ліній\nAVG_WVERT_LINE = []     # Усереднена довжина білих вертикальних ліній\nVERT_DIV = []           # Розбіжність вертикальних ліній\nRATIO_DET_REC = []      # Відношення детермінізму до частоти повторень\nRATIO_LAM_DET = []      # Відношення ламінарності до детермінізму\nWHITE_VERT_DIV = []     # Розбіжність білих вертикальних ліній\nDIAG_RR = []            # Діагональна частота рекурентних значень\n\nДля подальших розрахунків ми використовуватимемо метод complexity_rqa() бібліотеки neuralkit2. Синтаксис даного методу виглядає наступним чином:\ncomplexity_rqa(signal, dimension=3, delay=1, tolerance='sd', min_linelength=2, method='python', show=False)\nПараметри\n\nsignal (Union[list, np.ndarray, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі.\ndimension (int) — розмірність вкладень, \\(m\\).\ntolerance (float) — радіус \\(\\varepsilon\\) багатовимірного околу в межах якого шукаються рекурентні траєкторії (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються схожими. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\).\nmin_linelength (int) — мінімальна довжина діагональних та вертикальних ліній За замовчування дорівнює 2.\nmethod (str) — Може бути \"pyrqa\" для виконання рекурентного аналізу, але із використанням бібліотеки PyRQA (потребує додаткового встановлення).\nshow (bool) — візуалізувати рекурентну матрицю.\n\nПовертає\n\nrqa (DataFrame) — результати процедури RQA.\ninfo (dict) — словник, що містить інформацію відносно параметрів, що використовувались для виконання RQA.\n\nТепер можемо приступити до віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n    \n    resultRQA, _ = nk.complexity_rqa(fragm,\n                                     delay=tau,\n                                     dimension=m,\n                                     tolerance=eps)\n    \n    # Обчислення відношення ламінарності до детермінізму\n    resultRQA['LamiDet'] = resultRQA['Laminarity']/resultRQA['Determinism']\n\n    # Обчислення дивергенції чорних вертикальних ліній\n    resultRQA['VDiv'] = 1./resultRQA['VMax']\n\n    # Обчислення дивергенції білих вертикальних ліній\n    resultRQA['WVDiv'] = 1./resultRQA['WMax']\n\n    RR.append(resultRQA['RecurrenceRate'])\n    DET.append(resultRQA['Determinism'])\n    DIV.append(resultRQA['Divergence']) \n    AVG_DIAG_LINE.append(resultRQA['L'])\n    ENT_DIAG.append(resultRQA['LEn'])\n    LAM.append(resultRQA['Laminarity']) \n    TT.append(resultRQA['TrappingTime']) \n    ENT_VERT.append(resultRQA['VEn'])\n    ENT_WHITE_VERT.append(resultRQA['WEn'])\n    AVG_WVERT_LINE.append(resultRQA['W']) \n    VERT_DIV.append(resultRQA['VDiv'])\n    WHITE_VERT_DIV.append(resultRQA['WVDiv'])\n    RATIO_DET_REC.append(resultRQA['DeteRec']) \n    RATIO_LAM_DET.append(resultRQA['LamiDet'])\n    DIAG_RR.append(resultRQA['DiagRec'])\n\n100%|██████████| 7054/7054 [02:39&lt;00:00, 44.16it/s]\n\n\nЗберігаємо отримані результати в текстових файлах:\n\nname = f\"RQA_classic_name={symbol}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_m={m}_ \\\n    tau={tau}_eps={eps}.txt\"\n\nnp.savetxt(\"RR\" + name, RR)\nnp.savetxt(\"DIAG_RR\" + name, DIAG_RR)\nnp.savetxt(\"DET\" + name, DET)\nnp.savetxt(\"DIV\" + name, DIV)\nnp.savetxt(\"VERT_DIV\" + name, VERT_DIV)\nnp.savetxt(\"WHITE_VERT_DIV\" + name, WHITE_VERT_DIV)\nnp.savetxt(\"LAM\" + name, LAM)\nnp.savetxt(\"TT\" + name, TT)\nnp.savetxt(\"AVG_DIAG_LINE\" + name, AVG_DIAG_LINE)\nnp.savetxt(\"AVG_WRITE_VERT_LINE\" + name, AVG_WVERT_LINE)\nnp.savetxt(\"ENT_DIAG\" + name, ENT_DIAG)\nnp.savetxt(\"ENT_VERT\" + name, ENT_VERT)\nnp.savetxt(\"ENT_WHITE_VERT\" + name, ENT_WHITE_VERT)\nnp.savetxt(\"RATIO_DET_REC\" + name, RATIO_DET_REC)\nnp.savetxt(\"RATIO_LAM_DET\" + name, RATIO_LAM_DET)\n\n\n\n3.2.2 Рекурентні міри\nТепер займемося побудовою та інтерпретацією отриманих результатів. Для візуалізації графіків визначимо наступну функцію:\n\ndef plot_recurrence_measure(measure, label, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(time_ser.index[window:length:tstep], \n                  time_ser.values[window:length:tstep], \n                  \"b-\", label=fr\"{ylabel}\")\n    p2, = ax2.plot(time_ser.index[window:length:tstep],\n                   measure, \n                   color=clr, \n                   label=fr'${label}$')\n\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(f\"{ylabel}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(label +\n        f\" RQA_classic_name={symbol}_window={window}_step={tstep}_ \\\n        rettype={ret_type}_m={m}_tau={tau}_eps={eps}.jpg\")\n        \n    plt.show();\n\n\n3.2.2.1 Частота рекурентності (Recurrence rate)\nНайпростішим показником є частота рекурентності, яка визначає щільністю рекурентних точок на рекурентній діаграмі, ігноруючи лінію ідентичності:\n\\[\nRR = \\frac{1}{N^2}\\displaystyle\\sum_{i,j=1}^{N}R(i,j)\n\\]\nде \\(N\\) — кількість точок на траєкторії фазового простору.\nЧастота рекурентності відповідає ймовірності того, що певний стан повториться.\n\nplot_recurrence_measure(measure=RR, label='RR')\n\n\n\n\nРис. 3.5: Динаміка індексу Доу-Джонса та частоти рекурентності\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, міра рекурентності зростає при крахових подіях, що вказує на зростання ступеня самоорганізації та злагодженості торгівельної активності трейдерів на цьому ринку.\n\n\n3.2.2.2 Діагональна частота рекурентності (Diagonal recurrence rate)\nДаний підхід базується на діагональних рекурентних профілях часового ряду. Діагональний рекурентний профіль кількісно оцінює кількість рекурентних точок на різних лагах, подібно до функції автокореляцій. Для отримання діагонального профілю рекурентностей просто підраховується частка рекурентних точок на діагоналях, розташованих в нижньому правому або нижньому лівому куті рекурентної діаграми, і будується графік як функція відстані від головної діагоналі, тобто лагу.\nПо іншому можна сказати, що діагональна частота рекурентності фіксує величину автокореляції на різних лагах.\n\nplot_recurrence_measure(measure=DIAG_RR, label='DRR')\n\n\n\n\nРис. 3.6: Динаміка індексу Доу-Джонса та діагональної частоти рекурентності\n\n\n\n\nЗ представленого рисунку видно, що діагональна частота рекурентності зростає у передкризові та кризові стани, що вказує на зростання величини автокореляції, що в свою чергу демонструє зростання ступеню самоорганізації у кризові та передкризові стани.\n\n\n3.2.2.3 Детермінізм (Determinism)\nНаступним показником можна визначити частку рекурентних траєкторій, які формують діагональні лінії мінімальної довжини \\({\\displaystyle \\ell _{\\min }}\\). Ця міра називається детермінізмом і пов’язана з передбачуваністю динамічної системи:\n\\[\nDET={\\frac {\\sum _{\\ell =\\ell _{\\min }}^{N}\\ell \\,P(\\ell )}{\\sum _{\\ell =1}^{N}\\ell P(\\ell )}},\n\\]\nде \\(P(\\ell )\\) — частотний розподіл довжин \\(\\ell\\) діагональних ліній (тобто підраховує кількість діагональних профілів довжини \\(\\ell\\) ).\n\n\n\n\n\n\nДодаткова інформація по детермінізму\n\n\n\nДетерміновані системи характеризуються значною варіацією діагональних ліній різної довжинию. Періодичні сигнали будуть характеризуватися довгими діагональними лініями, в той час як для хаотичних сигналів діагональні лінії будуть короткими. Для стохастичним систем діагональні лінії взагалі будуть відсутніми, за винятком випадкових закономріностей, що утворюватимуть дуже короткі діагональні лінії.\nБілий шум, наприклад, мав би рекурентну діаграму з майже ізольованими рекурентними точками та дуже малих відсотком діагональних ліній, тоді як детермінований процес демонстрував би дуже малу кількість одиночних рекурентностей, але велику щільність довгих діагональних ліній.\n\n\n\nplot_recurrence_measure(measure=DET, label='DET')\n\n\n\n\nРис. 3.7: Динаміка індексу Доу-Джонса та детермінізму\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, як правило, у передкризові та кризові стани показник детермінізму починає зростати, що свідчить і про зростання ступеня передбачуваності (впорядкованості) флуктуацій системи.\n\n\n3.2.2.4 Ламінарність (Laminarity)\nКількість рекурентних станів, які утворюють вертикальні лінії, можна кількісно визначити таким же чином. Ця міра називається ламінарністю і пов’язана з кількістю ламінарних фаз (незмінностей) у системі:\n\\[\nLAM={\\frac {\\sum _{v=v_{\\min }}^{N}vP(v)}{\\sum _{v=1}^{N}vP(v)}},\n\\]\nде \\(P(v)\\) — частотний розподіл довжин \\(v\\) вертикальних ліній, які мають довжину принаймні \\(v_{\\min}\\).\n\n\n\n\n\n\nДодаткова інформація по ламінарності\n\n\n\nЛамінарність характеризує ймовірність системи перебувати в ламінарному (незмінному) стані. Зі збільшенням ізольованих рекурентних точок у системі, міра ламінарності спадатиме.\n\n\n\nplot_recurrence_measure(measure=LAM, label='LAM')\n\n\n\n\nРис. 3.8: Динаміка індексу Доу-Джонса та ламінарності\n\n\n\n\nМожна бачити, що при кризових станах ступінь ламінарності зростає. Як ми могли бачити, зростає і щільність діагональних точок, і загалом зростає кількість рекурентних траєкторій у фазовому просторі. Це вказує на те, що фінасовий індекс Доу-Джонса “застрягає” у стані кризи. Кризи характеризуються трендостійкістю, персистентністю та детермінованістю своєї поведінки.\n\n\n3.2.2.5 Середня довжина діагональних ліній (The average diagonal lines length)\nТакож можна виміряти середню довжину діагональних ліній. Cередня довжина діагональних лінії визначається як\n\\[\nL={\\frac  {\\sum _{{\\ell =\\ell _{\\min }}}^{N}\\ell \\,P(\\ell )}{\\sum _{{\\ell =\\ell _{\\min }}}^{N}P(\\ell )}}.\n\\]\nЗагалом цей показник характеризує середній період часу при якому дві траєкторії фазового простору знаходяться в достатній близькості один до одного.\n\n\n\n\n\n\nДодаткова інформація по середній довжині діагональних ліній\n\n\n\nСередня довжина діагональних ліній визначає середній час при якому система залишається передбачуваною.\n\n\n\nplot_recurrence_measure(measure=AVG_DIAG_LINE, label='AVG L')\n\n\n\n\nРис. 3.9: Динаміка індексу Доу-Джонса та середньої довжини діагональних ліній\n\n\n\n\nЯк і до цього, ми можемо бачити, що середній час перебування Доу-Джонса у детермінованому стані зростає під час кризових явищ, що говорить зростання ступеня колективізації трейдерів на ринку.\n\n\n3.2.2.6 Час захоплення/затримки (Trapping time)\nУсереднена довжина діагональної лінії пов’язана із часом передбачуваності динамічної системи та часом затримки. У даному випадку ми середню довжину вертикальних ліній:\n\\[\nTT={\\frac {\\sum _{{v=v_{\\min }}}^{{N}}vP(v)}{\\sum _{{v=v_{\\min }}} ^{{N}}P(v)}}.\n\\]\n\n\n\n\n\n\nДодаткова інформація по середній довжині вертикальних ліній\n\n\n\nСередня довжина вертикальних ліній визначає середній час перебування системи в ламінарному стані. Тобто, вона відповідає середньому періоду часу при якому система “завмирає” у певному стані. Очевидно, що зростання цiєї величини характеризує дедалi бiльший час затримки дослiджуваної системи в певному станi.\n\n\n\nplot_recurrence_measure(measure=TT, label='TT')\n\n\n\n\nРис. 3.10: Динаміка індексу Доу-Джонса та час затримки\n\n\n\n\nНа представленому рисунку видно, що \\(TT\\) зростає в (перед-)кризові стани, що вказує на потребу системи перебувати ще більший час у стані кризи.\n\n\n3.2.2.7 Середня довжина білих вертикальних лінії (Average white vertical lines length)\nСередня довжина білих вертикальних ліній може бути визначена як\n\\[\nWVL_{mean} = \\sum_{w=w_{min}}^{N} w \\cdot P(w) \\Big / \\sum_{w=w_{min}}^{N} P(w),\n\\]\nде \\(P(w)\\) — це частотний розподіл білих вертикальних ліній довжиною \\(w\\), а \\(w_{min}\\) відповідає найменшій довжині білих вертикальних ліній (найменшому періоду повернення до стану рекурентності).\n\n\n\n\n\n\nДодаткова інформація по середній довжині білих вертикальних ліній\n\n\n\nПредставлену міру можна охарактеризувати як середній горизонт непередбачуваності системи.\n\n\n\nplot_recurrence_measure(measure=AVG_WVERT_LINE, label='WVL_{mean}')\n\n\n\n\nРис. 3.11: Динаміка індексу Доу-Джонса та середньої довжини білих вертикальних ліній\n\n\n\n\nЗростання середньої довжини бiлих вертикальних лiнiй демонструє, що кризовi подiї характеризуються не лише детермiнiзмом динамiки фондового ринку, але i несхожiстю даних подiй у порiвняннi з попереднiми станами.\n\n\n3.2.2.8 Ентропія діагональних ліній (Diagonal lines entropy)\nДля відповідних діагональних сегментів можна розрахувати необхідну кількість інформації для опису всього розподілу цього типу ліній. Імовірність \\(p(\\ell )\\) того, що діагональна лінія має точну довжину \\(\\ell\\), можна оцінити за частотним розподілом \\(P(\\ell )\\) із \\(p( \\ell )={\\frac {P(\\ell )}{\\sum _{{\\ell = \\ell_{\\min }}}^{N}P(\\ell )}}\\). Ентропія Шеннона цієї ймовірності виглядає наступним чином:\n\\[\nDLEn = -\\sum_{{\\ell =\\ell _{\\min }}}^{N}p(\\ell )\\ln p(\\ell ).\n\\]\nДаний показник відображає складність досліджуваної структури.\n\n\n\n\n\n\nДодаткова інформація по ентропії діагональних ліній\n\n\n\nДля некорельованого шуму чи осциляцiй ми тримали б мале значення цієї ентропiї. Мале значення даної ентропії вказувало би на те, що розподіл діагональних ліній представляється асиметричним: існувала б невеличка частка діагональних ліній конкретної довжини, що характеризувала би всю рекурентність досліджуваної системи. Зростання даної ентропії характеризувало би зростання симетричності розподілу довжин діагональних ліній.\n\n\n\nplot_recurrence_measure(measure=ENT_DIAG, label='DLEn')\n\n\n\n\nРис. 3.12: Динаміка індексу Доу-Джонса та ентропії діагональних ліній\n\n\n\n\nВидно, що ентропія діагональних ліній зростає під час кризових явищ, що вказує на зростання впливу детермінованих процесів із різним ступенем передбачуваності.\n\n\n3.2.2.9 Ентропія вертикальних ліній (Vertical lines entropy)\nМи можемо визначити Шеннонівську ентропію для розподілу вертикальних структур рекурентної діаграми. Імовірність \\(p( v )\\) того, що вертикальна лінія має точну довжину \\(v\\), можна оцінити за частотним розподілом \\(P( v )\\) із \\(p( v )= P( v ) \\Big / \\sum _{{ v = v_{\\min }}}^{N}P( v )\\). Ентропія Шеннона цієї ймовірності визначається як\n\\[\nVLEn =-\\sum_{{ v = v_{\\min }}}^{N}p( v )\\ln p( v ).\n\\]\nЦя міра, по аналогії до попередньої ентропії, також є мірою складності системи.\n\n\n\n\n\n\nДодаткова інформація по ентропії вертикальних ліній\n\n\n\nДля синусоїдального процесу ми би очікували мале значення даної ентропії, оскільки це простий періодичний процес. Для складного процесу з пам’ятю ми би очiкуємо високе значення цього типу рекурентної ентропiї. Це означати- ме, що ламiнарнiсть процесу характеризуються рiзноманiтними перiодами довгостроковості пам’яті системи.\n\n\n\nplot_recurrence_measure(measure=ENT_VERT, label='VLEn')\n\n\n\n\nРис. 3.13: Динаміка індексу Доу-Джонса та ентропії вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що ентропія вертикальних ліній починає зростати під час крахових явищ, що вказує на зростання ступеню ламінарності, тобто зростання рівномірності розподілу вертикальних ліній різноманітних довжин.\n\n\n3.2.2.10 Дивергенція (Divergence)\nПоказник \\(L_{\\max }\\) може надати нам інформацію про максимальний ступінь передбачуваності досліджуваного періоду. Зворотнє значення максимальної довжини діагональних ліній \\(L_{\\max }\\) або дивергенція (розбіжність) може вказати нам на швидкість та тривалість розбіжності досліджуваних траєкторій. Даний показник можна визначити як\n\\[\n\\text{DIV} = {\\frac {1}{L_{\\max }}}.\n\\]\nДана міра схожа на старший показник Ляпунова. Однак взаємозв’язок між цією мірою та позитивним максимальним показником Ляпунова набагато складніший (щоб обчислити показник Ляпунова з RP, необхідно враховувати весь розподіл частот діагональних ліній). Дивергенція може мати тенденцію позитивного максимального показника Ляпунова, але не більше.\n\n\n\n\n\n\nДодаткова інформація по дивергенції\n\n\n\nЧим вище значення дивергенції, тим швидше розбігаються траєкторії фазового простору. І навпаки, чим нижче значення дивергенції, тим ближче досліджувані траєкторії прилягають один до одного.\n\n\n\nplot_recurrence_measure(measure=DIV, label='DIV')\n\n\n\n\nРис. 3.14: Динаміка індексу Доу-Джонса та дивергенції\n\n\n\n\nДаний рисунок показує, що дивергенція діагональних ліній починає спадати в кризові та передкризові періоди, що також вказує на зростання ступеня впорядкованості динаміки системи в дані періоди часу.\n\n\n3.2.2.11 Дивергенція вертикальних ліній (Vertical line divergence)\nЗворотнє значення максимальної довжини вертикальних ліній \\(V_{max}\\) або розбіжність вертикальних ліній можна визначити як:\n\\[\nVDIV = {\\frac {1}{V_{\\max }}}.\n\\]\n\n\n\n\n\n\nДодаткова інформація по дивергенції вертикальних ліній\n\n\n\nМаксимальна довижна вертикальних ліній надавала нам інформацію про максимальний ступінь незмінюваності системи. Вертикальна дивергенція дозволяє нам охарактеризувати швидкість настання або спаду ламінарності у системі. Чи вище значення \\(VDIV\\), тим швидше система виходить із ламінарного стану. І навпаки, чим нижчий даний показник, тим ближче траєкторії фазового простору один до одного, і тим вищий ступінь ламінарності системи в конкретний момент часу.\n\n\n\nplot_recurrence_measure(measure=VERT_DIV, label='VDIV')\n\n\n\n\nРис. 3.15: Динаміка індексу Доу-Джонса та дивергенції вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що періоди криз характеризуються спадом вертикальної дивергенції, тобто зростанням кількості вертикальних структур, що характеризують ще більший ступінь ламінарності станів.\n\n\n3.2.2.12 Дивергенція білих вертикальних ліній\nЗворотнє значення максимальної довжини білих вертикальних ліній (\\(WVL_{max}\\)) можна охарактеризувати як дивергенцію білих вертикальних ліній. Її можна визначити наступним чином:\n\\[\nWVDIV = \\frac{1}{WVL_{max}}.\n\\]\nЗростання даного показника має вказувати на зростання ступеня рекурентності системи, а його спад має демонструвати зростання непередбачуваності.\n\nplot_recurrence_measure(measure=WHITE_VERT_DIV, label='WVDIV')\n\n\n\n\nРис. 3.16: Динаміка індексу Доу-Джонса та дивергенції білих вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що дивергенція білих вертикальних ліній представляє доволі зашумлену динаміку, а тому не може бути використана в якості ефективного індикатора кризових явищ.\n\n\n3.2.2.13 Ентропія білих вертикальних ліній (White vertical lines entropy)\nІмовірність \\(p( \\omega )\\) того, що біла вертикальна лінія має точну довжину \\(\\omega\\), можна оцінити за частотним розподілом \\(P(\\omega )\\) із \\(p( \\omega )={\\frac {P( \\omega )}{\\sum _{{\\omega = \\omega_{\\min }}}^{N}P(\\omega )}}\\). Ентропія Шеннона цієї ймовірності,\n\\[\n{\\text{WVertEn}}=-\\sum_{{\\omega =\\omega _{\\min }}}^{N}p(\\omega )\\ln p(\\omega ),\n\\]\nде \\(\\omega_{min}\\) — мінімальна довжина білої вертикальної лінії.\n\nplot_recurrence_measure(measure=ENT_WHITE_VERT, label='WVLEN')\n\n\n\n\nРис. 3.17: Динаміка індексу Доу-Джонса та ентропії білих вертикальних ліній\n\n\n\n\nВидно, що ентропія білих вертикальних ліній спадає у кризові та передкризові періоди фондового ринку, що вказує на зростання загальної передбачуваності системи та зміщення розподілу білих вертикальних ліній до конкретних довжин. Тобто, їх розподіл у періоди криз стає менш симетричним, що вказує на поступове заміщення білих вертикальних ліній чорними.\n\n\n3.2.2.14 Співвідношення частоти рекурентності до детермінізму \\(DET/RR\\)\nСпіввідношення між \\(DET\\) і \\(RR\\) (\\(RATIO\\)) можна використовувати для виявлення прихованих фазових переходів у системи:\n\\[\nRATIO_1=\\frac{DET}{RR}=N^2\\frac{\\displaystyle\\sum_{l=l_{min}}^{N}lP(l)}{\\left(\\displaystyle\\sum_{l=1}^{N}lP(l)\\right)^2}\n\\]\n\nplot_recurrence_measure(measure=RATIO_DET_REC, label='RATIO_1')\n\n\n\n\nРис. 3.18: Динаміка індексу Доу-Джонса та співвідношення між мірою передбачуваності та рекурентності\n\n\n\n\nДаний показник спадає під час кризових явищ фондового ринку. Це говорить про те, що має зростати загальна щільність рекурентних точок, як ізольованих, так і просто розподілу вертикальних структур. Тобто, у кризові періоди \\(RR\\) представляється вищою за \\(DET\\).\n\n\n3.2.2.15 Співвідношення ламінарності до детермінізму (LAM/DET)\nТак само як і попередня міра, відношення ламінарності до детермінізму може дозволити нам виокремити приховані переходи в досліджуваному сигналі:\n\\[\nRATIO_2=\\frac{LAM}{DET}.\n\\]\n\nplot_recurrence_measure(measure=RATIO_LAM_DET, label='RATIO_2')\n\n\n\n\nРис. 3.19: Динаміка індексу Доу-Джонса та співвідношення між мірою ламінарності та детермінізмом\n\n\n\n\nЯкщо виходити з динаміки показника \\(RATIO_2\\), можна сказати, що загальний ступінь детермінізму починає переважати над ламінарністю під час кризових явищ.\nАле по результатам представлених показників ми можемо сказати, що досліджувані крахові та передкрахові події характеризуються зростанням рекурентності, і подібного роду поведінка може бути використана в якості передвісника подальших криз."
  },
  {
    "objectID": "lab_4.html#теоретичні-відомості",
    "href": "lab_4.html#теоретичні-відомості",
    "title": "4  Лабораторна робота № 4",
    "section": "4.1 Теоретичні відомості",
    "text": "4.1 Теоретичні відомості\n\n4.1.1 Складність. Кількісні міри складності. Інформаційні методи оцінки складності.\nДане століття називають століттям складності. Сьогодні питання “що таке складність?” вивчають фізики, біологи, математики і інформатики, хоча при теперішніх досягненнях у розумінні оточуючого світу, однозначної відповіді на це питання немає.\nЗ цієї причини, відповідно до ідеї І. Пригожина, будемо досліджувати прояви складності системи, застосовуючи при цьому сучасні методи кількісного аналізу складності.\nСеред таких методів на увагу заслуговують: - інформаційно-ентропійні; - засновані на теорії хаосу; - скейлінгово-мультифрактальні.\nЗрозуміло, виходячи з різної природи методів, покладених в основу формування міри складності, вони приділяють певні вимоги до часових рядів, що слугують вхідними даними. Наприклад, перші дві групи методів вимагають стаціонарності вхідних даних. При цьому мають різну чутливість до таких характеристик, як детермінованність, стохастичність, причинність та кореляції. Тому у подальшому, порівнюючи комплексно ефективність різних показників складності, на вказані обставини ми будемо звертати увагу, підкреслюючи спеціально застосовність того чи іншого показника для характеристики різних сторін складності досліджуваних систем.\nРозгляд першої групи методів почнемо з добре відомої міри складності, запропонованої А. Колмогоровим.\nКолмогорівська складність. Поняття колмогорівської складності (або, як ще говорять, алгоритмічної ентропії) з’явилося в 1960-і роки на стику теорії алгоритмів, теорії інформації і теорії ймовірностей.\nІдея А. Колмогорова полягала в тому, щоб вимірювати кількість інформації, що міститься в індивідуальних скінчених об’єктах (а не у випадкових величинах, як у шеннонівській теорії інформації). Виявилось, що це можливо (хоча лише з точністю до обмеженого доданку). А. Колмогоров запропонував вимірювати кількість інформації в скінчених об’єктах за допомогою теорії алгоритмів, визначивши складність об’єкту як мінімальну довжину програми, що породжує цей об’єкт. Дане визначення стало базисом алгоритмічної теорії інформації, а також алгоритмічної теорії ймовірностей: об’єкт вважається випадковим, якщо його складність наближена до максимальної.\nЩо ж собою являє колмогорівська складність і як її виміряти? На практиці ми часто стикаємося з програмами, які стискують файли (для економії місця в архіві). Найбільш поширені називаються zip, gzip, compress, rar, arj та інші. Застосувавши таку програму до деякого файлу (з текстом, даними, програмою), ми отримуємо його стислу версію (яка, як правило, коротше початкового файлу). За нею можна відновити початковий файл з допомогою парної програми-“декомпресора”. Отже, у першому наближенні колмогорівську складність файлу можна описати як довжину його стислої версії. Тим самим файл, що має регулярну структуру і добре стискуваний, має малу колмогорівську складність (порівняно з його довжиною). Навпаки, погано стискуваний файл має складність, близьку до довжини.\nПрипустимо, що ми маємо фіксований спосіб опису (декомпресор) \\(D\\). Для даного слова \\(x\\) розглянемо всі його описи, тобто всі слова \\(y\\), для яких \\(D(y)\\) визначене \\(і\\) рівне \\(x\\). Довжину найкоротшого з них \\(l(y)\\) і називають колмогорівською складністю слова \\(x\\) при даному способі опису \\(D\\):\n\\[\nKS_{D}(x) = \\min\\{l(y)\\,|\\,D(y)=x\\},\n\\]\nде \\(l(y)\\) позначає довжину слова \\(y\\). Індекс \\(D\\) підкреслює, що визначення залежить від вибору способу \\(D\\).\nМожна показати, що існують оптимальні способи опису. Спосіб опису тим краще, чим він коротше. Тому природно дати таке визначення: спосіб \\(D_1\\) не гірше за спосіб \\(D_2\\), якщо \\(KS_{D_1}(x) \\leq KS_{D_2}(x)+c\\) при деякому \\(c\\) і при всіх \\(x\\).\nОтже, за Колмогоровим, складність об’єкту (наприклад, тексту — послідовності символів) — це довжина мінімальної програми яка виводить даний текст, а ентропія — це складність, що ділиться на довжину тексту. На жаль, це визначення чисто умоглядне. Надійного способу однозначно визначити цю програму не існує. Але є алгоритми, які фактично якраз і намагаються обчислити колмогорівські складність тексту і ентропію.\n\n\n4.1.2 Оцінка складності Колмогорова за схемою Лемпела-Зіва\nУніверсальна (в сенсі застосовності до різних мовних систем) міра складності кінцевої символьної послідовності була запропонована Лемпелем і Зівом. У рамках їх підходу складність послідовності оцінюється числом кроків процесу, що її породжує. Припустимими (редакційними) операціями при цьому є:\n\nгенерація символу (необхідна, як мінімум, для синтезу елементів алфавіту) і\nкопіювання “готового” фрагмента з передісторії (тобто з уже синтезованої частини тексту).\n\nНехай \\(\\Sigma\\) — скінчений алфавіт, \\(S\\) — текст (послідовність символів), складений з елементів \\(\\Sigma\\); \\(S[i]\\) — \\(i\\)-й символ тексту; \\(S[i:j]\\) — фрагмент тексту з \\(i\\)-го по \\(j\\)-й символ включно \\((i&lt;j)\\); \\(N=|S|\\) — довжина тексту \\(S\\). Тоді схему синтезу послідовності можна представити у вигляді конкатенації\n\\[\nH(S)=S[1:i_1]S[i_1+1:i_2]...S[i_{k-1}+1:i_k]...S[i_{m−1}+1:N],\n\\tag{4.1}\\]\nде \\(S[i_{k−1}+1:i_k]\\) — фрагмент \\(S\\), породжуваний на \\(k\\)-му кроці, а $m=m_{H}(S) — число кроків процесу. З усіляких схем породження \\(S\\) обирається мінімальна за числом кроків. Таким чином, складність послідовності \\(S\\) за Лемпелем-Зівом\n\\[\nc_{LZ}(S) = \\min_{H}\\{ m_{H}(S) \\}.\n\\]\nМінімальність числа кроків забезпечується вибором для копіювання на кожному кроці максимально довгого прототипу з передісторії. Якщо позначити через \\(j(k)\\) номер позиції, з якої починається копіювання на \\(k\\)-му кроці, то довжина фрагмента копіювання\n\\[\nl_{j(k)} = i_k - i_{k-1} - 1 = \\max_{j \\leq i_{k-1}}\\{ l_{j} : S[i_{k-1}+1:i_{k-1}+l_j]=S[j:j+l_{j}-1] \\},\n\\tag{4.2}\\]\nа сам \\(k\\)-й компонент складнісного розкладання (Рівняння 4.1) можна записати у вигляді\n\\[\nS[i_{k-1}+1:i_{k}] =\n\\begin{cases}\n    S[j(k):j(k)+l_{j(k)}-1] & \\textrm{if} \\; j(k) \\neq 0, \\\\\n    S[i_{k-1}+1] & \\textrm{if} \\; j(k) = 0.\n\\end{cases}\n\\tag{4.3}\\]\nВипадок \\(j(k) = 0\\) відповідає ситуації, коли в позиції \\(i_{k−1}+1\\) стоїть символ, який раніше не зустрічався. При цьому ми застосовуємо операцію генерації символу.\nБудемо знаходити складність за Лемпелем-Зівом (LZ) для часового ряду, який являє собою, наприклад, щоденні значення індексу фондового ринку. Для дослідження динаміки LZ та порівняння з іншими фондовими ринками будемо знаходити дану міру складності для підряду фіксованої довжини (вікна). Для цього обчислимо логарифмічні прибутковості та перетворимо їх у послідовність бітів. При цьому можна задавати кількість станів, які диференційовані (система числення). Так, для двох різних станів маємо 0, 1, для трьох — 0, 1, 2 і т.д. Для двійкової системи кодування буде задаватися поріг по середньому значенню і стани, наприклад, прибутковостей (ret) кодуватимуться наступним чином:\n\\[\nret =\n\\begin{cases}\n0, & ret_t &lt; \\langle ret \\rangle \\\\\n1, & ret_t &gt; \\langle ret \\rangle.\n\\end{cases}\n\\tag{4.4}\\]\nТакож можна визначити так звану пермутаційну складність Лемпеля-Зіва (PLZС). У даному випадку би будемо опиратись на процедуру реконструкції фазового простору, що згадувалась в лабораторних 2 і 3. Згідно пермутаційній процедурі ми будемо брати фрагмент ряду довжини \\(m\\), що слугує розмірностю реконструйованого атрактора, та замінюємо кожне значення ряду його порядковим індексом. На подальшому ресунку представлено часовий ряд та його можливі порядкові шаблони:\n\n\n\nРис. 4.1: Фрагмент часового ряду (а) та 6 можливих порядкових шаблонів, що можуть бути в цьому сигналі (b)\n\n\nАлгоритм Лемпеля-Зіва виконує дві операції: (1) додає новий біт в уже існуючу послідовність; (2) копіює вже сформовану послідовність. Алгоритмічна складність представляє собою кількість таких операцій, необхідних для формування заданої послідовності.\nДля випадкової послідовності довжини \\(n\\) алгоритмічна складність обчислюється за виразом \\(LZC_r = n / \\log(n)\\). Тоді відносна алгоритмічна складність знаходиться як відношення отриманої складності до складності випадкової послідовності: \\(LZC = LZC / LZC_{r}\\).\nОднак навіть цього підходу може бути недостатньо. Справа в тому, що складні сигнали проявляють притаманну їм складність на різних просторових і часових масштабах, тобто мають масштабно інваріантні властивості. Вони, зокрема проявляються через степеневі закони розподілу. Тому розрахунки алгоритмічної складності на “поверховому” масштабі сигналу можуть бути неприйнятними і призводити до помилкових висновків.\nДля подолання таких труднощів використовуються мультимасштабні методи, до розгляду яких ми і переходимо.\n\n\n4.1.3 Процедура грануляції для мультискейлінгового дослідження часових рядів. Мультимасштабні міри складності\nІдея цієї групи методів включає дві послідовно виконувані процедури:\n\nпроцес “грубого дроблення” (coarse graining — “грануляції”) початкового часового ряду — усереднення даних на сегментах, що не перетинаються, розмір яких (вікно усереднення) збільшуватиметься на одиницю при переході на наступний за величиною масштаб;\nобчислення на кожному з масштабів певного (до сих пір мономасштабного) показника складності.\n\nПроцес “грубого дроблення” (“грануляція”) полягає в усереднені послідовних відліків ряду в межах вікон, що не перетинаються, а розмір яких \\(\\tau\\) — збільшується при переході від масштабу до масштабу. Кожен елемент “гранульованого” часового ряду \\(y_{j}^{\\tau}\\) знаходиться у відповідності до виразу:\n\\[\ny_{j}^{\\tau} = \\frac{1}{\\tau}\\sum_{i=(j-1)\\tau+1}^{j\\tau}x_i, \\; 1 \\leq j \\leq N/\\tau,\n\\]\nде \\(\\tau\\) характеризує фактор масштабування. Довжина кожного “гранульованого” ряду залежить від розміру вікна \\(і\\) рівна \\(N/\\tau\\). Для масштабу рівного 1 “гранульований” ряд просто тотожний оригінальному.\n\n\n\nРис. 4.2: Схематична ілюстрація процесу грубого дроблення (“грануляції”) початкового часового ряду для масштабів 2 і 3\n\n\nБібліотека neurokit2 представляє метод для обчислення як мономасштабного показника складності Лемпеля-Зіва, так і його мультимасштабного аналогу.\nСинтаксис мономасштабної процедури виглядає наступним чином:\ncomplexity_lempelziv(signal, delay=1, dimension=2, permutation=False, symbolize='mean', **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка, \\(\\tau\\). Використовується лише тоді, коли permutation=True.\ndimension (int) — розмірність вкладень, \\(m\\). Використовується лише коли permutation=True.\npermutation (bool) — якщо значення True, поверне складність Лемпеля-Зіва на основі порядкових патернів.\nsymbolize (str) — використовується тільки коли permutation=False. Метод перетворення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення None, щоб пропустити процес (якщо вхідний сигнал вже є дискретним). Можна скористатися методом complexity_symbolize() для використання іншої процедури символізації ряду.\nkwargs — інші аргументи, які передаються до complexity_ordinalpatterns() (якщо permutation=True) або complexity_symbolize().\n\nПовертає\n\nlzc (float) — складність Лемпеля-Зіва (LZC).\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення LZC.\n\nСинтаксис мультимасштабної процедури вже інший:\nentropy_multiscale(signal, scale='default', dimension=3, tolerance='sd', method='MSEn', show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму.\nscale (str або int або list) — список масштабних коефіцієнтів, що використовуються для процедури крос-грануляції часового ряду. Якщо значення \"default\", буде використано range(len(signal) / (dimension + 10)). Якщо \"max\", використовуватиме всі масштаби до половини довжини сигналу. Якщо ціле число, створить діапазон до вказаного цілого числа.\ndimension (int) — розмірність вкладення, \\(m\\).\ntolerance (float) — поріг пропускання \\(\\varepsilon\\) (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються подібними. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\).\nmethod (str) — яку версію мультимасштабного показника обчислювати. Переважна кількість показників за цим методом відповідають саме ентропійним підходам. Нас цікавитиме саме \"LZC\".\nshow (bool) — візуалізувати залежність показника від масштабу.\nkwargs — необов’язкові аргументи.\n\nПовертає\n\nfloat — точкова оцінка мультимасштабного показника окремого часового ряду, що відповідає площі під кривою значень цього показника, яка, по суті, є сумою вибіркових значень, наприклад, \"LZC\" в діапазоні масштабних коефіцієнтів.\ndict — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення мультимасштабного показника. Значення показника, що відповідають кожному фактору \"Scale\", зберігаються під ключем \"Value\".\n\n\n\n4.1.4 Шеннонівська складність\nЕнтропійний аналіз часових рядів за допомогою ентропійних показників різного роду буде проведено у наступних роботах. Зараз же ми розглянемо найпростішу з ентропій — ентропію Шеннона та порівняємо її можливості кількісно оцінювати складність часових послідовностей у порівнянні з мірою Лемпеля-Зіва.\nЕнтропія Шеннона — це статистичний квантифікатор, який широко використовується для характеристики складних процесів. Він здатний виявляти аспекти нелінійності в досліджуваних сигналах, сприяючи більш надійному поясненню нелінійної динаміки різних точок аналізу, що, в свою чергу, покращує розуміння природи складних систем, які характеризуються складністю та нерівноважністю. Окрім складності та нерівноважності, більшість, але не всі, складні системи також характеризуються неоднорідним розподілом зв’язків. Поняття ентропії було використано Шенноном в теорії інформації для передачі даних.\nЕнтропія - це міра невизначеності та випадковості в системі. Якщо припустити, що всі наявні дані належать до одного класу, то неважко передбачити клас нових даних. У цьому випадку ентропія дорівнює 0. Будучи величиною між 0 і 1, коли всі ймовірності рівні, ентропія набуває найбільшого значення. Невизначеність, що виникає, коли подія \\(E\\) відбувається з ймовірністю \\(p\\), можна позначити як \\(S(p)\\). Якщо ймовірність появи класу дорівнює 1, тоді ентропія мінімальна, \\(S(1) = 0\\). Відповідно до концепції Шеннона, якщо у нас наявні ймовірності реалізації певної події \\(p_1, p_2, p_3, ..., p_n\\), на виході отримується кількість інформації, що необхідна для опису цієї події. Тоді, Шеннонівська ентропія може бути визначена як\n\\[\nS = -\\sum_{i=1}^{n}p_i \\ln p_{i}.  \n\\]\nСинтаксис методу для розрахунку Шеннонівської ентропії виглядає наступним чином:\nentropy_shannon(signal=None, base=2, symbolize=None, show=False, freq=None, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\nbase (float) — основа логарифму, що за замовчуванням дорівнює 2, що дає одиницю в бітах. Зауважте, що scipy.stats.entropy() за замовчуванням використовує число Ейлера (np.e) (натуральний логарифм), що дає міру інформації, виражену в натах.\nsymbolize (str) — метод приведення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням дорівнює нулю, що пропускає процес (і припускає, що вхідні дані вже є дискретними).\nshow (bool) — якщо значення True, виводить дискретність сигналу.\nfreq (np.array) — замість сигналу можна надати вектор ймовірностей.\nkwargs — необов’язкові аргументи. Наразі не використовуються.\n\nПовертає\n\nshanen (float) — Шеннонівську ентропію.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення Шеннонівської ентопії.\n\n\n\n4.1.5 Інформація Фішера\nІнформацію Фішера було введено Р. А. Фішером у 1925 році як міру “внутрішньої точності” в теорії статистичних оцінок. Вона є центральною для багатьох статистичних галузей, що виходять далеко за межі теорії складності. Даний показник вимірює кількість інформації, яку спостережувана випадкова величина несе про невідомий параметр. В аналізі складності вимірюється кількість інформації, яку система несе “про себе”. Він базується на розкладанні за сингулярним значенням реконструйованого фазового простору. Значення показника Фішера зазвичай антикорельоване з іншими показниками складності (чим більше інформації система приховує про себе, тим більш передбачуваною і, відповідно, менш складною вона є).\nІнформацію Фішера можна визначити, використовуючи метод fisher_information() бібліотеки neurokit2. Її синтаксис виглядає наступним чином:\nfisher_information(signal, delay=1, dimension=2)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі, \\(\\tau\\).\ndimension (int) — розмірність векторів фазового простору, \\(m\\).\n\nПовертає\n\nfi (float) — обчислена міра інформації Фішера.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення інформації Фішера.\n\n\n\n4.1.6 Складність та параметри Хьорта\nПараметри Хьорта — це показники статистичних властивостей, які спочатку були введені Хьортом (Hjorth, 1970) для опису загальних характеристик сигналів електроенцифалограми у кількох кількісних термінах, але які можуть бути застосовані до будь-якого часового ряду. Параметрами є активність, рухливість і складність:\n\nПараметр активності (\\(Activity\\)) — це просто дисперсія сигналу, яка відповідає середній потужності сигналу (якщо його середнє значення дорівнює 0).\n\n\\[\nActivity = \\sigma^{2}_{signal}.\n\\]\n\nПараметр рухливості (\\(Mobility\\)) являє собою середню частоту або частку середньоквадратичного відхилення спектра потужності. Він визначається як квадратний корінь з дисперсії першої похідної сигналу, поділений на дисперсію сигналу.\n\n\\[\nMobility = \\frac{\\sigma_{dd}/\\sigma_{d}}{Complexity}.\n\\]\n\nПараметр складності (\\(Complexity\\)) дає оцінку смуги пропускання сигналу, яка вказує на схожість форми сигналу з чистою синусоїдою (для якої значення сходиться до 1). Іншими словами, це міра “надмірної деталізації” по відношенню до “найм’якшої” можливої форми кривої. Параметр “Складність” визначається як відношення рухливості першої похідної сигналу до рухливості самого сигналу.\n\n\\[\nComplexity = \\frac{\\sigma_d}{\\sigma_{signal}},\n\\]\nде \\(d\\) та \\(dd\\) представляють перші та другі похідні сигналу, відповідно.\n\n\n\nРис. 4.3: Характеристичні зміни форми кривої, що ілюструє залежність кожного параметра\n\n\nБібліотека neurokit2 представляє метод для отримання відповідних показників. Її синтаксис виглядає наступним чином:\ncomplexity_hjorth(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nhjorth (float) — показник складності Хьорта.\ninfo (dict) — словник, що містить додаткові показники Хьорта, такі як \"Mobility\" та \"Activity\".\n\n\n\n4.1.7 Час декореляції\nЧас декореляції (decorrelation time, DT) визначається як час (у відліках) першого перетину нуля функції автокореляції. Коротший час декореляції відповідає менш корельованому сигналу. Наприклад, зменшення часу декореляції в сигналах електроенцифалограми спостерігається перед нападами, що пов’язано зі зменшенням потужності низьких частот.\nБібліотека neurokit2 представляє функціонал для визначення часу декореляції, а саме метод complexity_decorrelation(). Її синтаксис є наступним:\ncomplexity_decorrelation(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nfloat — час декореляції.\ndict — словник, що містить додаткову інформацію про додаткові показники.\n\n\n\n4.1.8 Відносна грубість (нерівність, шорсткість)\nВідносна шорсткість — це відношення локальної дисперсії (автоковаріації з лагом 1) до глобальної дисперсії (автоковаріації з лагом 0), яке можна використовувати для класифікації різних “шумів”. Його також можна використовувати як індекс для перевірки застосовності фрактального аналізу (показники фрактальності будуть використовуватись у наступних роботах).\nСинтаксис даного методу в бібліотеці neurokit2 виглядає наступним чином:\ncomplexity_relativeroughness(signal, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (часовий ряд) у вигляді вектора значень.\nkwargs (optional) — інші аргументи, що потребуються методу nk.signal_autocor().\n\nПовертає\n\nrr (float) — значення відносної грубості.\ninfo (dict) — словник, що містить інформацію відносно параметрів, що використовувались для обчислення показника грубості.\n\n\n\n4.1.9 Взаємна інформація\nКоли йдеться про виявлення зв’язків між змінними, ми часто використовуємо кореляцію Пірсона. Проблема полягає в тому, що цей показник знаходить лише лінійні зв’язки, що іноді може призвести до неправильної інтерпретації зв’язку між двома змінними. Тим не менш, інші статистичні методи вимірюють нелінійні зв’язки, такі як взаємна інформація (mutual information, MI).\nВзаємна інформація між двома випадковими величинами вимірює нелінійний зв’язок між ними. Крім того, вона показує, скільки інформації можна отримати з випадкової величини, спостерігаючи за іншою випадковою величиною.\nВона тісно пов’язана з поняттям ентропії. Тобто, зменшення невизначеності випадкової величини пов’язане з отриманням інформації з іншої випадкової величини. Отже, високе значення взаємної інформації вказує на велике зменшення невизначеності, тоді як низьке значення вказує на мале зменшення. Якщо взаємна інформація дорівнює нулю, це означає, що дві випадкові величини є незалежними.\nВзаємну інформацію можна розрахувати наступним чином:\n\\[\nI(X; Y) = \\sum_{y \\in Y}\\sum_{x \\in X}p(x, y) \\cdot \\log{\\left( \\frac{p(x,y)}{p(x)p(y)} \\right)},\n\\]\nде \\(p(x)\\) та \\(p(y)\\) ймовірності спостереження окремо \\(x\\) або \\(y\\), а \\(p(x,y)\\) ймовірність спостереження одночасно \\(x\\) та \\(y\\).\nОсновна відмінність між кореляцією та взаємною інформацією полягає в тому, що кореляція є мірою лінійної залежності, тоді як взаємна інформація вимірює загальну залежність (включаючи нелінійні зв’язки). Тому взаємна інформація виявляє залежності, які не залежать тільки від коваріації. Таким чином, взаємна інформація дорівнює нулю, коли дві випадкові величини є строго незалежними.\nБібліотека neurokit2 представляє інструментарій для знаходження взаємної інформації між двома сигналами \\(x\\) та \\(y\\). У даній роботі ми спробуємо віднайти взаємну інформацію як між двома часовими рядами, так і авто-взаємну інформацію, подібно до автокореляції.\nСинтаксис потрібної нам процедури виглядає наступним чином:\nmutual_information(x, y, method='varoquaux', bins='default', **kwargs)\nПараметри\n\nx (Union[list, np.array, pd.Series]) — масив значень.\ny (Union[list, np.array, pd.Series]) — масив значень.\nmethod (str) — метод для обчислення взаємної інформації: \"nolitsa\", \"varoquaux\", \"knn\", \"max\".\nbins (int) — кількість бінів гістограми. Використовується лише для \"nolitsa\" та \"varoquaux\". Якщо \"default\", кількість бінів оцінюється згідно методики Hacine-Gharbi (2018).\nkwargs — додаткові ключові аргументи для обраного методу.\n\nПовертає\n\nfloat — розрахована взаємна інформація.\n\nІснують різноманітні підходи до розрахунку взаємної інформації:\n\nnolitsa: Класична взаємна інформація (трохи швидше, ніж метод \"sklearn\").\nvaroquaux: Застосовує фільтр Гауса до об’єднаної гістограми. Величину згладжування можна налаштовувати за допомогою аргументу sigma (за замовчуванням sigma=1).\nknn: Непараметрична (тобто не заснована на біннінгу) оцінка за найближчими сусідами. Додаткові параметри включають k (за замовчуванням, k=3), кількість найближчих сусідів для використання.\nmax: Максимальний коефіцієнт взаємної інформації, тобто \\(MI\\) є максимальним при певній комбінації кількості бінів.\n\nІснує безліч різноманітних показників складності, що базуються на теорії інформації та інших парадигах, які ми ще представлятимемо в подальшому. Розглянемо ефективність використання зазначених показників у якості індикаторів або індикаторів-передвісників крахових подій."
  },
  {
    "objectID": "lab_4.html#хід-роботи",
    "href": "lab_4.html#хід-роботи",
    "title": "4  Лабораторна робота № 4",
    "section": "4.2 Хід роботи",
    "text": "4.2 Хід роботи\nСпочатку імпортуємо необхідні модулі для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виведення:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nЦього разу розглянему можливість побудови індикаторів-передвісників на прикладі фондового індексу S&P 500, але, окрім цього, додамо ще Біткоїн для розрахунку взаємної інформації між фондовим ринком та криптовалютним. Очевидно, що фондовий індекс S&P 500 мав би проіснувати довше за Біткоїн. До того ж, криптовалютний ринок працює безперервно на відміну від фондового, а тому треба буде об’єднати значення двох активів за тими датами що співпадають.\nВиконуємо зчитування фондового індексу:\n\nsymbol_1 = '^GSPC'         # Символ першого індексу\nstart_1 = \"2014-01-01\"     # Дата початку зчитування даних\nend_1 = \"2023-08-24\"       # Дата закінчення зчитування даних\n\ndata_1 = yf.download(symbol_1, start_1, end_1)  # вивантажуємо дані\ntime_ser_1 = data_1['Adj Close'].copy()         # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'      # підпис по вісі Ох \nylabel_1 = symbol_1        # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиконуємо зчитування криптовалютного індексу:\n\nsymbol_2 = 'BTC-USD'       # Символ другого індексу\nstart_2 = \"2014-01-01\"     # Дата початку зчитування даних\nend_2 = \"2023-08-24\"       # Дата закінчення зчитування даних\n\ndata_2 = yf.download(symbol_2, start_2, end_2)  # вивантажуємо дані\ntime_ser_2 = data_2['Adj Close'].copy()         # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'      # підпис по вісі Ох \nylabel_2 = symbol_2        # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nТепер створимо новий масив даних, що об’єднуватиме в собі значення S&P 500 та BTC по їх спільним датам:\n\n# приводимо значення індексів до типу DataFrame, щоб мати змогу їх об'єднати \n# за допомогою бібліотеки pandas\ndf_time_ser_1 = pd.DataFrame(time_ser_1) \ndf_time_ser_2 = pd.DataFrame(time_ser_2)\n\n\njoined = df_time_ser_1.merge(df_time_ser_2, # об'єднуємо по датам тієї бази, що містить \n                             on='Date',     # більше дат\n                             how='left')  \n\njoined = joined.rename(columns={joined.columns[0]: symbol_1,  # переіменовуємо колонки по \n                                joined.columns[1]: symbol_2}) # змінним symbol_1 та symbol_2\n\njoined = joined.dropna()  # видаляємо рядки, що містять нульові значення\n\nВиводимо отриману базу:\n\njoined\n\n\n\n\n\n\n\n\n^GSPC\nBTC-USD\n\n\nDate\n\n\n\n\n\n\n2014-09-17\n2001.569946\n457.334015\n\n\n2014-09-18\n2011.359985\n424.440002\n\n\n2014-09-19\n2010.400024\n394.795990\n\n\n2014-09-22\n1994.290039\n402.152008\n\n\n2014-09-23\n1982.770020\n435.790985\n\n\n...\n...\n...\n\n\n2023-08-17\n4370.359863\n26664.550781\n\n\n2023-08-18\n4369.709961\n26049.556641\n\n\n2023-08-21\n4399.770020\n26124.140625\n\n\n2023-08-22\n4387.549805\n26031.656250\n\n\n2023-08-23\n4436.009766\n26431.640625\n\n\n\n\n2249 rows × 2 columns\n\n\n\nІ візуалізуємо сам графік. Спочатку оголосимо функцію для попарної візуалізації рядів зі збереженням їх абсолютних значень:\n\ndef plot_pair(x_values, y_values, x_label, y_label, file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y_values[0], \n                  \"b-\", label=fr\"{y_label[0]}\")\n    p2, = ax2.plot(x_values,\n                   y_values[1], \n                   color=clr, \n                   label=fr'${y_label[1]}$')\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y_label[0]}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nі тепер візуалізуємо отримані ряди:\n\nvalues_plot = joined.iloc[:,0].values, joined.iloc[:,1].values\nylabels = ylabel_1, ylabel_2\nfile_name = f'joined {symbol_1}_{symbol_2}'\n\n\nplot_pair(joined.index, values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРис. 4.4: Динаміка індексу S&P 500 та Біткоїна за досліджуваний період\n\n\n\n\n\n\n\n\n\n\nВажливо\n\n\n\nНе виконуйте блоки коду, що відповідають секції “Розрахунок взаємної інформації”, якщо ви працюєте з текстовим файлом.\n\n\n\n4.2.1 Розрахунок взаємної інформації\nРозглянемо взаємну інформацію як індикатор нелінійної кореляції між двома фінансовими активами, і спробуємо сказати, чи є між ними “істинний” взаємозв’язок. Виконуватимемо розрахунки із використанням алгоритму руховому вікна. Також визначимо функцію transform() для нормалізації ряду.\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\nret_type = 6                           # вид ряду\nwindow = 100                           # ширина вікна\ntstep = 1                              # часовий крок вікна \nlength = len(joined.iloc[:,0].values)  # довжина самого ряду\n\nMI = []                                # масив для віконної взаємної інформації\n\nТепер приступимо до розрахунків:\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагменти\n    fragm_1 = joined[symbol_1][i:i+window]  \n    fragm_2 = joined[symbol_2][i:i+window]\n\n    # виконуємо процедуру трансформації ряду \n    fragm_1 = transformation(fragm_1, ret_type)    \n    fragm_2 = transformation(fragm_2, ret_type)\n\n    # розраховуємо взаємну інформацію \n    mut_inf = nk.mutual_information(fragm_1, fragm_2)\n    \n    # та додаємо результат до масиву значень\n    MI.append(mut_inf)\n\n100%|██████████| 2149/2149 [00:03&lt;00:00, 706.05it/s]\n\n\nЗберігаємо отриманий результат у текстовому файлі:\n\nnp.savetxt(f\"mutual_inf_name1={symbol_1}_name2={symbol_2}_ \\\n    window={window}_step={tstep}_rettype={ret_type}.txt\" , MI)\n\nВізуалізуємо результат між відповідними показниками:\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(joined.index[window:length:tstep], \n                joined[symbol_1][window:length:tstep].values, \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(joined.index[window:length:tstep],\n                joined[symbol_2][window:length:tstep].values,\n                'red', \n                label=fr\"{symbol_2}\")\np3, = ax3.plot(joined.index[window:length:tstep],\n                MI,\n                'magenta', \n                label=r\"$MI$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"mutual_inf_name1={symbol_1}_name2={symbol_2}_ \\\n    window={window}_step={tstep}_rettype={ret_type}.jpg\")\n\nplt.show();\n\n\n\n\nРис. 4.5: Динаміка індексу S&P 500, Біткоїна та взаємної інформації\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, на фондовому та криптовалютному ринках дійсно спостерігалися фази зростання взаємної інформації між ними. Найкраще це видно напередодні кризи 2018-го року, під час 2019, після коронавірусної пандемії та напередодні 2023 року. Для даного індикатора залишається простір для експериментів, що можуть вивести його на рівень достатньо потужного передвісника криз на фондовому ринку чи криптовалютному.\nЯк вже зазначалося, окрім обчислення взаємної інформації для двох пар часових сигналів, ми можемо обчислити автовзаємну інформація, тобто взаємну інформацію ряду самого із собою по різним часовим лагам, як це було пророблено для автокореляції. Недолік автокореляції полягає в тому, що вони визначає саме лінійний зв’язок теперішніх значень з попередніми. Автовзаємна інформація в свою чергу є показником нелінійного зв’язку теперішніх значень із попередніми.\nДля обчислення автовзаємної інформації визначимо наступну функцію:\n\ndef automut(x, maxlag):\n    n = len(x)                               # визначаємо довжину сигналу\n    lags = np.arange(0, maxlag, dtype=\"int\") # оголошуємо масив лагів від 0 до maxlag\n    mi = np.zeros(len(lags))                 # оголошуємо масив під значення взаємної інформації\n    for i, lag in enumerate(lags):           # проходимось по кожному лагу\n        \n        # виконуємо зміщення на lag значень \n        y1 = x[:n-lag].copy()\n        y2 = x[lag:].copy()\n\n        # і розраховуємо взаємну інформацію між часовим рядом y1\n        # та його зміщенною на lag кроків копією \n        mi[i] = nk.mutual_information(y1, y2, bins=100)\n\n    return mi\n\nВиведемо залежність автовзаємної інформації від лагу для всього ряду S&P 500 та Біткоїна. Спочатку розрахуємо вихідні значення ряду, далі прибутковості і потім волатильності. Для кожного з відповідних сигналів виведемо взаємну інформацію.\nВиконуємо перетворення S&P 500 та Біткоїна\n\nsp_init = transformation(time_ser_1, ret_type=1)\nsp_ret = transformation(time_ser_1, ret_type=4)\nsp_vol = np.abs(sp_ret.copy())\n\nbtc_init = transformation(time_ser_2, ret_type=1)\nbtc_ret = transformation(time_ser_2, ret_type=4)\nbtc_vol = np.abs(btc_ret.copy())\n\nРозраховуємо автовзаємну інформацію S&P 500 та Біткоїна\n\nmax_lag = 100\n\nmu_sp_init = automut(sp_init, max_lag)\nmu_sp_ret = automut(sp_ret, max_lag)\nmu_sp_vol = automut(sp_vol, max_lag)\n\nmu_btc_init = automut(btc_init, max_lag)\nmu_btc_ret = automut(btc_ret, max_lag)\nmu_btc_vol = automut(btc_vol, max_lag)\n\nlags = np.arange(0, max_lag, dtype=\"int\") # оголошуємо масив лагів від 0 до maxlag\n\n\nfig, ax = plt.subplots()                     # Створюємо порожній графік\n\nax.plot(lags, mu_sp_init, label=r'$MI $ ' + f'{symbol_1}')  # Додаємо дані до графіку\nax.plot(lags, mu_sp_ret, label=r'$MI$ ' + r'$g(t)$')                          \nax.plot(lags, mu_sp_vol, label=r'$MI$ ' +  r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Automutual information\")     # Додаємо підпис для вісі Оу\n\nplt.savefig(f'Automutual information {symbol_1}.jpg')  # Зберігаємо графік \nplt.show();                                            # Виводимо графік\n\n\n\n\nРис. 4.6: Зміна з часом автовзаємної інформації для вихідного ряду x, нормалізованих прибутковостей g та модулів mod(g) фондового індексу S&P 500\n\n\n\n\n\nfig, ax = plt.subplots()                     # Створюємо порожній графік\n\nax.plot(lags, mu_btc_init, label=r'$MI $ ' + f'{symbol_2}')  # Додаємо дані до графіку\nax.plot(lags, mu_btc_ret, label=r'$MI$ ' + r'$g(t)$')                          \nax.plot(lags, mu_btc_vol, label=r'$MI$ ' +  r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Automutual information\")     # Додаємо підпис для вісі Оу\n\nplt.savefig(f'Automutual information {symbol_2}.jpg')  # Зберігаємо графік \nplt.show();                                            # Виводимо графік\n\n\n\n\nРис. 4.7: Зміна з часом автовзаємної інформації для вихідного ряду x, нормалізованих прибутковостей g та модулів mod(g) криптовалютного індексу BTC\n\n\n\n\nЯк ми можемо бачити з представлених графіків, ступінь взаємної інформації це показник, що найкращим чином працює саме для вихідних значень часових сигналів. Для вихідного ряду ступінь взаємної інформації залишається доволі високим. Для прибутковостей і волатильностей взаємна інформація спадає одразу на першому лагу, що свідчить про незалежність значень на подальших часових затримках.\n\n\n4.2.2 Розрахунок мономасштабної складності Лемпеля-Зіва\nПродовжимо розраховувати й інші показники складності. Розглянемо можливість використання показника складності Лемпеля-Зіва в якості індикатора катастрофічних подій.\n\nret_type = 4                           # вид ряду\nwindow = 250                           # ширина вікна\ntstep = 1                              # часовий крок вікна \nlength = len(time_ser_1.values)        # довжина самого ряду\nm = 4                                  # розмірність вкладень \ntau = 1                                # часова затримка         \n\nLZC = []                               # класична складність Лемпеля-Зіва\nPLZC = []                              # пермутаційна складність Лемпеля-Зіва\n\n\nfor i in tqdm(range(0,length-window,tstep)):    # фрагменти довжиною window  \n                                                # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо класичну складність Лемпеля-Зіва \n    lzc, _ = nk.complexity_lempelziv(fragm)\n\n    # та пермутаційну складність Лемпеля-Зіва\n    plzc, _ = nk.complexity_lempelziv(fragm, \n                                      delay=tau, \n                                      dimension=m, \n                                      permutation=True)\n\n\n    # та додаємо результати до масиву значень\n    LZC.append(lzc)\n    PLZC.append(plzc)\n\n100%|██████████| 2177/2177 [00:18&lt;00:00, 116.70it/s]\n\n\nЗберігаємо результати в текстових файлах:\n\nnp.savetxt(f\"lzc_name={symbol_1}_window={window}_step={tstep}_rettype={ret_type}.txt\" , LZC)\nnp.savetxt(f\"plzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.txt\" , PLZC)\n\nТа візуалізуємо їх:\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n                time_ser_1.values[window:length:tstep], \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep],\n                LZC,\n                'gold', \n                label=fr\"$LZC$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep],\n                PLZC,\n                'red', \n                label=fr\"$PLZC$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"plzc_lzc_name={symbol_1}_ \\\n    window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.jpg\")\n\nplt.show();\n\n\n\n\nРис. 4.8: Динаміка індексу S&P 500, класичної мономасштабної складності Лемпеля-Зіва та її пермутаційного аналогу\n\n\n\n\nНа даному рисунку видно, що 2 міри поводять себе асиметрично по відношенню один до одного: \\(LCZ\\) вказує на зростання складності, наприклад, події 2019 року. У той же час \\(PLCZ\\) вказує на спад складності системи в цей період. Варто дослідити мультимасштабну динаміку міри Лемпеля-Зіва для більш змістовних висновків.\n\n\n4.2.3 Обчислення мультимасштабної складності Лемпеля-Зіва\n\nret_type = 4\nret_sp = transformation(time_ser_1, ret_type)\n\n\nmslzc, info = nk.entropy_multiscale(ret_sp, method=\"LZC\", \n                                    scale=200, show=True)\n\n\n\n\nРис. 4.9: Залежність від масштабу класичної складності Лемпеля-Зіва для S&P 500\n\n\n\n\nМультимасштабна динаміка пермутаційного показника складності Лемпеля-Зіва\n\nmsplzc, info = nk.entropy_multiscale(ret_sp, \n                                        method=\"LZC\",  \n                                        permutation=True,\n                                        dimension=m,\n                                        delay=tau, \n                                        scale=200, \n                                        show=True)\n\n\n\n\nРис. 4.10: Залежність від масштабу пермутаційної складності Лемпеля-Зіва для S&P 500\n\n\n\n\nТепер розрахуємо віконну динаміку мультимасштабних показників Лемпеля-Зіва. Ми повертатимемо сумарну складність Лемпеля-Зіва за всіма масштабам.\n\nret_type = 4                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду\nm = 4                             # розмірність вкладень \ntau = 1                           # часова затримка         \n\nMSLZC = []                        # мультимасштабна складність Лемпеля-Зіва\nMSPLZC = []                       # мультимасштабна пермутаційна складність Лемпеля-Зіва\n\n\nfor i in tqdm(range(0,length-window,tstep)):    # фрагменти довжиною window  \n                                                # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо мультимасштабну складність Лемпеля-Зіва \n    mslzc, _ = nk.entropy_multiscale(fragm)\n\n    # та мультимасштабну пермутаційну складність Лемпеля-Зіва\n    msplzc, _ = nk.entropy_multiscale(fragm, \n                                      delay=tau, \n                                      dimension=m, \n                                      permutation=True)\n\n\n    # та додаємо результати до масиву значень\n    MSLZC.append(mslzc)\n    MSPLZC.append(msplzc)\n\n100%|██████████| 2177/2177 [00:49&lt;00:00, 43.90it/s]\n\n\n\nnp.savetxt(f\"mslzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}.txt\" , MSLZC)\nnp.savetxt(f\"msplzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.txt\" , MSPLZC)\n\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n                time_ser_1.values[window:length:tstep], \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep],\n                MSLZC,\n                'gold', \n                label=fr\"$MSLZC$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep],\n                MSPLZC,\n                'red', \n                label=fr\"$MSPLZC$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"msplzc_mslzc_name={symbol_1}_ \\\n    window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.jpg\")\n\nplt.show();\n\n\n\n\nРис. 4.11: Динаміка індексу S&P 500, класичної мультимасштабної складності Лемпеля-Зіва та її пермутаційного аналогу\n\n\n\n\nТепер бачимо однозначну картину: обидві міри поводять себе синхронно, та спадають у кризові та передкризові періоди, що вказує на зростання ступеня детермінованості та самоорганізації ринку.\n\n\n4.2.4 Обчислення Шеннонівської ентропії\nЯк уже зазначалося, Шеннонівська ентропія — це міра непередбачуваності стану, або, еквівалентно, його середнього інформаційного вмісту. Ентропія Шеннона є однією з перших і найбільш базових мір ентропії та фундаментальним поняттям теорії інформації.\nРозраховуватимемо її в ковзному вікні.\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду  \nlog_base = np.exp(1)      \n\nshannon = []                      # ентропія Шеннона\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо ентропію Шеннона\n    p, be = np.histogram(fragm,         # розраховуємо щільність ймовірностей\n                        bins='auto', \n                        density=True)  \n    r = be[1:] - be[:-1]                # знаходимо dx\n    P = p * r                           # представляємо ймовірність як f(x)*dx\n    P = P[P!=0]                         # фільтруємо по всім ненульовим ймовірностям\n\n    sh_ent, _ = nk.entropy_shannon(freq=P, base=log_base) # розраховуємо ентропію \n    sh_ent /= np.log(len(P))                              # та нормалізуємо\n\n    # та додаємо результат до масиву значень\n    shannon.append(sh_ent)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 2135.92it/s]\n\n\n\nnp.savetxt(f\"shannon_ent_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\" , shannon)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], shannon\nylabels = ylabel_1, \"ShEn\"\nfile_name = f\"shannon_ent_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], \n            values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРис. 4.12: Динаміка індексу S&P 500 та ентропії Шеннона\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, ентропія Шеннона реагує спадом на кризові періоди індексу S&P 500, що вказує на приріст ступеня періодизації системи, її детермінованості.\n\n\n4.2.5 Розрахунок інформаційного показника Фішера\nПерш за все задаємо параметри для розрахунків:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду  \nm = 3                             # розмірність вкладень\ntau = 1                           # часова затримка\n\nfisher = []                       # інформація Фішера\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    fish_inf, _ = nk.fisher_information(signal=fragm,\n                                        dimension=m, \n                                        delay=tau) \n\n    # та додаємо результат до масиву значень\n    fisher.append(fish_inf)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 2937.25it/s]\n\n\n\nnp.savetxt(f\"fisher_inf_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_dimension={m}_delay={tau}.txt\", fisher)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], fisher\nylabels = ylabel_1, \"FI\"\nfile_name = f\"fisher_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_dimension={m}_delay={tau}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРис. 4.13: Динаміка індексу S&P 500 та інформаційного показника Фішера\n\n\n\n\nНа даному рисунку видно, що показник Фішера спадає у кризові та передкризові періоди, що говорить про спад кількості інформації, що необхідна для опису самоорганізованої динаміки фінансових криз, тобто зростання корельованості між діями трейдерів на ринку.\n\n\n4.2.6 Обчислення часу декореляції\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\ndecorrelation_time = []           # час декореляції\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    dec_time, _ = nk.complexity_decorrelation(fragm) \n\n    # та додаємо результат до масиву значень\n    decorrelation_time.append(dec_time)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 1857.09it/s]\n\n\n\nnp.savetxt(f\"dec_time_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", decorrelation_time)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], decorrelation_time\nylabels = ylabel_1, \"DT\"\nfile_name = f\"dec_time_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, \n            xlabel, ylabels, file_name)\n\n\n\n\nРис. 4.14: Динаміка індексу S&P 500 та часу декореляції\n\n\n\n\nНа представленому рисунку видно, що час декореляції зростає у період краху, що вказує на зростання кореляції системи в цей період.\n\n\n4.2.7 Обчислення відносної шорсткості\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\nrelative_roughness = []           # відносна шорсткість\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    rr, _ = nk.complexity_relativeroughness(fragm) \n\n    # та додаємо результат до масиву значень\n    relative_roughness.append(rr)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 2094.81it/s]\n\n\n\nnp.savetxt(f\"rel_rough_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", relative_roughness)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], relative_roughness\nylabels = ylabel_1, \"RR\"\nfile_name = f\"rel_rough={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, \n            xlabel, ylabels, file_name)\n\n\n\n\nРис. 4.15: Динаміка індексу S&P 500 та показника відносної шорсткості\n\n\n\n\nПоказник відносної шорсткості демонструє, що крахові події як, наприклад, у 2015, 2016, 2019, 2020 та 2023 роках характеризуються зростанням шорсткості своєї динаміка. Подібного роду поведінка є індикатором зростання шумової активності ринку: кореляційних характеристик та загальної варіації ринку в цілому. Зростання цього показника в періоди криз є індикатором зростання фрактальності ринку в дані періоди часу.\n\n\n4.2.8 Розрахунок показників складності Хьорта\nЗавершуємо хід роботи показниками складності Хьорта:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\nactivity = []                     # параметр активності\nmobility = []                     # параметр рухливості\ncomplexity = []                   # параметр складності\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо показники складності Хьорта\n    cmpl, info = nk.complexity_hjorth(fragm) \n\n    # та додаємо результат до масиву значень\n    activity.append(info['Activity'])\n    mobility.append(info['Mobility'])\n    complexity.append(cmpl)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 3338.20it/s]\n\n\n\nnp.savetxt(f\"activity_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", activity)\nnp.savetxt(f\"mobility_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", mobility)\nnp.savetxt(f\"complexity_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", complexity)    \n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\nax4.spines.right.set_position((\"axes\", 1.19))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n              time_ser_1.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep], \n               activity, \"r--\", label=r\"$Act$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep], \n               mobility, \"g-\", label=r\"$Mob$\")\np4, = ax4.plot(time_ser_1.index[window:length:tstep],\n               complexity, \"m-\", label=r\"$Comp$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax4.legend(handles=[p1, p2, p3, p4])\n\nplt.savefig(f\"hjorth_name={symbol_1}_ret={ret_type}_wind={window}_step={tstep}.jpg\")\nplt.show();\n\n\n\n\nРис. 4.16: Динаміка індексу S&P500 наряду з показниками активності, мобільності та складності Хьорта\n\n\n\n\nНа даному рисунку видно, що параметр активності (\\(Act\\)) представляється найменш інформативним, оскільки він вказує тільки на зростання сукупної дисперсії сигналу. Видно тільки те, що активність значно почала зростати напередодні 2022 року, але для попередніх кризових станів ми не бачимо передвісницької поведінки цього індикатора, тому він ще вимагатиме додактових досліджень та експериментів, що виходять за рамки даного посібника.\nПитання передчасної ідентифікації наростання кризового явища найкраще вирішує показник мобільності (\\(Mob\\)). Ми бачимо, що даний показник зростає під час 2015-2016 років, напередодні 2019, при настанні коронавірусної пандемії, перед 2023 роком та 2024.\nПоказник складності Хьорта (\\(Comp\\)) реагує асиметричним чином: у той час коли мобільність зростає, показник складності спадає, вказуючи на те, що динаміки системи прагне до вищого ступеня періодичності або корельованості."
  },
  {
    "objectID": "lab_5.html#теоретичні-відомості",
    "href": "lab_5.html#теоретичні-відомості",
    "title": "5  Лабораторна робота № 5",
    "section": "5.1 Теоретичні відомості",
    "text": "5.1 Теоретичні відомості\nПитання динаміки розвитку і функціонування складних систем може розглядатись у двох варіантах:\n\nяк дослідження шумової активності;\nяк детерміністичного випадку з певним ступенем порядку.\n\nОстанніми роками було використано кілька підходів для ідентифікації механізмів, що лежать в основі розвитку та функціонування складних систем. Особливо корисні результати було отримано при їх дослідженні методами теорії випадкових матриць, моно- та мультифрактального аналізу, теорії хаосу з реконструкцією траєкторії системи у фазовому просторі та визначення її параметрів, рекурентного аналузу. Ми розглянули ці методи у попередніх роботах. Однак, застосування деяких із методів висуває вимоги до стаціонарності досліджуваних даних, потребує довгих часових рядів та комплексного обчислення кількох параметрів.\nІншим підходом до розгляду питання вивчення особливостей складних систем є обчислення характеристик ентропії. Для практичного застосування у якості міри невизначеності, а значить і складності сигналу, використовують десятки різновидів ентропії.\nКонцепція термодинамічної ентропії як міри хаосу системи добре відома у фізиці, однак, останніми роками поняття ентропії було застосоване до складних систем інших об’єктів (біологічних, економічних, соціальних тощо). Так, один із найбільш часто використовуваних методів визначення ентропії базується на обчисленні спектру потужності Фур’є та застосовується для вивчення сигналів (часових рядів) різної природи. Проте, використання дискретного перетворення Фур’є для аналізу часових рядів має свої недоліки, зокрема, на результати впливає нестаціонарність рядів, варіювання їх довжини від сотень до сотень тисяч, та обмеження самого методу (незмінність частотно-часових характеристик протягом всього часу функціонування системи). Тому виникає питання про розрахунок значень ентропії за допомогою інших методів.\nВведемо поняття ентропії, скориставшись інформацією, яку можна знайти у Вікіпедії.\nТермодинамічна ентропія \\(S\\), часто просто іменована ентропія, в хімії і термодинаміці є мірою кількості енергії у фізичній системі, яка не може бути використана для виконання роботи. Вона також є мірою безладдя, присутнього в системі.\nПоняття ентропії була вперше введено у 1865 році Рудольфом Клаузіусом. Він визначив зміну ентропії термодинамічної системи при оборотному процесі як відношення зміни загальної кількості тепла \\(\\Delta Q\\) до величини абсолютної температури \\(T\\):\n\\[\n\\Delta S = \\Delta Q / T.\n\\]\nРудольф Клаузіус дав величині \\(S\\) ім’я “ентропія”, що походить від грецького слова τρoπή, “зміна” (зміна, перетворення). Зверніть увагу на те, що рівність відноситься до зміни ентропії.\nУ 1877 році, Людвіг Больцман зрозумів, що ентропія системи може відноситися до кількості можливих “мікростанів” (мікроскопічних станів) що узгоджуються з їх термодинамічними властивостями. Розглянемо, наприклад, ідеальний газ у посудині. Мікростан визначений як позиції і імпульси кожного атома, що становить систему. Зв’язність пред’являє до нас вимоги розглядати тільки ті мікростани, для яких: (i) місцерозташування всіх частин розташовані в рамках судини, (ii) для отримання загальної енергії газу кінетичні енергії атомів підсумовуються. Больцман постулював що\n\\[\nS = k_{B}\\ln{\\Omega},\n\\]\nде константу \\(k_{B} = 1,38 \\cdot 10^{-23} Дж/К\\) ми знаємо тепер як сталу Больцмана, a \\(\\Omega\\) є числом мікростанів, які можливі в наявному макроскопічному стані. Цей постулат, відомий як принцип Больцмана, може бути оцінений як початок статистичної механіки, яка описує термодинамічні системи використовуючи статистичну поведінку компонентів, із яких вони складаються. Принцип Больцмана зв’язує мікроскопічні властивості системи (\\(\\Omega\\)) з однією з її термодинамічних властивостей (\\(S\\)).\nЗгідно визначенню Больцмана, ентропія є просто функцією стану. Більш того, оскільки (\\(\\Omega\\)) може бути тільки натуральним числом (1, 2, 3), ентропія повинна бути додатною — виходячи з властивостей логарифма.\nУ випадку дискретних станів квантової механіки кількість станів підраховується звичайним чином. В рамках класичної механіки мікроскопічний стан системи описується координатами \\(q_{i}\\) й імпульсами \\(p_{i}\\) окремих частинок, які пробігають неперервні значення. Для підрахунку станів у класичних системах фазовий простір розбивають на невеликі комірки із об’ємом, який відповідає сталій Планка. У такому випадку\n\\[\nS = k_{B}\\ln\\frac{1}{( 2\\pi\\hbar )^{s}} \\int \\prod_{i=1}^{s} dq_{i}dp_{i},\n\\]\nде \\(s\\) — число незалежних координат, \\(\\hbar\\) — приведена стала Планка, а інтегрування проводиться по області фазового простору, який відповідає певному макроскопічному стану.\nКлод Шеннон (Shannon, 1948) запропонував формулу для оцінки невизначеності кодової інформації в каналах зв’язку, звану ентропією Шеннона:\n\\[\nS = -k\\sum_{i=1}^{n}p_{i}\\ln{p_{i}},\n\\]\nде \\(p_{i}\\) — вірогідність того, що символ \\(i\\) зустрічається в коді, який містить \\(N\\) символів, \\(k\\) — розмірний множник.\nЗв’язок між ентропією і інформацією можна прослідкувати на наступному прикладі. Розглянемо тіло при абсолютному нулі температури, і хай ми маємо повну інформацію про координати і імпульси кожної частинки. Для простоти покладемо, що імпульси всіх частинок рівні нулю. В цьому випадку термодинамічна ймовірність рівна одиниці, а ентропія — нулю. При кінцевих температурах ентропія в рівновазі досягає максимуму. Можна зміряти всі макропараметри, що характеризують даний макростан. Проте ми практично нічого не знаємо про мікростан системи. Точніше кажучи, ми знаємо, що даний макростан можна реалізувати за допомогою дуже великого числа мікростанів. Таким чином, нульовій ентропії відповідає повна інформація (ступінь незнання рівний нулю), а максимальної ентропії — повне незнання мікростанів (ступінь незнання максимальний).\nУ теорії інформації ентропія (інформаційна ентропія) визначається як кількість інформації. Нехай \\(P\\) — апріорна вірогідність деякої події (ймовірність до проведення досвіду), а \\(P_{1}\\) — ймовірність цієї події після проведення досвіду. Для простоти вважатимемо, що \\(P_{1} = 1\\). За Шенноном, кількість інформації \\(I\\), яка дає точну відповідь (після проведення експерименту)\n\\[\nI = K \\log{P}.\n\\]\nЦя кількість інформації, за визначенням, дорівнює одному біту.\nФізичний сенс \\(I\\) — це міра нашого незнання. Іншими словами, \\(I\\) — це та інформація, яку ми можемо одержати, вирішивши завдання. У прикладі (тіло при абсолютному нулі температури), що розглядається вище, міра нашого незнання рівна нулю, оскільки \\(P = 1\\). Після проведення досвіду ми одержуємо нульову інформацію \\(I = 0\\), оскільки все було відомо до досвіду. Якщо розглядати тіло при кінцевих температурах, то до проведення досвіду число мікростанів, а отже, і \\(P\\) дуже велике. Після проведення досвіду ми одержуємо велику інформацію, оскільки нам стають відомими координати і імпульси всіх частинок.\nАналогія між кількістю інформації і ентропією \\(S\\), визначуваною з принципу Больцмана, очевидна. Досить покласти множник \\(K\\) рівним постійній Больцмана \\(k_{B}\\) і використовувати натуральний логарифм. Саме з цієї причини величину \\(I\\) називають інформаційною ентропією. Інформаційна ентропія (кількість інформації) була визначена по аналогії із звичайною ентропією, і вона має властивості, характерні для звичайній ентропії: адитивність, екстремальні властивості і т.д. Проте ототожнювати звичайну ентропію з інформаційною не можна, оскільки неясно, яке відношення має друге начало до інформації. Нагадаємо, що екстенсивна величина — ця така характеристика системи, яка росте із збільшенням розмірів системи, тобто, якщо наша система складається з двох незалежних підсистем \\(А\\) і \\(В\\), то ентропію всієї системи можна одержати складанням ентропій підсистем:\n\\[\nS(\\,A+B)\\, = S(\\,A)\\, + S(\\,B)\\,.\n\\]\nСаме ця властивість і означає екстенсивність, або адитивність, ентропії."
  },
  {
    "objectID": "lab_5.html#хід-роботи",
    "href": "lab_5.html#хід-роботи",
    "title": "5  Лабораторна робота № 5",
    "section": "5.2 Хід роботи",
    "text": "5.2 Хід роботи\nРозглянемо як ми можемо використовувати ентропійні показники в якості індикаторів або індикаторів-передвісників кризових подій. Перш за все імпортуємо необхідні модулі для подальшої роботи:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport yfinance as yf\nimport neurokit2 as nk\nimport EntropyHub as eh\nfrom tqdm import tqdm\nimport warnings\nimport scienceplots\n\nwarnings.filterwarnings('ignore')\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nУ даній роботі виконуватимемо розрахунки на прикладі одного з найважливіших фондових індексів Японії — Nikkei 225. Індекс обчислюється шляхом визначення простого середнього арифметичного значення цін акцій 225 провідних компаній, які входять до першої секції Токійської фондової біржі. Для отримання значень індексу скористаємось бібліотекою yfinance. Значення розглядатимемо за весь період, тому початкову та кінцеву дати вказувати не будемо.\n\nsymbol = '^N225'                      # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 5.1: Динаміка щоденних змін фондового індексу N225\n\n\n\n\nДля приведення ряду до стандартизованого вигляду або прибутковостей визначимо функцію transformations():\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nДля побудови пари часових рядів визначимо функцію plot_pair():\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n5.2.1 Approximate entropy (Апроксимаційна ентропія)\nТеоретичний опис методики розрахунку\nЕнтропія подібності (Approximate Entropy, ApEn) є “статистикою регулярності”, що визначає можливість передбачувати флуктуації в часових рядах. Інтуїтивно вона означає, що наявність повторюваних шаблонів (послідовностей певної довжини, побудованих із чисел ряду, що слідують одне за іншим) флуктуацій у часовому ряді призводить до більшої передбачуваності часового ряду порівняно із рядами, де повторюваності шаблонів немає. Порівняно велике значення ApEn показує ймовірність того, що подібні між собою шаблони спостережень не будуть слідувати один за одним. Іншими словами, часовий ряд, що містить велику кількість повторюваних шаблонів, має порівняно мале значення ApEn, а значення ApEn для менш передбачуваного (більш складного) процесу є більшим.\nПри розрахунку ApEn для даного часового ряду \\(S_{N}\\), що складається із \\(N\\) значень \\(t(\\,1)\\,, t(\\,2)\\,, t(\\,3)\\,, ... , t(\\,N)\\,\\) вибираються два параметри, \\(m\\) та \\(r\\). Перший з цих параметрів, \\(m\\), вказує довжину шаблона, а другий — \\(r\\) — визначає критерій подібності. Досліджуються підпослідовності елементів часового ряду \\(S_{N}\\), що складаються з \\(m\\) чисел, взятих, починаючи з номера \\(i\\), і називаються векторами \\(p_{m} (\\,i)\\,\\). Два вектори (шаблони), \\(p_{m}(\\,i)\\,\\) та $ p_{m}(,j),$, будуть подібними, якщо всі різниці пар їх відповідних координат є меншими за значення \\(r\\), тобто якщо\n\\[\n| t(\\,i+k)\\, - t(\\,j+k)\\, | &lt; r \\quad \\textrm{для} \\quad 0 \\leq k &lt; m.\n\\]\nДля розглядуваної множини \\(P_{m}\\) всіх векторів довжини \\(m\\) часового ряду \\(S_{N}\\) можна обраховуються значення\n\\[\nC_{im}(\\,r)\\, = \\frac{n_{im}(\\,r)\\,}{N-m+1},\n\\]\nде \\(n_{im}(\\,r)\\,\\) — кількість векторів у \\(P_{m}\\), що подібні вектору \\(p_{m}(\\,i)\\,\\) (враховуючи вибраний критерій подібності \\(r\\)). Значення \\(C_{im}(\\,r)\\,\\) є часткою векторів довжини \\(m\\), що мають схожість із вектором такої ж довжини, елементи якого починаються з номера \\(i\\). Для даного часового ряду обраховуються значення \\(C_{im}(\\,r)\\,\\) для кожного вектора у \\(P_{m}\\), після чого знаходиться середнє значення \\(C_{m}(\\,r)\\,\\), яке виражає розповсюдженість подібних векторів довжини \\(m\\) у ряду \\(S_{N}\\). Безпосередньо ентропія подібності для часового ряду \\(S_{N}\\) з використанням векторів довжини \\(m\\) та критерію подібності \\(r\\) визначається за формулою:\n\\[\nApEn(\\,S_{N}, m, r)\\, = \\ln(\\,\\frac{C_{m}(\\,r)\\,}{C_{m+1}(\\,r)\\,})\\,,\n\\]\nтобто, як натуральний логарифм відношення повторюваності векторів довжиною \\(m\\) до повторюваності векторів довжиною \\(m+1\\).\nТаким чином, якщо знайдуться подібні вектори у часовому ряді, ApEn оцінить логарифмічну ймовірність того, що наступні інтервали після кожного із векторів будуть відрізнятись. Менші значення ApEn відповідають більшій ймовірності того, що за векторами слідують подібні їм. Якщо часовий ряд дуже нерегулярний — наявність подібних векторів не може бути передбачуваною і значення ApEn є порівняно великим.\nЗауважимо, що ApEn є нестійкою до вхідних даних характеристикою, оскільки досить сильно залежить від параметрів \\(m\\) та \\(r\\).\n\nwindow = 500                   # ширина вікна\ntstep = 5                      # часовий крок\n\nm = 3                          # розмірність вкладень\ntau = 1                        # часова затримка\nr = 0.45                       # параметр подібності\n\nret_type = 4                   # вид ряду: \n                               # 1 - вихідний, \n                               # 2 - детрендований (різниця між теп. значенням та попереднім)\n                               # 3 - прибутковості звичайні, \n                               # 4 - стандартизовані прибутковості, \n                               # 5 - абсолютні значення (волатильності)\n                               # 6 - стандартизований ряд \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nApEn = []                      # масив для зберігання значень ентропії\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n    \n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # розраховуємо апроксимаційну ентропію\n    Ap, _ = nk.entropy_approximate(signal=fragm, \n                                    dimension=m,\n                                    delay=tau, \n                                    tolerance=r,\n                                    corrected=False)\n    ApEn.append(Ap)\n\n100%|██████████| 2792/2792 [00:16&lt;00:00, 169.59it/s]\n\n\nЗберігаємо значення апроксимаційної ентропії до текстового файлу:\n\nnp.savetxt(f\"ApEn_name={symbol}_window={window}_step={tstep}_\\\n           dim={m}_tau={tau}_radius={r}_sertype={ret_type}.txt\", ApEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_apen = fr'$ApEn$'\n\nfile_name_apen = f\"ApEn_name={symbol}_window={window}_step={tstep}_\\\n           dim={m}_tau={tau}_radius={r}_sertype={ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          ApEn, \n          ylabel, \n          label_apen,\n          xlabel,\n          file_name_apen)\n\n\n\n\nРис. 5.2: Динаміка фондового індексу N225 та апроксимаційної ентропії\n\n\n\n\n\n\n5.2.2 Fuzzy entropy (Нечітка ентропія)\n\nWeiting Chen, et al. Characterization of surface EMG signal based on fuzzy entropy, IEEE Transactions on neural systems and rehabilitation engineering, 15.2 (2007): 266-272.\nHong-Bo Xie, Wei-Xing He, and Hui Liu, Measuring time series regularity using nonlinear similarity-based sample entropy, Physics Letters A, 372.48 (2008): 7140-7146.\n\n\nwindow = 500                    # ширина вікна\ntstep = 5                       # часовий крок\n\nm = 3                           # розмірність вкладень\ntau = 1                         # часова затримка\n\ncharacteristic_func = \"default\" # вид функції приналежності: \n                                # default, \n                                # sigmoid, \n                                # gudermannian, \n                                # linear\n\nr = (0.4, 2.0)                  # параметри, що подаються до функції приналежності: \n                                # для 'default' та 'sigmoid' - два значення r, \n                                # для gudermannian та linear - 1 значення r, \n  \nret_type = 4                    # вид ряду: \n                                # 1 - вихідний \n                                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                                # 3 - прибутковості звичайні \n                                # 4 - стандартизовані прибутковості \n                                # 5 - абсолютні значення (волатильності)\n                                # 6 - стандартизований ряд \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nFuzzEn = [] #масив для зберігання значень ентропії\n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # обчислення нечіткої ентропії \n    Fuzz, _, _ = eh.FuzzEn(Sig=fragm, m=m, tau=tau, Fx=characteristic_func, r=r)  \n    FuzzEn.append(Fuzz[-1]) # дожаємо розрахованє значення до масиву значень \n\n100%|██████████| 2792/2792 [02:51&lt;00:00, 16.31it/s]\n\n\nЗберігаємо значення нечіткої ентропії до текстового файлу:\n\nnp.savetxt(f\"FuzzEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_radius={r}_sertype={ret_type}_\\\n        memberfunc={characteristic_func}.txt\", FuzzEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_fuzzen = fr'$FuzzEn$'\n\nfile_name_fuzzen = f\"FuzzEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_radius={r}_sertype={ret_type}_\\\n        memberfunc={characteristic_func}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          FuzzEn, \n          ylabel, \n          label_fuzzen,\n          xlabel,\n          file_name_fuzzen,\n          clr='red')\n\n\n\n\nРис. 5.3: Динаміка фондового індексу N225 та нечіткої ентропії\n\n\n\n\n\n\n5.2.3 Sample entropy (Ентропія шаблонів)\n\nJoshua S Richman and J. Randall Moorman, Physiological time-series analysis using approximate entropy and sample entropy, American Journal of Physiology-Heart and Circulatory Physiology (2000).\n\n\nwindow = 500    # ширина вікна\ntstep = 5       # часовий крок\n\nm = 3           # розмірність вкладень\ntau = 1         # часова затримка\nr = 0.4         # параметр подібності\n\nret_type = 4    # вид ряду: \n                # 1 - вихідний\n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні \n                # 4 - стандартизовані прибутковості \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд  \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nSampEn = []     # масив для зберігання значень ентропії шаблонів\n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислення ентропії шаблонів\n    Samp, _ = nk.entropy_sample(signal=fragm, \n                                dimension=m, \n                                delay=tau, \n                                tolerance=r)\n    SampEn.append(Samp)\n\n100%|██████████| 2792/2792 [00:15&lt;00:00, 184.11it/s]\n\n\nЗберігаємо значення ентропії шаблонів до текстового файлу:\n\nnp.savetxt(f\"SampEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_radius={r}_sertype={ret_type}.txt\", SampEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_sampen = fr'$SampEn$'\n\nfile_name_sampen = f\"SampEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_radius={r}_sertype={ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SampEn, \n          ylabel, \n          label_sampen,\n          xlabel,\n          file_name_sampen,\n          clr='darkgreen')\n\n\n\n\nРис. 5.4: Динаміка фондового індексу N225 та ентропії шаблонів\n\n\n\n\n\n\n5.2.4 Permutation entropy (Ентропія перестановок)\n\nChristoph Bandt and Bernd Pompe, Permutation entropy: A natural complexity measure for time series, Physical Review Letters, 88.17 (2002): 174102.\nXiao-Feng Liu, and Wang Yue, Fine-grained permutation entropy as a measure of natural complexity for time series, Chinese Physics B, 18.7 (2009): 2690.\nChunhua Bian, et al., Modiﬁed permutation-entropy analysis of heartbeat dynamics, Physical Review E, 85.2 (2012) : 021906\nBilal Fadlallah, et al., Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information, Physical Review E, 87.2 (2013): 022911.\nHamed Azami and Javier Escudero, Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation, Computer methods and programs in biomedicine, 128 (2016): 40-51.\nZhiqiang Huo, et al., Edge Permutation Entropy: An Improved Entropy Measure for Time-Series Analysis, 45th Annual Conference of the IEEE Industrial Electronics Soc, (2019), 5998-6003.\nZhe Chen, et al., Improved permutation entropy for measuring complexity of time series under noisy condition, Complexity, 1403829 (2019).\nMaik Riedl, Andreas MuЁller, and Niels Wessel, Practical considerations of permutation entropy, The European Physical Journal Special Topics, 222.2 (2013): 249-262.\n\n\nwindow = 500      # ширина вікна\ntstep = 5         # часовий крок\n\nm = 4             # розмірність вкладень\ntau = 3           # часова затримка\n\nType = 'none'     # none - класична \n                  # finegrain - Дрібнозерниста ентропія перестановок \n                  # modified - Модифікована ентропія перестановок \n                  # weighted - Зважена ентропія перестановок \n                  # ampaware - Амплітудно-орієнтована ентропія перестановок \n                  # edge - Ентропія перестановки граней\n                  # uniquant - Рівномірна ентропія перестановок на основі квантування \n            \ntpx = -1          # finegrain tpx - параметр α, додатний скаляр (за замовчуванням: 1)\n                  # ampaware tpx - параметр A, значення в діапазоні [0, 1] (за замовчуванням: 0.5)\n                  # edge tpx - параметр чутливості r, скаляр &gt; 0 (за замовчуванням: 1)\n                  # uniquant tpx - параметр L, ціле число &gt; 1 (за замовчуванням: 4)\n\nlog = np.exp(1)   # основа логарифма\nnorm = True       # нормування ентропії \n\nret_type = 1      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості звичайні \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nPEn = []        # масив для зберігання значень нормалізованої перм. ентропії\nCPEn = []       # масив для зберігання значень умовної перм. ентропії\n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислюємо ентропію перестановок \n    _, Pnorm, cPE = eh.PermEn(fragm, \n                              m=m, \n                              tau=tau, \n                              Typex=Type, \n                              tpx=tpx, \n                              Logx=log, \n                              Norm=norm)\n    \n    PEn.append(Pnorm[-1])\n    CPEn.append(cPE[-1])\n\n100%|██████████| 2792/2792 [02:02&lt;00:00, 22.84it/s]\n\n\nЗберігаємо значення пермутаційної ентропії до текстового файлу:\n\nnp.savetxt(f\"PEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}_type={Type}_param={tpx}.txt\", PEn)\n\nnp.savetxt(f\"CPEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}_type={Type}_param={tpx}.txt\", CPEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_permen = fr'$PEn$'\nlabel_condpermen = fr'$CPEn$'\n\nfile_name_perm = f\"PEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}_type={Type}_param={tpx}\"\nfile_name_condperm = f\"CPEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}_type={Type}_param={tpx}\"\n\nВиводимо результат для ентропії перестановок:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          PEn, \n          ylabel, \n          label_permen,\n          xlabel,\n          file_name_perm,\n          clr='indigo')\n\n\n\n\nРис. 5.5: Динаміка фондового індексу N225 та ентропії перестановок\n\n\n\n\nта умовної ентропії перестановок:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          CPEn, \n          ylabel, \n          label_condpermen,\n          xlabel,\n          file_name_condperm,\n          clr='indigo')\n\n\n\n\nРис. 5.6: Динаміка фондового індексу N225 та умовної ентропії перестановок\n\n\n\n\n\n\n5.2.5 Singular value decomposition entropy (ентропія сингулярного розкладу)\nЕнтропію сингулярного розкладу можна інтуїтивно розглядати як показник того, скільки власних векторів потрібно для адекватного пояснення набору даних. Іншими словами, вона вимірює багатство ознак: чим вища SVD-ентропія, тим більше ортогональних векторів потрібно для адекватного пояснення стану простору. Подібно до інформаційого показника Фішера, вона базується на розкладанні сингулярного значення реконструйованого методом часових затримок сигналу.\n\nwindow = 500      # ширина вікна\ntstep = 5         # часовий крок\n\nm = 3             # розмірність вкладень\ntau = 1           # часова затримка\n\nret_type = 6      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості звичайні \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nSVDEn = [] # масив для зберігання значень розподільної ентропії \n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислення розподільної ентропії\n    svden, _ = nk.entropy_svd(signal=fragm, \n                            dimension=m, \n                            delay=tau)\n\n    SVDEn.append(svden)\n\n100%|██████████| 2792/2792 [00:01&lt;00:00, 1787.05it/s]\n\n\nЗберігаємо значення розподільної ентропії до текстового файлу:\n\nnp.savetxt(f\"SVDEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}.txt\", SVDEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_svden = fr'$SVDEn$'\n\nfile_name_svden = f\"SVDEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SVDEn, \n          ylabel, \n          label_svden,\n          xlabel,\n          file_name_svden,\n          clr='darkorange')\n\n\n\n\nРис. 5.7: Динаміка фондового індексу N225 та ентропії сингулярного розкладу\n\n\n\n\n\n\n5.2.6 Dispersion entropy (Дисперсійна ентропія)\n\nMostafa Rostaghi and Hamed Azami, Dispersion entropy: A measure for time-series analysis IEEE Signal Processing Letters 23.5 (2016): 610-614.\nHamed Azami and Javier Escudero, Amplitude-and ﬂuctuation-based dispersion entropy, Entropy 20.3 (2018): 210.\nLi Yuxing, Xiang Gao and Long Wang, Reverse dispersion entropy: A new complexity measure for sensor signal, Sensors 19.23 (2019): 5203.\nWenlong Fu, et al., Fault diagnosis for rolling bearings based on ﬁne-sorted dispersion entropy and SVM optimized with mutation SCA-PSO, Entropy 21.4 (2019): 404.\n\n\nwindow = 500      # ширина вікна\ntstep = 5         # часовий крок\n\nfluct = False     # флуктуаційно-дисперсійна ентропія\nm = 3             # розмірність вкладень\ntau = 1           # часова затримка\nrho = 1           # параметр для Type=\"finesort\", позитивний скаляр (за замовчуванням 1)\nclasses = 6       # кількість символів, що задіяні при перетворені\n\ntype = 'ncdf'     # тип символьного перетворення ряду:\n                  # \"ncdf\" - Нормалізована кумулятивна функція розподілу\n                  # \"kmeans\" - Алгоритм кластеризації K-середніх\n                  # \"linear\" - Лінійна сегментація діапазону сигналу\n                  # \"finesort\" - Ентропія дрібнодисперсного розсіювання\n\nret_type = 4      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості звичайні \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nDispEn = [] # масив значень для зберігання дисперсійної ентропії \n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислюємо дисперсійну ентропію\n    Disp, _ = nk.entropy_dispersion(signal=fragm, \n                                    dimension=m, \n                                    delay=tau, \n                                    c=classes, \n                                    symbolize=type, \n                                    fluctuation=fluct, \n                                    rho=rho)\n    DispEn.append(Disp)\n\n100%|██████████| 2792/2792 [00:03&lt;00:00, 722.59it/s]\n\n\nЗберігаємо значення дисперсійної ентропії до текстового файлу:\n\nnp.savetxt(f\"DispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_\\\n           series_type={ret_type}_fluct={fluct}_rho={rho}_\\\n           classes={classes}_type={Type}.txt\", DispEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_dispen = fr'$DispEn$'\n\nfile_name_dispen = f\"DispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_\\\n           series_type={ret_type}_fluct={fluct}_rho={rho}_\\\n           classes={classes}_type={Type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          DispEn, \n          ylabel, \n          label_dispen,\n          xlabel,\n          file_name_dispen,\n          clr='coral')\n\n\n\n\nРис. 5.8: Динаміка фондового індексу N225 та дисперсійної ентропії\n\n\n\n\n\n\n5.2.7 Spectral Entropy (Спектральна ентропія)\nСпектральна ентропія (SE або SpEn) розглядає нормовану щільність спектра потужності (PSD) сигналу в частотній області як розподіл ймовірностей і обчислює його ентропію Шеннона:\n\\[\nH = -\\sum P(f)\\log_{2}[P(f)].\n\\tag{5.1}\\]\nСигнал з однією частотною складовою (наприклад, чиста синусоїда) має найменшу ентропію. З іншого боку, сигнал з усіма частотними компонентами однакової потужності (білий шум) дає найбільшу ентропію.\n\nwindow = 500      # ширина вікна\ntstep = 5         # часовий крок\n\nnum_bins = 30     # якщо передано ціле число, розділить PSD \n                  # на декілька частотних діапазонів\n\nmethod = 'fft'    # метод для розрахунку щільності спектра потужності:\n                  # welch\n                  # fft\n                  # multitapers\n                  # lombscargle\n                  # burg\n\nret_type = 4      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості звичайні \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nSpEn = [] # масив значень для зберігання спектральної ентропії \n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислюємо спектральну ентропію\n    spec, _ = nk.entropy_spectral(signal=fragm, \n                                  bins=num_bins,\n                                  method=method)\n\n    SpEn.append(spec)\n\n100%|██████████| 2792/2792 [00:17&lt;00:00, 156.13it/s]\n\n\nЗберігаємо значення спектральної ентропії до текстового файлу:\n\nnp.savetxt(f\"SpEn_symbol={symbol}_window={window}_step={tstep}_\\\n           series_type={ret_type}_bins={num_bins}_psd={method}.txt\", SpEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_spen = fr'$SpEn$'\n\nfile_name_spen = f\"SpEn_symbol={symbol}_window={window}_step={tstep}_\\\n                   series_type={ret_type}_bins={num_bins}_psd={method}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SpEn, \n          ylabel, \n          label_spen,\n          xlabel,\n          file_name_spen,\n          clr='deeppink')\n\n\n\n\nРис. 5.9: Динаміка фондового індексу N225 та спектральної ентропії"
  },
  {
    "objectID": "lab_6.html#теоретичні-відомості",
    "href": "lab_6.html#теоретичні-відомості",
    "title": "6  Лабораторна робота № 6",
    "section": "6.1 Теоретичні відомості",
    "text": "6.1 Теоретичні відомості\n\n6.1.1 Означення фрактала\nФракталами називають геометричні об’єкти: лінії, поверхні, просторові тіла, що мають сильно шорстку поверхню або форму і характеризуються властивістю самоподібності. Слово фрактал походить від латинського слова fractus і перекладається як дробовий, ламаний. Самоподібність як основна характеристика фрактала означає, що він більш-менш однорідно змінюється при широкому діапазоні масштабів. Так, при збільшенні маленькі фрагменти фрактала виходять дуже схожими на великі. В ідеальному випадку така самопподібність призводить до того, що фрактальний об’єкт виявляється інваріантним щодо розтягувань, тобто йому, як кажуть, притаманна дилатаційна симетрія. Вона передбачає незмінність основних геометричних особливостей фрактала при зміні масштабу.\nОчевидно, що фрактальні об’єкти реального світу не є нескінченно самоподібними й існує мінімальний масштаб \\(l_{min}\\), такий, що на масштабі \\(l \\approx l_{min}\\) властивість самоподібності зникає. Окрім цього, на достатньо великих масштабах довжин \\(l &gt; l_{max}\\), де \\(l_{max}\\) — характерний геометричний розмір об’єктів, ця властивість самоподібності також порушується. Тому властивості природніх фракталів розглядаються лише на масштабах \\(l\\), що задовільняють відношення \\(l_{min} \\ll l \\ll l_{max}\\). Такі обмеження природні, оскільки, коли ми приводимо в якості прикладу фракталу — ламану, негладку траєкторію броунівської частинки, то ми розуміємо, що цей образ представляє очевидну ідеалізацію. Справа в тому, що на малих масштабах приховується граничність маси і розмірів броунівської частнки, а також кінцевість часу зіткнення. При врахуванні цих обставин траєкторія броунівської частинки починає представляти гладку криву.\nВарто зазначити, що властивість точної самоподібності характерна лише для регулярних фракталів. Якщо замість детермінованого способу побудови включити в алгоритм їхнього створення деякий елемент випадковості (як це буває, наприклад, у багатьох процесах диференційованого зростання кластерів, електричному пробої тощо), то виникають так звані випадкові фрактали. Основна їхня відмінність від регулярних полягає в тому, що властивості самоподібності є справедливими тільки після відповідного усереднення за всіма статистично незалежними реалізаціями об’єкта. При цьому збільшена частина фрактала не точно ідентична вихідному фрагменту, проте їхні статистичні характеристики збігаються.\n\n\n6.1.2 Довжина берегової лінії\n\n\n\nРис. 6.1: Визначення довжини берегової лінії між точками А та В\n\n\nПершочергово поняття фрактала у фізиці виникло у зв’язку із завданням про визначення довжини берегової лінії. Під час її вимірювання за наявною картою місцевості з’ясувалася цікава деталь — чим більш великомасштабна карта береться, тим довшою виявляється ця берегова лінія. Нехай, наприклад, відстань по прямій між розсташованими на береговій лінії точками А та В дорівнює R (див. Рис. 6.1). Тоді, щоб виміряти довжину берегової лінії між цима точками, ми розташуємо по берегу жорстко пов’язані один з одним вузол так, що відстань між сусідніми вузлами дорівнювала б, наприклад, \\(l=10\\) км. Довжину берегової лінії в кілометрах між точками А і В ми приймемо тоді рівною числу вузлів мінус 1, помноженому на 10. Наступне вимірювання цієї довжини ми зробимо так само, але відстань між сусідніми вузлами зробимо вже рівною \\(l=1\\) км.\nВиявляється, що результат цих вимірювань буде різним. При зменшенні масштабу \\(l\\) ми отримуватимемо все більші й більші значення довжини. На відміну від гладкої кривої, лінія морського узбережжя виявляється найчастіше настільки порізаною (аж до найменших масштабів), що зі зменшенням довжини ланки \\(l\\) величина \\(L\\) — довжина берегової лінії — не прагне до кінцевого межі, а збільшується за степеневим законом\n\\[\nL \\approx l\\left( \\frac{R}{l} \\right)^{D},\n\\tag{6.1}\\]\nде \\(D &gt; 1\\) — деякий степеневий показник, котрий іменується фрактальною розмірністю берегової лінії. Чим більше значення \\(D\\), тим більш ломаною або деталізованою представляється ця берегова лінія. Походження залежності (Рівняння 6.1) має бути інтуїтивно зрозумілим: чим менший масштаб ми використовуємо, тим меньші деталі узбережжя будуть враховані і тим менший вклад вони внесуть у вимірювану довжину. Навпаки, збільшуючи масштаб, ми “розгортаємо” узбережжя, зменшуючи довжину \\(L\\).\nТаким чином, ми бачимо, що для визначення довжини берегової лінії \\(L\\) за допомогою жорсткого масштабу \\(l\\), необхідно зробити \\(N=L/l\\) кроків, причому величина \\(L\\) змінюється з \\(l\\) так, що \\(N\\) залежить від \\(l\\) за законом \\(N \\approx (R/I)^{D}\\). У результаті зі зменшенням масштабу довжина берегової лінії необмежено зростає. Ця обставина різко відрізняє фрактальну криву від звичайної гладкої кривої (типу кола, еліпса), для якої межа довжини апроксимованої ламаної \\(L\\), яка апроксимує, за наближення до нуля довжини її ланки \\(l\\) є скінченною. У результаті для гладкої кривої її фрактальна розмірність \\(D = 1\\), тобто збігається з топологічною.\n\n\n6.1.3 Фрактальна розмірність множин\nВище було введено поняття про фрактальну розмірність берегової лінії. Дамо тепер загальне визначення цієї величини. Нехай \\(d\\) — звичайна Евклідова розмірність простору, в якому розташований наш фрактальний об’єкт (\\(d=1\\) — лінія, \\(d=2\\) — площина, \\(d=3\\) — звичайний тривимірний простір). Покриємо тепер цей об’єкт цілком \\(d\\)-мірними “кулями” радіуса 1. Припустимо, що нам потребувалося для цього не менше, ніж \\(N(l)\\) куль. Тоді, якщо за досить малих \\(l\\) величина \\(N(l)\\) змінюється з \\(l\\) за степеневим законом\n\\[\nN(l) \\sim \\frac{1}{l^D},\n\\tag{6.2}\\]\nтоді \\(D\\) — називається хаусдорфовою або фрактальною розмірністю цього об’єкта. Очевидно, що ця формула еквівалентна відношеню \\(N \\approx \\left( R/l \\right)^{D}\\), що використовувалось вище для визначення довжини берегової лінії.\n\nможна переписати у вигляді\n\n\\[\nD = -\\lim_{l \\to 0} \\frac{\\ln{N(l)}}{\\ln{l}}.\n\\tag{6.3}\\]\nЦе відношення й слугує загальним визначенням фрактальної розмірності \\(D\\). У відповідності до нього величина \\(D\\) представляє локальну характеристику досліджуваного об’єкта.\n\n\n6.1.4 Процедури обчислення монофрактальних розмірностей\nНаразі існує багато визначень та методів вимірювання фрактальної розмірності. Найпоширенішими одновимірними фрактальними розмірностями є розмірність Хаусдорфа, розмірність Хігучі, розмірність Петросяна та Коробчаста розмірність. Розмірність Хаусдорфа є найпростішою фрактальною розмірністю. Але її обчислювальна складність є високою, що ускладнює її практичне застосування. Коробкова розмірність є відносно простою, і фрактальну розмірність сигналу можна отримати, регулюючи розмір довжини сторони коробки. Тому вона є широко впізнаваємою та застосовуваною. Який показник фрактальної розмірності найточніше описує складність сигналу та здатний ідентифікувати кризові явища і представляє ключовий момент цієї лабораторної роботи.\n\n6.1.4.1 R/S-аналіз\nМетод R/S-аналізу, розроблений Мандельбротом та Уоллесом, базується на попередньо створеному методі гідрологічного аналізу Херста, і дозволяє обчислювати параметр самоподібності \\(H\\), який вимірює інтенсивність довготривалих залежностей у часовому ряді. Коефіцієнт \\(H\\), який називають коефіцієнтом Херста, містить мінімальні прогнози стосовно природи системи, що вивчається, і може класифікувати часові ряди. За допомогою цього показника розрізняють випадкові (гаусові) та невипадкові ряди; окрім того, він пов’язаний із фрактальною розмірністю, що, у свою чергу, характеризує ступінь згладженості графіка, побудованого на основі часового ряду. Методом R/S-аналізу можливо також виявити максимальну довжину інтервалу (цикл), на якому значення зберігають інформацію про початкові дані системи (довготривала пам’ять).\nАналіз починається з побудови ряду логарифмічних прибутковостей, \\(G(t) \\equiv \\ln{x(t + \\Delta t)} - \\ln{x(t)}\\), де \\(x(t)\\) — значення вихідного часового ряду в момент \\(t\\), \\(\\Delta t\\) — часовий крок. Отримана послідовність \\(G(t)\\) розбивається на \\(d\\) підпослідовностей довжини \\(n\\).\nДля кожної підпослідовності \\(m=1,...,d\\):\n\nшукається середнє значення \\(\\mu_m\\) та стандартне відхилення \\(S_m\\);\nдані нормалізуються шляхом віднімання середнього значення послідовності \\(X_{i,m}=G_{i,m}-\\mu_m\\), \\(i=1,...,n\\);\nзнаходиться кумулятивна сума послідовності \\(X\\)ів: \\(Y_{i,m}=\\sum_{j=1}^{i}X_{j,m}\\), \\(i=1,...,n\\);\nу межах кожної підпослідовності знаходиться розмах між максимальним та мінімальним значеннями: \\(R_m = \\max\\{Y_{1,m},...,Y_{n,m}\\}-\\min\\{Y_{1,m},...,Y_{n,m}\\}\\), який стандартизується середнім квадратичним відхиленням \\(R_{m}/S_{m}\\);\nобчислюється середнє \\((R/S)_n\\) нормованих значень розмаху для всіх підпослідовностей довжини \\(n\\).\n\nR/S-статистика, обрахована таким чином, відповідає співвідношенню \\((R/S)_{n} \\cong cn^{H}\\), де значення \\(H\\) може бути отримане шляхом обчислення \\((R/S)_n\\) для послідовностей інтервалів зі збільшенням часового горизонту:\n\\[\n\\log{(R/S)}_{n} = \\log{c} + H\\log{n}.\n\\tag{6.4}\\]\nЗнайти коефіцієнт Херста можна, побудувавши залежність \\((R/S)_n\\) vs. \\(n\\) у подвійному логарифмічному масштабі і взявши коефіцієнт нахилу прямої, яка інтерполює точки отриманого графіка. Якщо значення \\(H=0.5\\), говорять про послідовність, що представляє собою білий шум; \\(0.5 &lt; H \\leq 1\\) свідчить про персистентний ряд, коли існує тенденція слідування великих значень ряду за великими і навпаки; \\(H&lt;0.5\\) вказує на антиперсистентний ряд.\nПри збільшенні часового горизонту коефіцієнт нахилу інтерполюючої прямої повинен прямувати до значення \\(H=0.5\\); сам процес переходу свідчить про втрату впливу початкових умов на поточні значення, і, таким чином, можна говорити про горизонт довгої пам’яті — це точка, до якої коефіцієнт нахилу інтерполюючої прямої відмінний від 0.5, а після — близько 0.5.\n\n\n\n\n\n\nПримітка до R/S-аналізу\n\n\n\nМіж фрактальною розмірністю та показником Херста також існує зв’язок\n\\[\nD_f = 2-H.\n\\]\nЯкщо для берегової лінії ми визначали масштабування її довжини \\(L\\) в залежності від зміни \\(l\\), то у випадку з R/S-аналізом ми визначаємо зміну нормованого розмаху значень ряду в межах масштабу \\(n\\).\n\n\n\n\n6.1.4.2 Аналіз детрендованих флуктуацій\nАналіз детрендований флуктуацій (Detrended fluctuation analysis, DFA) базується на гіпотезі про те, що корельований часовий ряд може бути відображений на самоподібний процес шляхом інтегрування. Таким чином, вимірювання властивостей самоподібності може непрямо свідчити про кореляційні властивості ряду. Переваги DFA порівняно з іншими методами (спектральний аналіз, R/S-аналіз) полягають в тому, що він виявляє довгочасові кореляції нестаціонарних часових рядів, а також дозволяє ігнорувати очевидні випадкові кореляції, що є наслідком нестаціонарності.\nІснують DFA різних порядків, що відрізняються трендами, які вилучаються з даних.\nРозглянемо DFA найнижчого порядку.\n\nДля часового ряду довжини \\(N\\) знаходиться кумулятивна сума, \\(y(k)=\\sum_{i=1}^{k}\\left( x_i - \\bar{x} \\right)\\), де \\(x_i\\) — це \\(i\\)-те значення часового ряду, \\(\\bar{x}\\) — його середнє значення, \\(k=1,...,N\\).\nОтриманий ряд \\(y(k)\\) розбивається на \\(m\\) підпослідовностей (вікон) однакової ширини \\(n\\) і для кожної підпослідовності (у кожному вікні) виконується наступне:\n\nза допомогою методу найменших квадратів знаходиться локальний лінійний тренд \\(y_{t}(k)\\);\n\nпідпослідовність детрендується шляхом віднімання значення локального тренду \\(y_{t}(k)\\) від значень ряду \\(y(k)\\), що належать послідовності \\(t\\);\nзнаходиться середнє \\(\\bar{y_t}\\) детрендований значень.\n\n\nДля отриманих таким чином значень на всіх підпослідовностях знаходиться:\n\\[\nF_n = \\sqrt{\\frac{1}{m}\\bar{y_t}},\n\\]\nде \\(n\\) — кількість точок у підпослідовності (ширина вікна), \\(m\\) — кількість підпослідовностей, \\(\\bar{y_t}\\) — середнє детрендованих значень для підпослідовності \\(t\\).\nВказана процедура повторюється для різних значень \\(n\\), внаслідок чого ми отримує набір залежностей \\(F_n\\) від \\(n\\). Побудова залежності \\(\\log{F(n)}\\) від \\(\\log{n}\\) та інтерполяція отриманих значень лінією регресії дає змогу обчислити показник скейлінга \\(\\alpha\\), що є коефіцієнтом кута нахилу інтерполяційної прямої і характеризує зміну кореляцій флуктуацій часового ряду \\(F_n\\) при збільшенні часового інтервалу \\(n\\).\nПорівняно із R/S-аналізом, DFA дає більші можливості інтерпретації скейлінгового показника \\(\\alpha\\):\n\nдля випадкового ряду (перемішаного чи “сурогатного”) \\(\\alpha = 0.5\\);\nпри наявності лише короткочасових кореляцій \\(\\alpha\\) може відрізнятись від 0.5, проте має тенденцію прямувати до 0.5 при збільшенні розміру вікна;\nЗначення \\(0.5 &lt; \\alpha \\leq 1.0\\) показує персистентні довгочасові кореляції, що відповідають степеневому закону;\n\\(0 &lt; \\alpha &lt; 0.5\\) означає антиперсистентний ряд;\ncпеціальний випадок, коли \\(\\alpha = 1\\), означає наявність \\(1/f\\) шуму.\nдля випадків, коли \\(\\alpha \\geq 1\\), кореляції існують, проте перестають відображувати степеневу залежність;\nвипадок \\(\\alpha = 1.5\\) свідчить про Броунівський шум, інтегрований білий шум.\n\nУ випадку степеневої залежності функції автокореляцій спостерігається спад автокореляції з показником \\(\\gamma\\):\n\\[\nC(L) \\sim L^{-\\gamma}.\n\\]\nНа додачу до цього, спектральна густина також спадає за степеневим законом:\n\\[\nP(f) \\sim f^{-\\beta}.\n\\tag{6.5}\\]\nВідповідні показники виражаються через наступні відношення:\n\n\\(\\gamma=2-2\\alpha\\);\n\\(\\beta=2\\alpha-1\\).\n\nУ DFA другого порядку (DFA2) обчислюються відхилення \\(F^2(v,s)\\) профілю від інтерполяційного многочлена другого порядку. Таким чином, вилучаються впливи можливих лінійних та параболічних трендів для масштабів, більших за розглядувані. Взагалі, у DFA порядку \\(n\\) обчислюються відхилення профілю від інтерполяційного многочлена \\(n\\)-го порядку, що вилучає вплив всіх можливих трендів порядків до (\\(n-1\\)) для масштабів, більших від розміру вікна.\nПотім обчислюється найближчий поліном \\(y_{ν}(s)\\) для профілю на кожному із \\(2N_s\\) сегментів \\(v\\) і визначається відхилення\n\\[\nF^2(v,s) \\equiv \\frac{1}{s}\\sum_{i=1}^{s}\\left( x_{(v-1)s+i} - y_{i}(i) \\right)^{2}.\n\\tag{6.6}\\]\nДалі знаходиться середнє значення флуктуацій всіх детрендованих профілів:\n\\[\nF_2(s) \\equiv \\sqrt{\\left( \\frac{1}{2N_s} \\sum_{v=1}^{2N_s}F^{2}(v,s) \\right)}.\n\\tag{6.7}\\]\nЗначення формули (Рівняння 6.7) можна трактувати як середньоквадратичний зсув (переміщення) точки випадкових блукань у ланцюжку після \\(s\\) кроків.\n\n\n6.1.4.3 Фрактальна розмірність Хігучі\nФрактальна розмірність Хігучі — це один з різновидів монофрактальної розмірності, яка визначається наступним чином:\nПрипустимо, що у нас є часовий ряд\n\\[\nx(1), x(2),...,x(N)\n\\]\nі реконструйований часовий ряд \\(x_{m}^{k}\\):\n\\[\\begin{align*}\nx_{m}^{k} = \\{ x(m), x(m+k), x(m+2k), ..., \\\\\nx\\left( m+\\left[ \\frac{N-m}{k} \\right] \\cdot k \\right) \\},\n\\end{align*}\\]\nдля \\(m=1,2,...,k\\); де \\(m\\) представляє початковий час; \\(k=2,...,k_{max}\\) представляють ступінь часового зміщення. Позначення \\([\\cdot]\\) представляє цілу частину \\(x\\). Для кожного реконструйованого часового ряду \\(x_{m}^{k}\\) розраховується середня довжина часової послідовності \\(L_{m}(k)\\):\n\\[\nL_{m}(k) = \\frac{\\sum_{i=1}^{[\\frac{(N-m)}{k}]} | x(m+ik) - x(m+(i-1)\\cdot k) | \\cdot (N-1)}{[\\frac{N-m}{k}] \\cdot k}\n\\]\nДалі, для всіх середніх довжин \\(L_{m}(k)\\), знаходиться загальне середнє:\n\\[\nL(k) = \\frac{1}{k}\\sum_{m=1}^{k}L_{m}(k).\n\\]\nЗгідно методу Хігучі узагальне середнє значення \\(L(k)\\) пропорційне масштабу \\(k\\), тобто\n\\[\nL(k) \\propto k^{-D}.\n\\]\nДалі логарифмуємо обидві сторони й отримуємо наступну рівність:\n\\[\n\\ln{L(k)} \\propto D \\cdot \\ln{\\left( \\frac{1}{k} \\right)}.\n\\]\nІнтерполювавши лінію регресії через залежність \\(\\ln{L(k)}\\) від \\(\\ln{\\left( \\frac{1}{k} \\right)}\\), ми можемо отримати показник фрактальності \\(D\\), отримавши кут нахилу цієї лінії. Показник \\(D\\) і представлятиме фрактальну розмірність Хігучі.\n\n\n6.1.4.4 Фрактальна розмірність Петросяна\nСпочатку, для часового ряду \\(\\{ x_1, x_2,...,y_{N} \\}\\), створюємо його дискретизовану (бінарну) версію, \\(z_i\\):\n\\[\nz_i =\n\\begin{cases}\n    1, & x_i &gt; \\langle x \\rangle, \\\\\n    -1, & x_i \\leq \\langle x \\rangle.\n\\end{cases}\n\\]\nФрактальну розмірність Петросяна може бути визначена як\n\\[\nD = \\frac{\\log_{10}{N}}{\\log_{10} + \\log_{10}{\\left( \\frac{N}{N+0.4N_{\\Delta}} \\right)}},\n\\]\nде \\(N_{\\Delta}\\) — кількість загальних змін знаку змінної \\(z_i\\):\n\\[\nN_{\\Delta} = \\sum_{i=1}^{N-2} \\left|\\frac{z_{i+1}-z_i}{2}\\right|.\n\\]\n\n\n6.1.4.5 Фрактальна розмірність Каца\nПредставимо, що сигнал складається з пари точок \\(\\left( x_i, y_i \\right)\\). Тоді, фрактальна розмірність Каца визначається як\n\\[\nD = \\frac{\\log{N}}{\\log{N} + \\log{\\frac{d}{L}}},\n\\]\nде \\(L\\) визначається наступним чином:\n\\[\nL = \\sum_{i=0}^{N-2}\\sqrt{\\left( y_{i+1}-y_{i} \\right)^{2} + \\left( x_{i+1}-x_{i} \\right)^{2}}.\n\\]\nЗначення \\(d\\) визначається як максимальна відстань від початкової точки \\(\\left( x_1, y_1 \\right)\\) до всіх інших точок, а саме \\(d\\) може бути розраховане наступним чином:\n\\[\nd = \\max{\\left( \\sqrt{\\left( x_i - x_1 \\right)^{2} - \\left( y_i - y_1 \\right)^{2}} \\right)}.\n\\]\n\n\n6.1.4.6 Фрактальна розмірність Севчика\nСпочатку, для множини значень \\(\\left( x_i, y_i \\right)\\) виконується нормалізація:\n\\[\nx_{i}^{*} = \\frac{x_i-x_{min}}{x_{max}-x_{min}}, \\; y_{i}^{*} = \\frac{y_i-y_{min}}{y_{max}-y_{min}}.\n\\]\nФрактальна розмірність Севчика може бути визначена як\n\\[\nD = 1 + \\frac{\\ln{L}}{\\ln{[2 \\cdot \\left( N-1 \\right)]}},\n\\]\nде \\(L\\) — це довжина сигналу, що може бути визначена як\n\\[\nL = \\sum_{i=0}^{N-2}\\sqrt{\\left( y_{i+1}^{*}-y_{i}^{*} \\right)^{2} + \\left( x_{i+1}^{*}-x_{i}^{*} \\right)^{2}}.\n\\]\n\n\n6.1.4.7 Фрактальна розмірність через нормалізовану щільність довжини\nДаний показник розраховується в наступний спосіб:\n\nДля часового ряду \\(\\{ x_1, x_2,...,x_n \\}\\) виконується стандартизація: \\(y_i = \\frac{x_i - \\mu}{\\sigma}\\), де \\(\\mu\\) — це середнє значення ряду, \\(\\sigma\\) — це стандартне відхилення.\nРозраховується нормалізована щільність довжини:\n\n\\[\nNLD = \\frac{1}{N}\\sum_{i=2}^{N}\\left| y_i - y_{i-1} \\right|\n\\]\nФактичний розрахунок фрактальної розмірності сигналу базується на побудові монотонної калібрувальної кривої, \\(D = f(NLD)\\), за набором функцій Вейєрштрасса, для яких значення \\(D\\) задаються теоретично.\n\nДля обчислювальних цілей необхідно створити математичну модель цієї залежності. Автори даного підходу тестували дві моделі:\n\nлогарифмічну модель: \\(D = a \\cdot \\log{\\left(NLD - NLD_{0} \\right)} + C\\)\nстепеневу модель: \\(D = a \\cdot \\left(NLD - NLD_{0} \\right)^{k}\\). Бібліотека neurokit2 використовує саме степеневу модель. Параметр \\(a=1.9079\\), \\(k=0.18383\\) і \\(NLD_{0}=0.097178\\), згідно статті Kalauzi et al. 2009.\n\n\n\n\n6.1.4.8 Фрактальна розмірність через нахил спектральної щільності потужності\nФрактальну розмірність можна обчислити на основі аналізу нахилу спектральної щільності потужності (power spectral density slope, PSD) в сигналах, що характеризуються частотною степеневою залежністю.\nСпочатку виконується перетворення часового ряду до частотної області і далі сигнал розбивається на синусоїдальні та косинусоїдальні хвилі певної амплітуди, які разом “складаються”, щоб представити вихідний сигнал. Якщо існує систематичний зв’язок між частотами в сигналі і потужністю цих частот, то в логарифмічних координатах це проявляється в лінійній залежності. Кут нахилу лінії регресії приймається як оцінка фрактальної розмірності.\nНахил 0 відповідає білому шуму, а нахил менше 0, але більше -1, відповідає рожевому шуму, тобто шуму \\(1/f\\). Спектральні нахили крутіші за -2 вказують на дробовий броунівський рух, що є втіленням процесів випадкового блукання.\n\n\n6.1.4.9 Кореляційна розмірність\nКореляційна розмірність (\\(D_2\\)) — це похідна величина від кореляційного інтеграла Кореляційний інтеграл (кореляційна сума) може бути поданий в такому вигляді:\n\\[\nC(\\varepsilon) = \\frac{1}{N^{2}}\\sum_{\\substack{i,j=1 \\\\ i\\neq j}}^{N}\\Theta \\left( \\varepsilon - \\| \\vec{x}(i) - \\vec{x}(j) \\| \\right), \\; \\vec{x}(i) \\in \\Re^{m}.\n\\]\nСама кореляційна розмірність може бути виведена з наступної степеневої залежності:\n\\[\nC(\\varepsilon) \\sim \\varepsilon^{\\nu},\n\\]\nабо слідуючим чином:\n\\[\nD_2 = \\lim_{M\\to\\infty}\\lim_{\\varepsilon\\to 0}\\frac{\\log{\\left( g_{\\varepsilon}/N^2 \\right)}}{\\log{\\varepsilon}},\n\\]\nде \\(g_{\\varepsilon}\\) — це сумарна кількість пар точок, відстань між якими менша за радіус \\(\\varepsilon\\).\nЗа формулою \\(C(\\varepsilon)\\) ми відбираємо \\(i\\)-ту траєкторію та всі інші \\(j\\)-ті траєкторії, і дивимося, чи потрапляють \\(j\\)-ті траєкторії в округ \\(i\\)-ої траєкторії з радіусом \\(\\varepsilon\\). Якщо відстань між ними не перевищує округ зі згаданим радіусом, ми ставимо 1. Але якщо відстань між траєкторіями більша за \\(\\varepsilon\\), тоді ставимо 0. Далі все це підсумовується, ділиться на загальну кількість траєкторій. По суті кореляційний інтеграл це середня ймовірність того, що дві розглянуті траєкторії фазового простору, що розглядаються, будуть знаходитися досить близько одна до одної. Чим тісніше розташовані точки фазового простору одна до одної, тим більше значення кореляційного інтеграла. Чим більш рівновіддаленими видаються траєкторії одна від одної, тим ближче значення кореляційного інтеграла до нуля.\nЗначення кореляційної розмірності ми можемо відшукати аналогічно попередним фрактальним показникам: ми шукаємо залежність кореляційного інтеграла від значення \\(\\varepsilon\\). Ця залежність будується в логарифмічному масштабі.\n\n\n\n\n\n\nДодаткова інформація по кореляційні розмірності\n\n\n\nКореляційна розмірність за аналогією з попередніми показниками — це теж тангенс кута нахилу лінії регресії, побудованої в логарифмічному масштабі, але для залежності кореляційного інтеграла від \\(\\varepsilon\\). За аналогією з іншими показниками, кореляційна розмірність визначає швидкість зміни значення кореляційного інтеграла (крутість нахилу лінії регресії).\n\n\n\n\n\nРис. 6.2: Зміна значення кореляційного інтеграла в залежності від ступеня розкиданості точок по фазовому простору системи\n\n\nЗа фазовим простором цієї хмари точок (Рис. 6.2) можна бачити, що більша згуртованість точок одна до одної має асоціюватися з меншою кореляційною розмірністю. Рисунок (а) характеризується найвищою близькістю точок одна до одної, але при цьому найменшою кореляційною розмірністю. Це можна пояснити так: якщо ми будуємо сітку з кіл радіусом \\(\\varepsilon\\) і поступово її збільшуємо, ми вже перестаємо бачити нові прилеглі траєкторії до тих \\(i\\)-их траєкторій, які ми розглядали із самого спочатку, на початкових значеннях \\(\\varepsilon\\). На рисунках (b) і (c) видно, що хмара траєкторій фазового простору є більш рівномірно розподіленою. За поступового збільшення радіуса кіл із центрами в кожній \\(i\\)-ій траєкторії ми повинні спостерігати пропорційне збільшення значення кореляційного інтеграла. У даному випадку при поступовому збільшенні \\(\\varepsilon\\) ми спостерігаємо появу все більшої і більшої кількості точок.\nУ періодичних системах кореляційна розмірність залишається постійною і дорівнює розмірності вкладення. Наприклад, для простої періодичної системи, такої як синусоїда, кореляційна розмірність дорівнюватиме 1 (оскільки вона лежить на одновимірній кривій), а якщо систему реконструюють у двовимірному фазовому просторі (за двома координатами), то кореляційна розмірність дорівнюватиме 2. У таких системах кореляційна розмірність не змінюється.\nДля хаотичних систем кореляційна розмірність має характерну поведінку, яка залежить від кількості змінних (розмірностей), необхідних для точного опису динаміки системи. На відміну від періодичних систем, кореляційна розмірність зростає в міру збільшення розмірності фазового простору, поки не досягне плато.\nЕлектрокардіограма (ЕКГ): ЕКГ-сигнали відображають електричну активність серця. Складність ЕКГ-сигналу може бути оцінена за допомогою кореляційної розмірності. Очікується, що кореляційна розмірність ЕКГ здорового серця буде вищою через наявність складних патернів і варіабельності. З іншого боку, аномальні ЕКГ-сигнали, наприклад, від пацієнтів з аритміями або серцевими захворюваннями, можуть мати нижчу кореляційну розмірність через втрату складності сигналу.\nЕлектроенцефалограма (ЕЕГ): Сигнали ЕЕГ реєструють електричну активність мозку. Кореляційна розмірність може використовуватися для аналізу складності мозкової активності, яка може змінюватися залежно від різних когнітивних станів, стадій сну або неврологічних розладів. У здорових людей сигнали ЕЕГ у стані бадьорості та уваги можуть мати вищу кореляційну розмірність порівняно з сигналами у стадії сну, коли активність мозку є більш регулярною і синхронізованою.\nДихальні сигнали: Дихальні сигнали, такі як частота дихання або повітряний потік, також можуть бути проаналізовані за допомогою кореляційної розмірності. Складність цих сигналів може змінюватися залежно від таких факторів, як стрес, фізичне навантаження або наявність респіраторних захворювань. За нормального дихання може спостерігатися вища кореляційна розмірність, тоді як порушення в дихальних сигналах, наприклад, за обструктивного апное уві сні або дихальних розладів, можуть призвести до зниження кореляційної розмірності.\nАналіз ходи: Кореляційна розмірність може бути використана для аналізу моделей ходи. Вона може допомогти в розумінні складності рухів людини під час ходьби або бігу. Зміни в кореляційній розмірності сигналів ходи можуть свідчити про зміну стабільності ходи або про наявність відхилень у ході, викликаних неврологічними або опорно-руховими захворюваннями.\nВаріативність динаміки серцевого ритму (ВСР): ВСР являє собою зміну часових інтервалів між послідовними ударами серця. Вона перебуває під впливом вегетативної нервової системи і відображає адаптивність і складність серцево-судинної системи. Вищий рівень ВСР, що відповідає вищій кореляційній розмірності, зазвичай асоціюється з кращим станом серцево-судинної системи та її адаптивністю до фізіологічних змін і змін навколишнього середовища. Її падіння може асоціюватися з аномальною динамікою серця.\nПослідовності ДНК: Кореляційна розмірність може бути використана і при аналізі послідовностей ДНК. Вона допомагає виявити самоподібні або фрактальні патерни всередині послідовностей, що може мати значення для розуміння генетичної складності, еволюційних зв’язків і регуляції генів. Висока кореляційна розмірність — висока складність ланцюжка ДНК. Мала кореляційна розмірність — спрощений ланцюжок ДНК.\nФінансові ринки: Вища кореляційна розмірність у даних часових рядів фінансового ринку свідчить про більшу складність та існування в їхній основі самоподібних моделей або фрактальних структур. Хаотична поведінка цін на акції може бути пов’язана з періодами високої волатильності та непередбачуваності. З іншого боку, нижча величина кореляційної розмірності може свідчити про більш передбачувані та менш складні рухи цін, що відповідає періодам стабільності або менш волатильним ринковим умовам.\n\n\n\n\n\n\nПримітка по кореляційні розмірності\n\n\n\nКореляційна розмірність розглядає кількість інформації (кубиків, кіл), необхідну для опису тільки пари точок у фазовому просторі."
  },
  {
    "objectID": "lab_6.html#хід-роботи",
    "href": "lab_6.html#хід-роботи",
    "title": "6  Лабораторна робота № 6",
    "section": "6.2 Хід роботи",
    "text": "6.2 Хід роботи\nРозглянемо як можна застосовувати зазначені показники в якості індикаторів кризових станів.\nСпочатку імпортуємо необхідні бібліотеки для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nУ даній роботі скористаємось монофрактальними показниками для ідентифікації кризових явищ на ринку золота. Розглянемо значення золота за весь період, що представляє Yahoo! Finance. Для цього нам не треба буде вказувати початкову та кінцеву дати:\n\nsymbol = 'GC=F'                       # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 6.3: Динаміка щоденних змін індексу золота\n\n\n\n\nВизначимо функцію transformation() для стандартизації ряду:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n6.2.1 Обчислення показника Херста із використанням R/S-аналізу\nДля подальших розрахунків використовуватимемо бібліотеку neurokit2 та fathon. Другу можна встановити в наступний спосіб:\n\n!pip install fathon\n\nДалі імпортуємо саму бібліотеку та дотичні до неї модулі:\n\nimport fathon\nfrom fathon import fathonUtils as fu\n\nБібліотека neurokit містить необхідний метод для R/S-аналізу — fractal_hurst. Його синтаксис виглядає наступним чином:\nfractal_hurst(signal, scale='default', corrected=True, show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму бібліотеки pandas.\nscale (list) — список, що містить довжини вікон (кількість точок даних у кожній підмножині ряду), на які розбито сигнал.\ncorrected (bool) — якщо значення True, до вихідних даних буде застосовано поправочний коефіцієнт Аніса-Ллойда-Пітерса відповідно до очікуваного значення для окремих значень (R/S).\nshow (bool) — якщо значення True, виводить залежність \\((R/S)_n\\) від \\(n\\) (scale) у подвійному логарифмічному масштабі.\n\nПовертає\n\nh (float) — показник Херста.\n**kwargs — словник, що містить інформацію відносно використовуваних у процедурі параметрів.\n\nРозглянемо ступінь трендостійкості в динаміці фондового індексу золота, використовуючи весь часовий ряд. Далі знайдемо значення показника Херста в рамках віконної процедури.\n\n6.2.1.1 Увесь часовий ряд\nПершочергово знайдемо значення прибутковостей для нашого ряду та стандартизуємо їх. Після цього виконаємо обчислення.\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rs = transformation(signal, ret_type) \n\nВиконуємо R/S-аналіз:\n\nh, info = nk.fractal_hurst(for_rs, corrected=False, show=True)\n\n\n\n\nРис. 6.4: Залежність значень R/S від скейлінгу побудованих в логарифмічному масштабі\n\n\n\n\nЯк ми можемо бачити з Рис. 6.4, значення \\(h=0.53\\), що свідчить про подібність динаміки золота до випадкового блукання. Але оскільки закони, що регулюють ринок, змінюються з часом, мають змінюватись і кореляції всередині системи, а одже коефіцієнт Херста також має залежати від періоду в якому він розглядається.\n\n\n6.2.1.2 Віконна процедура\nВизначимо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nПриступимо до віконної процедури:\n\n# встановлюємо параметри\nret_type = 4                   # вид ряду\nwindow = 250                   # ширина вікна\ntstep = 1                      # часовий крок вікна \nlength = len(time_ser.values)  # довжина самого ряду\ncorr = False                   # поправочний коефіцієнт Аніса-Ллойда-Пітерса\n\nH = []                         # масив для віконного Херсту\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо взаємну інформацію \n    h, _ = nk.fractal_hurst(fragm, corrected=corr, show=False)\n    \n    # та додаємо результат до масиву значень\n    H.append(h)\n\n  1%|          | 41/5531 [00:00&lt;00:13, 404.03it/s]\n\n\n100%|██████████| 5531/5531 [00:13&lt;00:00, 419.90it/s]\n\n\n\nnp.savetxt(f\"rs_hurst_name={symbol}_window={window}_step={tstep}_ \\\n           rettype={ret_type}_corrected={corr}.txt\" , H)\n\nВізуалізуємо результат:\n\nmeasure_label = r'$H$'\nfile_name = f\"rs_hurst_name={symbol}_window={window}_step={tstep}_ \\\n           rettype={ret_type}_corrected={corr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          H, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРис. 6.5: Динаміка індексу золота та показника Херста\n\n\n\n\nНа представленому рисунку (Рис. 6.5) можемо бачити, що показник Херста зростає в передкризовий період та спадає під час кризи. Перед кризою динаміка ринку характеризується зростанням трендостійкості (персистентності), що відзеркалює зростання скорельованості дій між трейдерами ринку.\n\n\n\n6.2.2 Обчислення на основі DFA\nБібліотека fathon представляє інструментарій як для виконання класичного аналізу детрендованих флуктуацій, так і для його мультифрактального аналогу, мова про який піде в наступній лабораторній.\n\n6.2.2.1 Для всього ряду\nСпочатку представимо значення \\(\\alpha\\) для всього ряду. Процедура розрахунків на основі бібліотеки fathon виглядатиме наступним чином:\n\nзнаходимо стандартизовані прибутковості ряду\n\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_dfa = transformation(signal, ret_type) \n\n\ncumulat = fu.toAggregated(for_dfa) # знаходимо кумулятивні накопичення\n\nrev = True # чи повторювати розрахунок ф-ції флуктуацій з кінця\norder = 2  # порядок локального лінійного тренду \n\npydfa = fathon.DFA(cumulat) # ініціалізація об'єкту DFA\n                            # для виконання подальших обчислень\n\nwin_beg = 100               # початкова ширина сегментів\nwin_end = 2000              # кінцева ширина сегментів\n\nwins = fu.linRangeByStep(win_beg, win_end) # генеруємо масив \n                                           # лінійно розділених \n                                           # елементів.\n\nn, F = pydfa.computeFlucVec(wins, \n                            polOrd=order, \n                            revSeg=rev)    # знаходимо функцію флуктуацій\n\nH, H_intercept = pydfa.fitFlucVec()        # знаходимо показник альфа\n\nВиводимо залежність функції флуктуацій від характеристичного масштабу:\n\npolyfit = np.polyfit(np.log(n), np.log(F), 1)\nfluctfit = np.exp(1) ** np.polyval(polyfit, np.log(n))\n\nБудуємо залежність функції флуктуацій від масштабу в подвійному логарифмічному масштабі:\n\nfig, ax = plt.subplots()\nfig.suptitle(\"Показник Херста на основі DFA\")\n\nax.scatter(\n        np.log(n),\n        np.log(F),\n        marker=\"o\",\n        zorder=1,\n        label=\"_no_legend_\",\n    )\n\nlabel = fr\"$\\alpha$ = {H:.2f}\"\nax.plot(np.log(n), np.log(fluctfit), \n        color=\"#E91E63\", zorder=2, \n        linewidth=3, label=label)\n\nax.set_ylabel(r'$\\ln{F_{2}(n)}$')\nax.set_xlabel(r'$\\ln{n}$')\n\nax.legend(loc=\"lower right\")\n\nplt.show()\n\n\n\n\nРис. 6.6: Логарифмічна залежність значень функції флуктуацій від скейлінгу\n\n\n\n\nПроцедура DFA показує, що значення індексу золота представляються скоріше антиперсистентними, але представлений результат доволі близький до того, що був отриманий за допомогою R/S-аналізу. Розглянемо значення \\(\\alpha\\) в рамках алгоритму ковзного вікна.\n\n\n6.2.2.2 Віконна процедура\nВизначимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nrev = True      # чи повторювати розрахунок ф-ції флуктуацій з кінця\norder = 2       # порядок поліноміального тренду\n\nperiods = 1\n\nwin_beg = 10             # початковий масштаб сегментів\nwin_end = window-1       # кінцевий масштаб сегментів\n\n\n\nlength = len(time_ser.values) # довжина ряду\n\nalpha = []               # масив показників альфа (Херста)\nD_f = []                 # фрактальна розмірність\nbeta = []                # показник спектральної щільності\ngamma = []               # показник автокореляції\n\nЗнайдемо показник Херста (\\(\\alpha\\)), фрактальну розмірність (\\(D_f\\)), показник спектральної щільності (\\(\\beta\\)) та показник автокореляції (\\(\\gamma\\)):\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # знаходимо кумулятивні накопичення\n    cumulat_wind = fu.toAggregated(fragm) \n\n    # ініціалізація об'єкту DFA\n    pydfa = fathon.DFA(cumulat_wind) \n\n    # генеруємо масив лінійно розділених елементів\n    wins = fu.linRangeByStep(win_beg, win_end) \n\n    # знаходимо функцію флуктуацій\n    n, F_wind = pydfa.computeFlucVec(wins, polOrd=order, revSeg=rev)    \n\n    # знаходимо показник альфа\n    H_wind, _ = pydfa.fitFlucVec()\n\n    # знаходимо фрактальну розмірність        \n    D = 2. - H_wind\n\n    # показник спектральної щільності\n    bi = 2. * H_wind - 1 \n\n    # показник автокореляції\n    gi = 2. - 2. * H_wind\n\n    alpha.append(H_wind)\n    D_f.append(D)\n    beta.append(bi)\n    gamma.append(gi)\n\n100%|██████████| 5531/5531 [00:57&lt;00:00, 96.13it/s] \n\n\nЗберігаємо абсолютні значення показників до текстових файлів:\n\nnp.savetxt(f\"alpha_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", alpha)\nnp.savetxt(f\"D_f_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", D_f)\nnp.savetxt(f\"beta_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", beta)\nnp.savetxt(f\"gamma_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", gamma)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_alpha = fr'$\\alpha$'\nlabel_d = fr'$D_f$'\nlabel_beta = fr'$\\beta$'\nlabel_gamma = fr'$\\gamma$'\n\nfile_name_alpha = f\"alpha_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_d = f\"D_f_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_beta = f\"beta_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_gamma = f\"gamma_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\n\nВиводимо результати:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alpha, \n          ylabel, \n          label_alpha,\n          xlabel,\n          file_name_alpha)\n\n\n\n\nРис. 6.7: Динаміка індексу золота та показника альфа\n\n\n\n\nЯкщо порівнювати з R/S-аналізом, Рис. 6.7 демонструє, що динаміка узагальненого показника Херста отриманого за допомогою DFA є набагато стабільнішою. Тепер ми здатні диференціювати значну частку крахових подій, що мали місце на ринку золота. Узагальнений Херст показує, що передкризові явища характеризуються зростанням трендостійкості ринку або, іншими словами, підвищенням ступеня самоорганізації системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_f, \n          ylabel, \n          label_d,\n          xlabel,\n          file_name_d)\n\n\n\n\nРис. 6.8: Динаміка індексу золота та фрактальної розмірності\n\n\n\n\nРис. 6.8 показує, що \\(D_f\\) характеризується спадом при кризових станах. Це є індикатором того, що вищий ступень організованості ринку відзеркалюється в більш згладженій або менш шорстких флуктуаціях досліджуваного сигналу.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          beta, \n          ylabel, \n          label_beta,\n          xlabel,\n          file_name_beta)\n\n\n\n\nРис. 6.9: Динаміка індексу золота та показника спектральної щільності\n\n\n\n\nНа даному рисунку ми бачимо динаміку показника \\(\\beta\\), що відноситься до спектральної густини потужності (Рівняння 6.5), зростає в кризові періоди, що говорить про спад потужності сигналу, що припадає на одиничний інтервал частоти. Це також є свідченням зростання кореляційних властивостей системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          gamma, \n          ylabel, \n          label_gamma,\n          xlabel,\n          file_name_gamma)\n\n\n\n\nРис. 6.10: Динаміка індексу золота та показника автокореляції\n\n\n\n\nНа Рис. 6.10 видно, що показник \\(\\gamma\\) спадає в кризові та передкризові періоди. Це є показником сповільнення спаду функції автокореляції, що в свою чергу також вказує на зростання корельованності динаміки системи.\n\n\n\n6.2.3 Обчислення фрактальної розмірності Хігучі\nЯк уже зазначалося, фрактальна розмірність Хігучі є одним із різновидів фрактальної розмірності для часових рядів. Вона обчислюється шляхом реконструкції \\(k\\)-максимальної кількості нових наборів даних. Для кожного відновленого набору даних обчислюється довжина кривої і відкладається проти відповідного k-значення в логарифмічній шкалі. HFD відповідає нахилу лінійного тренду за методом найменших квадратів.\nРозрахуємо оптимальне значення \\(k\\) для всього часового ряду. Бібліотека neurokit2 представляє готову процедуру для автоматизованого підбору даного параметру. Оптимальний \\(k_{max}\\) розраховується на основі точки, в якій значення фрактальної розмірності досягає плато для діапазону значень \\(k_{max}\\) (див. Vega, 2015).\nСинтаксис даної функції виглядає наступним чином:\ncomplexity_k(signal, k_max='max', show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\\(k_{max}\\) (Union[int, str, list], optional) — максимальна кількість інтервалів (має бути більше або дорівнювати 3), які потрібно перевірити. Якщо \\(k_{max}\\)=default, тоді вибирається максимально можливе значення, що відповідає половині довжини сигналу.\nshow (bool) — візуалізовується нахил кривої для обраного значення \\(k_{max}\\).\n\nПовертає\n\nk (float) — оптимальний \\(k_{max}\\) часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимального \\(k_{max}\\).\n\n\n6.2.3.1 Для всього ряду\nДля подальших розрахунків спочатку виконаємо перетворення ряду. Будемо використовувати вихідний часовий ряд для подальших розрахунків:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_higuchi = transformation(signal, ret_type) \n\nІ тепер отримаємо оптимальне значення \\(k_{max}\\) згідно зазначеної процедури:\n\nk_max, info = nk.complexity_k(for_higuchi, k_max=100, show=True)\n\n\n\n\nРис. 6.11: Залежність розмірності Хігучі від діапазону значень kmax\n\n\n\n\nТепер побудуємо залежність довжини сигналу від часового зміщення в логарифмічному масштабі. Для фрактального сигналу має зберігатися лінійна залежність. Бібліотека neurokit2 містить метод для розрахунку даної фрактальної розмірності. Синтаксис цієї процедури виглядає наступним чином:\nfractal_higuchi(signal, k_max='default', show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\\(k_{max}\\) (Union[int, str, list], optional) — максимальна кількість інтервалів (має бути більше або дорівнювати 3), які потрібно перевірити.\nshow (bool) — візуалізовується нахил кривої для обраного значення \\(k_{max}\\).\n\nПовертає\n\nHFD (float) — фрактальна розмірність Хігучі для досліджуваного часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення фрактальної розмірності Хігучі.\n\n\nhfd, info = nk.fractal_higuchi(for_higuchi, k_max=k_max, show=True)\n\n\n\n\nРис. 6.12: Залежність довжини сигналу від часового зміщення\n\n\n\n\nУ подальшому будемо послуговуватись отриманим оптимальним значенням для розрахунку розмірності Хігучі в рамках алгоритму ковзного вікна.\n\n\n6.2.3.2 Віконна процедура\nСкористаємось наступними параметрами:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nk_max_wind = 30                    # максимальне часове зміщення\n\nlength = len(time_ser.values)      # довжина ряду\n\nhfd_wind = []                      # масив показників Хігучі\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Хігучі\n    higuchi, _ = nk.fractal_higuchi(fragm, \n                                    k_max=k_max_wind, \n                                    show=False)\n\n    # зберігаємо результат до масиву значень\n    hfd_wind.append(higuchi)\n\n100%|██████████| 5531/5531 [00:15&lt;00:00, 362.90it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_higuchi_name={symbol}_kmax={k_max_wind}_\\\n           wind={window}_step={tstep}.txt\", hfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_higuchi = fr'$HFD$'\n\nfile_name_higuchi = f\"fd_higuchi_name={symbol}_kmax={k_max_wind}_\\\n           wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          hfd_wind, \n          ylabel, \n          label_higuchi,\n          xlabel,\n          file_name_higuchi)\n\n\n\n\nРис. 6.13: Динаміка індексу золота та фрактальної розмірності Хігучі\n\n\n\n\nЯк можна бачити з представленого рисунку, фрактальна розмірність Хігучі може працювати як індикатор або індикатор-передвісник кризових явищ. Видно, що даний показник починає спадати у передкризові періоди чи у сам момент кризи, вказуючи на зростання згладженності динаміки системи, ступеня кореляцій та трендостійкості динаміки ринку.\n\n\n\n6.2.4 Обчислення фрактальної розмірності Петросяна\nПетросян (1995) запропонував швидкий метод оцінки фрактальної розмірності шляхом перетворення сигналу в двійкову послідовність, з якої оцінюється фрактальна розмірність. Існує кілька варіацій алгоритму (neurokit2, наприклад, пропонує \"A\", \"B\", \"C\" або \"D\"), що відрізняються насамперед способом створення дискретної (символьної) послідовності (див. complexity_symbolize() для деталей). Найпоширеніший метод (\"C\", за замовчуванням) бінаризує сигнал за знаком послідовних різниць.\nБільшість з цих методів дискретизації припускають, що сигнал є періодичним (без лінійного тренду). Для усунення лінійних трендів може бути корисним лінійне детрендування.\nСинтаксис даної процедури має наступний вигляд:\nfractal_petrosian(signal, symbolize='C', show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\nsymbolize (str) - метод перетворення неперервного вхідного сигналу в символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення None, що припускає, що вхідний сигнал вже є дискретним.\nshow (bool) — виводить дискретизацію сигналу.\n\nПовертає\n\nPFD (float) — фрактальна розмірність Петросяна для досліджуваного часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення фрактальної розмірності Петросяна.\n\nМи не розглядатимемо детально синтаксис функції complexity_symbolize(). Опишемо лише ті методи дискретизації, що дотичні до фрактальної розмірності Петросяна: - Метод A бінаризує сигнал за більшими та меншими значеннями порівняно із середнім значенням сигналу. Еквівалентом є method=\"mean\" (method=\"median\" також є допустимим). - Метод B використовує значення, що знаходяться в діапазоні \\(\\pm 1\\sigma\\), проти значень, що виходять за межі цього діапазону. - Метод C обчислює різницю між послідовними вибірками та бінаризує їх залежно від знаку. - Метод D відокремлює послідовні відліки, що перевищують \\(1\\sigma\\) сигналу, від інших менших змін.\nТепер розглянемо віконну динаміку даного показника.\n\n6.2.4.1 Віконна процедура\nОскільки більшість з даних методів дискретизації вимагаються детрендування ряду, тоді будемо виконувати розрахунки для прибутковостей індексу золота.Скористаємось наступними параметрами:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nsymb = \"B\"                    # тип дискретизації ряду\n\nlength = len(time_ser.values) # довжина ряду\n\npetr_wind = []                 # масив показників Петросяна\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Петросяна\n    petrocian, _ = nk.fractal_petrosian(fragm, \n                                        symbolize=symb, \n                                        show=False)\n\n    # зберігаємо результат до масиву значень\n    petr_wind.append(petrocian)\n\n100%|██████████| 5531/5531 [00:05&lt;00:00, 1034.18it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_petrosian_name={symbol}_method={symb}_\\\n           wind={window}_step={tstep}.txt\", petr_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_petrocian = fr'$PFD$'\n\nfile_name_petrocian = f\"fd_petrosian_name={symbol}_method={symb}_\\\n           wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          petr_wind, \n          ylabel, \n          label_petrocian,\n          xlabel,\n          file_name_petrocian)\n\n\n\n\nРис. 6.14: Динаміка індексу золота та фрактальної розмірності Петросяна\n\n\n\n\nРис. 6.14 показує, що показник Петросяна також спадає під час кризових подій, що вказує на зростання періодизації ринку та синхронізації активності трейдерів у відповідні моменти часу.\n\n\n\n6.2.5 Обчислення фрактальної розмірності Каца\nТепер обчислюємо фрактальну розмірність Каца. Евклідові відстані між послідовними точками сигналу підсумовуються і усереднюються, а також визначається максимальна відстань між початковою точкою і будь-якою іншою точкою у вибірці.\nФрактальна розмірність варіюється від 1.0 для прямих ліній, приблизно до 1.15 для випадкових блукань і наближається до 1.5 для найбільш “дивних” форм сигналу.\nСинтаксис процедури для розрахунку даної розмірності виглядає наступним чином:\nfractal_katz(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nKFD (float) — фрактальна розмірність Каца для досліджуваного часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію (наразі порожній, але повертається для узгодженості з іншими функціями).\n\n\n6.2.5.1 Віконна процедура\nОскільки даний показник є параметронезалежним, нам достатньо буде лише розміру часового вікна, кроку та типу ряду:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser.values)      # довжина ряду\n\nkz_wind = []                      # масив показників Каца\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Хігучі\n    katz, _ = nk.fractal_katz(fragm)\n\n    # зберігаємо результат до масиву значень\n    kz_wind.append(katz)\n\n100%|██████████| 5531/5531 [00:01&lt;00:00, 3412.20it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_katz_name={symbol}_wind={window}_step={tstep}.txt\", kz_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_katz = fr'$KFD$'\n\nfile_name_katz = f\"fd_katz_name={symbol}_wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          kz_wind, \n          ylabel, \n          label_katz,\n          xlabel,\n          file_name_katz)\n\n\n\n\nРис. 6.15: Динаміка індексу золота та фрактальної розмірності Каца\n\n\n\n\nНа представленому рисунку видно, що фрактальна розмірність Каца також спадає у кризові та передкризові періоди. Це також є індикаторов зростання ступеня корельованості системи в дані періоди.\n\n\n\n6.2.6 Обчислення фрактальної розмірності Севчика\nАлгоритм цієї фрактальної розмірності був запропонований для обчислення фрактальної розмірності сигналів Севчиком (1998). Цей метод можна використовувати для швидкого вимірювання складності та випадковості сигналу.\nСинтаксис методу, що ми використовуватимемо в подальшому, має наступний вигляд:\nfractal_sevcik(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nSFD (float) — фрактальна розмірність Севчика для досліджуваного часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію (наразі порожній, але повертається для узгодженості з іншими функціями).\n\n\n6.2.6.1 Віконна процедура\nДля цього показника нам також не потребується нічого зайвого:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser.values)      # довжина ряду\n\nsfd_wind = []                      # масив показників Севчика\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Севчика\n    sevcik, _ = nk.fractal_sevcik(fragm)\n\n    # зберігаємо результат до масиву значень\n    sfd_wind.append(sevcik)\n\n100%|██████████| 5531/5531 [00:01&lt;00:00, 3440.38it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_cevcik_name={symbol}_wind={window}_step={tstep}.txt\", sfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_sevcik = fr'$SFD$'\n\nfile_name_sevcik = f\"fd_cevcik_name={symbol}_wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          sfd_wind, \n          ylabel, \n          label_sevcik,\n          xlabel,\n          file_name_sevcik)\n\n\n\n\nРис. 6.16: Динаміка індексу золота та фрактальної розмірності Севчика\n\n\n\n\nБачимо, що і фрактальна розмірність Севчика реагує спадом на крахові події на ринку золота. Особливо характерними є спади під час кризи 2008, 2011, 2015 та 2020 років, що є ключовими та дослідженими багатьма вченими. Динаміка індексу золота під час зазначених кризових подій також характеризувалися зростанням персистентності (кореляцій).\n\n\n\n6.2.7 Обчислення фрактальної розмірності через нормалізовану щільність довжини\nЦе доволі простий показник, що відповідає середнім абсолютним послідовним різницям (стандартизованого) сигналу (np.mean(np.abs(np.diff(std_signal)))). Цей метод було розроблено для вимірювання складності сигналів дуже короткої тривалості (&lt; 30 відліків), і його можна використовувати, наприклад, коли цікавлять безперервні зміни фрактальної розмірності сигналу, обчислюючи його в межах ковзних вікон.\nДля таких методів, як для фрактальної розмірності Хігучі, стандартне відхилення віконної фрактальної розмірності різко зростає, коли довжина ряду стає коротшою. Цей метод дає менше стандартне відхилення, особливо для коротших фрагментів, хоча і за рахунок меншої точності середнього значення фрактальної розмірності вікна.\nСинтаксис процедури має наступний вигляд:\nfractal_nld(signal, corrected=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ncorrected (bool) — якщо значення True, змінює масштаб вихідного значення фрактальної розмірності відповідно до степеневої моделі, щоб зробити його більш порівнянним з “істинним” значенням: FD = 1.9079*((NLD-0.097178)^0.18383). Зауважте, що це може призвести до np.nan, якщо результат різниці буде від’ємним.\n\nПовертає\n\nNLDFD (float) — фрактальна розмірність через нормалізовану щільність довжини.\ninfo (dict) — словник, що містить додаткову інформацію (наразі порожній, але повертається для узгодженості з іншими функціями).\n\n\n6.2.7.1 Віконна процедура\nДля цього показника нам також не потребується нічого зайвого:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nnld_corrected = True               # нормалізація фрактальної розмірності\n\nlength = len(time_ser.values)      # довжина ряду\n\nnldfd_wind = []                    # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    nld, _ = nk.fractal_nld(fragm, \n                            corrected=nld_corrected)\n\n    # зберігаємо результат до масиву значень\n    nldfd_wind.append(nld)\n\n100%|██████████| 5531/5531 [00:05&lt;00:00, 994.20it/s] \n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_nld_name={symbol}_wind={window}_\\\n           step={tstep}_corrected={nld_corrected}.txt\", nldfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_nld = fr'$NLDFD$'\n\nfile_name_nld = f\"fd_nld_name={symbol}_wind={window}_\\\n                step={tstep}_corrected={nld_corrected}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          nldfd_wind, \n          ylabel, \n          label_nld,\n          xlabel,\n          file_name_nld)\n\n\n\n\nРис. 6.17: Динаміка індексу золота та фрактальної розмірності через нормалізовану щільність довжини\n\n\n\n\nРис. 6.17 показує, що \\(NLDFD\\) спадає під час кризових та передкризових подій, що вказує на зростання кореляцій у дані періоди.\n\n\n\n6.2.8 Обчислення фрактальної розмірності через нахил спектральної щільності потужності\nСкористаємось наступним методом бібліотеки neurokit2:\nfractal_psdslope(signal, method='voss1988', show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\nmethod (str) — метод оцінки фрактальної розмірності за нахилом, може бути \"voss1988\" (за замовчуванням) або \"hasselman2013\".\nshow (bool) — якщо значення True, повертає графік залежності спектральної щільності потужності від частоти в логарифмічному масштабі.\nkwargs — інші аргументи, які слід передати до signal_psd().\n\nПовертає\n\nslope (float) — оцінка фрактальної розмірності, отримана в результаті аналізу нахилу спектральної щільності потужності.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для аналізу нахилу спектральної щільності потужності.\n\n\n6.2.8.1 Для всього ряду\nДля подальших розрахунків спочатку виконаємо перетворення ряду. Будемо використовувати вихідний часовий ряд для подальших розрахунків:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_psd = transformation(signal, ret_type) \n\nІ тепер виведемо графік залежності спектральної щільності потужності від частоти в логарифмічному масштабі:\n\npsdslope, info = nk.fractal_psdslope(for_psd,\n                                     method=\"voss1988\",\n                                     show=True)\n\n\n\n\nРис. 6.18: Залежність спектральної щільності потужності від частоти в логарифмічному масштабі\n\n\n\n\nЯк можна бачити з представленого графіку, спектральний нахил спектральної щільності потужності на різних частотах має лінійну залежність, а кут нахилу прямої, побудованої по спектру, близький до -2, що вказує на те, що динаміка індексу золота близька до дробового броунівського руху.\nТепер розглянемо варіацію кута нахилу спектра в рамках алгоритму ковзного вікна.\n\n\n6.2.8.2 Віконна процедура\nВстановимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nmethod_psd = \"voss1988\"        # метод для розрахунку\n                               # спектральної щільності\n\nlength = len(time_ser.values)  # довжина ряду\n\npsd_wind = []                  # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    psd, _ = nk.fractal_psdslope(fragm, method=method_psd)\n\n    # зберігаємо результат до масиву значень\n    psd_wind.append(psd)\n\n100%|██████████| 5531/5531 [00:16&lt;00:00, 337.75it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_psd_name={symbol}_method{method_psd}_\\\n           wind={window}_step={tstep}.txt\", psd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_psd = fr'$PSDFD$'\n\nfile_name_psd = f\"fd_psd_name={symbol}_method{method_psd}_\\\n                wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          psd_wind, \n          ylabel, \n          label_psd,\n          xlabel,\n          file_name_psd)\n\n\n\n\nРис. 6.19: Динаміка індексу золота та фрактальної розмірності через нахил спектральної щільності потужності\n\n\n\n\nРисунок вище показує, що даний показник також реагує спадом у кризові та передкризові події, вказуючи на зростання автокореляції часового ряду під час даних подій. Також зрозуміло, що має місце варіації нахилу спектра щільності потужності. В одні моменти часу динаміка сигналу може бути подібна до броунівського руху, а в інші до білого шуму.\n\n\n\n6.2.9 Обчислення кореляційної розмірності\nКореляційна розмірність (також позначається як \\(D_2\\)) є нижньою границею оцінки фрактальної розмірності досліджуваного фазового простору.\nСпочатку здійснюється реконструкція фазового простору сигналі згідно методу часової затримки, і далі обчислюються відстані між усіма точками траєкторії. Потім обчислюється “кореляційна сума”, яка є часткою пар точок, відстань між якими менша за заданий радіус. Остаточна кореляційна розмірність апроксимується графіком залежності кореляційної суми від радіусу багатовимірного околу досліджуваних траєкторій, що будується в логарифмічному масштабі.\nЦю розмірність можна викликати через fractal_correlation(). Її синтаксис виглядає наступним чином:\nfractal_correlation(signal, delay=1, dimension=2, radius=64, show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка (\\(\\tau\\)).\ndimension (int) — розмірність вкладень (\\(m\\)).\nradius (Union[str, int, list]) — послідовність радіусів для перевірки. Якщо передано ціле число, буде отримано експоненціальну послідовність довжиною заданим скалярним значенням radius у межах від 2.5% до 50% від діапазону відстані. Методи, реалізовані в інших пакетах, можна використовувати, вказуючи \"nolds\", \"Corr_Dim\" або \"boon2008\".\nshow (bool) — графік кореляційної розмірності, якщо True. За замовчуванням — False.\nkwargs — інші аргументи для передачі (наразі не використовуються).\n\nПовертає\n\ncd (float) — кореляційна розмірність часового ряду.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення кореляційної розмірності.\n\n\n6.2.9.1 Для всього часового ряду\nРозглянемо залежність кореляційної суми від радіусу для всього часового ряду. Перш за все виконаємо перетворення ряду:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_corr = transformation(signal, ret_type) \n\nТепер розрахуємо кореляційну розмірність, побудувавши залежність кореляційної суми від радіусу в логарифмічному масштабі:\n\ncd, info = nk.fractal_correlation(for_corr,\n                                  delay=1, \n                                  dimension=1,\n                                  radius=\"nolds\", \n                                  show=True)\n\n\n\n\nРис. 6.20: Залежність кореляційної суми від радіусу багатовимірного околу досліджуваних траєкторій. Графік побудований у логарифмічному масштабі\n\n\n\n\nЯк ми можемо бачити, кореляційна сума дійсно має лінійну залежність для різних значень радіусу околу певної траєкторії, що вказує на фрактальність системи. Тепер подивимось як варіюється значення кореляційної розмірності в періоди турбулентності.\n\n\n6.2.9.2 Віконна процедура\nДля цього показника визначимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nd_wind = 2          # розмірність вкладень\ntau_wind = 1        # часова затримка\nrad_wind = \"nolds\"  # метод для визначення масиву радіусів\n\nlength = len(time_ser.values)      # довжина ряду\n\ncorr_wind = []                     # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    cd_wind, _ = nk.fractal_correlation(fragm,\n                                        delay=tau_wind, \n                                        dimension=d_wind,\n                                        radius=rad_wind)\n\n    # зберігаємо результат до масиву значень\n    corr_wind.append(cd_wind)\n\n100%|██████████| 5531/5531 [00:19&lt;00:00, 290.43it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_correlation_name={symbol}_wind={window}_\\\n                step={tstep}_dim={d_wind}_tau={tau_wind}_\\\n                radius={rad_wind}.txt\", corr_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_cd = fr'$CD$'\n\nfile_name_cd = f\"fd_correlation_name={symbol}_wind={window}_\\\n                step={tstep}_dim={d_wind}_tau={tau_wind}_\\\n                radius={rad_wind}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          corr_wind, \n          ylabel, \n          label_cd,\n          xlabel,\n          file_name_cd)\n\n\n\n\nРис. 6.21: Динаміка індексу золота та кореляційної фрактальної розмірності\n\n\n\n\nРис. 6.21 демонструє, що кореляційна розмірність для індексу золота також спадає у кризові та передкризові періоди, вказуючи на зростання корельованості теперішніх цін на золото з попередніми. Можна сказати і по іншому: у період криз трейдери починають самоорганізовуватись та колективно скупати або продавати відповідний актив; іншими словами, їх динаміка стає більш синхронною. Оскільки кореляційна розмірність вимірюється для траєкторій фазового простору, спад цього показника свідчить про зростання щільності досліджуваних траєкторій. Тобто, фазовий простір стає більш розрідженим, а всі його траєкторії концентрованими лише в одній конкретній області, що є індикатором згуртованості прихованих змінних досліджуваної системи."
  },
  {
    "objectID": "lab_7.html#теоретичні-відомості",
    "href": "lab_7.html#теоретичні-відомості",
    "title": "7  Лабораторна робота № 7",
    "section": "7.1 Теоретичні відомості",
    "text": "7.1 Теоретичні відомості\n\n7.1.1 Означення мультифракталів\nУ цій лабораторній ми викладемо основи теорії мультифракталів — неоднорідних фрактальних об’єктів, для повного опису яких, на відміну від регулярних фракталів, недостатньо введення лише однієї величини, його фрактальної розмірності \\(D\\), а потрібен цілий спектр таких розмірностей, кількість яких, взагалі кажучи, нескінченна. Причина цього полягає в тому, що поряд із суто геометричними характеристиками, які визначаються величиною \\(D\\), такі фрактали характеризуються й деякими специфічними статистичними властивостями.\n\n\n\nРис. 7.1: Трикутник Серпинського, області якого згенеровані з нерівномірними ймовірностями\n\n\nПростіше всього пояснити, що розуміється під “неоднорідним фракталом” на прикладі трикутника Серпинського, отриманого за допомогою методу випадкових ітерацій.\nПрипустімо, однак, що в методі випадкових ітерацій ми тепер із якоїсь причини віддали перевагу одній із вершин трикутника, наприклад, вершині А, і стали вибирати її з імовірністю 90%. Дві ж інші вершини В і С для нас рівноцінні, але на їхню частку тепер припадає всього лише по 5%. Результат такої несиметричної гри” зображено нижче на рисунку вище.\nВидно, що точки всередині трикутника АВС розподілені тепер вкрай нерівномірно. Більша їх частина перебуває біля вершини А та її прообразів. Водночас у вершин В і С (і їхніх прообразів) їх є вкрай мало. Проте, за звичайною термінологією, ця множина точок (за умови прагнення числа ітерацій до нескінченності) є фракталом, тому що збереглася основна властивість фракта — самоподібність. Дійсно, трикутник DFC, хоча в ньому у 20 разів менше точок, за своїми статистичними і геометричними властивостями повністю подібний до великого трикутника АВС. Так само, як і у великому трикутнику, точки в ньому концентруються здебільшого поблизу вершини D — аналогу вершини А.\n\n\n\nРис. 7.2: Розподіл точок по трикутнику Серпинського, представленого на попередньому рисунку\n\n\nРис. 7.2 більш детально демонструє результуючий розподіл точок по трикутнику Серпинського. Цифри в кожному з маленьких трикутників показують його відносну заселеність точками множини.\nОднак, не дивлячись на нерівномірність розподілу точок фрактала, його фрактальна розмірність залишилась при цьому такою ж, \\(D=\\frac{\\ln{3}}{\\ln{2}}\\). Покриття цієї множини все меншими трикутниками можна здійснити по тому ж алгоритму, що й раніше. Таке співпадіння змушує замислитись над пошуком нових кількісних характеристик, котрі могли б відрізнити нерівномірний розподіл точок від рівномірного.\nІнший, складніший приклад неоднорідного фрактала, який ми б хотіли ще навести, показано на наступному рисунку. Ліворуч продемонстровано великий квадрат зі стороною, що дорівнює одиниці, який на цьому (нульовому) етапі повністю покриває собою деяку фрактальну множину точок \\(M\\). На наступному (першому) етапі, у центрі малюнка, показано, як ту саму множину можна покрити трьома меншими квадратами зі сторонами \\(l_1=1/2, \\ l_2=l_3=5/16\\), у яких, відповідно, міститься частка \\(p_1=1/2, \\ p_2=1/3\\) та \\(p_3=1/6\\) усіх точок.\n\n\n\nРис. 7.3: Приклад мультифрактала, що підкоряється ренормалізаційній схемі\n\n\nНаступний етап покриття (зображений на рисунку праворуч) містить уже 9 квадратів зі сторонами \\(l_{1}^{2}=1/4, \\ l_{1}l_{2}=l_{1}l_{3}=5/32\\) (у нижньому правому куті) і \\(l_{2}l_{1}=5/32, \\ l_{2}^{2}=l_{2}l_{3}=25/256\\) (угорі праворуч і ліворуч). Відносна заселеність цих квадратів точками множини показана на рисунку. Вона відповідає добутку чинників заселеності (імовірностей): \\(p_{1}^{2}=1/4, \\ p_{1}p_{2}=1/6, \\ p_{1}p_{3}=1/12\\) — для нижньої правої групи, \\(p_{2}p_{1}=1/6, \\ p_{2}^{2}=1/9, \\ p_{2}p_{3} = 1/18\\) — для верхньої лівої та \\(p_{3}p_{1}=1/12, \\ p_{3}p_{2}=1/18, \\ p_{3}^{2}=1/36\\) — для верхньої правої групи. Зазначимо, що є чітка відповідність між заселеністю квадрата \\(p_{j}p_{i}\\) і його розмірами \\(l_{i}l_{i}\\).\nПодальший процес розбиття і покриття множини \\(M\\) здійснюється згідно із цією ренормалізаційною схемою. Кожен квадрат, що має на \\(n\\)-му кроці розмір \\(l\\) і заселеність \\(р\\), замінюється на \\(n+1\\) кроці трьома квадратами з розмірами \\(ll_{1}, \\ ll_{2}, \\ ll_{3}\\) і заселеностями \\(pp_{1}, \\ pp_{2}, \\ pp_{3}\\) відповідно, розміщеними таким самим чином відносно один одного, як показано на попередньому посередні рисунку.\nДвоє розглянутих вище випадки являють собою приклади неоднорідних фракталів. Під словом “неоднорідний” ми тут розуміємо нерівномірний розподіл точок множини по фракталу або нерівномірний розподіл малих та великих флуктуацій у часовому ряді. Причина неоднорідності в попередніх випадках одна й та сама — різні ймовірності заповнення геометрично однакових елементів фрактала, або в загальному випадку невідповідність імовірностей заповнення геометричним розмірам відповідних областей. Такі неоднорідні фрактальні об’єкти в літературі називають мультифракталами, і їх вивченням ми й займемося надалі.\n\n\n7.1.2 Узагальнені фрактальні розмірності \\(D_{q}\\)\nДамо загальне визначення мультифрактала. Розглянемо фрактальний об’єкт, що займає якусь обмежену ділянку \\(\\Omega\\) розміру \\(L\\) у Евклідовому просторі з розмірністю \\(d\\). Нехай на якомусь етапі його побудови він являє собою множину з \\(N \\gg 1\\) точок, якось розподілених у цій області. Ми будемо припускати, що врешті-решт \\(N \\to \\infty\\). Прикладом такої множини може слугувати трикутник Серпінського, побудований методом випадкових ітерацій. Кожен крок ітераційної процедури додає до цієї множини одну нову точку.\nРозіб’ємо всю область \\(\\Omega\\) на кубічні клітинки зі стороною \\(\\varepsilon \\ll L\\) та об’ємом \\(\\varepsilon^{d}\\). Далі нас будуть цікавити тільки зайняті клітинки, у яких міститься хоча б одна точка. Нехай номер зайнятих комірок \\(i\\) змінюється в межах \\(і=1, 2,..., N(\\varepsilon)\\), де \\(N(\\varepsilon)\\) — сумарна кількість зайнятих клітинок, яка, звісно, залежить від розміру клітинки \\(\\varepsilon\\).\nНехай \\(n_{i}(\\varepsilon)\\) представляє собою кількість точок у клітинці з номером \\(i\\), тоді величина\n\\[\np_{i}(\\varepsilon) = \\lim_{N\\to\\infty}\\frac{n_{i}(\\varepsilon)}{N}\n\\]\nпредставляє собою ймовірність того, що навмання взята точка з нашої множини знаходиться в комірці \\(i\\). Інакше кажучи, ймовірності \\(р_{i}\\) характеризують відносну заселеність комірок. З умови нормування ймовірності випливає, що\n\\[\n\\sum_{i=1}^{N(\\varepsilon)}p_{i}(\\varepsilon)=1.\n\\]\nУведемо тепер у розгляд узагальнену статистичну суму \\(Z(q,\\varepsilon)\\), що характеризується показником ступеня \\(q\\), який може набувати будь-яких значень в інтервалі \\(-\\infty&lt;q&lt;+\\infty\\)\n\\[\nZ(q,\\varepsilon)=\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon).\n\\]\nСпектр узагальнених фрактальних розмірностей \\(D_{q}\\), що характеризує даний розподіл точок в області \\(\\Omega\\), визначається за допомогою співвідношення\n\\[\nD_{q} = \\frac{\\tau(q)}{q-1},\n\\]\nде функція \\(\\tau(q)\\) має вид\n\\[\n\\tau(q)=\\lim_{\\varepsilon\\to 0}\\frac{\\ln{Z(q,\\varepsilon)}}{\\ln{\\varepsilon}}.\n\\]\nЯк ми покажемо нижче, якщо \\(D_{q}=D=\\text{const}\\), тобто не залежить від \\(q\\), то дана множина точок являє собою звичайний, регулярний фрактал, який характеризується лише однією величиною — фрактальною розмірністю \\(D\\). Навпаки, якщо функція \\(D_{q}\\) якось змінюється з \\(q\\), то розглянута множина точок представляє мультифрактал.\nТаким чином, мультифрактал у загальному випадку характеризується деякою нелінійною функцією \\(\\tau(q)\\), що визначає поведінку статистичної суми \\(Z(q,\\varepsilon)\\) при \\(\\varepsilon\\to 0\\)\n\\[\nZ(q,\\varepsilon)=\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon) \\approx \\varepsilon^{\\tau(q)}.\n\\tag{7.1}\\]\nСлід мати на увазі, що в реальній ситуації ми завжди маємо скінченне, хоча й дуже велике число дискретних точок \\(N\\), тому при комп’ютерному моделювані конкретної множини граничний перехід \\(\\varepsilon\\to 0\\) треба виконувати з обережністю, пам’ятаючи, що йому завжди передує ліміт \\(N \\to 0\\).\nПокажемо тепер, як поводиться узагальнена статистична сума у випадку звичайного регулярного фрактала з фрактальною розмірністю \\(D\\). У цьому випадку в усіх зайнятих комірках міститься однакова кількість точок\n\\[\nn_{i}(\\varepsilon)=\\frac{N}{N(\\varepsilon)},\n\\]\nтобто фрактал представляється однорідним. Тоді очевидно, що відносні населеності клітинок, \\(p_{i}(\\varepsilon)=1/N(\\varepsilon)\\), також однакові, і узагальнена статистична сума набуває вигляду\n\\[\nZ(q,\\varepsilon) = N^{1-q}(\\varepsilon).\n\\tag{7.2}\\]\nВрахуємо тепер, що, згідно визначеню фрактальної розмірності \\(D\\), кількість зайнятих клітинок при достатньо малому \\(\\varepsilon\\) поводить себе наступним чином:\n\\[\nN(\\varepsilon) \\approx \\varepsilon^{-D}.\n\\tag{7.3}\\]\nПідставляючи (Рівняння 7.3) у формулу (Рівняння 7.2), і порівнюючи з (Рівняння 7.1), отримуємо\n\\[\n\\varepsilon^{\\tau(q)} = \\varepsilon^{-D(1-q)} \\to \\tau(q)=(q-1)D.\n\\tag{7.4}\\]\nМи приходимо до висновку, що у випадку звичайного фрактала функція (Рівняння 7.4) є лінійною. Тоді всі \\(D_{q}\\) дійсно не залежать від \\(q\\). Фрактал у якого всі узагальнені фрактальні розмірності \\(D_{q}\\) співпадають називається монофракталом.\nЯкщо розподіл точок по клітинкам неоднаковий, тоді фрактал називається неоднорідним, тобто представляє із себе мультифрактал, і для його характеристики необхідний цілий спектр узагальнених фрактальних розмірностей \\(D_{q}\\), кількість котрих, у загальному випадку, нескінченна.\nТак, наприклад, при \\(q \\to +\\infty\\) основний внесок в узагальнену статистичну суму (Рівняння 7.1) вносять комірки, що містять найбільшу кількість частинок \\(n_{i}\\) у них і, відповідно, що характеризуються найбільшою ймовірністю їх заповнення \\(p_{i}\\). Навпаки, при \\(q \\to -\\infty\\) основний внесок в узагальнену статистичну суму вносять найбільш розрідженні комірки з найменшою ймовірністю їх заповнення \\(p_{i}\\). Таким чином, функція \\(D_{q}\\) показує, наскільки неоднорідним представляється досліджувана множина точок \\(\\Omega\\).\nУ подальшому для характеристики розподілу точок необхідно знати не тільки функцію \\(\\tau(q)\\), але і її похідну:\n\\[\n\\frac{d\\tau(q)}{dq} = \\lim_{\\varepsilon\\to 0}\\frac{\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}\\ln{p_{i}}}{\\left( \\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q} \\right)\\ln{\\varepsilon}}.\n\\]\nЦя похідна має важливий фізичний зміст, який буде продемонстровано пізніше. Зараз знову зазначимо, що для мультифрактальної системи вона не залишається константною і змінюється з \\(q\\).\n\n\n7.1.3 Функція мультифрактального спектра \\(f(\\alpha)\\)\n\n7.1.3.1 Спектр фрактальних розмірностей\nУ попередньому пункті ми ввели поняття мультифрактала — об’єкта, що представляє собою неоднорідний фрактал. Для його опису ми ввели множину узагальнених фрактальних розмірностей \\(D_{q}\\), де \\(q\\) приймає будь-які значення в інтервалі \\(-\\infty&lt;q&lt;+\\infty\\). Однак величини \\(D_{q}\\) не є, строго кажучи, фрактальними розмірностями в загальному розумінні цього слова.\nТому часто поряд із ними для характеристики мультифрактальної множини використовують так звану функцію мультифрактального спектра \\(f(\\alpha)\\) (спектр сингулярностей мультифрактала), до якої, як ми побачимо надалі, більше підходить термін фрактальна розмірність. Ми покажемо, що величина \\(f(\\alpha)\\) фактично дорівнює хаусдорфовій розмірності якоїсь однорідної фрактальної підмножини із вихідної множини \\(\\Omega\\), що дає домінантний внесок у статистичну суму при заданій величині \\(q\\).\nОднією з основних характеристик мультифрактала є набір імовірностей \\(р_{i}\\), що показують відносну заселеність клітинок \\(\\varepsilon\\), якими ми покриваємо цю множину. Чим менший розмір клітинки, тим менша величина її заселеності. Для самоподібних множин залежність \\(p_{i}\\) від розміру клітинки \\(\\varepsilon\\) має степеневий характер:\n\\[\np_{i}(\\varepsilon) \\approx \\varepsilon^{\\alpha_{i}},\n\\]\nде \\(\\alpha_{i}\\) являє собою деякий показник ступеня (різний для різнок клітинок \\(i\\)).\n\n\n\n\n\n\nДодатково по \\(\\alpha\\)\n\n\n\nСпрямовуючи значення \\(\\varepsilon\\) до нуля, можна сказати, що фрактальність можна розглядати локально для кожної точки (елемента) досліджуваної системи, і таким чином показник \\(\\alpha\\) можна розглядати як локальну фрактальну розмірність. Його так само називають показником Гьолдера або силою сингулярності.\nМожемо спостерігати саме степеневу залежність, оскільки, вочевидь, розподіл маси (флуктуацій) концентрується з різною “силою” \\(\\alpha\\), тож і імовірнісна міра змінюється пропорційно розмірам вікон, що використовуються, з розміром \\(\\varepsilon\\).\n\n\n\n\n\nРис. 7.4: Схематичне представлення залежності сили сингулярності та густини порівняно з околицями\n\n\nСірий масштаб являє собою ймовірнісну міру для кожної локації, як показано на кожній панелі. На рисунку (a) тільки \\(i\\)-та локація має ненульову щільність, інші місця порожні. Імовірнісна міра на комірці залишається \\(\\rho\\), навіть коли розмір клітинки \\(\\varepsilon\\) збільшується. Розмір ящика \\(\\varepsilon\\) збільшується, що підкреслюється жирною лінією. Проте, через те, що далі ми не спостерігаємо зростання щільності, показник \\(\\alpha\\) залишається нульовим. На рисунку (b) усі комірки мають однакову щільність. Імовірнісні міри комірок дорівнюють \\(\\rho\\), \\(9\\rho\\) і \\(25\\rho\\) для найменшої, другої найменшої та найбільшої комірки (підкреслено жирною лінією). Таким чином, сила сингулярності \\(i\\)-го осередку дорівнює 2. На рисунку (c) \\(i\\)-й осередок є розрідженим порівняно з навколишніми осередками. Імовірнісна міра осередків дорівнює \\(\\rho\\), \\(27\\rho\\) і \\(125\\rho\\) для найменшого, другого найменшого і найбільшого осередку (підкреслено жирною лінією). Таким чином, сила сингулярності \\(i\\)-ої комірки дорівнює 3.\n\n\n\n\n\n\nДодатково по \\(\\alpha\\)\n\n\n\nМожна сказати, що чим більш гладкою видається поверхня системи, чим менше елементів задіяно в її розвитку, тим менший показник сингулярності. Що більше елементів системи вступають у взаємозв’язок один з одним, що більше процесів протікає під час еволюції системи, то більший показник сингулярності.\n\n\nВідомо, що для регулярного (однорідного) фрактала всі показники ступеня \\(\\alpha_{i}\\) однакові й рівні фрактальній розмірності \\(D\\):\n\\[\np_{i} = \\frac{1}{N(\\varepsilon)} \\approx \\varepsilon^{D}.\n\\]\nУ даному випадку статистична сума (Рівняння 7.1) приймає наступний вигляд:\n\\[\nZ(q,\\varepsilon) = \\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon) = N(\\varepsilon)\\varepsilon^{Dq}=\\varepsilon^{-D}\\varepsilon^{Dq} \\approx \\varepsilon^{D(q-1)}.\n\\]\nТому \\(\\tau(q)=D(q-1)\\) і всі узагальнені фрактальні розмірності \\(D_{q}=D\\) у цьому випадку співпадають та не залежать від \\(q\\).\nОднак, для такого складного об’єкта, як мультифрактал, унаслідок його неоднорідності, ймовірності заповнення клітинок \\(p_{i}\\) у загальному випадку різняться, і показник ступеня \\(\\alpha_{i}\\) для різних клітинок може приймати різні значення. Достатньо типовою є ситуація, коли ці значення неперервно заповнюють деякий закритий інтервал \\(\\left( \\alpha_{min}, \\alpha_{max} \\right)\\), причому\n\\[\np_{min} \\approx \\varepsilon^{\\alpha_{max}}, \\; \\text{a} \\; p_{max} \\approx \\varepsilon^{\\alpha_{min}}.\n\\]\nТепер перейдемо до питання о розподілі ймовірностей різних значень \\(\\alpha_{i}\\). Нехай \\(n(\\alpha)d\\alpha\\) є ймовірністю того, що \\(\\alpha_{i}\\) знаходиться в інтервалі від \\(\\alpha\\) до \\(\\alpha+d\\alpha\\). Іншими словами, \\(n(\\alpha)d\\alpha\\) представляє собою відносну кількість клітинок \\(i\\), що характеризуються однією і тією самою мірою \\(p_{i}\\) з \\(\\alpha_{i}\\), що лежать у цьому інтервалі. У випадку монофрактала, для котрого всі \\(\\alpha_{i}\\) однакові (і рівні фрактальній розмірності \\(D\\)), це число, очевидно, пропорційно повній кількості клітинок \\(N(\\varepsilon) \\approx \\varepsilon^{-D}\\), степеневим чином залежних від розміру клітинки \\(\\varepsilon\\). Показник ступеня в цьому співвідношені визначається фрактальною розмірністю множини \\(D\\).\nДля мультифрактала, однак, це не так, і різні значення \\(\\alpha_{i}\\) зустрічаються з ймовірністю, що характеризується не однією і тією ж величиною \\(D\\), а різними (в залежності від \\(\\alpha\\)) значеннями показниками ступеня \\(f(\\alpha)\\),\n\\[\nn(\\alpha) \\approx \\varepsilon^{-f(\\alpha)}.\n\\tag{7.5}\\]\nТаким чином, фізичний сенс функції \\(f(\\alpha)\\) полягає в тому, що вона представляє собою розмірність хаусдорфа деякої однорідної підмножини \\(\\Omega_{\\alpha}\\) із вихідної множини \\(\\Omega\\), що характеризується однаковими ймовірностями заповнення клітинок \\(p_{i} \\approx \\varepsilon^{\\alpha}\\). Оскільки фрактальна розмірність підмножини очевидно завжди менша або рівна фрактальній розмірності вихідної множини \\(D_{0}\\), має місце важлива нерівність для функції \\(f(\\alpha)\\):\n\\[\nf(\\alpha) \\leq D_{0}.\n\\]\nУ результаті можна зробити висновок, що множина різних значень функції \\(f(\\alpha)\\) (при різних \\(\\alpha\\)) представляє собою спектр фрактальних розмірностей однорідних підмножин \\(\\Omega_{\\alpha}\\), на які можна розбити вихідну множину \\(\\Omega\\), кожна з яких характеризується власним значенням фрактальної розмірності \\(f(\\alpha)\\).\nОскільки будь-якій підмножині належить лише частина загальної кількості клітинок \\(N(\\varepsilon)\\), на котрі ми розділили вихідну множину \\(\\Omega\\), умова нормування ймовірностей, очевидно, не виконується при підсумовуванні тільки по цій підмножині. Сума цих імовірностей стає менше одиниці. Тому й самі ймовірності \\(p_i\\) з одним і тим самим значенням \\(\\alpha_i\\) очевидно менше (або в крайньому випадку одного порядку), ніж величина \\(\\varepsilon^{f(\\alpha_i)}\\), яка обернено пропорційна кількості наявних клітинок, що покривають дану підмножину (нагадаємо, що у випадку монофрактала \\(p_i \\approx 1/N(\\varepsilon)\\)). У результаті ми приходимо до наступної важливої нерівності для функції \\(f(\\alpha)\\). А саме, при всіх значеннях \\(\\alpha\\)\n\\[\nf(\\alpha) \\leq \\alpha.\n\\]\nЗнак рівності має місце, наприклад, для повністю однорідного фрактала, де \\(f(\\alpha)=\\alpha=D\\).\n\n\n7.1.3.2 Перетворення Лежандра\nВстановимо зв’язок функції \\(f(\\alpha)\\) із введенною раніше функцією \\(\\tau(q)\\). Обчислимо для цього статистичну суму \\(Z(q,\\varepsilon)\\). Підставляє у статистичну суму ймовірності \\(p_i \\approx \\varepsilon^{\\alpha_i}\\), та переходячи від підсумовування по \\(i\\) до інтегрування по \\(\\alpha\\) з плотністю ймовірностей (Рівняння 7.5), ми отримаємо\n\\[\nZ(q,\\varepsilon) = \\sum_{i=1}^{N(\\varepsilon)} p_{i}^{q}(\\varepsilon) \\approx \\int d\\alpha n(\\alpha)\\varepsilon^{q\\alpha} \\approx \\int d\\alpha\\varepsilon^{q\\alpha-f(\\alpha)}.\n\\tag{7.6}\\]\nТак як величина \\(\\varepsilon\\) дуже мала, основний внесок у цей інтеграл вноситимуть ті значення \\(\\alpha(q)\\), при яких показник ступеня \\(q\\alpha-f(\\alpha)\\) виявляється мінімальним (а підінтегральна функція — максимальною). Цей вклад буде пропорційним значенню підінтегральної функції у точці максимума. Саме ж значення \\(\\alpha(q)\\) визначається при цьому з наступної умови:\n\\[\n\\left. \\frac{d}{d\\alpha}[ q\\alpha-f(\\alpha) ] \\right \\vert_{\\alpha=\\alpha(q)} = 0.\n\\]\nТакож, з умови мінімуму ми маємо\n\\[\n\\left. \\frac{d^{2}}{d\\alpha^{2}}[ q\\alpha-f(\\alpha) ] \\right \\vert_{\\alpha=\\alpha(q)} &gt; 0.\n\\]\nУ результаті отримуємо, що залежність \\(\\alpha(q)\\) неявним чином визначається з рівняння\n\\[\nq = \\frac{df(\\alpha)}{d\\alpha}\n\\]\nі що функція \\(f(\\alpha)\\) усюди є випуклою\n\\[\nf^{''}(\\alpha)&gt;0.\n\\]\nЦе значить, що величина \\(f(\\alpha(q))\\) дійсно визначає фрактальну розмірність підмножини \\(\\Omega_{\\alpha(q)}\\), що має найбільший домінуючий внесок у статистичну суму (Рівняння 7.6) при заданій величині показника ступеня \\(q\\).\nОскільки \\(Z(q,\\varepsilon)=\\tau(q)\\), приходимо до висновку, що\n\\[\n\\tau(q) = q\\alpha(q) - f(\\alpha(q)).\n\\tag{7.7}\\]\nПам’ятаючи, що \\(\\tau(q)=D(q-1)\\), можемо віднайти функцію \\(D_{q}\\):\n\\[\nD_{q} = \\frac{1}{q-1}[ q\\alpha(q)-f(\\alpha(q)) ].\n\\tag{7.8}\\]\nТаким чином, якщо ми знаємо функцію мультифрактального спектра \\(f(\\alpha)\\), то за домомогою співвідношень (Рівняння 7.8) та (Рівняння 7.9) ми можемо знайти функцію \\(D_{q}\\). Навпаки, знаючи \\(D_{q}\\), ми можемо знайти залежність \\(\\alpha(q)\\) з допомогою рівняння\n\\[\n\\alpha(q) = \\frac{d}{dq}[(q-1)D_{q}]\n\\tag{7.9}\\]\nі після цього знайти із (Рівняння 7.9) залежність \\(f(\\alpha(q))\\). Ці два рівняння і визначають функцію \\(f(\\alpha)\\).\n\\[\n\\frac{d\\tau}{dq}\\frac{dq}{d\\alpha} = q + \\alpha\\frac{dq}{d\\alpha} - \\frac{df}{d\\alpha}.\n\\]\nПриймаючи до уваги те, що \\(q=df/d\\alpha\\), і скорочуючи це рівняння на \\(dq/d\\alpha\\), приходимо до співвідношення\n\\[\n\\alpha = \\frac{d\\tau(q)}{dq},\n\\]\nщо еквівалентне виразу (Рівняння 7.9).\nВирази для \\(\\tau(q)\\) та \\(\\alpha(q)\\) задають перетворення Лежандра від змінних \\(\\{ q, \\tau(q) \\}\\) до змінних \\(\\{\\alpha, f(\\alpha)\\}\\):\n\\[\n\\alpha = \\frac{d\\tau}{dq}\n\\tag{7.10}\\]\nта\n\\[\nf(\\alpha) = q\\frac{d\\tau}{dq} - \\tau.\n\\tag{7.11}\\]\nЯк відомо, для однорідного фрактала \\(D_{q}=D=\\text{const}\\). Тому \\(\\alpha=d\\tau/dq=D\\) і \\(f(\\alpha)=q\\alpha-\\tau(q)=qD-D(q-1)=D\\). У цьому випадку “графік” функції \\(f(\\alpha)\\) на площині \\(\\left( \\alpha, f(\\alpha) \\right)\\) складається лише з однієї точки \\(\\left( D, D \\right)\\).\n\n\n\n7.1.4 Мультифрактальний аналіз детрендованих флуктуацій\nМонофрактальні та мультифрактальні структури фінансових сигналів є особливим різновидом масштабно-інваріантних структур. Найчастіше монофрактальна структура фінансових часових рядів визначається одним степеневим показником і передбачає, що масштабо-інваріантність не залежить від часу і простору. Однак, часто ми маємо змогу спостерігати просторово-часову варіацію масштабно-інваріантної структури досліджуваної складної системи. Ці просторово-часові варіації вказують на мультифрактальність фінансового сигналу, яка визначається мультифрактальним спектром. Ширина і форма мультифрактального спектра можуть також допомогти диференціювати варіативність серцевого ритму у пацієнтів із серцевими захворюваннями, такими як шлуночкова тахікардія, фібриляція шлуночків і застійна серцева недостатність. Мультифрактальний спектр може допомогти кількісно визначити асиметрію підйомів та спадів на фондовому чи криптовалютному ринках, передбачити фінансову кризу, що поступово наближується, і, таким чином, сприяти успішності подальших торгівельних рішень. Основна мета цього розділу — представити одну з найточніших процедур для визначення множини фрактальних показників — мультифрактальний аналіз детрендованих флуктуацій (multifractal detrended fluctuation analysis, MFDFA).\nПобудова MFDFA складається з восьми кроків:\n\nРозділ “Шум і випадкові блукання у часовому ряді” представляє метод приведення часового ряду до такого, що подібний до випадкового блукання, що є попереднім кроком для MFDFA.\nРозділ “Обчислення середньоквадратичної варіації часового ряду” представляє середньоквадратичну варіацію, яка є основною процедурою для подальших обчислень в MFDFA і типовим способом обчислення середньої варіації часових рядів різної природи.\nРозділ “Локальна середньоквадратична варіація часового ряду” представляє обчислення локальної варіації часового ряду як середньоквадратичного відхилення часового ряду в межах сегментів, що можуть як перекриватися, так і не перекриватися.\nУ розділі “Локальне детрендування часового ряду” таке ж локальне середньоквадратичне відхилення обчислюється навколо трендів, які часто зустрічаються у фінансових часових рядах.\nУ розділі “Монофрактальний аналіз детрендованих флуктуацій” амплітуди локальних середніх квадратичних відхилень підсумовуються в узагальнене середнє квадратичне відхилення. У сумарному середньоквадратичному відхиленні для сегментів з малими розмірами вибірки переважають швидкі флуктуації часового ряду. На противагу цьому, у сумарному середньоквадратичному відхиленні для сегментів з великими розмірами вибірки переважають повільні коливання. Степенева залежність між загальним середнім квадратичним відхиленням для декількох розмірів вибірки (тобто масштабів) визначається за допомогою монофрактального аналізу дентрендованих флуктуацій (monofractal detrended fluctuation analysis, DFA) і називається показником Херста (Hurst exponent, \\(H\\)).\nУ розділі “Мультифрактальний аналіз детрендованих флуктуацій” MFDFA отримують шляхом розширення на \\(q\\)-й порядок узагальненого середньоквадратичного відхилення. Середньоквадратичне відхилення \\(q\\)-го порядку може розрізняти сегменти з малими та великими флуктуаціями. Степенева залежність між середньоквадратичним відхиленням \\(q\\)-го порядку чисельно визначається як показник Херста \\(q\\)-го порядку.\nУ розділі “Мультифрактальний спектр часових рядів” на основі показника Херста \\(q\\)-го порядку обчислено декілька мультифрактальних спектрів.\nРозділ “Узагальнені фрактальні розмірності” представляє більш детальний опис показників \\(D_{q}\\), що будуть описані в подальшому.\nРозділ “Аналогії мультифракталів із термодинамікою” показує, що отримані кількісні мультифрактальні показники мають зв’язок із термодинамічними показниками, що дозволило нам вивести мультифрактальну “теплоємність”.\n\nДля подальшої візуалізації кожного кроку процедури MFDFA імпортуємо наступні модулі:\n\nimport matplotlib.pyplot as plt \nimport matplotlib.gridspec as gridspec\nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom scipy.integrate import cumulative_trapezoid\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виведення:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),          # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 24,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,              # товщина ліній\n    'axes.titlesize': 'small',         # розмір титулки над рисунком\n    'font.family': \"Times New Roman\",  # сімейство стилів підписів \n    'font.serif': [\"Times\"],           # стиль підпису\n    'mathtext.fontset': \"dejavuserif\", # стиль математичних виразів \n    'savefig.dpi': 300                 # якість збережених зображень\n}\n\nplt.rcParams.update(params)            # оновлення стилю згідно налаштувань\n\nЦього разу розглянему можливість побудови індикаторів або індикаторів-передвісників на прикладі індексу сирої нафти West Texas Intermediate (WTI). При описі процедури MFDFA порівнюватимемо мультифрактальність даного ряду зі штучно зненерованими монофрактальними рядами складність яких має представлятися меншою у порівняні з нафтою.\nСам сайт Yahoo! Finance містить досить коротку історію щодених цін на нафту даної марки. Цього разу, в якості альтернативного прикладу, будемо послуговуватись альтернативним джерелом фінансових даних — федеральним резервом економічних даних федерального резервного банку Сент-Луїса (Federal Reserve Economic Data of the Federal Reserve Bank of St. Louis, FRED). На Python було створено бібліотеку для вивантаження даних із даного джерела — pandas-datareader. Для її встановлення достатньо прописати наступну команду:\n\n!pip install pandas-datareader\n\nТепер імпортуємо відповідну бібліотеку:\n\nimport pandas_datareader.data as web\n\nта виконаємо зчитування індексу з FRED, використовуючи функціонал даної бібліотеки. Завантажимо дані за весь період, що нам буде доступний:\n\nsymbol = 'DCOILWTICO'    # cимвол індексу, як указано на сайті FRED\nstart = \"1986-01-01\"     # Дата початку зчитування даних\nend = \"2023-01-21\"       # Дата закінчення зчитування даних\n\nwti = web.DataReader(symbol, 'fred', start, end) # зчитуємо значення ряду \ntime_ser = wti[symbol].copy()                    # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\nВиведемо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 7.5: Динаміка щоденних змін індексу сирої нафти WTI\n\n\n\n\nРозглянемо опис нашого масиву даних:\n\ntime_ser.describe()\n\ncount    9336.000000\nmean       46.078743\nstd        29.597897\nmin       -36.980000\n25%        19.990000\n50%        35.955000\n75%        67.242500\nmax       145.310000\nName: DCOILWTICO, dtype: float64\n\n\nНа представленому рисунку видно, що в значеннях досліджуваного індексу існують пропуски та негативні значення на ціну нафти. Для того, щоб позбутися від’ємного значення, можна виконати заміну значення на таке, що близьке до нуля. Для видалення значень NaN достатньо скористатися методом dropna() бібліотеки pandas.\n\ntime_ser = time_ser.dropna()    # видаляємо значення NaN\ntime_ser[time_ser.values&lt;0] = 5 # замінюємо від'ємне значення на 5\n\nЗнову візуалізуємо результат:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}_filtered.jpg')      # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 7.6: Динаміка щоденних змін індексу сирої нафти WTI (із видаленими NaN та заміненим від’ємним значенням)\n\n\n\n\nОстаннє, що нам залишається, це приведення вихідного ряду до прибутковостей. Для цього визначимо функцію transformation() та знайдемо з її допомогою прибутковості:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nwti_ret = transformation(signal, ret_type) # знаходимо прибутковості \nwti_length = len(wti_ret)                  # визначаємо довжину прибутковостей\n\nЯк вже зазначалося, при описі процедури MFDFA, ми будемо послуговуватись для порівняння і монофрактальними сигналами. Для подальших розрахунків згенеруємо сигнал білого та рожевого шумів. У цьому нам може допомогти функція signal_noise() бібліотеки neurokit2. Ця функція генерує чистий гаусовий (1/f)**beta шум. Спектр потужності згенерованого шуму пропорційний S(f) = (1/f)**beta. Було описано наступні категорії шуму:\n\nфіолетовий шум: beta = -2\nсиній шум: beta = -1\nбілий шум: beta = 0\nфлікер / рожевий шум: beta = 1\nкоричневий шум: beta = 2\n\nЇї синтаксис виглядає наступним чином:\nsignal_noise(duration=10, sampling_rate=1000, beta=1, random_state=None)\nПараметри\n\nduration (float) — бажана тривалість (у секундах).\nsampling_rate (int) — бажана частота дискретизації (у Гц, тобто відліків на секунду).\nbeta (float) — експонента шуму.\nrandom_state (None, int, numpy.random.RandomState або numpy.random.Generator) — початкове значення (зерно) для генератора випадкових чисел.\n\nПовертає\n\nnoise (array) — сигнал чистого шуму.\n\nТепер можемо згенерувати 2 види шумів:\n\nwhite_noise = nk.signal_noise(duration=wti_length, # генеруємо білий шум \n                              sampling_rate=1, \n                              beta=0, \n                              random_state=123)\n\npink_noise = nk.signal_noise(duration=wti_length,  # генеруємо рожевий шум \n                              sampling_rate=1, \n                              beta=1, \n                              random_state=123)\n\nІ тепер приступаємо до подальшої роботи.\n\n7.1.4.1 Шум і випадкові блукання у часовому ряді\nМультифрактальний аналіз детрендованих флуктуацій базується на класичному аналізі детрендованих флуктуацій (DFA). Класичний DFA застосовується до часових рядів зі структурою, подібною до випадкових блукань. Однак більшість фінансових часових рядів мають коливання, які більш схожі на прирости випадкових блукань. Якщо фінансовий часовий ряд має структуру, подібну до шуму, як у прибутковостей, його слід перетворити на випадково-блукаючий часовий ряд перед застосуванням DFA. Шуми можна перетворити на випадкові блукання шляхом віднімання середнього значення та інтегрування часового ряду (знаходження його кумулятивної суми). Часовий ряд з білим шумом, монофрактал (рожевий шум) і мультифрактал є шумовими часовими рядами і перетворюються на випадкові блукання за допомогою коду, що наведеного нижче:\n\nRW1 = np.cumsum(white_noise-np.mean(white_noise)) # випадкове блукання білого шуму\nRW2 = np.cumsum(pink_noise-np.mean(pink_noise))   # випадкове блукання монофракталу\nRW3 = np.cumsum(wti_ret-np.mean(wti_ret))         # випадкове блукання нафти\n\n\nfig, ax = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n\nax[0].plot(time_ser.index[1:], wti_ret)\nax[0].plot(time_ser.index[1:], RW3, 'r')\nax[0].grid(False)\nax[0].margins(x=0)\nax[0].set_title('Мультифрактальний часовий ряд')\n\nax[1].plot(time_ser.index[1:], pink_noise, label='Шумоподібний часовий ряд')\nax[1].plot(time_ser.index[1:], RW2, 'r', label='Випадкове блукання')\nax[1].grid(False)\nax[1].margins(x=0)\nax[1].set_title('Монофрактальний часовий ряд')\nax[1].legend()\n\nax[2].plot(time_ser.index[1:], white_noise)\nax[2].plot(time_ser.index[1:], RW1, 'r')\nax[2].grid(False)\nax[2].margins(x=0)\nax[2].set_title('Білий шум')\n\nplt.show();\n\n\n\n\nРис. 7.7: Мультифрактальний (верхня панель), монофрактальний (середня панель) та подібний до білого шуму (нижня панель) часові ряди\n\n\n\n\n\n\n7.1.4.2 Обчислення середньоквадратичної варіації часового ряду\nТрадиційний аналіз варіації часових рядів полягає в обчисленні середнього значення варіації як середньоквадратичного відхилення. Читач може скористатися наведеним нижче кодом для обчислення середньоквадратичного відхилення для часових рядів з білим шумом, монофрактальних і мультифрактальних даних:\n\nRMS_ordinary = np.sqrt(np.mean(white_noise**2))    # середньоквадратична варіація білого шуму\nRMS_monofractal = np.sqrt(np.mean(pink_noise**2))  # середньоквадратична варіація монофрактала\nRMS_multifractal = np.sqrt(np.mean(wti_ret**2))    # середньоквадратична варіація мультифрактала\n\n\nfig, ax = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n\nax[0].plot(time_ser.index[1:], wti_ret, label='Шумоподібний часовий ряд')\nax[0].axhline(y=np.mean(wti_ret), c='r', linestyle='--', label='Середнє')\nax[0].axhline(y=np.mean(wti_ret)+RMS_multifractal, c='r', linestyle='-', label='+/- 1 RMS')\nax[0].axhline(y=np.mean(wti_ret)-RMS_multifractal, c='r', linestyle='-')\nax[0].set_ylim(-20, 20)\nax[0].grid(False)\nax[0].margins(x=0)\nax[0].set_title('Мультифрактальний часовий ряд')\n\nax[1].plot(time_ser.index[1:], pink_noise)\nax[1].axhline(y=np.mean(pink_noise), c='r', linestyle='--')\nax[1].axhline(y=np.mean(pink_noise)+RMS_monofractal, c='r', linestyle='-')\nax[1].axhline(y=np.mean(pink_noise)-RMS_monofractal, c='r', linestyle='-')\nax[1].grid(False)\nax[1].margins(x=0)\nax[1].set_title('Монофрактальний часовий ряд')\n\nax[2].plot(time_ser.index[1:], white_noise)\nax[2].axhline(y=np.mean(white_noise), c='r', linestyle='--')\nax[2].axhline(y=np.mean(white_noise)+RMS_ordinary, c='r', linestyle='-')\nax[2].axhline(y=np.mean(white_noise)-RMS_ordinary, c='r', linestyle='-')\nax[2].grid(False)\nax[2].margins(x=0)\nax[2].set_title('Білий шум')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center')\n\nplt.show();\n\n\n\n\nРис. 7.8: Мультифрактальни (верхня панель), монофрактальний (середня панель) та подібний до білого шуму (нижня панель) часові ряди з нульовим середнім значенням (червона штрихова лінія) і \\(\\pm\\) RMS (червона суцільна лінія)\n\n\n\n\nРис. 7.8 ілюструє, що середня амплітуда варіації (тобто середньоквадратичне відхилення) є однаковою для всіх трьох часових рядів — білого шуму, монофрактального та мультифрактального, навіть якщо вони мають досить різну структуру. MFDFA може розрізняти ці структури, як ми побачимо в наступних розділах.\n\n\n7.1.4.3 Локальна середньоквадратична варіація часового ряду\nМультифрактальні часові ряди на верхній панелі мають локальні флуктуації як з великими, так і з малими величинами. Середньоквадратичне відхилення (RMS) у попередньому коді можна обчислити для сегментів часового ряду, щоб розрізнити величини локальних флуктуацій. Проста процедура полягає в тому, щоб розділи часовий ряд на однакові за розміром сегменти, що не перекриваються, і обчислити локальне середнє квадратичне відхилення для кожного сегмента. Це можна зробити за допомогою коду наведеного нижче, і це є основною MFDFA:\n\ndef calc_rms(arr, scale=1335, m=1):\n    \n    # симулюємо випадкове блукання (X)\n    X = np.cumsum(arr - np.mean(arr))\n\n    # транспонуємо значення X \n    X = X.T                           \n\n    # визначаємо довжини сегментів \n    scale = scale\n    \n    # визначаємо порядок полінома\n    m = m\n\n    # визначаємо кількість сегментів\n    segments = np.floor(len(X) / scale).astype(int)\n\n    Index = {}  # словник індексів значень\n    fit = {}    # словник для збереження отриманих поліноміальних кривих\n                # для кожного сегмента\n    RMS = []    # список середньоквадратичних відхилень\n\n    for v in range(segments+1):       # проходимо по кожному сегменту\n        Idx_start = v * scale         # визначаємо початкове значення сегмента                                           \n        Idx_stop = (v + 1) * scale    # визначаємо кінцеве значення\n        \n        # формуємо масив індексів значень досліджуваного сегмента\n        Index[v] = np.arange(Idx_start, min(Idx_stop, len(X)))  \n\n        # вилучаємо значення по індексам\n        X_Idx = X[Index[v]]                       \n\n        # визначаємо поліноміальні коефіцієнти порядку m\n        C = np.polyfit(Index[v], X_Idx, m) \n\n        # будуємо поліноміальну криву по визначеним коефіцієнтам\n        fit[v] = np.polyval(C, Index[v])         \n                 \n        # визначаємо варіацію ряду навколо поліноміального тренда\n        RMS.append(np.sqrt(np.mean((X_Idx - fit[v]) ** 2)))  \n\n    return fit, RMS, Index, X\n\nПерший рядок коду функції calc_rms() перетворює зашумлений часовий ряд, мультифрактал, на часовий ряд випадкового блукання \\(X\\). Третій рядок коду задає масштаб параметра, який визначає розмір вибірки сегментів, що не перетинаються, для яких обчислюється локальне середнє квадратичне відхилення, RMS. П’ятий рядок — це кількість сегментів, на які можна розбити часовий ряд \\(X\\), де len(X) — розмір вибірки часового ряду \\(X\\). Таким чином, segments = 6 при len(X) = 9335 і scale = 1335. З дев’ятого по шістнадцятий рядок — це цикл, який обчислює локальне середньоквадратичне значення навколо тренду fit[v] для кожного сегмента, оновлюючи часовий індекс. У першому циклі v = 0, Index[0] переходить від 0 до 1335 значення сегмента (не включно). У другому циклі \\(v = 1\\), Index[1] переходить від 1335 до 2670 значення другого сегмента. В останньому циклі \\(v = 6\\), Index[6] переходить від 8010-го до 9345-го значення (не включно).\n\n\n7.1.4.4 Локальне детрендування часового ряду\nУ складних системах наявні повільні мінливі тренди, тому для кількісної оцінки масштабо-інваріантності флуктуацій навколо цих трендів необхідно провести детрендування сигналу. У попередньому коді до \\(X\\) на кожному сегменті \\(v\\) підбирається поліноміальний тренд fit[v]. Параметр \\(m\\) визначає порядок полінома. Поліноміальний тренд є лінійним, якщо \\(m = 1\\), квадратичним, якщо \\(m = 2\\), і кубічним, якщо \\(m = 3\\). Рядок C = np.polyfit(Index[v], X[Index[v]], m) визначає коефіцієнти полінома C, які використовуються для створення поліноміальної залежності тренду fit[v] для кожного сегмента. Потім для залишкової варіації, X(Index[v])-fit[v], обчислюється локальне середньоквадратичне відхилення, RMS[v], в межах кожного сегмента \\(v\\). Локальна середньоквадратична варіація, RMS[v], представлено на наступному рисунку як відстань між червоними пунктирними трендами і червоними суцільними лініями.\n\nfit_1, RMS_1, Index_1, X = calc_rms(wti_ret, scale=1335, m=1) # оцінка локального відхилення для мультифрактала\nfit_2, RMS_2, Index_2, X = calc_rms(wti_ret, scale=1335, m=2) # оцінка локального відхилення для монофрактала\nfit_3, RMS_3, Index_3, X = calc_rms(wti_ret, scale=1335, m=3) # оцінка локального відхилення для білого шуму\n\n\nfig, ax = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n\n\nax[0].plot(time_ser.index[1:], X)\nfor v in list(fit_1.keys()):\n    ax[0].plot(time_ser.index[Index_1[v]], fit_1[v], 'r--')\n    ax[0].plot(time_ser.index[Index_1[v]], fit_1[v]+RMS_1[v], c='r', linestyle='-')\n    ax[0].plot(time_ser.index[Index_1[v]], fit_1[v]-RMS_1[v], c='r', linestyle='-')\n\nax[0].grid(False)\nax[0].margins(x=0)\nax[0].set_title('Лінійне детрендування ' + r'$(m=1)$')\n\n\n\nax[1].plot(time_ser.index[1:], X, label='Випадкове блукання мультифрактального сигналу')\nfor v in list(fit_2.keys()):\n    if v == 1:\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v], 'r--', label='Локальний тренд')\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v]+RMS_2[v], c='r', linestyle='-', label='+/- 1 RMS')\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v]-RMS_2[v], c='r', linestyle='-')\n    else:\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v], 'r--')\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v]+RMS_2[v], c='r', linestyle='-')\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v]-RMS_2[v], c='r', linestyle='-')\n\nax[1].grid(False)\nax[1].margins(x=0)\nax[1].set_title('Квадратичне детрендування ' + r'$(m=2)$')\n\n\n\nax[2].plot(time_ser.index[1:], X)\nfor v in list(fit_3.keys()):\n    ax[2].plot(time_ser.index[Index_3[v]], fit_3[v], 'r--')\n    ax[2].plot(time_ser.index[Index_3[v]], fit_3[v]+RMS_3[v], c='r', linestyle='-')\n    ax[2].plot(time_ser.index[Index_3[v]], fit_3[v]-RMS_3[v], c='r', linestyle='-')\nax[2].grid(False)\nax[2].margins(x=0)\nax[2].set_title('Кубічне детрендування ' + r'$(m=3)$')\n\nhandles, labels = ax[1].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center')\n\nplt.show();\n\n\n\n\nРис. 7.9: Обчислення локальних флуктуацій, RMS, навколо лінійного, квадратичного та кубічного трендів за допомогою функції calc_rms() (\\(m = 1\\), \\(m = 2\\) та \\(m = 3\\), відповідно). Червона пунктирна лінія — це підігнаний тренд, fit[v], у шести сегментах вибірки розміром 1335. Відстань між червоним штриховим трендом і суцільними червоними лініями становить \\(\\pm\\) RMS. Локальні коливання, RMS, поблизу поліноміальних трендів є основою аналізу детрендованих флуктуацій\n\n\n\n\n\n\n7.1.4.5 Монофрактальний аналіз детрендованих флуктуацій\nУ DFA варіації локального середньоквадратичного відхилення кількісно оцінюються загальним середньоквадратичним відхиленням (\\(F\\)).\nШвидкі коливання часового ряду \\(X\\) впливатимуть на загальне середньоквадратичне відхилення, \\(F\\), у сегментах малої довжини (масштабу), тоді як повільні коливання впливатимуть на \\(F\\) у сегментах великої довжини (масштабу). Таким чином, функція флуктуацій, \\(F\\), повинна бути обчислена для декількох масштабів, щоб виокремити вплив як швидкоплинних, так і повільних коливань, які у свою чергу визначають структурні перетворення часового ряду. Функція флуктуацій \\(F(ns)\\) може бути обчислена для декількох масштабів шляхом модифікації попереднього коду:\n\ndef calc_F(arr, scale, m=1):\n    \n    X = np.cumsum(arr - np.mean(arr)) # симулюємо випадкове блукання (X)\n    X = X.T                           # транспонуємо значення X\n\n    scale = scale\n    m = m\n    segments = np.zeros(len(scale), dtype=int)\n    F = np.zeros(len(scale))\n\n    Index = {}  # словник індексів значень\n    fit = {}    # словник для збереження отриманих поліноміальних кривих\n                # для кожного сегмента\n    RMS = {}    # словник середньоквадратичних відхилень\n\n    for ns in range(len(scale)):\n        segments[ns] = np.floor(len(X) / scale[ns]).astype(int)\n        RMS[ns] = np.zeros(segments[ns])\n\n        for v in range(segments[ns]):         # проходимо по кожному сегменту\n            # визначаємо початкове значення сегмента\n            Idx_start = v * scale[ns]  \n                       \n            # визначаємо кінцеве значення\n            Idx_stop = (v + 1) * scale[ns] if v &lt; segments[ns] - 1 else len(X)    \n            \n            # формуємо масив індексів значень досліджуваного сегмента\n            Index[v, ns] = np.arange(Idx_start, Idx_stop)  \n\n            # вилучаємо значення по індексам\n            X_Idx = X[Index[v, ns]]                       \n\n            # визначаємо поліноміальні коефіцієнти порядку m\n            C = np.polyfit(Index[v, ns], X_Idx, m) \n            \n            # будуємо поліноміальну криву по визначеним коефіцієнтам\n            fit[v, ns] = np.polyval(C, Index[v, ns])  \n\n            # оцінюємо середньоквадратичне відхилення для фрагмента v на масштабі ns \n            RMS[ns][v] = np.sqrt(np.mean((X_Idx - fit[v, ns]) ** 2)) \n\n        # оцінюємо загальне середньоквадратичне відхилення в межах масштабу ns\n        F[ns] = np.sqrt(np.mean(RMS[ns] ** 2))\n\n    return F, RMS, Index, X\n\n\nscales = [16, 32, 64, 128, 256, 512, 1024][::-1]\nF, RMS, Index, X = calc_F(wti_ret, scale=scales) # оцінка узагальненої функції флуктуацій по різним масштабам\n\n\nfig, ax = plt.subplots(len(scales), 1, figsize=(10, 8), sharex=True)\n\nfor scale, val in enumerate(scales):\n    l = [Index[val] for val in Index.keys() if (val[1] == scale)]\n\n    x = np.array([])\n    for v in l:\n        x = np.concatenate([x, v])\n\n    y = np.array([])\n    for idx, v in enumerate(l): \n        y = np.concatenate([y, RMS[scale][idx]*np.ones(len(v))])\n\n    if scales[scale] == 16:\n        ax[scale].plot(time_ser.index[1:], y, c='b', label=\"Локальні флуктуацій: RMS\")\n        ax[scale].axhline(y=F[scale], c='r', linestyle='-', label=r\"RMS локальних флуктуацій: $F$\")\n        ax[scale].grid(False)\n        ax[scale].set_title(f\"Масштаб = {scales[scale]}\")\n        ax[scale].margins(x=0)\n    else: \n        ax[scale].plot(time_ser.index[1:], y, c='b')\n        ax[scale].axhline(y=F[scale], c='r', linestyle='-')\n        ax[scale].grid(False)\n        ax[scale].set_title(f\"Масштаб = {scales[scale]}\")\n        ax[scale].margins(x=0)       \n\nhandles, labels = ax[-1].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper left', fontsize=12)\n\nfig.tight_layout(pad=0.1)\nplt.show();\n\n\n\n\nРис. 7.10: Локальні флуктуації, RMS[ns], обчислені для сегментів із різними масштабами. Функція флуктуацій F[ns] є загальним середньоквадратичним відхиленням локальних коливань RMS[ns]. Зверніть увагу, що F[ns] зменшується на менших масштабах\n\n\n\n\nDFA визначає монофрактальну структуру часового ряду відповідно до степеневої залежність між загальним середнім квадратичним відхиленням (тобто \\(F\\)), обчисленим для декількох масштабів. Степенева залежність між загальним середнім квадратичним відхиленням позначається нахилом (\\(H\\)) лінії регресії, розрахованим за допомогою коду, що представлений нижче:\n\nC = np.polyfit(np.log(scales), np.log(F), 1)\nH = C[0]\nRegLine = np.polyval(C, np.log(scales))\n\nМодифікуємо попередній код, додавши нові фрагменти:\n\ndef calc_H(arr, scale, m=1):\n    \n    X = np.cumsum(arr - np.mean(arr)) # симулюємо випадкове блукання (X)\n    X = X.T                           # транспонуємо значення X\n\n    scale = scale\n    m = m\n    segments = np.zeros(len(scale), dtype=int)\n    F = np.zeros(len(scale))\n\n    Index = {}  # словник індексів значень\n    fit = {}    # словник для збереження отриманих поліноміальних кривих\n                # для кожного сегмента\n    RMS = {}    # словник середньоквадратичних відхилень\n\n    for ns in range(len(scale)):\n        segments[ns] = np.floor(len(X) / scale[ns]).astype(int)\n        RMS[ns] = np.zeros(segments[ns])\n\n        for v in range(segments[ns]):         # проходимо по кожному сегменту\n            # визначаємо початкове значення сегмента\n            Idx_start = v * scale[ns]  \n                       \n            # визначаємо кінцеве значення\n            Idx_stop = (v + 1) * scale[ns] if v &lt; segments[ns] - 1 else len(X)    \n            \n            # формуємо масив індексів значень досліджуваного сегмента\n            Index[v, ns] = np.arange(Idx_start, Idx_stop)  \n\n            # вилучаємо значення по індексам\n            X_Idx = X[Index[v, ns]]                       \n\n            # визначаємо поліноміальні коефіцієнти порядку m\n            C = np.polyfit(Index[v, ns], X_Idx, m) \n            \n            # будуємо поліноміальну криву по визначеним коефіцієнтам\n            fit[v, ns] = np.polyval(C, Index[v, ns])  \n\n            # оцінюємо середньоквадратичне відхилення для фрагмента v на масштабі ns \n            RMS[ns][v] = np.sqrt(np.mean((X_Idx - fit[v, ns]) ** 2)) \n\n        # оцінюємо загальне середньоквадратичне відхилення в межах масштабу ns\n        F[ns] = np.sqrt(np.mean(RMS[ns] ** 2))\n\n    # знаходимо коефіцієнти рівняння прямої \n    C = np.polyfit(np.log(scale), np.log(F), 1) \n    \n    # беремо кут нахилу прямої в якості показника Херста\n    H = C[0]\n\n    # будуємо саме рівняння\n    RegLine = np.polyval(C, np.log(scale))\n\n    return H, RegLine, F\n\nТепер розглянемо залежність загальної функції флуктуацій \\(F\\) від різних довжин (масштабів) локальних сегментів ряду для досліджуваних нами рядів:\n\nscmin = 16\nscmax = 1024\nscres = 19\nexponents = np.linspace(np.log(scmin), np.log(scmax), scres)\n\nscales_exp = np.round(np.exp(1)**exponents).astype(int)\n\nH_multifrac, RegLine_multifrac, F_multifrac = calc_H(wti_ret, scale=scales_exp, m=1)\nH_monofrac, RegLine_monofrac, F_monofrac = calc_H(pink_noise, scale=scales_exp, m=1)\nH_white_noise, RegLine_white_noise, F_white_noise = calc_H(white_noise, scale=scales_exp, m=1)\n\n\nfig, ax = plt.subplots(1, 1)\n\nax.set_xscale('log')\nax.set_yscale('log')\nax.scatter(scales_exp, F_multifrac, \n           label=fr\"Мультифрактальний ряд ($H$={H_multifrac:.2f})\", \n           color='darkblue')\nplt.plot(scales_exp, np.exp(RegLine_multifrac),  color='darkblue')\n\nax.scatter(scales_exp, F_monofrac, \n           label=fr\"Монофрактальний ряд ($H$={H_monofrac:.2f})\", \n           color='magenta')\nplt.plot(scales_exp, np.exp(RegLine_monofrac), color='magenta')\n\n\nax.scatter(scales_exp, F_white_noise, \n           label=fr\"Білий шум ($H$={H_white_noise:.2f})\", \n           color='red')\nplt.plot(scales_exp, np.exp(RegLine_white_noise), color='red')\n\nax.set_xlabel(r'$\\log{ns}$')\nax.set_ylabel(r\"$\\log{F(ns)}$\")\n\nplt.legend(fontsize=12)\n\nfig.tight_layout()\nplt.show();\n\n\n\n\nРис. 7.11: Графік залежності загального середньоквадратичного відхилення (тобто функції флуктуацій \\(F\\)) від масштабу, де і \\(F\\), і масштаб представлені в логарифмічних координатах. Масштабно-інваріантна залежність позначається нахилом, \\(H\\), ліній регресії. Нахил, \\(H\\), є показником степеневого закону, який називається показником Херста, оскільки \\(F\\) і масштаб представлені в логарифмічних координатах\n\n\n\n\nЯк уже зазначалося в попередній лабораторній роботі, кут нахилу \\(H\\) ліній регресії називається показником Херста. Показник Херста визначає монофрактальну структуру часового ряду, показуючи, наскільки швидко зростає загальне середньоквадратичне відхилення, \\(F\\), локальних коливань, RMS, зі збільшенням розміру локальних сегментів ряду (тобто, масштабу). Рис. 7.11 показує, що загальне середньоквадратичне значення локальних флуктуацій, \\(F\\), зростає швидше зі збільшенням розміру вибірки сегментів для монофрактального рожевого шуму порівняно з індексом нафти та білим шумом. Більший показник Херста, \\(H\\), візуально свідчить про повільнішу еволюцію варіацій (тобто більш стійку структуру) в рожевому шумі порівняно з нафтою та білим шумом. Рис. 7.12 ілюструє, що показник Херста визначає континуум між часовими рядами, подібними до шуму, і часовими рядами, подібними до випадкового блукання. Показник Херста знаходиться в інтервалі від 0 до 1 для часових рядів, подібних до шуму, тоді як для часових рядів, подібних до випадкових блукань, він перевищує 1. Часовий ряд має довгострокову залежну (тобто корельовану) структуру, коли показник Херста знаходиться в інтервалі 0.5-1, і антикорельовану структуру, коли показник Херста знаходиться в інтервалі 0-0.5. Часовий ряд має незалежну або короткострокову залежну структуру в окремому випадку, коли показник Херста дорівнює 0.5. Згідно з попереднім рисунком, часові ряди білого шуму та нафти представляються непередбачуваними, оскільки показник Херста близький до 0.5, тоді як рожевий шум довгостроково залежну структуру з показником Херста близьким до 1.\n\nbetas = np.linspace(0.0, 2.0, 12)[::-1]\nscmin = 16\nscmax = 1024\nscres = 19\nexponents = np.linspace(np.log(scmin), np.log(scmax), scres)\nscales_exp = np.round(np.exp(1)**exponents).astype(int)\n\n\ncolor = iter(plt.cm.rainbow(np.linspace(0, 1, len(betas))))\n\nfig, ax = plt.subplots(len(betas), 1, figsize=(10, 8), sharex=True)\n\nfor idx, beta in enumerate(betas):\n\n    noise = nk.signal_noise(duration=wti_length,  # генеруємо шум із різними значеннями beta \n                              sampling_rate=1, \n                              beta=beta, \n                              random_state=123)   \n\n    H_noise, _, _ = calc_H(arr=noise, scale=scales_exp, m=1)\n\n    c = next(color)\n    ax[idx].plot(np.arange(len(noise)), noise, label=fr\"$H$ = {H_noise:.2f}\", c=c)\n    ax[idx].grid(False)\n    ax[idx].legend(loc=\"upper left\")\n    ax[idx].margins(x=0)\n\nfig.subplots_adjust(hspace=0)\n\nplt.show();\n\n\n\n\nРис. 7.12: Діапазон показників Херста визначає континуум фрактальних структур між білим шумом (\\(Н = 0.5\\)) і коричневим шумом (\\(H = 1.5\\)). Рожевий шум \\(H = 1\\) розділяє шуми \\(H &lt; 1\\), які мають більш помітні швидкі флуктуації, і випадкові блукання \\(H &gt; 1\\), які мають більш помітні повільні флуктуації\n\n\n\n\n\n\n7.1.4.6 Мультифрактальний аналіз детрендованих флуктуацій\nСтруктури монофрактального та мультифрактального часових рядів відрізняється, хоча вони мають схожі загальні середньоквадратичні значення. Мультифрактальні часові ряди мають локальні флуктуації як з екстремально малими, так і з екстремально великими значеннями, що не характерно монофрактальним часовим рядам. Відсутність флуктуацій з екстремально великими та малими значеннями призводить до нормального розподілу для монофрактального часового ряду, де варіація описується лише статистичним моментом другого порядку (тобто дисперсією). Отже, монофрактальний DFA базується на статистиці другого порядку загального середньоквадратичного відхилення (тобто, \\(F\\)). У мультифрактальному часовому ряді локальні коливання, RMS[ns][v], будуть екстремально великими для сегментів \\(v\\) в межах часових періодів великих коливань і екстремально малими для сегментів \\(v\\) в межах часових періодів малих коливань. Отже, мультифрактальні часові ряди не є нормально розподіленими і слід враховувати всі статистичні моменти \\(q\\)-го порядку. Таким чином, необхідно розширити загальне середньоквадратичне значення монофрактального DFA (тобто \\(F\\)) до середньоквадратичної функції флуктуацій \\(q\\)-го порядку мультифрактального DFA (\\(F_{q}\\)):\n\ndef calc_Fq(arr, scale, q, m=1):\n    \n    X = np.cumsum(arr - np.mean(arr)) # симулюємо випадкове блукання (X)\n    X = X.T                           # транспонуємо значення X\n\n    scale = scale \n    qs = q\n    m = m\n    segments = np.zeros(len(scale), dtype=int)\n    Fq = np.zeros((len(qs), len(scale)))\n    Index = {}\n    RMS = {}    # словник локальних середньоквадратичних відхилень\n    fit = {}    # словник для збереження отриманих поліноміальних кривих\n                # для кожного сегмента\n    qRMS = {}   # словник локальних відхилень зважених показником q\n\n    for ns in range(len(scale)):\n        segments[ns] = np.floor(len(X) / scale[ns]).astype(int)\n        RMS[ns] = np.zeros(segments[ns])\n\n        # проходимо по кожному сегменту\n        for v in range(segments[ns]): \n\n            # визначаємо початкове значення сегмента\n            Idx_start = v * scale[ns]  \n                       \n            # визначаємо кінцеве значення\n            Idx_stop = (v + 1) * scale[ns] if v &lt; segments[ns] - 1 else len(X)    \n            \n            # формуємо масив індексів значень досліджуваного сегмента\n            Index[v] = np.arange(Idx_start, Idx_stop)  \n\n            # вилучаємо значення по індексам\n            X_Idx = X[Index[v]]                       \n\n            # визначаємо поліноміальні коефіцієнти порядку m\n            C = np.polyfit(Index[v], X_Idx, m) \n            \n            # будуємо поліноміальну криву по визначеним коефіцієнтам\n            fit = np.polyval(C, Index[v])  \n\n            # оцінюємо середньоквадратичне відхилення для фрагмента v на масштабі ns \n            RMS[ns][v] = np.sqrt(np.mean((X_Idx - fit) ** 2)) \n        \n        # приводимо q значення до типу float\n        qs = np.asarray_chkfinite(qs, dtype=float)\n\n        # для мультифрактальності\n        # ----------------------------\n        for nq, qval in enumerate(qs):\n            if (qval != 0.): \n                qRMS[nq, ns] = RMS[ns] ** q[nq]\n                Fq[nq, ns] = np.mean(qRMS[nq, ns]) ** (1 / q[nq])\n            else:\n                Fq[nq, ns] = np.exp(0.5 * np.mean(np.log(RMS[ns] ** 2)))\n        # ----------------------------\n\n    return Fq, qRMS, Index\n\nУ новому блоці коду запускається цикл, який обчислює загальне середньоквадратичне значення \\(q\\)-порядку, \\(F_{q}(nq)\\), від від’ємних до додатних \\(q\\). Порядок \\(q\\) зважує вплив сегментів ряду з великими та малими коливаннями, RMS, як показано на наступному рисунку. На \\(F_{q}(nq)\\) для від’ємних \\(q\\) впливають сегменти \\(v\\) з малими RMS(v). Навпаки, на \\(F_{q}(nq)\\) для додатних \\(q\\) впливають відрізки \\(v\\) з великими RMS(v). Локальні флуктуації RMS з великими та малими величинами класифікуються за величиною від’ємного або додатного порядку \\(q\\) відповідно. На \\(F_{q}\\) для \\(q = -3\\) і \\(3\\) більше впливають відрізки \\(v\\) з найменшим і найбільшим RMS(v), відповідно, порівняно з \\(F_{q}\\) для \\(q = -1\\) і \\(1\\). Середня точка \\(q = 0\\) є нейтральною до впливу відрізків з малим та великим RMS. Зверніть увагу, що в останньому рядку коду нового блоку перевизначено окремий випадок \\(q(nq) = 0\\), оскільки \\(1/0\\) прямує до нескінченності (тобто, \\(1/q(q = 0) = \\infty\\)). Читач також повинен помітити, що \\(F_{q}[q == 2]\\) дорівнює статистиці другого порядку \\(F\\), оскільки \\(\\sqrt{x} = x^{1/2}\\). Монофрактальний DFA тепер розширюється до MFDFA.\n\nscales = np.array([32])\nnq = np.array([-3, -1, 1, 3])\n\nFq, qRMS, Index = calc_Fq(wti_ret, scale=scales, q=nq, m=1)\nFq_pink, qRMS_pink, Index = calc_Fq(pink_noise, scale=scales, q=nq, m=1)\n\n\nfig, ax = plt.subplots((len(nq)+1), 1, figsize=(12, 10), sharex=True)\n\nax[0].plot(time_ser.index[1:], wti_ret, label=\"Мультифрактал\")\nax[0].plot(time_ser.index[1:], pink_noise, label=\"Монофрактал\")\nax[0].grid(False)\nax[0].margins(x=0)\nax[0].legend(loc='upper left', fontsize=12)\nax[0].get_xaxis().set_visible(False)\n\n\nfor idx in range(1, len(nq)+1):\n    l = [Index[val] for val in Index.keys()]\n\n    x = np.array([])\n    for v in l:\n        x = np.concatenate([x, v])\n\n    y = np.array([])\n    y_pink = np.array([])\n    for i, v in enumerate(l): \n        y = np.concatenate([y, qRMS[(idx-1, 0)][i]*np.ones(len(v))])\n        y_pink = np.concatenate([y_pink, qRMS_pink[(idx-1, 0)][i]*np.ones(len(v))])\n    \n    ax[idx].set_title(fr\"Локальні варіації для {scales[0]}-го масштабу при $q=${nq[idx-1]}\", fontsize=12)\n    ax[idx].plot(time_ser.index[1:], y)\n    ax[idx].plot(time_ser.index[1:], y_pink)\n    ax[idx].grid(False)\n    ax[idx].margins(x=0)       \n\nhandles, labels = ax[0].get_legend_handles_labels()\n\nplt.show();\n\n\n\n\nРис. 7.13: Ілюстрація залежності локальних флуктуацій qRMS від \\(q\\) при масштабі 32. qRMS — це \\(q\\)-порядок локальних флуктуацій (тобто, RMS) і є складовою частиною загального \\(q\\)-порядку RMS (тобто, $F_{q}). qRMS представлено для монофрактального (зелена смуга) та мультифрактальних (синя смуга) часових рядів. Від’ємний порядок \\(q\\) (\\(q = -3\\) і \\(-1\\)) підсилює сегменти в мультифрактальному часовому ряді з екстремально малими RMS, тоді як додатний порядок \\(q\\) (\\(q = 3\\) і \\(1\\)) підсилює відрізки з екстремально великими RMS. Зверніть увагу, що \\(q = -3\\) і \\(q = 3\\) підсилюють малу і велику варіацію відповідно більше, ніж \\(q = -1\\) і \\(q = 1\\). Зауважте також, що монофрактальний часовий ряд не має відрізків з екстремально великими або малими коливаннями і, таким чином, не має піків у qRMS. Загальне середньоквадратичне відхилення \\(q\\)-го порядку здатне розрізняти структуру малих і великих флуктуацій і, відповідно, монофрактальних і мультифрактальних часових рядів\n\n\n\n\nТепер можна визначити показники Херста \\(q\\)-го порядку як нахили (\\(h(q)\\)) ліній регресії для кожного середньоквадратичного значення \\(q\\)-го порядку (\\(F_{q}\\)). І \\(h(q)\\), і лінія регресії визначаються в циклі для кожного \\(q\\)-го порядку:\n\ndef calc_Hq(arr, scale, q, m=1):\n    \n    X = np.cumsum(arr - np.mean(arr)) # симулюємо випадкове блукання (X)\n    X = X.T                           # транспонуємо значення X\n\n    scale = scale \n    qs = q\n    m = m\n    segments = np.zeros(len(scale), dtype=int) \n    Fq = np.zeros((len(qs), len(scale)))       # масив для збереження загальної функції флуктуацій \n    hq = np.zeros(len(qs), dtype=float)        # масив для збереження Херста q-го порядку\n    qRegLine = {} # словник для збереження ліній регресій\n    Index = {}    # словник для збереження індексів сегментів ряду\n    RMS = {}      # словник локальних середньоквадратичних відхилень\n    fit = {}      # словник для збереження отриманих поліноміальних кривих\n                  # для кожного сегмента\n    qRMS = {}     # словник локальних відхилень зважених показником q\n\n    for ns in range(len(scale)):\n        segments[ns] = np.floor(len(X) / scale[ns]).astype(int)\n        RMS[ns] = np.zeros(segments[ns])\n\n        # проходимо по кожному сегменту\n        for v in range(segments[ns]): \n\n            # визначаємо початкове значення сегмента\n            Idx_start = v * scale[ns]  \n                       \n            # визначаємо кінцеве значення\n            Idx_stop = (v + 1) * scale[ns] if v &lt; segments[ns] - 1 else len(X)    \n            \n            # формуємо масив індексів значень досліджуваного сегмента\n            Index[v] = np.arange(Idx_start, Idx_stop)  \n\n            # вилучаємо значення по індексам\n            X_Idx = X[Index[v]]                       \n\n            # визначаємо поліноміальні коефіцієнти порядку m\n            C = np.polyfit(Index[v], X_Idx, m) \n            \n            # будуємо поліноміальну криву по визначеним коефіцієнтам\n            fit = np.polyval(C, Index[v])  \n\n            # оцінюємо середньоквадратичне відхилення для фрагмента v на масштабі ns \n            RMS[ns][v] = np.sqrt(np.mean((X_Idx - fit) ** 2)) \n        \n        # приводимо q значення до типу float\n        qs = np.asarray_chkfinite(qs, dtype=float)\n\n        # для мультифрактальності\n        # ----------------------------\n        for nq, qval in enumerate(qs):\n            if (qval != 0.): \n                qRMS[nq, ns] = RMS[ns] ** q[nq]\n                Fq[nq, ns] = np.mean(qRMS[nq, ns]) ** (1 / q[nq])\n            else:\n                Fq[nq, ns] = np.exp(0.5 * np.mean(np.log(RMS[ns] ** 2)))\n\n        for nq, _ in enumerate(qs): \n            # якщо флуктуації дорів. 0, log2 стикнеться з діленням на 0 \n            old_setting = np.seterr(divide=\"ignore\", invalid=\"ignore\")\n            C = np.polyfit(np.log(scale), np.log(Fq[nq, :]), m)\n            np.seterr(**old_setting)\n            hq[nq] = C[0]\n            qRegLine[nq] = np.polyval(C, np.log(scale))\n        # ----------------------------\n\n    return hq, qRegLine, Fq \n\n\nscmin = 16\nscmax = 1024\nscres = 19\n\nq_min = -5.0\nq_max = 5.0\nq_step = 0.1\n\nnq = np.arange(q_min, q_max+q_step, q_step)\n\nexponents = np.linspace(np.log(scmin), np.log(scmax), scres)\nscales_exp = np.round(np.exp(1)**exponents).astype(int)\n\nHq_multifrac, qRegLine_multifrac, Fq_multifrac = calc_Hq(wti_ret, scale=scales_exp, q=nq, m=1)\nHq_monofrac, qRegLine_monofrac, Fq_monofrac = calc_Hq(pink_noise, scale=scales_exp, q=nq, m=1)\nHq_white_noise, qRegLine_white_noise, Fq_white_noise = calc_Hq(white_noise, scale=scales_exp, q=nq, m=1)\n\n\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\n\nax[0][0].set_title(\"Мультифрактал\")\nax[0][0].set_xlabel(r\"$ns$\")\nax[0][0].set_ylabel(r\"$F_{q}(ns)$\")\nax[0][0].set_xscale('log')\nax[0][0].set_yscale('log')\nfor i in range(len(nq)):\n    ax[0][0].scatter(scales_exp, Fq_multifrac[i, :], color='darkblue')\n    ax[0][0].plot(scales_exp, np.exp(qRegLine_multifrac[i]),  color='darkblue')\n\nax[0][1].set_title(\"Монофрактал\")\nax[0][1].set_xlabel(r\"$ns$\")\nax[0][1].set_xscale('log')\nax[0][1].set_yscale('log')\nfor i in range(len(nq)):\n    ax[0][1].scatter(scales_exp, Fq_monofrac[i, :], color='magenta')\n    ax[0][1].plot(scales_exp, np.exp(qRegLine_monofrac[i]),  color='magenta')\n\nax[1][0].set_title(\"Білий шум\")\nax[1][0].set_xlabel(r\"$ns$\")\nax[1][0].set_ylabel(r\"$F_{q}(ns)$\")\nax[1][0].set_xscale('log')\nax[1][0].set_yscale('log')\nfor i in range(len(nq)):\n    ax[1][0].scatter(scales_exp, Fq_white_noise[i, :], color='red')\n    ax[1][0].plot(scales_exp, np.exp(qRegLine_white_noise[i]),  color='red')\n\nax[1][1].set_title(r\"Показники Херста $q$-го порядку\")\nax[1][1].set_xlabel(r\"$q$\")\nax[1][1].set_ylabel(r\"$h(q)$\")\nax[1][1].plot(nq, Hq_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[1][1].plot(nq, Hq_monofrac,linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[1][1].plot(nq, Hq_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax[1][1].legend()\n\nfig.tight_layout()\nplt.show();\n\n\n\n\nРис. 7.14: Середньоквадратичні значення \\(F_{q}\\) для різних \\(q\\)-их порядків та відповідні лінії регресії обчислені за допомогою MFDFA для мультифракталу, монофракталу та білого шуму\n\n\n\n\nМожемо бачити, що узагальнена функція флуктуацій для мультифракталу залежить не лише від масштабу, але й від \\(q\\), що демонструють різні нахили ліній регресії \\(h(q)\\). Масштабуючі узагальнені функції флуктуацій \\(F_{q}\\) для монофракталу та білого шуму є \\(q\\)-незалежними, оскільки їх лінії регресії для різних масштабів мають один і той самий кут нахилу. Показник Херста \\(q\\)-го порядку \\(h(q)\\) для мультифрактального (синя лінія) ряду представляється незалежним для \\(q&lt;0\\) і змінним для \\(q&gt;0\\). Це вказує на те, що джерелом мультифрактальності нафти є аномально великі флуктуацій як, наприклад, криза коронавірусної пандемії. Для монофракталу (рожева лінія) та білого шуму (червона лінія) \\(h(q)\\) залишаються сталими.\n\n\n7.1.4.7 Мультифрактальний спектр часових рядів\nПоказник Херста \\(q\\)-го порядку \\(h(q)\\) є лише одним з декількох типів масштабних показників, що використовуються для параметризації мультифрактальної структури часових рядів. Як уже було представлено попередньо, ми можемо вивести показник маси \\(q\\)-го порядку (\\(\\tau(q)\\)), а потім через \\(\\tau(q)\\) отримати показник сингулярності \\(q\\)-го порядку (\\(\\alpha(q)\\)) і фрактальну розмірність (\\(f(\\alpha)\\)) флуктуацій (областей) із ступенем сингулярності \\(\\alpha(q)\\). Графік залежності \\(\\alpha(q)\\) від \\(f(\\alpha)\\) представляє мультифрактальний спектр. Показники маси, сингулярності та фрактальності можна обчислити згідно коду, що наведений нижче:\n\ntau_multifrac = nq * Hq_multifrac - 1 \ntau_monofrac = nq * Hq_monofrac - 1 \ntau_white_noise = nq * Hq_white_noise - 1 \n\nalpha_multifrac = np.gradient(tau_multifrac, nq)\nalpha_monofrac = np.gradient(tau_monofrac, nq)\nalpha_white_noise = np.gradient(tau_white_noise, nq)\n\nf_multifrac = nq * alpha_multifrac - tau_multifrac\nf_monofrac = nq * alpha_monofrac - tau_monofrac\nf_white_noise = nq * alpha_white_noise - tau_white_noise\n\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 8))\n\nax[0].set_xlabel(r\"$q$\")\nax[0].set_ylabel(r\"$\\tau(q)$\")\nax[0].plot(nq, tau_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[0].plot(nq, tau_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[0].plot(nq, tau_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax[0].legend()\n\nax[1].set_xlabel(r\"$\\alpha$\")\nax[1].set_ylabel(r\"$f(\\alpha)$\")\nax[1].plot(alpha_multifrac, f_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[1].plot(alpha_monofrac, f_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[1].plot(alpha_white_noise, f_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\n\nax[2].set_xlabel(r\"$q$\")\nax[2].set_ylabel(r\"$f(\\alpha)$\")\nax[2].plot(nq, f_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[2].plot(nq, f_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[2].plot(nq, f_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\n\nfig.tight_layout(pad=0.3) \nplt.show();\n\n\n\n\nРис. 7.15: Множинне представляння мультифрактального спектру для мультифрактала, монофрактала та білого шуму\n\n\n\n\nПоказники сингулярності \\(\\alpha\\) для великих висококонцентрованих флуктуацій малі й розташовані в лівому хвості спектра, тоді як \\(\\alpha\\) для малих флуктуацій великі й розташовані в правому хвості спектра.\nТаким чином, сила мультифрактальності описується великим відхиленням експоненти локальної сингулярності \\(\\alpha\\) від центральної тенденції \\(\\alpha(0)\\). Монофрактальний сигнал — це випадок, коли \\(\\alpha\\) залишається майже константної, і в деяких випадках мультифрактальний спектр зводиться в одну точку за даною \\(\\alpha\\).\nДіапазон \\(\\alpha\\) вказує на різноманітність експонент сингулярності, що описують динаміку системи, а величина \\(f(\\alpha)\\) вказує на величину внеску елементів із відповідним показником \\(\\alpha\\).\nМультифрактальний спектр може характеризуватися різною шириною, що вказує на варіативність процесів, які відбуваються всередині системи. Так само він може бути як симетричним, так і асиметричним. Асиметрія може бути як правосторонньою, так і лівосторонньою, що вказуватиме на різний ступінь впливу висококонцентрованих і низькоконцентрованих елементів (флуктуацій). Мультифрактальний спектр матиме довгий лівий хвіст, коли часовий ряд має мультифрактальну структуру, чутливу до локальних флуктуацій з великими амплітудами. Навпаки, мультифрактальний спектр матиме довгий правий хвіст, коли часовий ряд має мультифрактальну структуру, чутливу до локальних флуктуацій з малими амплітудами.\nПроілюструємо залежність ширини спектра мультифрактальності від ступеня флуктуацій у ряді. Дану залежність будемо демонструвати на прикладі рядів, що розподілятимуться згідно альфа-стабільному розподілу Леві, який ми ще розглядатимемо в наступних роботах. Для генерації випадкових величин із даного розподілу, використовуватимемо модуль scipy.stats. З нього імпортуємо клас levy_stable для використання методу rvs(). Метод приймає показник \\(\\alpha\\), що відповідає за, простіше кажучи, частоту виникнення подій, що виходять за межі нормального розподілу. Розглянемо діапазон таких значень \\(\\alpha\\) та відповідні спектри згенерованих рядів.\n\nfrom scipy.stats import levy_stable\n\nalphas = np.linspace(1.5, 2.0, 7)\nscmin = 16\nscmax = 1024\nscres = 19\n\nq_min = -5.0\nq_max = 5.0\nq_step = 0.1\nnq_levy = np.arange(q_min, q_max+q_step, q_step)\n\nexponents = np.linspace(np.log(scmin), np.log(scmax), scres)\nscales_exp = np.round(np.exp(1)**exponents).astype(int)\n\n\ncolor = iter(plt.cm.plasma(np.linspace(0, 0.8, len(alphas))))\n\nfig = plt.figure(figsize=(12, 10))\nsubfigs = fig.subfigures(1, 2)\nax1 = subfigs[0].subplots(len(alphas), 1, sharex=True)\nax2 = subfigs[1].subplots(1, 1)\n\nfor i in range(len(alphas)):\n    \n    # генеруємо альфа-стабільний процес\n    r = levy_stable.rvs(alpha=alphas[i], beta=0, loc=0, \n                        scale=1, size=len(wti_ret), random_state=123)\n\n    Hq_levy, qRegLine_levy, Fq_levy = calc_Hq(r, scale=scales_exp, q=nq_levy, m=1)\n    tau_levy = nq_levy * Hq_levy - 1\n    alpha_levy = np.gradient(tau_levy, nq_levy)\n    f_levy = nq_levy * alpha_levy - tau_levy\n    \n    c = next(color)\n    ax1[i].plot(np.arange(len(r)), r, label=fr'$\\alpha$={alphas[i]:.2f}', c=c)\n    ax1[i].margins(x=0)\n    ax1[i].legend(loc=\"upper left\")\n    ax2.plot(alpha_levy, f_levy, marker='o', c=c)\n\nax1[0].set_title(\"Мультифрактальні часові ряди\")\nax1[-1].set_xlabel(\"Час (порядковий номер)\")\nax1[len(alphas) // 2].set_ylabel('Амплітуда коливань')\n\nax2.set_title(\"Мультифрактальні спектри\")\nax2.set_xlabel(r\"$\\alpha$\")\nax2.set_ylabel(r\"$f(\\alpha)$\")\n\nfig.subplots_adjust(hspace=0.1)\n\nplt.show();\n\n\n\n\nРис. 7.16: Ілюстрація множини мультифрактальних часових рядів (Леві альфа-стабільних процесів) та їх мультифрактальних спектрів, що були згенеровані з різними значеннями \\(\\alpha\\). Зверніть увагу на зростання структурних відмінностей між періодами з малими і великими флуктуаціями зі збільшенням ширини мультифрактального спектра\n\n\n\n\nСистема, складність якої зумовлена висококонцентрованими елементами, матиме чітко виражений лівосторонній спектр. Складність системи, зумовлена слабко концентрованими елементами, буде відображена в правому хвості мультифрактального спектра. Якщо складність системи розвивається за рахунок елементів двох типів, тоді спектр представлятиметься симетричним. Іншими словами, елементи двох типів будуть рівноймовірними. Для згенерованих вище Леві альфа-стабільних процесів видно, що чим нижче значення \\(\\alpha\\), тим сильніша домінація висококонцентрованих (великих) флуктуацій. При \\(\\alpha=2.0\\) спектр усе сильніше звужується до сингулярної точки.\nДалі буде показано, що для отриманої мультифрактальної параболи мультифрактального спектра можуть бути розраховані показники як всієї ширини спектра (\\(\\Delta\\alpha\\)), так і окремо його правого та лівого хвостів (\\(R\\) та \\(L\\)). Також можна розрахувати значення сингулярності, де \\(f(\\alpha)\\) приймає максимальне значення \\(\\alpha_0\\), і навіть так звану “асиметрію” цього спектра (\\(\\Delta f\\)). На наступному рисунку (Рис. 7.17) схематично представлено положення ключових індикаторів мультифрактальності в спектрі.\n\n\n\nРис. 7.17: Графік мультифрактального спектра із відміченними на ньому показниками ширини спектра мультифрактальності (\\(\\Delta\\alpha\\)), значень мінімальної, центральної та максимальної сингулярності (\\(\\alpha_{min}, \\alpha_0, \\alpha_{max}\\)), ширини лівого та правого хвостів спектра (\\(L, R\\)) та різниці між фрактальними розмірностями в кінцях параболи (\\(\\Delta f\\))\n\n\nТакож варто зазначити, що дана схема не представляє весь вичерпний список індикаторів мультифрактальності системи, що ми використовуватимемо в подальшому, але має надавати інтуїтивне розуміння того, як виводиться більшість мультифрактальних показників.\n\n\n7.1.4.8 Узагальнені фрактальні розмірності\nНаряду з мультифрактальним спектром корисно буде розглянути спектр узагальнених фрактальних розмірностей або по іншому, розмірностей Реньї, оскільки вони також мають інформаційно-теоретичне значення. З’ясуємо фізичний сенс узагальнених фрактальних розмірностей для деяких значень \\(q\\). При \\(q=0\\) слідує, що\n\\[\nZ(0, \\varepsilon) = N(\\varepsilon).\n\\]\nЗ іншого боку, можна визначити, що\n\\[\nZ(0, \\varepsilon) \\approx \\varepsilon^{\\tau(0)} = \\varepsilon^{-D_{0}}.\n\\]\nСпівставляючи зазначені рівності, можемо прийти до співвідношення \\(N(\\varepsilon) \\approx \\varepsilon^{-D_{0}}\\). Таким чином можна сказати, що величина \\(D_{0}\\) представляє собою звичайну хаусдорфову розмірність множини \\(\\Omega\\). Також вона відповідає максимуму мультифрактального спектра, \\(f(\\alpha)\\), що завжди дорівнює одиниці для одновимірного сигналу. Отже для задач передчасного розпізнавання кризових явищ ця характеристика є найгрубішою і не несе інформації про статистичні властивості системи.\nТепер з’ясуємо сенс величини \\(D_{1}\\). Оскільки при \\(q=1\\) статистична сума рівна\n\\[\nZ(1, \\varepsilon) = 1,\n\\]\nто \\(\\tau(1)=0\\). Таким чином, ми маємо невизначеність коли \\(D_{1}=\\tau(1)/(1-1)\\). Розкриємо цю невизначеність за допомогою наступної рівності:\n\\[\nZ(q, \\varepsilon) = \\sum_{i=1}^{N(\\varepsilon)} p_{i}^{q} = \\sum_{i=1}^{N(\\varepsilon)} p_{i}\\exp{[(q-1)\\ln{p_{i}}]}.\n\\]\nТепер, спрямовуючи \\(q \\to 1\\), розкладаючи експоненту і враховуючи умову нормування ймовірностей \\(p_{i}\\), отримуємо\n\\[\nZ(q \\to 1, \\varepsilon) \\approx \\sum_{i=1}^{N(\\varepsilon)} [p_{i} + (q-1)p_{i}\\ln{p_{i}}] = 1 + (q-1)\\sum_{i=1}^{N(\\varepsilon)} p_{i}\\ln{p_{i}}.\n\\]\nУ результаті ми приходимо до наступного виразу:\n\\[\nD_{1} = \\lim_{\\varepsilon \\to 0}\\frac{\\sum_{i=1}^{N(\\varepsilon)}p_{i}\\ln{p_{i}}}{\\ln{\\varepsilon}}.\n\\]\nЗ точністю до знака, чисельник у цій формулі представляє собою інформаційну ентропію фрактальної множини \\(S(\\varepsilon)\\):\n\\[\nS(\\varepsilon) = -\\sum_{i=1}^{N(\\varepsilon)}p_i\\ln{p_i}.\n\\]\nТаким чином, результуюча величина узагальненої фрактальної розмірності \\(D_{1}\\) пов’язана з ентропією \\(S(\\varepsilon)\\) наступним співвідношенням:\n\\[\nD_{1} = - \\lim_{\\varepsilon \\to 0}\\frac{S(\\varepsilon)}{\\ln{\\varepsilon}}\n\\]\nПовертаючись до задачі о розподілі точок на фрактальній множині \\(\\Omega\\), можно сказати, що оскільки\n\\[\nS(\\varepsilon) \\approx \\varepsilon^{-D_1},\n\\]\nвеличина \\(D_{1}\\) характеризує інформацію, необхідну для опису положення точки в деякій комірці.\n\n\n\n\n\n\nДодаткова інформація по інформаційній розмірності\n\n\n\nІнформаційна розмірність може бути використана для опису просторової неоднорідності системи. Що однорідніший атрактор, то вищим має бути цей показник. Тобто, чим більшу кількість конфігурацій здатні займати елементи даної системи, тим більша кількість інформації нам потрібна для обліку кожного елемента. За просторової однорідності інформаційна ентропія так само зростає, що пов’язує інформаційну розмірність із поняттям ентропії. Оскільки \\(D_{1}\\) це тангенс кута нахилу лінії регресії, що будується для залежності між ентропією та радіусом кіл, у яких вимірюється частота влучання окремих елементів атрактора, можна сказати, що інформаційна розмірність відображає швидкість (крутизну) зміни інформаційної ентропії. Чим вища \\(D_{1}\\), тим стрімкіше зростає ентропія — міра нашого нинішнього незнання про систему. Чим нижча \\(D_{1}\\), тим менша сама ентропія. Інакше кажучи, тим більша просторова асиметрія, упорядкованіша складність, вищі наші знання про поточний стан системи, і тим менше інформації нам потрібно для опису тих конфігурацій, які система може займати.\n\n\nДля узагальненої фрактальної розмірності при \\(q=2\\) справедливий наступний вираз:\n\\[\nD_{2} = \\lim_{\\varepsilon \\to 0}\\frac{\\ln{\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{2}}}{\\ln{\\varepsilon}}.\n\\]\nВеличина \\(p_{i}\\), згідно своєму визначеню, представляє собою ймовірність попадання точки в комірку розміром \\(\\varepsilon\\). Таким чином величина \\(p_{i}^{2}\\) представляє собою ймовірність попадання в цю комірку двох точок. Знаходячи суму \\(p_{i}^{2}\\) по всім зайнятим коміркам, ми отримуємо ймовірність того, що дві навмання обрані точки з множини \\(\\Omega\\) знаходяться всередині однієї комірки з розміром \\(\\varepsilon\\). Отже, відстань між цима двома точками буде менше або порядку \\(\\varepsilon\\). Ймовірність знаходження двох траєкторій у межах околиці з радіусом \\(\\varepsilon\\) можна знайти за допомогою кореляційного інтеграла, що був представлений у попередній роботі.\nУ такому разі ми приходимо до висновку, що узагальнена розмірність визначає залежність кореляційного інтеграла \\(C(\\varepsilon)\\) від \\(\\varepsilon\\). З цієї причини величину \\(D_{2}\\) в літературі іменують кореляційною розмірністю.\nПроаналізуємо тепер поведінку \\(f(\\alpha)\\). Значення функції в максимумі легко визначити, якщо скористатися виразом (Рівняння 7.7), де\n\\[\n\\tau(q) = q\\alpha(q) - f(\\alpha(q))\n\\]\nабо\n\\[\n(q-1)D_{q} = q\\alpha(q) - f(\\alpha(q)).\n\\]\nПри \\(q=0\\) ми отримаємо, що \\(f(\\alpha_0)=D_{0}\\), тобто максимальне значення спектра дорівнює хаусдорфовій розмірності.\n\n\n\nРис. 7.18: Максимум функції \\(f(\\alpha)\\) дорівнює фрактальній розмірності \\(D_{0}\\)\n\n\nРозглянемо випадок, коли \\(q=1\\). Оскільки \\(\\tau(1) = 0\\), тоді з рівняння вище слідує, що \\(\\alpha(1) = f(\\alpha(1))\\). З іншого боку ми знаємо, що оскільки\n\\[\nq = \\frac{df(\\alpha)}{d\\alpha},\n\\]\nпохідна \\(f(\\alpha)\\) в цій точці дорівнює 1. Диференцюючи співвідношення \\(\\tau(q) = (q-1)D_{q}\\) по \\(q\\),\n\\[\n\\frac{d\\tau}{dq} = D_{q} + (q-1)D_{q}^{'} = \\alpha(q),\n\\]\nі припускаючи, що \\(q=1\\), ми отримуємо, що \\(\\alpha(1)=D_{1}\\). Таким чином, ми маємо\n\\[\nD_{1} = \\alpha(1) = f(\\alpha(1)).  \n\\]\nОтже, інформаційна розмірність \\(D_{1}\\) лежить на кривій \\(f(\\alpha)\\) в точці, де \\(\\alpha=f(\\alpha)\\) і \\(f^{'}(\\alpha)=1\\).\n\n\n\nРис. 7.19: Положення інформаційної розмірності \\(D_{1}: D_{1}=\\alpha=f(\\alpha)\\)\n\n\nТепер розглянемо випадок, коли \\(q=2\\). Користуючись попередньою формулою, отримаємо, що\n\\[\nD_{2} = 2\\alpha(2) - f(\\alpha(2))\n\\]\nабо \\(f(\\alpha(2)) = 2\\alpha(2)-D_{2}\\).\n\n\n\nРис. 7.20: Геометричне визначення кореляційної розмірності \\(D_{2}\\)\n\n\nДалі розглянемо залежність узагальненої фрактальної розмірності \\(D_{q}\\) від різних значень \\(q\\) для мультифрактального ряду, монофрактального та білого шуму.\n\ndifference_zero = np.absolute(nq-0)\nidx_zero = difference_zero.argmin()\n\ndifference_one = np.absolute(nq-1)\nidx_one = difference_one.argmin()\n\ndifference_two = np.absolute(nq-2)\nidx_two = difference_two.argmin()\n\n# ініціалізуємо масиви під розмірності\nDq_multifrac = np.zeros(len(nq))\nDq_monofrac = np.zeros(len(nq))\nDq_white_noise = np.zeros(len(nq))\n\n# Визначаємо узагальнені фрактальні розмірності там де q!=1 \nDq_multifrac[nq!=nq[idx_one]] = tau_multifrac[nq!=nq[idx_one]] / (nq[nq!=nq[idx_one]] - 1)\nDq_monofrac[nq!=nq[idx_one]] = tau_monofrac[nq!=nq[idx_one]] / (nq[nq!=nq[idx_one]] - 1)\nDq_white_noise[nq!=nq[idx_one]] = tau_white_noise[nq!=nq[idx_one]] / (nq[nq!=nq[idx_one]] - 1)\n\n# Визначаємо окремо узагальнені фрактальні розмірності при q=1\nDq_multifrac[nq==nq[idx_one]] = -tau_multifrac[nq==nq[idx_one]] \nDq_monofrac[nq==nq[idx_one]] = -tau_monofrac[nq==nq[idx_one]]\nDq_white_noise[nq==nq[idx_one]] = -tau_white_noise[nq==nq[idx_one]]\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\nax.plot(nq, Dq_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax.plot(nq, Dq_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax.plot(nq, Dq_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax.set_xlabel(r\"$q$\")\nax.set_ylabel(r\"$D_{q}$\")\nax.legend(loc=\"upper right\")\n\nax.annotate(fr'$D_{0}$={Dq_multifrac[nq==nq[idx_zero]][0]:.2f}', \n            xy=(nq[idx_zero], Dq_multifrac[nq==nq[idx_zero]]), \n            xytext=(nq[idx_zero]-2, Dq_multifrac[nq==nq[idx_zero]]+2),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nax.annotate(fr'$D_{1}$={Dq_multifrac[nq==nq[idx_one]][0]:.3f}', \n            xy=(nq[idx_one], Dq_multifrac[nq==nq[idx_one]]), \n            xytext=(nq[idx_one]-3, Dq_multifrac[nq==nq[idx_one]]-1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nax.annotate(fr'$D_{2}$={Dq_multifrac[nq==nq[idx_two]][0]:.2f}', \n            xy=(nq[idx_two], Dq_multifrac[nq==nq[idx_two]]), \n            xytext=(nq[idx_two], Dq_multifrac[nq==nq[idx_two]]-1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.show();\n\n\n\n\nРис. 7.21: Залежність узагальнених фрактальних розмірностей \\(D(q)\\) від \\(q\\)\n\n\n\n\nЗ представленого рисунку видно, що, по-переше, для всіх сигналів \\(D_{0}=1\\), що відповідає теоретичним міркуванням. Інформаційна розмірність \\(D_{1}\\) для мультифрактала та білого шуму однакова, що може говорити і про інформаційну зачущість обох сигналів. Для монофрактала вона близька до нуля. Кореляційна розмірність \\(D_{2}\\) показує, що загалом як WTI, так і білий шум доволі схожі: їх значення в більшості своїй постають незалежними один від одного. Це розбігається з тими висновками, що були зробені в попередній роботі, те наближення \\(D_{2}\\) до нуля вказувало на зростання ступеня корельованості системи. Для монофракталу \\(D_{2}\\) знаходиться на рівні 1, що вказує на вищий ступінь кореляцій у цьому сигналі, порівнюючи з мульти- та монофракталами.\n\n\n7.1.4.9 Аналогії мультифракталів із термодинамікою\nВикористовуючи концепції MFDFA, ми можемо по-новому поглянути на часовий сигнал як на термодинамічну систему. В рамках MFDFA показник маси \\(\\tau(q)\\) можна розглядати як аналоги вільної енергії, показник сингулярності \\(\\alpha\\) як аналог внутрішньої енергії \\(U\\), мультифрактальний спектр \\(f(\\alpha)\\) як ентропію. Дійсно, форма мультифрактального спектра нагадує залежність ентропії термодинамічної системи від енергії \\(U\\). Показники \\(\\alpha_{min}\\) та \\(\\alpha_{max}\\) можна охарактеризувати як верхній та нижній ліміти внутрішньої енергії системи. Функція \\(Z(q)\\) є формальним аналогом статистичної суми \\(Z(\\beta)\\) в термодинаміці, де \\(\\beta=1/T=q\\).\n\n\n\nРис. 7.22: Схематичне представлення аналогії мультифракталів із концепціями термодинаміки\n\n\nОкрім цього, ми можемо прорахувати “температуру” досліджуваного сигнала. Більш конкретно, мультифрактальну “питому теплоємність” \\(C(q)\\), що можна визначити як\n\\[\nC(q) \\equiv -\\frac{\\partial^{2} \\tau(q)}{\\partial q^{2}} = -\\frac{\\partial \\alpha}{\\partial q} \\approx \\tau(q+1) - 2\\tau(q) + \\tau(q-1).\n\\]\nПитома теплоємність, як міра швидкості зміни енергії, слугує індикатором явищ фазового переходу. У термодинамічній системі фаза характеризується однорідними фізичними властивостями, а фазовий перехід — стрибкоподібною зміною певних властивостей за критичної зовнішньої умови. Вивчення фазових переходів у мультифрактальному спектрі обмежувалося простими системами, такими як множина Кантора та логістична карта. Однак наш аналіз показує наявність фазових переходів у в мультифрактальному спектрі фінансових систем. Зокрема, “енергія” \\(\\alpha\\) виявляє значні флуктуації в околі \\(q_c\\), які відображаються піком питомої теплоємності \\(C(q_c)\\).\n\nC_q_multifrac = -np.gradient(alpha_multifrac, nq, edge_order=2)\nC_q_monofrac = -np.gradient(alpha_monofrac, nq, edge_order=2)\nC_q_white_noise = -np.gradient(alpha_white_noise, nq, edge_order=2)\n\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 8))\n\nax[0].plot(nq, C_q_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[0].plot(nq, C_q_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[0].plot(nq, C_q_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax[0].set_xlabel(r\"$q$\")\nax[0].set_ylabel(r\"$C(q)$\")\nax[0].legend()\n\nax[1].plot(alpha_multifrac, C_q_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[1].plot(alpha_monofrac, C_q_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[1].plot(alpha_white_noise, C_q_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax[1].set_xlabel(r\"$\\alpha$\")\n\nfig.tight_layout(pad=0.3) \n\nplt.show();\n\n\n\n\nРис. 7.23: Залежність мультифрактальної теплоємності \\(C(q)\\) від \\(q\\) та \\(\\alpha\\)\n\n\n\n\nРис. 7.23 демонструє, що \\(C(q)\\) досягає максимуму в діапазоні \\(q\\) від 2 до 3, що свідчить про те, що індекс WTI стає вкрай нерегулярним через велику флуктуацію катастрофи “COVID-19”, яка слугує квазіфазовим переходом досліджуваного індексу. Можна стверджувати, що колапс коронавірусної пандемії має найбільший вплив на мультифрактальність досліджуваного ряду."
  },
  {
    "objectID": "lab_7.html#хід-роботи",
    "href": "lab_7.html#хід-роботи",
    "title": "7  Лабораторна робота № 7",
    "section": "7.2 Хід роботи",
    "text": "7.2 Хід роботи\nЗвісно ж фрактальний аналіз усього ряду є важливим, але такий підхід ігнорує припущення про існування в часовій послідовності як монофрактальних, так і мультифрактальних ділянок. Тобто, ігнорується припущення про зміну ступеня складності з плином часу. Кількісні міри мультифрактальності, розраховані в рамках підходу ковзного вікна, є найбільш об’єктивними та практичними при аналізі систем. Окрім цього кількісні показники можуть бути використані як індикатори або індикатори-провісники аномальних явищ, або можуть бути використані як базис для побудови іншої прогностичної моделі. Мультифрактальний спектр може послужити хорошою основою для побудови таких показників.\nПовторно зчитаємо значення ряду із сайту FRED:\n\nsymbol = 'DCOILWTICO'    # cимвол індексу, як указано на сайті FRED\nstart = \"1986-01-01\"     # Дата початку зчитування даних\nend = \"2023-01-21\"       # Дата закінчення зчитування даних\n\nwti = web.DataReader(symbol, 'fred', start, end) # зчитуємо значення ряду \ntime_ser = wti[symbol].copy()                    # зберігаємо саме ціни закриття\n\n#------- фільтрація від'ємних значень -------\ntime_ser = time_ser.dropna()    # видаляємо значення NaN\ntime_ser[time_ser.values&lt;0] = 5 # замінюємо від'ємне значення на 5\n#--------------------------------------------\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance або FRED, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 7.24: Динаміка щоденних змін індексу сирої нафти, що використовуватиметься для подальших розрахунків\n\n\n\n\nДеякі графіки будуть представляти парну побудову лише часового ряду та мультифрактального індикатора. Скористаємось функцією plot_pair(), що ми визначали в попередніх лабораторних:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, \n              clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n7.2.0.1 Віконна процедура\nДля подальших розрахунків скористаємось бібліотекою fathon, що ми використовували попередньо для виконання класичного аналізу детрендованих флуктуацій. Переваги саме цієї бібліотеки полягають у можливості використання процедури розрахунку розбиття ряду на сегменти починаючи з кінця ряду, оскільки довжина ряду не завжди дозволяє поділити нам сам на локальні сегменти націло. Тобто, теоретично у нас залишається сегмент ряду, що не піддається розбиттю на локальні сегменти. Тому повторна процедура розбиття на локальні сегменти починаючи з кінця ряду дозволяє обійти дану проблему. Для спрощення представлення теоретичного матеріалу ми не стали імплементувати дану процедуру, але вона доступна в бібліотеці fathon. Окрім цього, бібліотека надає можливість для обчислення крос-кореляційного аналізу детрендованих флуктуацій та його мультифрактального аналогу.\nДалі імпортуємо бібліотеку та приступимо до розрахунків.\n\nimport fathon\nfrom fathon import fathonUtils as fu\n\nПриступимо до ініціалізації параметрів.\n\nwindow = 500        # розмір вікна\ntstep = 5           # крок вікна\nret_type = 4        # вид ряду: \n                    # 1 - вихідний, \n                    # 2 - детрендований (різниця між теп. значенням та попереднім)\n                    # 3 - прибутковості звичайні, \n                    # 4 - стандартизовані прибутковості, \n                    # 5 - абсолютні значення (волатильності)\n                    # 6 - стандартизований ряд\n\nwin_beg = 10        # Початкова ширина сегменту\nwin_end = window-1  # Кінцева ширина сегменту\n\nscales_exp_wind = fu.linRangeByStep(win_beg, win_end) # генеруємо масив \n                                                      # лінійно розділених \n                                                      # елементів\n\nrev = True      # чи повторювати розрахунок ф-ції флуктуацій з кінця\n\nlength = len(time_ser.values)\n\nq_min = -5         # мінімальне значення q\nq_max = 5          # максимальне значення q\nq_step = 1         # крок збільшення q\n\nnq = np.arange(q_min, \n               q_max+q_step, \n               q_step)\n\norder = 1           # порядок поліноміального тренду\n\ndelta_alph = []\ndelta_spec = []\nmax_alph = []\nmin_alph = []\nmean_alph = []\nalpha_zero = []\ndelta_alph_right = []\ndelta_alph_left = []\nassym = []\ndelta_s = []\nD_0 = []\nD_1 = []\nD_2 = []\nD_left = []\nD_right = []\nC_q = []\nh_q = []\ntau_q = []\nD_q = []\nmfSpect = []\nalpha = []\nhFI = []\nalphaCF = []\nC_q_area_wind = []\n\nРозпочнемо процедуру рухомого вікна, що об’єднуватиме у собі попередні етапи розрахунків:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # знаходження кумулятивного ряду\n    cumulative = fu.toAggregated(fragm)\n\n    # інціалізації процедури mfdfa\n    pymfdfa = fathon.MFDFA(cumulative)\n\n    # обчислення функції флуктуацій та отримання узагальненого показника Херста\n    n, F = pymfdfa.computeFlucVec(scales_exp_wind, nq, revSeg=rev, polOrd=order)\n    Hq_fragm, _ = pymfdfa.fitFlucVec()\n\n    # отримання показника tau\n    tau_wind = nq * Hq_fragm - 1\n\n    # отримання показника сингулярності\n    alpha_wind = np.gradient(tau_wind, nq, edge_order=2)\n\n    # отримання мультифрактального спектра\n    f_wind = nq * alpha_wind - tau_wind\n\n    # отримання мультифрактальної теплоємності\n    C_q_wind = -np.gradient(alpha_wind, nq, edge_order=2)\n\n    # інтегральний показник C(q) \n    C_q_area = cumulative_trapezoid(np.abs(C_q_wind), nq, initial=0)[-1]\n\n    # ширина спектра мультифрактальності\n    delta_alpha_wind = alpha_wind.max() - alpha_wind.min()\n\n    # відстань між кінцями спектра мультифрактальності\n    delta_phi = f_wind[-1] - f_wind[0]\n\n    # максимальне значення альфи\n    maximal_alpha = alpha_wind.max()\n\n    # мінімальне значення альфи\n    minimal_alpha = alpha_wind.min()\n\n    # середнє значення альфи\n    mean_alpha = np.mean(alpha_wind)\n\n    # значення сингулярності при якому спектр приймає максимальне значення (α0)\n    alpha_0 = alpha_wind[np.nanargmax(f_wind)]\n\n    # ширина правого хвоста спектра\n    delt_alpha_right = maximal_alpha - alpha_0\n\n    # ширина лівого хвоста спектра\n    delt_alpha_left = alpha_0 - minimal_alpha\n\n    # різниця між шириною лівого та правого хвостів\n    delt_s = delt_alpha_right - delt_alpha_left\n\n    # показник асиметрії\n    A = (delt_alpha_left - delt_alpha_right)/(delt_alpha_left + delt_alpha_right)\n\n    # визначаємо індекс при q=0\n    difference_zero = np.absolute(nq-0)\n    idx_zero = difference_zero.argmin()\n\n    # визначаємо індекс при q=1\n    difference_one = np.absolute(nq-1)\n    idx_one = difference_one.argmin()\n\n    # визначаємо індекс при q=2\n    difference_two = np.absolute(nq-2)\n    idx_two = difference_two.argmin()\n\n    # ініціалізуємо масиви під розмірності\n    Dq_wind = np.zeros(len(nq))\n\n    # Визначаємо узагальнені фрактальні розмірності там де q!=1 \n    Dq_wind[nq!=nq[idx_one]] = tau_wind[nq!=nq[idx_one]] / (nq[nq!=nq[idx_one]] - 1)\n\n    # Визначаємо окремо узагальнені фрактальні розмірності при q=1\n    Dq_wind[nq==nq[idx_one]] = -tau_wind[nq==nq[idx_one]] \n\n    # Узагальнені фрактальні розмірності отримані з мультифрактального спектра\n    D_zero = f_wind[nq==nq[idx_zero]]\n    D_one = f_wind[nq==nq[idx_one]]\n    D_two = 2 * alpha_wind[nq==nq[idx_two]] - f_wind[np.where(alpha_wind[nq==nq[idx_two]])]\n\n    # відстань від центру розподілу узагальнених розмірностей до лівого кінця\n    delta_D_Q_left = Dq_wind[nq==q_min] - Dq_wind[nq==nq[idx_zero]]\n\n    # відстань від центру розподілу узагальнених розмірностей до правого кінця\n    delta_D_Q_right = Dq_wind[nq==nq[idx_zero]] - Dq_wind[nq==q_max]\n\n    # індекс h-флуктуацій (hFI)\n    fluct = np.sum(np.gradient(np.gradient(Hq_fragm, nq, edge_order=2), nq, edge_order=2) ** 2)/(2 * np.max(np.abs(nq)) + 2)\n\n    # кумулятивний індекс інкрементів узагальнених показників Херста (αCF)\n    incr = np.sum(np.gradient(Hq_fragm, edge_order=2) ** 2 / np.gradient(nq, edge_order=2))\n\n    delta_alph.append(delta_alpha_wind)\n    delta_spec.append(delta_phi)\n    max_alph.append(maximal_alpha)\n    min_alph.append(minimal_alpha)\n    mean_alph.append(mean_alpha)\n    alpha_zero.append(alpha_0)\n    delta_alph_right.append(delt_alpha_right)\n    delta_alph_left.append(delt_alpha_left)\n    delta_s.append(delt_s)\n    assym.append(A)\n    D_0.append(D_zero)\n    D_1.append(D_one)\n    D_2.append(D_two)\n    D_left.append(delta_D_Q_left)\n    D_right.append(delta_D_Q_right)\n    C_q.append(C_q_wind)\n    mfSpect.append(f_wind)\n    alpha.append(alpha_wind)\n    hFI.append(fluct)\n    alphaCF.append(incr)\n    C_q_area_wind.append(C_q_area)\n    h_q.append(Hq_fragm)\n    tau_q.append(tau_wind)\n    D_q.append(Dq_wind)\n\n100%|██████████| 1768/1768 [04:05&lt;00:00,  7.21it/s]\n\n\nЗберігаємо абсолютні значення показників до текстових файлів.\n\n# перелік назв кожного індикатора для збереження до txt\nsubtitle_of_txts = ['delta_alpha', 'delta_f', 'max_alpha', 'min_alpha', 'mean_alpha', \n                    'zero_alpha', 'delta_alpha_right', 'delta_alpha_left', 'assymetry',\n                    'delta_s', 'D_0', 'D_1', 'D_2', 'hFI', 'alphaCF', 'C_q_area',\n                    'delta_d_left', 'delta_d_right']\n\n# перелік вихідних значень індикаторів для збереження до txt\nmfdfa_indicators = [delta_alph, delta_spec, max_alph, min_alph, mean_alph, alpha_zero,\n                    delta_alph_right, delta_alph_left, assym, delta_s, D_0, D_1, D_2,\n                    hFI, alphaCF, C_q_area_wind, D_left, D_right]\n\nfor i in range(len(subtitle_of_txts)):\n    np.savetxt(f\"mfdfa_{subtitle_of_txts[i]}_name={symbol}_ret={ret_type}_ \\\n               order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n               wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}.txt\", mfdfa_indicators[i])\n\nРозглянемо динаміку отриманих індикаторів.\n\n\n7.2.1 Ширина спектра мультифрактальності (\\(\\Delta\\alpha\\))\nПершим і одним із найпрактичніших індикаторів складності системи є ширина спектра мультифрактальності, \\(\\Delta\\alpha\\), яку можна подати як різницю між максимальним ступенем синґулярності та мінімальним:\n\\[\n\\Delta\\alpha = \\alpha_{max} - \\alpha_{min}.\n\\tag{7.12}\\]\nЯкщо проводити аналогію з термодинамічними показниками, то в такому разі ширина спектра мультифрактальності становитиме різницю між найбільшим і найменшим значенням внутрішньої енергії системи. Розглянемо динаміку даного показника для ринку нафти.\n\nmeasure_label = r'$\\Delta\\alpha$'\nfile_name = f\"mfdfa_delta_alpha_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          delta_alph, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='red')\n\n\n\n\nРис. 7.25: Динаміка індексу сирої нафти WTI та показника ширини спектра мультифрактальності \\(\\Delta\\alpha\\)\n\n\n\n\nНа рисунку (Рис. 7.25) видно, що ширина спектра мультифрактальності зростає під час кризових подій, що вказує на зростання загального ступеня складності та періодизації. Тобто, даний показник слугує ще одним підтвердженням, що трейдери на ринку, наприклад, нафти поводять себе у синхронній манері в ході кризи. Зростання загального ступеня мультифрактальності є індикатором зростання кореляцій в системі, що підтверджувалось і попередніми індикаторами складності.\n\n\n7.2.2 Різниця між кінцями спектра мультифрактальності (\\(\\Delta f\\))\nОднак проста ширина спектра мультифрактальності не показує, наприклад, флуктуації якого типу є найвірогіднішими, елементи якого типу щільності відіграють найбільшу роль у зростанні або зниженні складності системи. Надалі було запропоновано такий показник мультифрактальності як \\(\\Delta f\\), який можна представити наступним чином:\n\\[\n\\Delta f = f(\\alpha_{min}) - f(\\alpha_{max}).\n\\tag{7.13}\\]\n\nmeasure_label = r'$\\Delta f$'\nfile_name = f\"mfdfa_delta_f_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          delta_spec, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='brown')\n\n\n\n\nРис. 7.26: Динаміка індексу сирої нафти WTI та показника відстані між кінцями спектра мультифрактальності \\(\\Delta f\\)\n\n\n\n\nСенс даного показника полягає в тому, що він дає нам змогу визначити ступінь імовірності появи елементів з великими щільностями і малими. Якщо цей показник менший за нуль, тоді флуктуації, що відображають елементи з найбільшою концентрацією (найбільшими фруктуаціями), мають найбільшу ймовірність. Якщо цей показник вищий за нуль, тоді флуктуації, що відображають малоконцентровані елементи (малі флуктуації), визначають динаміку системи. Якщо цей показник перебуває в нулі, тоді як високосингулярні, так і малосингулярні елементи мають рівномірний внесок у динаміку системи.\nЗвертаючись до термодинаміки, можна згадати, що \\(f(\\alpha)\\) — це ентропія системи. Тоді стає зрозуміло, що варіативність спектра мультифрактальності дає нам змогу визначити ступінь внеску висококонцентрованих і малоконцентрованих елементів у мінімізацію ентропії системи. Лівостороння асиметрія мультифрактального спектра (\\(\\Delta f &gt; 0\\)) підказує нам, що висококонцентровані елементи фазового простору роблять найбільший внесок у мінімум термодинамічної ентропії. Іншими словами, ці елементи і є двигуном зростання впорядкованості динаміки системи. Своєю чергою, правостороння асиметрія мультифрактального спектра (\\(\\Delta f &lt; 0\\)) вказує на мінімізацію ентропії за рахунок малоконцентрованих елементів. Симетрія кінців спектра вказує на рівний внесок високощільних і розріджених областей на мінімізацію ентропії. Як уже згадувалося, трапляються випадки, коли мультифрактальний спектр практично сходиться в сингулярність. У такому разі ми маємо справу з простою монофрактальною системою, яка в нашому випадку характеризувалася незалежними і нормально розподіленими випадковими величинами. Для такого спектра і \\(\\Delta\\alpha\\), і \\(\\Delta f\\) будуть прагнути до нуля. Для такого часового ряду вже спостерігається не множина фрактальних розмірностей, а тільки один фрактальний показник, \\(f[\\alpha(q=0)] = 1\\). Це саме та область, де система досягає своєї термодинамічної рівноваги — максимуму ентропії. У свою чергу \\(\\Delta f\\) можна охарактеризувати як різницю ентропій за граничної максимальної та мінімальної внутрішньої енергії системи.\n\n\n7.2.3 Ширина лівого (\\(\\Delta\\alpha_{L}\\)) та правого (\\(\\Delta\\alpha_{R}\\)) хвостів мультифрактального спектра\nКрім цього, ми можемо дослідити ступінь складності динаміки окремо високощільних областей (з великими флуктуаціями) і низькощільних (з малими флуктуаціями). Для цього ми можемо виміряти ширину окремо лівого і правого хвостів. Ширину лівого хвоста можна визначити як\n\\[\n\\Delta\\alpha_{L} = \\alpha_{0} - \\alpha_{min},\n\\tag{7.14}\\]\nа ширина правого хвоста визначається як\n\\[\n\\Delta\\alpha_{R} = \\alpha_{max} - \\alpha_{0}.\n\\tag{7.15}\\]\nУ свою чергу ширина лівого хвоста вимірює ступінь складності флуктуацій з великою амплітудою, а ширина правого хвоста вимірює ступінь складності малих флуктуацій, які рефлексують малоконцентровані елементи.\nЗростання ширини кожного з хвостів відображатиме зростання ступеня кореляцій між елементами\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser.index[window:length:tstep], \n              time_ser.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window:length:tstep], \n               delta_alph_left, color=\"r\", label=r\"$\\Delta\\alpha_{L}$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], \n               delta_alph_right, color=\"g\", label=r\"$\\Delta\\alpha_{R}$\")\n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"mfdfa_delta_alpha_left_right_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}.jpg\")\nplt.show();\n\n\n\n\nРис. 7.27: Динаміка індексу сирої нафти WTI та ширини лівого і правого хвостів мультифрактального спектра\n\n\n\n\nНа рисунку (Рис. 7.27) видно, що досліджувані індикатори реагують у характерний спосіб на кризові події. Ширина лівої сторони спектра мультифрактальності зростає під час 1992, 1996-2000, під час 2008, 2016 та коронавірусної пандемії. Це вказує на зростання домінації висококонцентрованих флуктуацій (з великою амплітудою коливань). Окрім цього, зростання ширини лівого хвоста вказує на те, що флуктуації з великою амплітудою коливань характеризуються зростанням ступеня кореляцій під час кризових подій, що в свою чергу може слугувати індикатором зростання процесів самоорганізації.\nХоча і меншою, але не менш примітною є динаміка ширини правого хвоста мультифрактального спектра. Як ми можемо бачити з представленого рисунку, цей індикатор працює майже аналогічно до ширини лівого хвоста, але сам характеризує динаміку малоконцентрованих величин — флуктуацій з малою амплітудою коливань. Майже синхронна динаміка двох показників указує на зростання впливу коливань як флуктуацій з великою амлпітудою коливань, так і флуктуацій з малою амплітудою коливань. Тобто, два типи флуктуацій в ряді є джерелом зростання нелінійних кореляцій під час кризових подій.\n\n\n7.2.4 Показник сингулярності \\(\\alpha\\) та його різновиди\nВ якості можливих індикаторів складності системи можна взяти \\(\\alpha_{min}\\), \\(\\alpha_{max}\\), \\(\\alpha_{mean}\\) і \\(\\alpha_0\\), які, відповідно, характеризують мінімальну силу сингулярності, максимальну, середню і сингулярність за умови рівноважного врахування як великих флуктуацій, так і малих.\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\nax5 = ax.twinx()\n\nax3.spines.right.set_position((\"axes\", 1.08))\nax4.spines.right.set_position((\"axes\", 1.18))\nax5.spines.right.set_position((\"axes\", 1.27))\n\np1, = ax.plot(time_ser.index[window:length:tstep], time_ser[window:length:tstep], \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window:length:tstep], max_alph, \"r-\", label=r\"$\\alpha_{max}$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], min_alph, \"g-\", label=r\"$\\alpha_{min}$\")\np4, = ax4.plot(time_ser.index[window:length:tstep], mean_alph, \"c-\", label=r\"$\\alpha_{mean}$\")\np5, = ax5.plot(time_ser.index[window:length:tstep], alpha_zero, \"m-\", label=r\"$\\alpha_{0}$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\nax5.yaxis.label.set_color(p5.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax5.tick_params(axis='y', colors=p5.get_color(), **tkw)\nax.tick_params(axis='x', **tkw, pad=10)\n\nax5.legend(handles=[p1, p2, p3, p4, p5])\n\nplt.savefig(f\"mfdfa_alpha_min_max_mean_zero_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n            wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\nРис. 7.28: Динаміка індексу сирої нафти WTI та показників сингулярності\n\n\n\n\nЯк видно з рисунку (Рис. 7.28), усі показники сингулярності зростають в області фінансового фазового переходу зі стану стабільності до стану кризи. Це говорить про зростання складності системи: різькому прирості кількості агентів, що задіяні в самоорганізованому розвитку досліджуваної системи. З погляду термодинаміки можна було б сказати, що при фінансових крахових подіях зростає внутрішня енергія системи.\n\n\n7.2.5 Тип довгого хвоста мультифрактального спектра (\\(\\Delta S\\))\nКрім такої міри як, наприклад, \\(\\Delta f\\) можна представити й інші міри асиметрії мультифрактального спектра. Наприклад, ми можемо визначити тип довгого хвоста мультифрактального спектра за допомогою показника \\(\\Delta S\\), що визначається як\n\\[\n\\Delta S = \\Delta\\alpha_{R} - \\Delta\\alpha_{L}.\n\\tag{7.16}\\]\nЯкщо \\(\\Delta S &lt; 0\\), тоді мультифрактальний спектр має довгий лівий хвіст, що свідчить про те, що структура часового ряду чутлива до локальних флуктуацій з великою амплітудою. Якщо \\(\\Delta S &gt; 0\\), тоді мультифрактальний спектр має довгий правий хвіст, який вказує на те, що структура сигналу чутлива до локальних флуктуацій з малою амплітудою коливань. У тих випадках, коли високо- і низькофлуктуаційні компоненти сигналу характеризуються однаковою мірою складністю, спектр сингулярностей буде приблизно симетричним і \\(\\Delta\\alpha_{R} = \\Delta\\alpha_{L}\\). У разі більшої складності однієї з цих компонент спектр сингулярності стає несиметричним, унаслідок чого один із хвостів виявляється ширшим за інший.\n\nmeasure_label = r'$\\Delta S$'\nfile_name = f\"mfdfa_delta_s_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          delta_s, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkorange')\n\n\n\n\nРис. 7.29: Динаміка індексу сирої нафти WTI та показника типу хвоста мультифрактального спектра \\(\\Delta S\\)\n\n\n\n\nЗ огляду на цей рисунок (Рис. 7.29) видно, що \\(\\Delta S &lt; 0\\), що свідчить про те, що найбільш крахові ділянки нафтового ринку зумовлені флуктуаціями з великою амплітудою коливань. Особливо помітними тут предстають кризи 1992 та 2020 років.\n\n\n7.2.6 Показник асиметрії (\\(A\\))\nДалі ми можемо визначити наступний параметр асиметрії:\n\\[\nA = \\frac{\\Delta\\alpha_{L} - \\Delta\\alpha_{R}}{\\Delta\\alpha_{R} + \\Delta\\alpha_{L}} = \\frac{-\\Delta S}{\\Delta\\alpha}.\n\\tag{7.17}\\]\nПараметр асиметрії пов’язаний з предомінуючим типом коливань у досліджуваній системі. Якщо \\(A=0\\) (\\(\\Delta\\alpha_{L}=\\Delta\\alpha_{R}\\)), тоді динаміку системи представляє симетричний спектр. Якщо \\(A&lt;0\\) (\\(\\Delta\\alpha_{L}&lt;\\Delta\\alpha_{R}\\)), тоді мультифрактальний спектр має правосторонню асиметрію, що підкреслює сильніший вплив малих флуктуацій на мультифрактальність системи, тобто в часовому ряду переважає мультифрактальна природа малих шумоподібних флуктуацій. І навпаки, коли \\(A&gt;0\\) (\\(\\Delta\\alpha_{L}&gt;\\Delta\\alpha_{R}\\)), тоді ми маємо справу з лівостороннім спектром, який позначає більшу неоднорідність для великих флуктуацій і вказує на те, що в часовому ряді переважає мультифрактальна природа неоднорідностей з високими щільностями. Оскільки асиметрію виявляють за знаком \\(A\\), який еквівалентний знаку \\(\\Delta S\\), то, ґрунтуючись тільки по знаку \\(\\Delta S\\), можна зробити висновки як про тип довгого хвоста, так і про знак показника \\(A\\) мультифрактального спектра, тобто нечутливість і тип домінантних коливань мультифрактальності часового ряду.\n\nmeasure_label = r'$A$'\nfile_name = f\"mfdfa_A_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          assym, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkviolet')\n\n\n\n\nРис. 7.30: Динаміка індексу сирої нафти WTI та показника асиметрії мультифрактального спектра \\(A\\)\n\n\n\n\nНа представленому рисунку видно, що, як правило, показник асиметрії зростає під час крахових подій, що вказує на домінацію лівостороннього спектра (висококонцентрованих флуктуацій з великою амплітудою коливань). Окрім цього видно, що, наприклад, для криз 2008 та 2015-2016 років спостерігалась помітна короткочасна правостороння асиметрія, що характеризується швидкоплинним сплеском показника асиметрії у сторону від’ємних значень. Асоціювати малі та великі флуктуації з конкретними настроями або поведінковими патернами ринку доволі складно. На даний момент ми можемо відзначити тільки те, що дані події представляли найбільш багату варіацію як короткострокових, так і довгострокових кореляцій.\n\n\n7.2.7 Індекс \\(h\\)-флуктуацій (\\(hFI\\))\nФлуктуацію можна проаналізувати за допомогою другої похідної узагальненого показника Херста. Зауважимо, що амплітуда другої похідної у випадку мультифрактальних сигналів більша, ніж для монофрактальних. Для вилучення потрібної інформації з \\(h(q)\\) було запропоновано наступну міру, яка називається \\(h\\)-індекс флуктуації (\\(hFI\\)), яка визначається як степінь другої похідної від \\(h(q)\\),\n\\[\nhFI = \\frac{1}{2|q_{max}|+2}\\sum_{q=q_{min}-2}^{q_{max}}\\left[ h(q) - 2h(q-1) + h(q-2) \\right]^{2}.\n\\tag{7.18}\\]\nЧим вище значення даного показника, тим вища самоорганізованість системи.\n\nmeasure_label = r'$hFI$'\nfile_name = f\"mfdfa_hFI_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          hFI, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='green')\n\n\n\n\nРис. 7.31: Динаміка індексу сирої нафти WTI та індексу \\(h\\)-флуктуацій \\(hFI\\)\n\n\n\n\nЗ рисунку (Рис. 7.31) видно, що найвищий ступінь мультифрактальності, згідно \\(hFI\\), проявляється саме для криз 1992, 2008, 2016 та 2020 років. Це свідчить про те, що дані крахові події включають в себе найбільшу кількість різноманітних факторів, що впливали на динаміку досліджуваної системи. Особливо помітно це для коронавірусної пандемії, що має найвищу амплітуду другої похідної узагальненого показника Херста.\n\n\n7.2.8 Кумулятивний індекс інкрементів узагальнених показників Херста (\\(\\alpha CF\\))\nКумулятивна функція квадратів інкриментів (\\(\\alpha CF\\)) узагальнених показників Херста між послідовними моментними порядками є більш надійним показником розподілу узагальнених показників Херста.\n\nmeasure_label = r'$\\alpha CF$'\nfile_name = f\"mfdfa_alphaCF_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alphaCF, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='crimson')\n\n\n\n\nРис. 7.32: Динаміка індексу сирої нафти WTI та кумулятивний індексу інкрементів узагальнених показників Херста \\(\\alpha CF\\)\n\n\n\n\nПредставлений кумулятивний індекс дещо відрізняється від \\(hFI\\), але хід думок приблизно однаковий: події з найвищим ступенем мультифрактальності характеризуються вищою амплітудою \\(\\alpha CF\\). Представлений показник виділяє ті самі кризи, що й попередній, але динаміка цього показника більш виразна, що, в теорії, робить його більш надійним для ідентифікації періодів самоорганізації системи.\n\n\n7.2.9 Інтегральна мультифрактальна теплоємність \\(C(q)\\)\nЗагальний ступінь мультифрактальності, інтегральну мультифрактальну питому теплоємність (\\(C_{area}\\)), можна виразити через рівняння (Рівняння 7.19):\n\\[\nC_{area} = \\int C(q)dq.\n\\tag{7.19}\\]\nРозглянемо динаміку цього показника.\n\nmeasure_label = r'$C_{area}$'\nfile_name = f\"mfdfa_C_q_area_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          C_q_area_wind, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkslateblue')\n\n\n\n\nРис. 7.33: Динаміка індексу сирої нафти WTI та інтегральної мультифрактальної теплоємності \\(C_{area}\\)\n\n\n\n\nЗ представленого рисунку видно, що динаміка інтегральної теплоємності дуже подібна до ширини спектра мультифрактальності. Тобто, \\(C_{area}\\) є показником складності, що вказує на ступінь самоорганізованості фінансового фазового переходу. Видно, що фінансові крахи представляють доволі трендостійку динаміку, що є наслідком цілеспрямованих та колективних дій трейдерів на ринку.\n\n\n7.2.10 Хаусдорфова розмірність (\\(D_{0}\\))\nЯк уже зазначалось, \\(D_{0}\\) представляє верхню межу змін розмірностей фрактальних підмножин атрактора системи. Інформацію про статистичні властивості системи він не має нести, тому не представляє особливої цікавості.\n\nmeasure_label = r'$D_{0}$'\nfile_name = f\"mfdfa_D_0_area_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_0, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkred')\n\n\n\n\nРис. 7.34: Динаміка індексу сирої нафти WTI та хаусдорфової розмірності цього часового сигналу \\(D_{0}\\)\n\n\n\n\n\n\n7.2.11 Інформаційна розмірність (\\(D_{1}\\))\nЯк уже зазначалося в теорії, інформаційна розмірність тісно пов’язана з інформаційною ентропією Шеннона. Чим вище значення \\(D_1\\), тим швидше збільшується ентропія, що є показником того, наскільки мало ми знаємо про поточний стан системи. Зі зменшенням \\(D_1\\) ентропія знижується, що, своєю чергою, свідчить про збільшення асиметрії в просторі, зменшення складності та розширення нашого розуміння поточного стану системи. Це також означає, що для опису можливих конфігурацій системи нам буде потрібно менше інформації.\n\nmeasure_label = r'$D_{1}$'\nfile_name = f\"mfdfa_D_1_area_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_1, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkred')\n\n\n\n\nРис. 7.35: Динаміка індексу сирої нафти WTI та інформаційної розмірності цього часового сигналу \\(D_{1}\\)\n\n\n\n\nНа представленому рисунку (Рис. 7.35) видно, що інформаційна розмірність характеризується спадом під час крахових подій. Це вказує на зростання ступеня впорядкованості в системі та колективного скупчення агентів ринку в межах конкретної області фазового простору досліджуваної системи. Для рівномірнорозподіленої динаміки цього ринку інформаційна розмірність зростала б, вказуючи на незалежність агентів ринку один від одного.\n\n\n7.2.12 Кореляційна розмірність (\\(D_{2}\\))\nКореляційну розмірність, аналогічно інформаційній розмірності, можна подати як тангенс кута нахилу лінії регресії, побудованої в логарифмічному масштабі, щодо залежності кореляційного інтеграла \\(C(\\varepsilon)\\) від \\(\\varepsilon\\). Подібно до \\(D_1\\), кореляційна розмірність також визначає, як швидко змінюється значення кореляційного інтеграла.\n\nmeasure_label = r'$D_{2}$'\nfile_name = f\"mfdfa_D_2_area_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_\\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_2, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkred')\n\n\n\n\nРис. 7.36: Динаміка індексу сирої нафти WTI та кореляційної розмірності цього часового сигналу \\(D_{2}\\)\n\n\n\n\nКореляційна розмірність на представленому рисунку (Рис. 7.36) характеризується зростанням у передкризовий період та спадом під час кризи. Як уже було показано в попередній лабораторній роботі, спадаючи, кореляційна розмірність вказує на зростання ступеня періодизації у системі та зміщення всіх траєкторій фазового простору в одну конкретну область. Це говорить про те, що більшість агентів ринку починає орієнтуватися на один конкретний вектор розвитку системи в котру вони залучені. Тобто, їх думки в цей момент стають більш синхронізованими.\n\n\n7.2.13 Кривизна лівого (\\(\\Delta D_{L}\\)) та правого (\\(\\Delta D_{R}\\)) хвостів розподілу узагальнений фрактальних розмірностей\nЯкщо фрактальні показники дорівнюють один одному, це може вказувати на те, що система є доволі простою, і її елементи значною мірою незалежні один від одного, а її поведінка наближається до нормального Гаусового розподілу. Однак, якщо поведінка системи є досить складною, тоді узагальнені фрактальні розмірності в сукупності можуть мати форму сигмоїди, вказуючи на більш складні внутрішні взаємозв’язки в системі.\nОхарактеризувати ступінь цієї складності можна за кривизною окремо правого та лівого хвостів узагальнених фрактальних розмірностей. Праву сторону (\\(\\Delta D_{R}\\)) можна визначити як\n\\[\n\\Delta D_{R} = D_0 - D_{q_{max}}.\n\\tag{7.20}\\]\nІ чим більшим буде значення цієї міри, тим сильнішим буде ступінь впливу елементів із найбільшою концентрацією (щільністю, амплітудою флуктуацій) на загальну складність системи.\nМи можемо дізнатися так само кривизну лівого хвоста кривої узагальнених фрактальних розмірностей (\\(\\Delta D_{L}\\)). Вона визначатиметься як\n\\[\n\\Delta D_{L} = D_{q_{min}} - D_{0}.\n\\tag{7.21}\\]\nЦей показник буде говорити нам про те, наскільки сильним є вплив найменш концентрованих елементів на складність системи.\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser.index[window:length:tstep], \n              time_ser.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window:length:tstep], \n               D_left, color=\"g\", label=r\"$\\Delta D_{L}$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], \n               D_right, color=\"r\", label=r\"$\\Delta D_{R}$\")\n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"mfdfa_delta_D_left_right_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}.jpg\")\nplt.show();\n\n\n\n\nРис. 7.37: Динаміка індексу сирої нафти WTI та кривизни лівого і правого хвостів спектра узагальнених фрактальних розмірностей\n\n\n\n\nНа рисунку (Рис. 7.37) видно, що спостерігаються етапи при який два показники можуть вести себе як асинхронно, так і синхронно. Для 1992 року видно, що у передкризовий етап спостерігалась короткочасна домінація великих флуктуацій, на що і вказує \\(\\Delta D_{R}\\). Під час краху спостерігалась домінація малоконцентрованих елементів, що демонструє зростання \\(\\Delta D_{L}\\). Для 2001 року ми бачимо зростання впливу правого хвоста і зменшення впливу лівого. Для краху 2008 року видно зростання впливу висококонцентрованих елементів напередодні кризи. Після цього домінація малоконцентрованих елементів, що відзеркалює зростання зеленої кривої. Бачимо зростання впливу малоконцентрованих елементів під час кризи 2016 року, і закономірне зменшення участі великоконцентрованих елементів. Пандемія 2020-2021 років характеризувалась активною участю як малоконцентрованих елементів, так і великоконцентрованих, на що вказує зростання двох показників кривизни хвостів узагальнених розмірностей.\n\n\n7.2.14 Дво- та тривимірна візуалізація показників мультифрактальності\nПопередньо ми дивилися на залежність \\(h(q)\\), \\(\\tau(q)\\), \\(D(q)\\), \\(C(q)\\) та \\(f(\\alpha)\\) для всього часового ряду. Тепер, скориставшись процедурою ковзного вікна, ми можемо подивитися на їх зміну з плином часу.\nПерш за все огосимо функції для побудови двовимірних графіків\n\ndef plot_2d(X, Y, Z, subtitle_jpg, subtitle_fig, ylabel, barlabel, cmap, lims):\n\n    fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n\n    cp = ax.contourf(X, Y, Z, alpha=0.8, cmap=cmap)\n    plt.colorbar(cp, ax=ax, extend='both', label=barlabel)\n\n    ax.set_xlim((time_ser.index[window:length:tstep][0], \n                 time_ser.index[window:length:tstep][-1]))\n    ax.set_ylim((np.min(lims), np.max(lims)))\n\n    ax.set_xlabel(xlabel, fontsize=22)\n    ax.set_ylabel(ylabel, fontsize=22)\n\n    ax.set_title(subtitle_fig, pad=10, fontsize=22)\n\n    ax.tick_params(axis='both', which='major', pad=10, labelsize=22)\n\n    fig.tight_layout()\n\n    plt.savefig(f\"mfdfa_{subtitle_jpg}_name={symbol}_ret={ret_type}_order={order}_ \\\n                qmin={q_min}_qmax={q_max}_qinc={q_step}_windbeg={win_beg}_winden={win_end}.jpg\", \n                bbox_inches=\"tight\")\n    plt.show(); \n\nта тривимірних\n\ndef plot_3d(X, Y, Z, subtitle_jpg, ylabel, zlabel, cmap):\n\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(10, 8))\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, rstride=2, cstride=2, linewidth=0)\n\n    ax.set_xlabel(xlabel, fontsize=22, labelpad=15)\n    ax.set_ylabel(ylabel, fontsize=22, labelpad=15)\n    ax.set_zlabel(zlabel, fontsize=22, labelpad=15)\n    ax.tick_params(axis='both', which='major', labelsize=18, pad=5)\n\n    fig.colorbar(surf, shrink=0.5, aspect=10, location='right', pad=0.1)\n\n    fig.tight_layout()\n\n    plt.savefig(f\"mfdfa_{subtitle_jpg}_name={symbol}_ret={ret_type}_order={order}_ \\\n                qmin={q_min}_qmax={q_max}_qinc={q_step}_windbeg={win_beg}_ \\\n                winden={win_end}.jpg\", bbox_inches=\"tight\")\n    \n    plt.show();\n\nПісля оголошення необхідний функцій можна приступати до візуалізації.\n\n7.2.14.1 Динаміка \\(h(q)\\) з ходом часу в дво- та тривимірному просторах\n\nX, Y = np.meshgrid(time_ser.index[window:length:tstep], nq)\nZ = np.array(h_q).T\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_h(q)', \n        subtitle_fig=fr\"Colormap of $h(q)$\", \n        ylabel=r\"$q$\", \n        barlabel=r\"$h(q)$\",\n        cmap='jet',\n        lims=nq)\n\n\n\n\nРис. 7.38: Двовимірна контурна діаграма динаміки узагальненого показника Херста \\(h(q)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\n\nX, Y = np.meshgrid(np.arange(window, length, tstep), nq)\nZ = np.array(h_q).T\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_h(q)', \n        ylabel=r\"$q$\", \n        zlabel=r\"$h(q)$\",\n        cmap='jet')\n\n\n\n\nРис. 7.39: Тривимірна діаграма динаміки узагальненого показника Херста \\(h(q)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\nНа представлений рисунках (Рис. 7.38 та Рис. 7.39) видно, що узагальнений показник Херста характеризується значним ростом саме в період криз. Особливо високим \\(h(q)\\) предстає для \\(q &lt; 0\\), що говорить про значну персистентність малих флуктуацій в періоди турбулентності. Найвищим ступінь нелінійності в даному випадку представляють кризи 1992, 2008-2009, 2015-2016 та 2020-2021 років, що підтверджується й попередніми індикаторами.\n\n\n7.2.14.2 Динаміка \\(\\tau(q)\\) з ходом часу в дво- та тривимірному просторах\n\nX, Y = np.meshgrid(time_ser.index[window:length:tstep], nq)\nZ = np.array(tau_q).T\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_tau(q)', \n        subtitle_fig=fr\"Colormap of $\\tau(q)$\", \n        ylabel=r\"$q$\", \n        barlabel=r\"$\\tau(q)$\",\n        cmap='viridis',\n        lims=nq)\n\n\n\n\nРис. 7.40: Двовимірна контурна діаграма динаміки показника \\(\\tau(q)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\n\nX, Y = np.meshgrid(np.arange(window, length, tstep), nq)\nZ = np.array(tau_q).T\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_tau(q)', \n        ylabel=r\"$q$\", \n        zlabel=r\"$\\tau(q)$\",\n        cmap='viridis')\n\n\n\n\nРис. 7.41: Тривимірна діаграма динаміки показника \\(\\tau(q)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\nЯк видно з представлений рисунків (Рис. 7.40 та Рис. 7.41), \\(\\tau(q)\\) стає більш нелінійним для всіх значень \\(q\\). На кінцях хвостів цього індикатора можна помітити значні впадини, що можуть слугувати індикаторами крахових подій, але в порівнянні з тим же показником Херста даний індикатор є менш виразним.\n\n\n7.2.14.3 Динаміка \\(D(q)\\) з ходом часу в дво- та тривимірному просторах\n\nX, Y = np.meshgrid(time_ser.index[window:length:tstep], nq)\nZ = np.array(D_q).T\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_D(q)', \n        subtitle_fig=fr\"Colormap of $D(q)$\", \n        ylabel=r\"$q$\", \n        barlabel=r\"$D(q)$\",\n        cmap='magma',\n        lims=nq)\n\n\n\n\nРис. 7.42: Двовимірна контурна діаграма динаміки узагальненої фрактальної розмірності \\(D(q)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\n\nX, Y = np.meshgrid(np.arange(window, length, tstep), nq)\nZ = np.array(D_q).T\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_D(q)', \n        ylabel=r\"$q$\", \n        zlabel=r\"$D(q)$\",\n        cmap='magma')\n\n\n\n\nРис. 7.43: Тривимірна діаграма динаміки узагальненої фрактальної розмірності \\(D(q)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\nДво- та тривимірні представлення узагальненої фрактальної розмірності показують, що \\(D(q)\\) зростає під час кризових подій, що вже згадувались попередньо. Узагальнена фрактальна розмірність також представляє найбільш індикативну динаміку для негативних значень \\(q\\), хоча для позитивних \\(q\\) також спостерігаються незначні коливання.\n\n\n7.2.14.4 Динаміка \\(C(q)\\) з ходом часу в дво- та тривимірному просторах\n\nX, Y = np.meshgrid(time_ser.index[window:length:tstep], nq)\nZ = np.array(C_q).T\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_C(q)', \n        subtitle_fig=fr\"Colormap of $C(q)$\", \n        ylabel=r\"$q$\", \n        barlabel=r\"$C(q)$\",\n        cmap='hot',\n        lims=nq)\n\n\n\n\nРис. 7.44: Двовимірна контурна діаграма динаміки мультифрактальної теплоємності \\(C(q)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\n\nX, Y = np.meshgrid(np.arange(window, length, tstep), nq)\nZ = np.array(C_q).T\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_C(q)', \n        ylabel=r\"$q$\", \n        zlabel=r\"$C(q)$\",\n        cmap='hot')\n\n\n\n\nРис. 7.45: Тривимірна контурна діаграма динаміки мультифрактальної теплоємності \\(C(q)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\nНа даних рисунках (Рис. 7.44 та Рис. 7.45) спостерігається мультифрактальної теплоємності під час кризових подій, що у свою чергу вказує схожість фізичних фазових переходів та кризових подій. Можна бачити, що при різних ринкових режимах \\(C(q)\\) може бути симетричною, демонструючи рівномірний вплив на динаміку ринку як висококонцентрованих елементів, так і низькоконцентрованих. Також \\(C(q)\\) може зміщуватись як у ліву сторону, так і вправу, що говорить про мінливість ринку та впливовість різних початкових умов на його структуризацію.\n\n\n7.2.14.5 Динаміка \\(f(\\alpha)\\) з ходом часу в дво- та тривимірному просторах\n\nX = time_ser.index[window:length:tstep].values\nX = np.expand_dims(X, axis=1)\nX = np.repeat(a=X, repeats=nq.shape[0], axis=1)\n\nY = np.array(alpha)\nZ = np.array(mfSpect)\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_f(alpha)', \n        subtitle_fig=fr\"Colormap of $f(\\alpha)$\", \n        ylabel=r\"$\\alpha$\", \n        barlabel=r\"$f(\\alpha)$\",\n        cmap='hsv',\n        lims=alpha)\n\n\n\n\nРис. 7.46: Двовимірна контурна діаграма динаміки мультифрактального спектра \\(f(\\alpha)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\n\nX = np.arange(window, length, tstep)\nX = np.expand_dims(X, axis=1)\nX = np.repeat(a=X, repeats=nq.shape[0], axis=1)\n\nY = np.array(alpha)\nZ = np.array(mfSpect)\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_f(alpha)', \n        ylabel=r\"$\\alpha$\", \n        zlabel=r\"$f(\\alpha)$\",\n        cmap='hsv')\n\n\n\n\nРис. 7.47: Тривимірна діаграма динаміки мультифрактального спектра \\(f(\\alpha)\\), що змінюється з плином часу. Даний показник було розраховано в ковзному вікні для індеку сирої нафти WTI\n\n\n\n\nЯк ми можемо бачити з представлених рисунків (Рис. 7.46 та Рис. 7.47), ширина спектра мультифрактальності змінюється у формі з плином часу, і стає ширшою під час кризових подій, що підтверджувалось таким індикатором як, наприклад, \\(\\Delta\\alpha\\). Видно, що у передкризові періоди зростає лівостороння асиметрія, що характеризує флуктуації значної амплітуди коливань. Самі кризи представляють зміщення \\(f(\\alpha)\\) у праву сторону, що вказує на домінацію флуктуацій з малою амплітудою. У будь-якому разі, зростання ширини спектра є індикатором зростання ступеня самоорганізованості елементів, що залучені до досліджуваної системи. Тобто, як \\(f(\\alpha)\\), так і попередні індикатори можна пробувати використовувати в якості індикаторів або індикаторів-передвісників кризових подій."
  },
  {
    "objectID": "lab_8.html#теоретичні-відомості",
    "href": "lab_8.html#теоретичні-відомості",
    "title": "8  Лабораторна робота № 8",
    "section": "8.1 Теоретичні відомості",
    "text": "8.1 Теоретичні відомості\nВивчення статистичних властивостей матриць з незалежними випадковими елементами — випадкових матриць — має багату історію, що починається з ядерної фізики, де проблема з’явилася 50 років тому при дослідженні енергетичних рівнів складних ядер, що існуючі на той час моделі були не в змозі пояснити. Теорія випадкової матриці (ТВМ) була розвинена в цьому контексті Вігнером (Wigner), Дайсоном (Dyson), Метою (Mehta) та іншими для пояснення статистики рівнів енергії складних квантових систем. Дослідники постулювали, що функція Гамільтона, яка описує важкі ядра, може бути задана матрицею \\(H\\) з незалежними випадковими елементами \\(H_{ij}\\), отриманими з розподілу імовірності. Відштовхуючись від цього припущення було зроблено низку вражаючих передбачень, які було підтверджено експериментально. Для складних квантових систем передбачення на основі ТВМ представляють середнє за всіма можливими взаємодіями. Відхилення від універсальних передбачень ТВМ відображують системну специфіку, невипадкові властивості системи, забезпечуючи ключові підходи до розуміння базової взаємодії системи. Недавні дослідження, що використовували методи аналізу ТВМ до аналізу властивостей матриці взаємних кореляцій \\(C\\), показують, що близько 98% власних значень матриці \\(C\\) співпадають зі значеннями, отримуваними з використанням ТВМ, таким чином пропонуючи задовільний рівень хаотичності у вимірюваних крос-кореляціях. Також було знайдено, що існують відхилення від передбачень за допомогою ТВМ у близько 2% найбільших власних значень. Ці результати викликають наступні питання:\n\nЯка можлива інтерпретація для відхилень від ТВМ?\nЩо можна сказати про структуру C з цих результатів?\nЯке практичне значення отриманих результатів?\n\nШляхом комп’ютерного моделювання виявлено, що найбільше власне значення матриці \\(C\\) представляє вплив усього ринку, що є звичайним для всіх акцій. Аналіз змісту власних значень, що відхиляються від ТВМ, показує існування взаємних кореляцій між акціями того ж самого типу промисловості, найбільш капіталізованими акціями, і акціями фірм, що мають бізнес у певному географічному секторі (локалізовані територіально). Обчислюючи скалярний добуток власних векторів від одного періоду часу до наступного, можна побачити, що “власні вектори, що відхиляються”, мають різні ступені стабільності в часі, визначеному кількісно величиною скалярного добутку. Найбільші два-три власних вектори стійкі протягом тривалих періодів часу, у той час як для іншої частини власних векторів, що відхиляються, стабільність у часі зменшується як тільки відповідні власні значення наближаються до верхньої межі ТВМ.\n\n8.1.1 Знаходження коефіцієнтів матриці крос-кореляцій\nВизначення кореляцій між різними акціями — тема, цікава не лише з точки зору наукових причин розуміння економіки як складної динамічної системи, але також і з практичних поглядів, зокрема, з точки зору розміщення активів і оцінки портфельного ризику. Ми будемо аналізувати взаємні кореляції між акціями, застосовуючи поняття і методи теорії випадкових матриць, що використовуються в контексті складних квантових систем, де точний характер взаємодій між підодиницями невідомий.\nДля визначення кількісно кореляцій спочатку обчислюється зміна цін (прибутковості) акції \\(i=1,...,N\\) за час \\(\\Delta t\\),\n\\[\nG_{i}(t) = \\ln S_i(t+\\Delta t) - \\ln S_i(t),\n\\tag{8.1}\\]\nде \\(S_i(t)\\) позначає ціну акції \\(i\\). Оскільки різні ціни мають різні рівні змінюванності (стандартні відхилення), визначатимемо стандартизовану прибутковість\n\\[\ng_i(t) \\equiv \\frac{G_i(t) - \\left\\langle G_i \\right\\rangle}{\\sigma_i},\n\\tag{8.2}\\]\nде \\(\\sigma_i \\equiv \\sqrt{\\left\\langle G_{i}^{2} \\right\\rangle - \\left\\langle G_i \\right\\rangle^{2}}\\) — стандартне відхилення \\(G_i\\), а \\(\\left\\langle...\\right\\rangle\\) позначає середнє значення за досліджуваний період часу. Тобі обчислення кореляцій \\(C\\) зводиться до обчислення формули:\n\\[\nC_{ij} \\equiv \\left\\langle g_i(t)g_j(t) \\right\\rangle.\n\\tag{8.3}\\]\nЗгідно з побудовою елементи \\(C_{ij}\\) обмежені областю \\(−1 \\leq C_{ij} \\leq 1\\), де \\(C_{ij} = 1\\) відповідає повним кореляціям, \\(C_{ij} = -1\\) — повним антикореляціям, і \\(C_{ij} = 0\\) свідчить про некорельованність пар акцій.\nТруднощі в аналізі важливості та значення коефіцієнтів крос-кореляції \\(C_{ij}\\) виникають внаслідок кількох причин, що полягають в наступному:\n\nринкові умови з часом змінюються і взаємна кореляція, що існує між будь-якою парою акцій, може бути не постійною (нестаціонарною);\nскінчена довжина досліджуваного ряду, доступного для оцінювання взаємних кореляцій, додає так званий “шум вимірювання” — чим коротший досліджуваний ряд — тим менш точними будуть отримувані значення.\n\nЯкщо буде використано довгий ряд для вилучення проблеми скінченної довжини, на отримані значення буде впливати нестаціонарність крос-кореляцій. З цих причин, емпірично виміряні крос-кореляції будуть містити “випадкові” складові, і найбільш важливою (і одночасно важкою проблемою) є оцінка в складі матриці \\(C\\) таких взаємних кореляцій, що не є результатом випадковості.\nЯким же чином можна виділяти з \\(C_{ij}\\) ті акції, що залишилися корельованими на розглядуваному періоді часу? Щоб відповісти на це питання, перевіримо статистику \\(C\\) у порівнянні із так званою “нульовою гіпотезою” випадкової кореляційної матриці — матриці кореляцій, побудованої із взаємно некорельованих часових рядів. Якщо властивості \\(C\\) відповідають властивостям для випадкової матриці кореляцій, тоді можна говорити про те, що значення емпірично вимірюваних властивостей \\(C\\) випадкові. Навпаки, відхилення властивостей \\(C\\) від таких же властивостей для випадкової кореляційної матриці передає інформацію про “справжні” кореляції. Таким чином, нашою метою є порівняння властивостей \\(C\\) з такими ж властивостями випадкової матриці кореляцій і розділ властивостей \\(C\\) на дві групи: (a) частина \\(C\\), що відповідає властивостям випадкової кореляційної матриці (“шум”) і (b) частина \\(C\\), що відхиляється (“інформація”).\n\n\n8.1.2 Розподіл власних значень\nДля отримання інформації про взаємні кореляції \\(C\\) необхідно порівняти властивості \\(C\\) з такими ж властивостями випадкової матриці крос-кореляцій. У матричній нотації така матриця може бути виражена як\n\\[\nC = \\frac{1}{L} GG^{T},\n\\tag{8.4}\\]\nде \\(G\\) — матриця розміру \\(N \\times L\\) з елементами \\(g_{im}=g_i(m\\Delta t), i=1,...,N; m=0,...,L-1\\) і \\(G^{T}\\) позначає транспонування \\(G\\). Розглянемо випадкову кореляційну матрицю\n\\[\nR = \\frac{1}{L} AA^{T},\n\\tag{8.5}\\]\nде \\(A\\) — матриця розміру \\(N \\times L\\), що містить \\(N\\) часових рядів із \\(L\\) випадковими елементів \\(a_{im}\\) з нульовим середнім і одиничним відхиленням, що означають взаємну некорельованість.\nСтатистичні властивості випадкових матриць \\(R\\) відомі. Зокрема, у наближенні \\(N \\to \\infty\\), \\(L \\to \\infty\\), такому, що \\(Q \\equiv \\frac{L}{N}(&gt;1)\\) фіксоване, показано аналітично, що функція розподілу щільності імовірності \\(P_{rm}(\\lambda)\\) власних значень \\(\\lambda\\) випадкової матриці кореляції \\(R\\) визначається як\n\\[\nP_{rm}(\\lambda) = \\frac{Q}{2\\pi}\\frac{\\sqrt{(\\lambda_{+} - \\lambda)(\\lambda - \\lambda_{-})}}{\\lambda}\n\\tag{8.6}\\]\nде \\(\\lambda\\) в межах границь \\(\\lambda_{-} \\leq \\lambda_{i} \\leq \\lambda_{+}\\), де \\(\\lambda_{-}\\) і \\(\\lambda_{+}\\) — найменше та найбільше власні значення \\(R\\), які можна визначити аналітично як\n\\[\n\\lambda_{\\pm} = 1 + \\frac{1}{Q} \\pm 2\\sqrt{\\frac{1}{Q}}.\n\\tag{8.7}\\]\nЗвертаємо вашу увагу, що вираз (Рівняння 8.6) є точним для випадку розподілених за Гаусом матричних елементів \\(a_{im}\\).\nПорівняємо розподіл власних значень \\(P(\\lambda)\\) для \\(C\\) з \\(P_{rm}(\\lambda)\\). Для цього обчислимо власні значення \\(\\lambda_i\\) матриці \\(C\\), причому \\(\\lambda_i\\) впорядкуємо за зростанням (\\(\\lambda_{i+1} &gt; \\lambda_{i}\\)). При дослідженнях зверніть увагу на присутність чіткої “великої частини” власних значень, що спадають у межах границь \\([\\lambda_{-}, \\lambda_{+}]\\) для \\(P_{rm}(\\lambda)\\). Також зверніть увагу на відхилення для деяких найбільших і найменших власних значень власних значень отриманих за допомогою ТВМ.\nОскільки рівняння (Рівняння 8.6) є таким, що строго відповідає лише для \\(L \\to \\infty\\) і \\(N \\to \\infty\\), необхідно перевірити також відхилення від ідеального випадку, оскільки робота проводиться завжди із скінченими рядами. При дослідженнях виявляється, що для кількох найбільших (найменших) власних значень ефект впливу скінчених величин \\(L\\) і \\(N\\) відсутній.\n\n\n8.1.3 Обернене відношення участі\nВивчивши інтерпретацію найбільшого власного значення, що значно відхиляється від результатів ТВМ, зосередимось на власних значеннях, що залишаються. Відхилення розподілу компонентів власного вектора \\(u_k\\) від ТВМ Гаусового передбачення більш явне, коли відстань від верхньої границі ТВМ \\(\\lambda_k - \\lambda_{+}\\) збільшується. Оскільки близькість до \\(\\lambda_{+}\\) збільшує ефекти хаотичності визначаємо кількість компонентів, що беруть значну участь в кожному власному векторі, що, у свою чергу, відображає ступінь відхилення від ТВМ для розподілу компонентів власного вектора. Для цього використовується поняття оберненого відношення участі (ОВУ), що часто застосовується в теорії локалізації. ОВУ власного вектора \\(u_k\\) визначається як\n\\[\nI^{k} \\equiv \\sum_{l=1}^{N}\\left[ u_{l}^{k} \\right]^4,\n\\tag{8.8}\\]\nде \\(u_{l}^{k}\\), \\(l=1,...,N\\) — компоненти власного вектора \\(u^{k}\\). Значення \\(I^{k}\\) може бути проілюстровано двома граничними випадками:\n\nвектор з ідентичними компонентами \\(u_{l}^{k} \\equiv \\frac{1}{\\sqrt{N}}\\) має \\(I^{k}=\\frac{1}{N}\\);\n\nвектор з одним компонентом \\(u_{1}^{k}=1\\) і нульовими іншими має \\(I^{k}=1\\).\n\nТаким чином, ОВУ визначає кількість даних з числа компонентів власного вектора, що значний впливають на ринок, заданий системою часових рядів. Наявність векторів з великими значеннями \\(I^{k}\\) також виникає в теорії локалізації Андерсона. У контексті теорії локалізації часто знаходять “випадкову смугу матриць”, що містять узагальнені стани з маленьким \\(I^{k}\\) в більшій частині спектра власних значень, тоді як основні стани локалізовані і мають великі \\(I^{k}\\). Виявлення локалізованих станів для маленьких і великих власних значень матриці крос-кореляцій \\(C\\) нагадує про локалізацію Андерсона і припускає, що C може мати випадкову зону матричної структури."
  },
  {
    "objectID": "lab_8.html#хід-роботи",
    "href": "lab_8.html#хід-роботи",
    "title": "8  Лабораторна робота № 8",
    "section": "8.2 Хід роботи",
    "text": "8.2 Хід роботи\nІмпортуємо необхідні бібліотеки:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport pandas as pd\nimport yfinance as yf\nimport scienceplots\nimport requests\n\nВизначаємо стиль рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nВиконуємо парсинг та фільтрацію заголовків акцій компаній:\n\nheaders = {\n    'authority': 'api.nasdaq.com',\n    'accept': 'application/json, text/plain, */*',\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',\n    'origin': 'https://www.nasdaq.com',\n    'sec-fetch-site': 'same-site',\n    'sec-fetch-mode': 'cors',\n    'sec-fetch-dest': 'empty',\n    'referer': 'https://www.nasdaq.com/',\n    'accept-language': 'en-US,en;q=0.9',\n}\n\nparams = (\n    ('tableonly', 'true'),\n    ('limit', '25'),\n    ('offset', '0'),\n    ('download', 'true'),\n)\n\nr = requests.get('https://api.nasdaq.com/api/screener/stocks', headers=headers, params=params)\ndata = r.json()['data']\ndf = pd.DataFrame(data['rows'], columns=data['headers'])\ndf = df.dropna(subset={'marketCap'})\ndf = df[~df['symbol'].str.contains(\"\\/|\\.|\\^\")]\ndf.head()\n\n\n\n\n\n\n\n\nsymbol\nname\nlastsale\nnetchange\npctchange\nmarketCap\ncountry\nipoyear\nvolume\nsector\nindustry\nurl\n\n\n\n\n0\nA\nAgilent Technologies Inc. Common Stock\n$103.68\n-1.96\n-1.855%\n30335448465.00\nUnited States\n1999\n2255378\nIndustrials\nElectrical Products\n/market-activity/stocks/a\n\n\n1\nAA\nAlcoa Corporation Common Stock\n$23.86\n0.42\n1.792%\n4257807575.00\nUnited States\n2016\n4466232\nIndustrials\nAluminum\n/market-activity/stocks/aa\n\n\n2\nAAC\nAres Acquisition Corporation Class A Ordinary ...\n$10.77\n-0.01\n-0.093%\n760407880.00\n\n2021\n62409\nIndustrials\nMetal Fabrications\n/market-activity/stocks/aac\n\n\n3\nAACG\nATA Creativity Global American Depositary Shares\n$1.02\n-0.0089\n-0.865%\n32254613.00\nChina\n2008\n2205\nReal Estate\nOther Consumer Services\n/market-activity/stocks/aacg\n\n\n4\nAACIW\nArmada Acquisition Corp. I Warrant\n$0.0869\n-0.0011\n-1.25%\n0.00\nUnited States\n2021\n1100\nFinance\nBlank Checks\n/market-activity/stocks/aaciw\n\n\n\n\n\n\n\nФільтруємо та сортуємо заголовків акцій за їх капіталізацією:\n\ndef cust_filter(mkt_cap):\n    if 'M' in mkt_cap:\n        return float(mkt_cap[1:-1])\n    elif 'B' in mkt_cap:\n        return float(mkt_cap[1:-1]) * 1000\n    elif mkt_cap == '':\n        return 0.0\n    else:\n        return float(mkt_cap[1:]) / 1e6\n    \ndf['marketCap'] = df['marketCap'].apply(cust_filter)\ndf = df.sort_values('marketCap', ascending=False)\ndf.head()\n\n\n\n\n\n\n\n\nsymbol\nname\nlastsale\nnetchange\npctchange\nmarketCap\ncountry\nipoyear\nvolume\nsector\nindustry\nurl\n\n\n\n\n18\nAAPL\nApple Inc. Common Stock\n$171.13\n-2.31\n-1.332%\n675486.122160\nUnited States\n1980\n42331738\nTechnology\nComputer Manufacturing\n/market-activity/stocks/aapl\n\n\n2995\nGOOG\nAlphabet Inc. Class C Capital Stock\n$126.83\n-13.29\n-9.485%\n599199.470000\nUnited States\n2004\n53289059\nTechnology\nComputer Software: Programming Data Processing\n/market-activity/stocks/goog\n\n\n2996\nGOOGL\nAlphabet Inc. Class A Common Stock\n$125.74\n-13.07\n-9.416%\n585455.660000\nUnited States\n2004\n74470572\nTechnology\nComputer Software: Programming Data Processing\n/market-activity/stocks/googl\n\n\n4477\nMSFT\nMicrosoft Corporation Common Stock\n$340.64\n10.11\n3.059%\n530874.714262\nUnited States\n1986\n47823790\nTechnology\nComputer Software: Prepackaged Software\n/market-activity/stocks/msft\n\n\n408\nAMZN\nAmazon.com Inc. Common Stock\n$121.35\n-7.21\n-5.608%\n252059.059095\nUnited States\n1997\n64091789\nConsumer Discretionary\nCatalog/Specialty Distribution\n/market-activity/stocks/amzn\n\n\n\n\n\n\n\nВизначаємо найпередовіші акцій за їх капіталізацією:\n\ntop = 200\ntickers_list = df.iloc[:top]['symbol'].tolist()\ntickers_list[:10]\n\n['AAPL', 'GOOG', 'GOOGL', 'MSFT', 'AMZN', 'ACN', 'JNJ', 'UNH', 'CRM', 'CVX']\n\n\nЗчитуємо дані з Yahoo Finance згідно створенного списку акцій:\n\nstart = \"2001-12-31\"\nend = \"2023-10-25\"\ndata = yf.download(tickers_list, start, end)[\"Adj Close\"]\n\n\nperc = 5.0 \nmin_count =  int(((100-perc)/100)*data.shape[0] + 1)\ndata = data.dropna(axis=1, thresh=min_count)\ndata\n\n\n\n\n\n\n\n\nAAPL\nABEV\nABT\nACN\nADBE\nADI\nADM\nAEP\nAJG\nALGN\n...\nUMC\nUNH\nUNP\nUPS\nUSB\nVZ\nWAB\nWBA\nWMT\nXOM\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2001-12-31\n0.331940\n0.284807\n14.984426\n19.391705\n15.452660\n28.258905\n8.876990\n17.383923\n17.229757\n4.500000\n...\n5.445521\n14.341556\n9.328091\n30.550989\n10.333231\n14.899978\n5.713789\n21.294727\n38.050941\n19.562275\n\n\n2002-01-02\n0.353160\n0.308827\n15.008610\n18.880257\n15.845817\n28.870050\n8.765645\n17.551647\n16.954996\n4.300000\n...\n5.502244\n14.270627\n9.262630\n30.528574\n10.145624\n15.223341\n5.709144\n20.927788\n38.381535\n19.711607\n\n\n2002-01-03\n0.357404\n0.308827\n15.022053\n18.289574\n16.462931\n30.340614\n8.672855\n17.427847\n16.635281\n4.480000\n...\n5.655400\n14.110532\n9.603026\n31.089144\n10.194996\n15.728801\n5.685916\n21.769196\n38.335232\n19.741465\n\n\n2002-01-04\n0.359071\n0.308827\n14.995171\n19.953569\n17.866360\n30.525223\n8.351179\n17.280092\n16.395496\n4.500000\n...\n5.604348\n14.185513\n9.819045\n31.756235\n10.387540\n15.888920\n5.690560\n21.864100\n38.083996\n19.910711\n\n\n2002-01-07\n0.347098\n0.308827\n14.890348\n19.053139\n17.995756\n29.824957\n8.307874\n17.551647\n15.915919\n5.010000\n...\n5.485227\n14.094324\n9.807587\n32.014080\n10.362859\n15.791601\n5.597653\n22.066542\n37.945152\n19.736486\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-18\n175.839996\n2.450000\n95.559998\n303.769989\n557.869995\n171.270004\n74.519997\n75.169998\n234.339996\n272.399994\n...\n7.400000\n536.059998\n205.919998\n153.820007\n33.369999\n31.049999\n101.660004\n21.250000\n161.539993\n112.949997\n\n\n2023-10-19\n175.460007\n2.450000\n95.440002\n302.940002\n555.739990\n168.720001\n73.839996\n74.419998\n231.029999\n270.290009\n...\n7.550000\n531.630005\n210.330002\n152.059998\n32.750000\n31.580000\n100.080002\n20.959999\n160.770004\n113.019997\n\n\n2023-10-20\n172.880005\n2.460000\n96.779999\n297.000000\n540.960022\n166.520004\n72.849998\n73.349998\n227.570007\n269.880005\n...\n7.430000\n527.030029\n211.339996\n151.960007\n30.930000\n31.570000\n99.699997\n21.260000\n158.759995\n111.080002\n\n\n2023-10-23\n173.000000\n2.450000\n95.779999\n294.940002\n540.409973\n163.869995\n72.400002\n73.349998\n227.699997\n265.720001\n...\n7.420000\n521.570007\n207.759995\n148.160004\n31.260000\n31.389999\n98.739998\n21.959999\n161.009995\n109.449997\n\n\n2023-10-24\n173.440002\n2.500000\n94.809998\n296.089996\n539.559998\n164.929993\n69.470001\n74.739998\n231.210007\n265.459991\n...\n7.440000\n525.000000\n205.440002\n149.320007\n31.389999\n34.299999\n99.940002\n21.370001\n163.250000\n108.389999\n\n\n\n\n5492 rows × 135 columns\n\n\n\n\ndata = data.dropna(axis=0)\ndata\n\n\n\n\n\n\n\n\nAAPL\nABEV\nABT\nACN\nADBE\nADI\nADM\nAEP\nAJG\nALGN\n...\nUMC\nUNH\nUNP\nUPS\nUSB\nVZ\nWAB\nWBA\nWMT\nXOM\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2002-12-16\n0.225083\n0.23162\n11.242016\n12.822151\n13.237535\n17.169249\n8.008354\n11.473660\n15.008780\n3.430000\n...\n2.165762\n16.765938\n10.106949\n36.486992\n10.987369\n13.026583\n6.303859\n18.376917\n34.474094\n18.295858\n\n\n2002-12-17\n0.228569\n0.23162\n11.255742\n13.023845\n13.377089\n17.131050\n7.819630\n11.700225\n14.826304\n3.280000\n...\n2.136574\n16.697002\n10.053861\n36.271317\n10.901052\n12.772179\n6.336473\n18.503960\n33.810364\n18.122383\n\n\n2002-12-18\n0.220839\n0.23162\n11.189867\n12.959014\n12.973382\n16.074289\n7.687520\n11.755794\n14.577931\n3.050000\n...\n2.043172\n16.626055\n9.954317\n36.294014\n10.677649\n12.801530\n6.103514\n18.364222\n33.487984\n18.132597\n\n\n2002-12-19\n0.215230\n0.23162\n11.085575\n12.634859\n12.729166\n15.883307\n7.681227\n11.657471\n14.496831\n2.900000\n...\n2.037334\n16.624023\n9.954317\n36.118042\n10.652270\n12.651503\n6.178061\n18.408678\n33.341740\n17.933613\n\n\n2002-12-20\n0.214321\n0.23162\n10.621729\n12.353925\n12.828848\n16.201601\n7.807046\n11.875497\n14.689438\n3.050000\n...\n2.049009\n16.439529\n10.101971\n36.203201\n11.119382\n13.046148\n6.289883\n18.866041\n33.760506\n18.214230\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-09\n178.990005\n2.53000\n96.227318\n310.721283\n529.289978\n173.320007\n73.900002\n73.660004\n233.110001\n283.940002\n...\n7.090000\n526.510010\n204.300003\n154.339996\n32.009998\n31.450001\n103.370003\n22.270000\n155.839996\n110.919998\n\n\n2023-10-10\n178.389999\n2.60000\n97.022911\n311.029999\n532.719971\n175.970001\n74.150002\n74.230003\n233.490005\n286.510010\n...\n7.170000\n524.239990\n206.350006\n155.550003\n32.590000\n31.760000\n104.940002\n22.379999\n157.600006\n110.449997\n\n\n2023-10-11\n179.800003\n2.60000\n92.129997\n312.540009\n549.909973\n174.919998\n73.699997\n75.279999\n234.899994\n277.519989\n...\n7.250000\n524.130005\n209.479996\n155.300003\n32.509998\n31.459999\n105.739998\n22.600000\n158.229996\n106.489998\n\n\n2023-10-12\n180.710007\n2.54000\n90.190002\n304.359985\n559.630005\n173.600006\n72.769997\n73.480003\n233.440002\n268.000000\n...\n7.210000\n525.539978\n207.990005\n155.380005\n32.259998\n30.910000\n104.010002\n24.190001\n158.949997\n106.470001\n\n\n2023-10-13\n178.850006\n2.55000\n90.870003\n301.829987\n548.760010\n171.070007\n73.320000\n74.470001\n233.880005\n265.989990\n...\n7.100000\n539.400024\n207.750000\n155.080002\n32.169998\n30.670000\n102.570000\n23.250000\n159.830002\n109.870003\n\n\n\n\n5243 rows × 135 columns\n\n\n\n\ndata = data.T\ndata\n\n\n\n\n\n\n\nDate\n2002-12-16\n2002-12-17\n2002-12-18\n2002-12-19\n2002-12-20\n2002-12-23\n2002-12-24\n2002-12-26\n2002-12-27\n2002-12-30\n...\n2023-10-02\n2023-10-03\n2023-10-04\n2023-10-05\n2023-10-06\n2023-10-09\n2023-10-10\n2023-10-11\n2023-10-12\n2023-10-13\n\n\n\n\nAAPL\n0.225083\n0.228569\n0.220839\n0.215230\n0.214321\n0.219626\n0.217656\n0.218262\n0.213108\n0.213260\n...\n173.750000\n172.399994\n173.660004\n174.910004\n177.490005\n178.990005\n178.389999\n179.800003\n180.710007\n178.850006\n\n\nABEV\n0.231620\n0.231620\n0.231620\n0.231620\n0.231620\n0.231620\n0.231620\n0.231620\n0.231620\n0.231620\n...\n2.550000\n2.500000\n2.550000\n2.490000\n2.540000\n2.530000\n2.600000\n2.600000\n2.540000\n2.550000\n\n\nABT\n11.242016\n11.255742\n11.189867\n11.085575\n10.621729\n10.465286\n10.347263\n10.059079\n10.094759\n10.182588\n...\n95.202988\n95.262657\n95.123428\n95.670395\n96.346657\n96.227318\n97.022911\n92.129997\n90.190002\n90.870003\n\n\nACN\n12.822151\n13.023845\n12.959014\n12.634859\n12.353925\n12.389941\n12.584430\n12.865371\n12.742911\n12.836556\n...\n306.986786\n304.965179\n308.112122\n308.380981\n310.900543\n310.721283\n311.029999\n312.540009\n304.359985\n301.829987\n\n\nADBE\n13.237535\n13.377089\n12.973382\n12.729166\n12.828848\n13.232551\n13.073062\n12.803926\n12.549742\n12.405205\n...\n521.130005\n507.029999\n518.419983\n516.440002\n526.679993\n529.289978\n532.719971\n549.909973\n559.630005\n548.760010\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nVZ\n13.026583\n12.772179\n12.801530\n12.651503\n13.046148\n13.013534\n12.883075\n12.945032\n12.687381\n12.713467\n...\n31.096254\n31.311655\n30.880852\n31.154999\n30.850000\n31.450001\n31.760000\n31.459999\n30.910000\n30.670000\n\n\nWAB\n6.303859\n6.336473\n6.103514\n6.178061\n6.289883\n6.219993\n6.294541\n6.397041\n6.322494\n6.471588\n...\n105.239998\n104.500000\n105.000000\n102.099998\n102.930000\n103.370003\n104.940002\n105.739998\n104.010002\n102.570000\n\n\nWBA\n18.376917\n18.503960\n18.364222\n18.408678\n18.866041\n18.554777\n18.453140\n18.357862\n18.294350\n18.345161\n...\n22.420000\n22.549999\n22.290001\n22.180000\n21.809999\n22.270000\n22.379999\n22.600000\n24.190001\n23.250000\n\n\nWMT\n34.474094\n33.810364\n33.487984\n33.341740\n33.760506\n32.962849\n33.035984\n33.075851\n32.677044\n33.660805\n...\n160.100006\n159.089996\n161.000000\n159.080002\n156.410004\n155.839996\n157.600006\n158.229996\n158.949997\n159.830002\n\n\nXOM\n18.295858\n18.122383\n18.132597\n17.933613\n18.214230\n18.239742\n18.071362\n18.020346\n17.673412\n17.729530\n...\n115.629997\n115.830002\n111.500000\n108.989998\n107.169998\n110.919998\n110.449997\n106.489998\n106.470001\n109.870003\n\n\n\n\n135 rows × 5243 columns\n\n\n\n\n8.2.1 Знаходження коефіцієнтів матриці крос-кореляцій\n\nlog_ret = data.pct_change(axis=1).iloc[:,1:]\nlog_ret = log_ret.values\nlog_ret = (log_ret - np.mean(log_ret, axis=1, keepdims=True)) / np.std(log_ret, axis=1, keepdims=True)\n\n\nN, T = log_ret.shape\n\n\nC = (1/T)*np.dot(log_ret, log_ret.T)\ndi = np.diag_indices(N)\nccoef = np.ma.asarray(C)\nccoef[di] = np.ma.masked\nccoef_flat = ccoef.compressed()\n\n\nnp.random.seed(1234)\nrandom_stocks = np.random.normal(size=(N,T))\nR = (1/T)*np.dot(random_stocks, random_stocks.T)\ndi_rand = np.diag_indices(N)\nccoef_rand = np.ma.asarray(R)\nccoef_rand[di_rand] = np.ma.masked\nccoef_flat_rand = ccoef_rand.compressed()\n\n\nfig = plt.figure(figsize=(15, 10))\ngs = gridspec.GridSpec(2, 2)\n\nax1 = fig.add_subplot(gs[0, 0])\nim1 = ax1.imshow(C, cmap='hot', interpolation='nearest')\nfig.colorbar(im1, ax=ax1)\n\nax2 = fig.add_subplot(gs[0, 1])\nim2 = ax2.imshow(R, cmap='hot', interpolation='nearest')\nfig.colorbar(im2, ax=ax2)\n\nax3 = fig.add_subplot(gs[1, :])\nax3.hist(ccoef_flat, bins='auto', density=True, label=r'$P^{init}(C_{ij})$')\nax3.hist(ccoef_flat_rand, bins='auto', density=True, label=r'$P^{rand}(C_{ij})$')\nax3.set_xlabel(r'$C_{ij}$')\nax3.set_ylabel(r'$P(C_{ij})$')\nax3.legend()\n\nfig.align_labels()\nplt.show();\n\n\n\n\n\n\n8.2.2 Розподіл власних значень та векторів\n\nw, v = np.linalg.eig(C)\nw_rand, v_rand = np.linalg.eig(R)\n\n\nQ = T/N\n\n\nlambda_plus = 1 + 1/Q + 2*np.sqrt(1/Q)\nlambda_minus = 1 - 1/Q - 2*np.sqrt(1/Q)\n\n\nlambda_plus\n\n1.3467116522335216\n\n\n\nlambda_minus\n\n0.6532883477664784\n\n\n\nlambda_random = np.linspace(lambda_minus, lambda_plus, 1000)\nP = (Q/(2*np.pi))*np.sqrt((lambda_plus-lambda_random)*(lambda_random-lambda_minus))/lambda_random\n\n\nlambda_max = w.max()\nlambda_max\n\n53.422930355995014\n\n\n\nfig, ax = plt.subplots(2, 1, figsize=(13, 10))\nax[0].hist(w, bins=50, density=True, color=\"skyblue\", label='$P^{init}_{\\it{empiric}}$')\nax[0].hist(w_rand, bins='auto', density=True, color=\"red\", label='$P^{rand}_{\\it{empiric}}$')\nax[0].set_xlabel('$\\lambda_{i}$')\nax[0].set_ylabel('$P(\\lambda_{i})$')\nax[0].text(20, 0.5, '$\\lambda_{max}=$'+f'{lambda_max:.2f}', ha='center', va='center')\nax[0].set_yscale('log')\nax[0].legend()\n\nax[1].hist(w, bins='auto', density=True, color=\"skyblue\", label='$P^{init}_{\\it{empiric}}$')\nax[1].hist(w_rand, bins='auto', density=True, color=\"red\", label='$P^{rand}_{\\it{empiric}}$')\nax[1].set_xlabel('$\\lambda_{i}$')\nax[1].set_ylabel('$P(\\lambda_{i})$')\nax[1].set_xlim(lambda_minus-3*np.std(w_rand), lambda_plus+3*np.std(w_rand))\nax[1].scatter(lambda_random, P, label='$P_{\\it{RMT}}$', color='green')\nax[1].legend()\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(3, 1, figsize=(15, 10), sharex=True, sharey=True)\nax[0].hist(v[:, 0], bins=50, density=True, label=r'$\\rho(u)_{empiric}^{init}$')\nax[0].hist(v_rand[:, 0], bins=50, density=True, alpha=0.7, label=r'$\\rho(u)_{empiric}^{rand}$')\nax[0].set_xlabel('$u_{1}$')\nax[0].set_ylabel('$P(u)$')\nax[0].set_title('Eigenvector components of $ \\lambda_{1} $')\nax[0].legend()\n\nax[1].hist(v[:, 30], bins=50, density=True, label=r'$\\rho(u)_{empiric}^{init}$')\nax[1].hist(v_rand[:, 30], bins=50, density=True, alpha=0.7, label=r'$\\rho(u)_{empiric}^{rand}$')\nax[1].set_xlabel('$u_{30}$')\nax[1].set_ylabel('$P(u)$')\nax[1].set_title('Eigenvector components of $ \\lambda_{30} $')\nax[1].legend()\n\nax[2].hist(v[:, 70], bins=50, density=True, label=r'$\\rho(u)_{empiric}^{init}$')\nax[2].hist(v_rand[:, 70], bins=50, density=True, alpha=0.7, label=r'$\\rho(u)_{empiric}^{rand}$')\nax[2].set_xlabel('$u_{70}$')\nax[2].set_ylabel('$P(u)$')\nax[2].set_title('Eigenvector components of $ \\lambda_{70} $')\nax[2].legend()\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n8.2.3 Обернене відношення участі\n\nIPR = np.sum(v**4, axis=0)\nIPR_rand = np.sum(v_rand**4, axis=0)\n\n\nfig, ax = plt.subplots(1, 1)\n\nax.scatter(w, IPR, color='green', label=r'$IPR_{\\it{init}}$', s=10**2, marker='x')\nax.scatter(w_rand, IPR_rand, color='red', label=r'$IPR_{\\it{rand}}$', s=10**2, marker='x')\nax.set_xlabel(r'$\\lambda_{i}$')\nax.set_ylabel(r'$IPR$')\nax.set_xscale('log')\n\nfig.tight_layout()\nplt.legend()\nplt.show();"
  },
  {
    "objectID": "lab_9.html#теоретичні-відомості",
    "href": "lab_9.html#теоретичні-відомості",
    "title": "9  Лабораторна робота № 9",
    "section": "9.1 Теоретичні відомості",
    "text": "9.1 Теоретичні відомості\nЗдавалося б, випадкові коливання у складних системах часто демонструють різний рівень складності та хаотичності. В умовах обмеженості даних стає важко визначити межі їх передбачуваності. Аналіз таких систем, процесів, що визначають їх динаміку, теорія хаосу розглядалася в різних галузях, таких як економіка, фінанси, фізика та ін. Що стосується аналізу, наприклад, динаміки Біткоїна, то знання про його абсолютно випадкові і, водночас, детерміновані процеси потенційно можуть пояснити флуктуації часових рядів різної природи. З огляду на фінансовий сектор, докази детермінованого хаосу, знання таких моментів, коли дві спочатку близькі траєкторії починають розходитися, і періодів, протягом яких вони залишатимуться близькими одна до одної, матимуть важливе значення для регуляторів і трейдерів, які розроблятимуть ефективні короткострокові торгівельні стратегії. Протягом багатьох років теорія хаосу надавала підходи до вивчення деяких цікавих властивостей часових рядів. Найбільш поширеними є: кореляційна розмірність, BDS тест, ентропія Колмогорова, показники Ляпунова тощо.\nЗастосувавши до часового ряду досліджуваної системи підхід ковзного вікна та показників Ляпунова, ми відзеркалюватимемо його перехід між хаотичною та нехаотичною поведінкою.\n\n9.1.1 Показники Ляпунова\nЕволюція системи демонструє чутливу залежність від початкових умов. Це означає, що спочатку близькі траєкторії, які розвиваються, можуть швидко відхилятися одна від одної і мати абсолютно різні результати. Відповідно, при малих невизначеностях, які надзвичайно швидко посилюються, довгострокові прогнози виявляються неможливими. З іншого боку, в системі з точками тяжіння або стабільними точками відстань між ними асимптотично зменшується з часом або з кількістю точок, які мають тенденцію до зближення.\nЩоб представити ідею більш точно, розглянемо дві послідовні траєкторії — \\(x(t)\\) та наближчого сусіда цієї траєкторії з невеликим зміщенням, \\(x(t) + \\delta(t)\\), де \\(\\delta(t)\\) представляє собою крихітне відхилення в часі \\(t\\), як показано на наступному рисунку (Рис. 9.1).\n\n\n\nРис. 9.1: Розбіжність двох початково близьких траєкторій у динамічній системі\n\n\nКоли динаміка двох початково близьких траєкторій порушуються певною подією, вони починають розходитися, і відстань між ними зростає за експоненціальним законом:\n\\[\n\\| \\delta(t) \\| \\approx \\| \\delta(0) \\| \\exp(\\lambda t),\n\\tag{9.1}\\]\nде \\(\\lambda\\) позначає показник Ляпунова (ПЛ); \\(\\delta(t)\\) — відстань між точкою що розглядається та її наближчим сусідом після часу \\(t\\) (або після \\(t\\) ітерацій); \\(\\delta(0)\\) — це початкова відстань між точкою що розглядається та її найближчим сусідом у початковий момент часу (\\(t=0\\)).\nПЛ є мірою швидкості експоненціальної розбіжності близьких один до одного траєкторій у фазовому просторі динамічної системи. Іншими словами, ПЛ показує, наскільки швидко зближуються або розходяться траєкторії, які починаються близько одна від одної, вимірюючи ступінь хаосу в системі.\nУ тих випадках, коли наша система \\(n\\)-вимірна, ми маємо стільки ПЛ, скільки вимірів у ній. Для їх визначення розглянемо еволюцію нескінченно малої сфери, що зазнала збурень за різними осями (зазначала початкових умов). Визначивши величину збурення по вісі \\(i\\) як \\(\\delta_i(t)\\), отримаємо \\(n\\) показників Ляпунова, що мають вид\n\\[\n\\| \\delta_i(t) \\| \\approx \\| \\delta_i(0) \\| \\exp(\\lambda_i t), \\; \\text{для} \\; i=1,...,n.\n\\tag{9.2}\\]\nДля визначення того, чи є рух періодичним або хаотичним, особливо для великих \\(t\\), рекомендується розглядати внесок системи в найбільший показник Ляпунова (НПЛ), оскільки діаметр \\(n\\)-розмірного еліпсоїда починає залежати від неї. Саме НПЛ використовується для кількісної оцінки передбачуваності систем, оскільки експоненціальна розбіжність означає, що в системі, де початкове збурення було нескінченно малим, починаються втрати передбачуванності. Однак слід зазначити, що інші експоненти також містять важливу інформацію про стійкість системи, в тому числі про напрямки збіжності та розбіжності траєкторій.\nІснування принаймні одного позитивного ПЛ зазвичай розглядається як сильний індикатор хаосу. Позитивний ПЛ означає, що початково близькі траєкторії у фазовому просторі, чутливі до початкових умов і розходяться експоненціально швидко, характеризують хаотичну поведінку системи. Негативний ПЛ відповідає випадкам, коли траєкторії залишаються близькими одна до одної, але це не обов’язково означає стабільність, і ми повинні дослідити нашу систему більш детально. Нульові або дуже близькі до нуля експоненти вказують на те, що збурення, внесені вздовж траєкторії, не розходяться і не зближуються.\nУ зв’язку з великою зацікавленістю в ПЛ, з’являється все більше пропозицій щодо їх розрахунку. На жаль, досі не отримано загальноприйнятого та універсального методу оцінки всього спектру показників Ляпунова за значеннями часового ряду. Одні з найбільш поширених і популярних алгоритмів були застосовані Вольфом та ін., Сано і Савадою, а пізніше вдосконалені Екманом, Розенштейном, Парліцом, Бальцержаком тощо.\nТеорія хаосу та її інструментарій залишаються величезним викликом для дослідників різних галузей науки. У світі показників Ляпунова зберігається зростаючий інтерес до їх визначення, чисельних методів та застосування до різних складних систем. Підсумовуючи, СПЛ дозволяє встановити: - область чутливості до початкових умов; - область хаосу; - область стабільності.\n\n9.1.1.1 Метод Екмана\nПо-перше, згідно з підходом Екмана та ін., ми повинні реконструювати динаміку атрактора з одного часового ряду \\(\\{x(i) \\,|\\, i=1,...N \\}\\) з розмірністю вкладень \\(d_E\\), і після цього побудувати \\(d_E\\)-вимірну орбіту, що представляє часову еволюцію\n\\[\n\\vec{X}(i) = \\left[ x(i), x(i+1), ..., x(i+(d_E - 1)) \\right], \\; \\text{для} \\; i=1,...,N-d_E+1.\n\\]\nДалі, ми маємо визначити найближчи до \\(\\vec{X}(i)\\) траєкторії:\n\\[\n\\| \\vec{X}(i) - \\vec{X}(j) \\| = \\max_{0\\leq \\alpha \\leq d_E-1} \\left| x(i+\\alpha) - x(j+\\alpha) \\right|.   \n\\tag{9.3}\\]\nМи сортуємо \\(x(i)\\) так, щоб \\(x(\\Pi(1)) \\leq x(\\Pi(2)) \\leq ... \\leq x(\\Pi(N))\\) і зберігаємо перестановку \\(\\Pi\\) та її зворотню версію \\(\\Pi^-1\\). Далі ми намагаємось знайти сусідів \\(x(i)\\), переглядаючи \\(k=\\Pi^{-1}(i)\\) і скануємо \\(x(\\Pi(s))\\) при $s=k+1, k+2, … k-1, k-2, … $ до тих пір, до поки не виконається умова \\(x(\\Pi(𝑠)) - x(i) &gt; r\\). Для вибраної розмірності вкладення \\(d_E &gt; 1\\) вибираємо значення \\(s\\), для якого виконується наступна умова\n\\[\n\\left| x(\\Pi(s) + \\alpha) - x(j + \\alpha) \\right| \\leq r, \\; \\text{для} \\; \\alpha=0,1,...,d_E-1.\n\\]\nПісля того, як ми реконструювали нашу систему у розмірність \\(d_E\\), нам потрібно визначити матрицю \\(M_i\\) розмірності \\(d_E \\times d_E\\), яка описуватиме часову еволюцію векторів, що оточують траєкторію \\(\\vec{X}(i)\\), і те, як вони відображаються на стан \\(\\vec{X}(i+1)\\). Матриця \\(M_i\\) отримується шляхом пошуку сусідів\n\\[\nM_i(\\vec{X}(i) - \\vec{X}(j)) \\approx \\vec{X}(i+1) - \\vec{X}(j+1).\n\\tag{9.4}\\]\nПроте вектори \\(\\vec{X}(i)-\\vec{X}(j)\\) можуть і не покривати \\(\\mathbb{R}^{d_E}\\). У цьому випадку така невизначеність може призвести до хибних експонент, які можуть зіпсувати аналіз. Для подолання таких перешкод проекція траєкторій визначається на підпростір розмірності \\(d_M \\leq d_E\\). Таким чином, простір, на якому відбувається динаміка, відповідає локальній розмірності \\(d_M\\), де \\(d_E\\) має бути дещо більшим за \\(d_M\\), щоб уникнути наявності хибних сусідів. Звідси випливає, що траєкторія \\(\\vec{X}(i)\\) асоціюється з \\(d_M\\)-вимірним вектором\n\\[\n\\vec{X}(i) = \\left[ x(i), x(i+\\tau), ..., x(i+(d_M - 1)\\tau) \\right] = \\left[ x(i), x(i+\\tau), ..., x(i+d_E - 1) \\right],\n\\tag{9.5}\\]\nде \\(\\tau=(d_E-1)/(d_M-1)\\). Коли \\(\\tau&gt;1\\), умова (Рівняння 9.4) замінюється наступним виразом:\n\\[\nM_i(\\vec{X}(i) - \\vec{X}(j)) \\approx \\vec{X}(i+\\tau) - \\vec{X}(j+\\tau).\n\\tag{9.6}\\]\nМатриця \\(M_i\\) потім визначається методом найменших квадратів. Останнім кроком є QR декомпозиція, для знаходження ортогональних матриць \\(Q_i\\) і верхніх трикутних матриць \\(R_i\\) при яких\n\\[\nM_{1+i\\tau}Q_i = Q_{i+1}R_{i+1}, \\; \\text{для} \\; i=0,1,2,... .\n\\]\nЯк було запропоновано Екманом, для знаходження \\(d_M\\) показників Ляпунова, знаючи \\(K\\) кількість точок на атракторі, діагональні власні значення матриці \\(R_i\\) та крок дискретизації \\(\\Delta t\\), можна визначити наступне рівняння для знаходження \\(k\\)-го ПЛ:\n\\[\n\\lambda_k = \\frac{1}{\\Delta t}\\frac{1}{\\tau}\\frac{1}{K}\\sum_{i=0}^{K-1}\\ln{(R_i)_{kk}}.\n\\]\nТаким чином, за допомогою лінеаризації із використанням діагональних елементів з QR-розкладу ми можемо обчислити показники Ляпунова.\n\n\n9.1.1.2 Метод Розенштейна\nАлгоритм Розенштейна використовує метод реконструкції вкладень із часовою затримкою, який реконструює найважливіші особливості багатовимірного атрактора в один одновимірний часовий ряд деякого скінченного розміру \\(N\\). Для часового ряду кожен вектор \\(\\vec{X}(i)\\) буде представлений подібно до вектора (Рівняння 9.5) з розмірністю вкладень \\(d_E\\) і часовою затримкою \\(\\tau\\). Потім на відновленій траєкторії ми ініціалізуємо пошук у просторі станів найближчого найближчого сусіда \\(\\vec{X}(j)\\) для траєкторії \\(\\vec{X}(i)\\):\n\\[\n\\delta_i(0) = \\min_{\\vec{X}(i)} \\| \\vec{X}(i) - \\vec{X}(j) \\|, \\; \\text{для} \\; \\left| i-j \\right| &gt; \\text{середній період},\n\\]\nде \\(\\| \\cdot \\|\\) — це Евклідова норма, \\(\\vec{X}(j)\\) — найближчий сусідня траєкторія, \\(\\vec{X}(i)\\) — розглядувана траєкторія.\nЗ рівняння (Рівняння 9.1) ми вже знаємо, що відстань між станами \\(\\vec{X}(i)\\) та \\(\\vec{X}(j)\\) зростає з часом відповідно до степеневого закону, де \\(\\lambda\\) є хорошим наближенням СПЛ. Для подальших оцінок ми розглянемо логарифм траєкторії відстані \\(\\ln{\\delta_i(k)} \\approx \\lambda(k\\cdot \\Delta t) + \\ln{c_i}\\), де \\(\\delta_i(k)\\) — відстань між \\(i\\)-ою парою найближчих сусідів, визначених у рівнянні (Рівняння 9.6) через \\(k\\) часових кроків, \\(c_i\\) — початкова відстань між ними, а \\(\\Delta t\\) — часовий інтервал між вимірюваннями (період дискретизації часового ряду).\nПодальший результат цього алгоритму представляє функцію від часу\n\\[\ny(k, \\Delta t) = \\frac{1}{\\Delta t}\\frac{1}{M}\\sum_{i=1}^{M}\\ln{\\delta_i(k)},\n\\]\nде \\(M=N-(d_E-1)\\tau\\) представляє розмір реконструйованого часового ряду, а \\(\\delta_i(k)\\) — представляє \\(i\\)-у лінію, нахил котрої приблизно рівний СПЛ. Тоді пропонується обчислювати СПЛ як кут нахилу найбільш лінійної ділянки. Знаходження такої ділянки виявляється нетривіальною задачею, а іноді взагалі неможливо вказати таку ділянку. Незважаючи на цю проблему, метод Розенштейна є простим для реалізації та обчислення."
  },
  {
    "objectID": "lab_9.html#хід-роботи",
    "href": "lab_9.html#хід-роботи",
    "title": "9  Лабораторна робота № 9",
    "section": "9.2 Хід роботи",
    "text": "9.2 Хід роботи\nРозглянемо, як можна використовувати зазначені підходи для розрахунку відповідних хаос-динамічних індикаторів. Спочатку імпортуємо необхідні бібліотеки.\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nДалі виконаємо налаштування формату виведення рисунків.\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nРозглянемо значення фондового індексу Доу Джонса за весь період, що представляє Yahoo! Finance. Початкову та кінцеві дати зазначати не будемо.\n\nsymbol = '^DJI'                       # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 9.2: Динаміка щоденних змін фондового індексу Доу Джонса\n\n\n\n\nВизначимо функцію transformation() для виконання перетворення ряду до прибутковостей або стандартизованих значень:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nВизначимо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n9.2.1 Обчислення показників Ляпунова із використанням віконної процедури\nДля подальших розрахунків використовуватимемо бібліотеку neurokit2. Ключовою функцією для отримання відповідних показників є complexity_lyapunov(). Вона надає доступ до розрахунків згідно з наступними алгоритмами:\n\nРозенштейна та ін. (1993) запропонували алгоритм для обчислення СПЛ з невеликих наборів даних. Спочатку часовий ряд реконструюється за допомогою методу вбудовування затримки, а найближчий сусід кожного вектора обчислюється за допомогою евклідової відстані. Потім ці дві сусідні точки відстежуються вздовж їхніх траєкторій відстані для ряду точок даних. Нахил лінії з використанням методу найменших квадратів до середньої логарифмічної траєкторії відстаней дає остаточне значення LLE.\nМаковскі — це спеціальна модифікація алгоритму Розенштейна, що використовує процедуру \\(k\\)-вимірного дерева для більш ефективного обчислення найближчих сусідів. Крім того, СПЛ обчислюється як нахил до точки зміни швидкості розбіжності (точки, де вона вирівнюється), що робить його більш стійким до параметра довжини траєкторії.\nЕкман та ін. (1986) обчислюють ПЛ, спочатку реконструюючи часовий ряд за допомогою методу вбудовуваних вкладень, і отримують дотичну, яка відображає реконструйовану динаміку за допомогою методу найменших квадратів.\n\nРозглянемо її синтаксис більш детально:\ncomplexity_lyapunov(signal, delay=1, dimension=2, method='rosenstein1993', separation='auto', **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка (часто позначається \\(\\tau\\) іноді називають запізненням). Ще розглянемо метод complexity_delay() для оцінки оптимального значення цього параметра.\ndimension (int) — розмірність вкладень (\\(m\\), іноді позначається як \\(d\\) або порядок). Далі звернемось до методу complexity_dimension(), щоб оцінити оптимальне значення для цього параметра. Якщо метод має значення \"eckmann1986\", рекомендується використовувати більші значення розмірності.\nmethod (str) — метод, який визначає алгоритм обчислення ПЛ. Може бути \"rosenstein1993\", \"makowski\" або \"eckmann1986\".\nlen_trajectory (int) — застосовується, якщо метод \"rosenstein1993\". Кількість точок даних, в яких простежуються сусідні траєкторії.\nmatrix_dim (int) — застосовується, якщо метод \"eckmann1986\". Відповідає кількості ПЛ, які потрібно повернути.\nmin_neighbors (int, str) — застосовується, якщо метод \"eckmann1986\". Мінімальна кількість сусідів. Якщо \"default\", використовується min(2 * matrix_dim, matrix_dim + 4).\nkwargs (необов’язково) — інші аргументи, які передаються до signal_psd() для обчислення мінімального часового розділення двох сусідів.\n\nПовертає\n\nlle (float) — оцінка СПЛ, якщо метод \"rosenstein1993\", і масив ПЛ, якщо \"eckmann1986\".\ninfo (dict) — словник, що містить додаткову інформацію щодо параметрів, які використовуються для обчислення СПЛ.\n\nПеред розрахунками виконаємо оновлення бібліотеки neurokit2:\n\n!pip install --upgrade neurokit2 \n\n\n9.2.1.1 Обчислення старшого показника Ляпунова на основі методу Розенштейна\nСпочатку виконаємо розрахунки для всього ряду прибутковостей індексу Доу Джонса. Прибутковості обчислимо за допомогою процедури transformation():\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\ntime_ser_ret = transformation(signal, ret_type) \n\nДалі визначимо наступні параметри:\n\nd_E = 3                      # розмірність вкладень\ntau = 10                     # часові затримка\napproach_lyap = \"makowski\"   # метод для розрахунку старшого показника\nmax_len = \"auto\"             # встановлюємо максимальну довжину траєкторії у 10 разів більшу за затримку\nsep = \"auto\"                 # оцінка середнього періоду як величину, обернену до середньої частоти спектра потужності \n\nі візуалізуємо результат:\n\nlle, _ = nk.complexity_lyapunov(signal=time_ser_ret, \n                                method=approach_lyap, \n                                dimension=d_E, \n                                delay=tau,\n                                max_length=max_len,\n                                separation=sep, \n                                show=True)\n\n\n\n\nРис. 9.3: Діаграма розбіжності траєкторій реконструйованого фазового простору індексу Доу Джонса, що представляє розрахований СПЛ\n\n\n\n\nНа рисунку (Рис. 9.3) показано типовий графік (суцільна крива) залежності середньої розбіжності траєкторій від часу \\(\\Delta t\\); помаранчева лінія має нахил, що дорівнює теоретичному значенню \\(\\lambda_{max}\\). Коротка синя ділянка до переходу через червону пунктирну лінію використовується для вилучення найбільшого показника Ляпунова. Як ми можемо бачити, крива змінюється при більших часових періодах, оскільки система обмежена у фазовому просторі і середня дивергенція не може перевищувати “довжину” атрактора. Отриманий показник Ляпунова вказує на те, що індекс Доу Джонса знаходиться на межі між хаосом та стабільність, тобто індекс дивергенція динаміки ряду врівноважується конвергенцією.\nЯк ми вже мали змогу переконатись, складні системи мінливі, що говорить про те, що система з плином часу може проявляти як конвергенцію, так і дивергенцію, так і повну незмінність із плином часу.\nДалі розглянемо динаміку досліджуваної системи з плином часу в рамках процедури ковзного вікна. Визначимо наступні параметри:\n\nwindow = 500            # ширина вікна\ntstep = 1               # часовий крок вікна \nlength = len(time_ser)  # довжина самого ряду\nret_type = 1            # вид ряду: \n                        # 1 - вихідний, \n                        # 2 - детрендований (різниця між теп. значенням та попереднім)\n                        # 3 - прибутковості звичайні, \n                        # 4 - стандартизовані прибутковості, \n                        # 5 - абсолютні значення (волатильності)\n                        # 6 - стандартизований ряд\n\nd_E = 3                      # розмірність вкладень\ntau = 1                      # часові затримка\napproach_lyap = \"makowski\"   # метод для розрахунку старшого показника: rosenstein1993, makowski \nmax_len = \"auto\"             # встановлюємо максимальну довжину траєкторії у 10 разів більшу за затримку: auto\nsep = \"auto\"                 # оцінка середнього періоду як величину, обернену до середньої частоти спектра потужності\n\nLLE = []                # масив для збереження СПЛ\n\nТепер можна приступати до віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n    \n    lle, _ = nk.complexity_lyapunov(signal=fragm, \n                                method=approach_lyap, \n                                dimension=d_E, \n                                delay=tau,\n                                max_length=max_len,\n                                separation=sep, \n                                show=False)\n    \n    LLE.append(lle)\n\n100%|██████████| 7559/7559 [00:30&lt;00:00, 244.02it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"LLE_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    d_E={d_E}_tau={tau}_approach={approach_lyap}_max_len={max_len}_separation={sep}.txt\"\n\nnp.savetxt(name, LLE)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника Ляпунова в легенді рисунку \nlabel_lyap = r'$\\lambda_{max}$'  \n\n# назва рисунку\nfile_name = f\"LLE_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    d_E={d_E}_tau={tau}_approach={approach_lyap}_max_len={max_len}_separation={sep}\"\n\n# колір показника\ncolor = 'red'  \n\nта виводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          LLE, \n          ylabel, \n          label_lyap,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\nРис. 9.4: Динаміка індексу Доу Джонса та старшого показника Ляпунова\n\n\n\n\nЯк ми можемо бачити з представленого рисунку (Рис. 9.4), СПЛ починає спадати в кризові та передкризові стани, що вказує на зростання корельованності досліджуваної динаміки. У момент кризи СПЛ починає зростати, що вказує на зростання дивергенції в кризові періоди.\n\n\n9.2.1.2 Обчислення показників Ляпунова на основі методу Екмана\nРозглянемо динаміку показників Ляпунова в межах процедури рухомого вікна. Визначимо наступні параметри:\n\nwindow = 500            # ширина вікна\ntstep = 1               # часовий крок вікна \nlength = len(time_ser)  # довжина самого ряду\nret_type = 1            # вид ряду: \n                        # 1 - вихідний, \n                        # 2 - детрендований (різниця між теп. значенням та попереднім)\n                        # 3 - прибутковості звичайні, \n                        # 4 - стандартизовані прибутковості, \n                        # 5 - абсолютні значення (волатильності)\n                        # 6 - стандартизований ряд\n\nd_E = 4                         # розмірність вкладень вихідного простору (кількість показників)\nd_M = 3                         # розмірність вкладень підпростору \n\napproach_lyap = \"eckmann1986\"   # метод для розрахунку старшого показника\nsep = \"auto\"                    # оцінка середнього періоду як величину, обернену до середньої частоти спектра потужності\nmin_neighb = \"default\"          # min(2 * matrix_dim, matrix_dim + 4)        \n\nLE = []                         # масив для збереження ПЛ  \n\nТепер переходимо до розрахунків:\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n    \n    le, _ = nk.complexity_lyapunov(signal=fragm, \n                                method=approach_lyap, \n                                dimension=d_E,\n                                matrix_dim=d_M, \n                                min_neighbors=min_neighb,\n                                separation=sep, \n                                show=False)\n\n    LE.append(le)\n\n100%|██████████| 7559/7559 [09:00&lt;00:00, 13.98it/s]\n\n\nЗберігаємо отримані результати в текстових файлах:\n\nLE = np.array(LE)\n\nfor i in range(d_E):\n    np.savetxt(f\"LE number={i+1}_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    d_E={d_E}_d_M={d_M}_approach={approach_lyap}_min_neighbors={min_neighb}_separation={sep}.txt\", LE[i])\n\nВізуалізуємо отримані результати:\n\nfig, ax = plt.subplots(LE.shape[1]+1, 1, figsize=(10, 10), sharex=True)\n\nax[0].plot(time_ser.index[window:length:tstep], time_ser.values[window:length:tstep], label=symbol)\nax[0].set_ylabel(symbol)\nax[0].legend()\n\nfor i in range(1, LE.shape[1]+1):\n    ax[i].plot(time_ser.index[window:length:tstep], LE[:,i-1], color='red', label=fr'$\\lambda_{i}$')\n    ax[i].set_ylabel(r\"$\\lambda$\")\n    ax[i].legend()\n\nax[-1].set_xlabel(xlabel)\nfig.subplots_adjust(hspace=0)\n\nplt.savefig(f\"LE name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    #d_E={d_E}_d_M={d_M}_approach={approach_lyap}_min_neighbors={min_neighb}_separation={sep}.jpg\")\nplt.show();\n\n\n\n\nРис. 9.5: Динаміка індексу Доу Джонса та спектра показників Ляпунова\n\n\n\n\nЯк показано на даному рисунку (Рис. 9.5), спектр показників Ляпунова реагує особливим чином на кризові події фондового ринку. Видно, що, по-перше, \\(\\lambda\\) спадає в передкризові періоди та зростає під час кризи. Особливо характерною є дана динаміка перед кризами 1997, 2001, 2008, 2011, 2015, 2020 років. У передкризові періоди спостерігається конвергенція траєкторій у фазовому просторі системі, що говорить про зростання її впорядкованості. Сам кризовий та посткризовий період характеризується дивергенцією, тобто розбіжністю траєкторій системи. По-друге, видно, що, спускаючись від 1-го до 4-го показника Ляпунова, ми поступово втрачаємо інформацію про динаміку системи. Тобто, перші найбільші показники представляються в даному випадку найбільш інформативними. Можливо, у даному випадку, має сенс розглядати лише найперший ПЛ."
  },
  {
    "objectID": "lab_10.html#теоретичні-відомості",
    "href": "lab_10.html#теоретичні-відомості",
    "title": "10  Лабораторна робота № 10",
    "section": "10.1 Теоретичні відомості",
    "text": "10.1 Теоретичні відомості\n\n10.1.1 Неекстенсивна термодинаміка і кризи фінансово-економічних систем\nВеликий виклик теорії складності, що лежить в основі сучасної наукової парадигми, бере початок ще із старих та таких важливих проблем, як: стріла часу, існування простого та фундаментального фізичного рівня для єдиного опису макроскопічного та мікроскопічного рівнів, взаємозв’язок між спостерігачем та досліджуваним об’єктом, і т. д. Загалом, що стосується теорії складності та кожного нового рівня реальності, потрібні нові концепції та нові класифікації.\nЗокрема, теорія складності включає: хаотичну динаміку в просторі станів, далеку від рівноважних фазових переходів, довготривалі кореляції, самоорганізацію та мультимасштабність, фрактальні процеси в просторі і часі та інші значущі явища. Теорія складності розглядається як третя наукова революція минулого століття (після теорії відносності та квантової теорії). Однак теорія складності ще далека від своєї академічної зрілості. У цьому напрямку вагомий внесок щодо питання “що таке складність” можна знайти в книзі Г. Ніколісa та І. Пригожина, де можна знайти деякі доповнюючі визначення складності. Як правило, ми можемо узагальнити основну концепцію теорії складності наступним чином:\n\nТеорія складності — це узагальнення статистичної фізики для критичних станів термодинамічної рівноваги та для далеких від рівноваги процесів.\nСкладність — це поширення динаміки на нелінійність і дивну динаміку.\nТакож, згідно Іллі Пригожину, теорія складності пов’язана з динамікою кореляцій замість динаміки траєкторій або хвильових функцій.\n\nЗгідно з теорією складності, різні фізичні явища, що відбуваються в розподілених фізичних системах, таких як космічна плазма, рідини або тверді тіла, хімія, біологія, екосистеми, динаміка ДНК, соціально-економічні чи інформаційні системи, мережі можна описати і зрозуміти подібним чином. Цей опис базується на принципі максимізації ентропії. Також згідно з теорією складності, вказані системи є цілісно стійкими дисипативними структурами, що утворюються загальним природним процесом, спрямованим на максимізацію ентропії. З точки зору складності, немає суттєвої диференціації між групою галактик, зірками, тваринами, квітами або елементарними частинками, оскільки скрізь ми маємо відкриті, динамічні та самоорганізовані системи і всюди природа працює з метою максимізації ентропії.\nПід час дослідження складних фізичних систем та явищ, зокрема, самоорганізаційних і фрактальних структур, субдифузії, турбулентності, хімічних реакцій, а також різних економічних, соціальних і біологічних систем розподіл Гіббса не забезпечує узгодження із спостережуваними явищами. Як виявляється у багатьох дослідженнях, для таких систем характерні степеневі розподіли. Вони не отримуються з принципу максимуму ентропії Гіббса-Шеннона, на якому ґрунтується як рівноважна, так і нерівноважна статистична термодинаміка. Це спричинило численні спроби побудови узагальненої статистики, яка б забезпечила степеневу асимптотику функції розподілу. Таку узагальнену статистику можна будуватина основі кількох ентропій. Серед них важливе місце посідає ентропія Тсалліса (Tsallis).\nДослідження в області механіки неекстенсивних (неадитивних) систем стали останнім часом предметом значного інтересу в зв’язку з проявами неаддитивних властивостей в аномальних фізичних явищах. Це пояснюється як новизною виникаючих тут загальнотеоретичних проблем, так і важливістю практичних застосувань (див. бібліографію, представлену на сайті, яка постійно оновлюється). Початок систематичного вивчення в цьому напрямку пов’язаний з роботою К. Тсалліса, в якій автором була введена параметрична формула статистичної \\(q\\)-ентропії, залежної від деякого дійсного числа \\(q\\) (так званого параметра деформації) і неадитивної для сукупності незалежних складних систем. Теорія неекстенсивних систем, заснована на ентропії Тсалліса, в даний час інтенсивно розвивається, на жаль, в основному зарубіжними фахівцями. Ці роботи стали значним кроком у розвитку теоретико-інформаційного підходу і при розробці принципів неекстенсивної статистичної механіки та рівноважної термодинаміки відкритих систем. При цьому важливо відзначити, що діапазон застосування цих та багатьох інших неекстенсивних параметричних ентропій в даний час постійно розширюється, охоплюючи різні напрямки в науці, такі як космологія і космогонія, теорія плазми, квантова механіка і статистика, нелінійна динаміка і фрактали, геофізика, біомедицина і багато інших.\nЕкономічну динаміку з фізичної точки зору можна розглядати як просторово розподілену динаміку та пов’язану із загальною категорією нелінійних розподілених систем. Аналіз економічних часових рядів демонструє складну та хаотичну динаміку у фазовому просторі. Теорема Такенса (за допомогою методу затримок) дозволяє реконструювати топологічний еквівалент до вихідного фазового простору, який зберігає основні геометричні та динамічні властивості, такі як ступені свободи, фрактальна розмірність, мультифрактальність, показники Ляпунова, матриця прогнозування тощо. Реконструйований фазовий простір може бути використаний для оцінки всіх вищезазначених величин, а також фазових переходів, статистичної поведінки, генерування ентропії тощо. Крім того, фазовий простір може мати мультифрактальні властивості та характеристики переривчастої турбулентності, які вказують на існування дальніх взаємодій у просторі та часі, а також мультимасштабну взаємодію.\nЦі характеристики також вказують на існування дробової динаміки у фазовому просторі, яку можна описати за допомогою дробово-диференціальних рівнянь Фоккера-Планка та аномальних дифузійних рівнянь. Рішеннями цих рівнянь є дробові просторово-часові функції та негаусові функції розподілу, які належать до категорії розподілів Леві та розподілів Тсалліса. Нерівноважні стаціонарні стани економічної динаміки походять від процесів сильної самоорганізації, що відповідає локальним максимумам ентропії Тсалліса, тоді як зміни параметрів управління економічної системи можуть спричинити фазовий перехід та зміщення економічної динаміки до нової стійкої рівноваги, стійкого стану з максимальною ентропією Тсалліса. Цей фазовий перехід призводить до мультифрактальної зміни у формуванні фазового простору та до зміни феноменології економічної системи. Нарешті, статистику динаміки в мультифрактальному фазовому просторі можна описати за допомогою степеневих функцій розподілу Тсалліса з «важкими» хвостами, які можуть бути використані для вдосконалення методів прогнозування.\nВ останні роки статистична механіка розширила своє початкове призначення: застосування статистики до великих систем, стани яких регулюються якимись гамільтоновими функціоналами. Їх здатність пов’язувати мікроскопічні стани окремих складових системи з макроскопічними властивостями сьогодні використовується повсюдно. Безумовно, найважливішим із цих зв’язків все-таки є визначення термодинамічних властивостей через відповідність між поняттям ентропії, спочатку введеним Рудольфом Клаузіусом в 1865 р., та кількістю дозволених мікроскопічних станів, введеним Людвігом Больцманом близько 1877 р. коли він вивчав підхід до рівноваги ідеального газу. Цей зв’язок можна виразити як\n\\[\nS = k\\ln{W},\n\\tag{10.1}\\]\nде \\(k\\) — позитивна константа, а \\(W\\) — кількість мікростанів, сумісних з макроскопічним станом ізольованої системи. Це рівняння, відоме як принцип Больцмана, є одним із наріжних каменів стандартної статистичної механіки. Коли система не ізольована, а замість цього контактує з деяким великим резервуаром, можна модифікувати рівняння (Рівняння 10.1) і отримати ентропію Больцмана-Гіббса (БГ-BG):\n\\[\nS_{BG} = -k\\sum_{i=1}^{W}p_i\\ln{p_i},\n\\tag{10.2}\\]\nде \\(p_i\\) — ймовірність мікроскопічної конфігурації \\(i\\). Статистична механіка BG все ще ґрунтується на таких гіпотезах, як молекулярний хаос та ергодичність. Незважаючи на відсутність фактичного фундаментального виведення, статистика BG, безсумнівно, мала успіх у вивченні систем, в яких домінують короткі просторово-часові взаємодії. Отже, цілком можливо, що інші фізичні ентропії, крім BG, можуть бути визначені для належного опису аномальних систем, для яких спрощена гіпотеза про ергодичність та/або незалежність не виконується. Натхненний такими концепціями в 1988 р. Константіно Тсалліс (К. Tsallis) запропонував узагальнення статистичної механіки BG, яка охоплює системи, що порушують ергодичність, системи, мікроскопічні конфігурації яких не можна вважати незалежними. Це узагальнення базується на неадитивних ентропіях, \\(S_q\\), що характеризується індексом \\(q\\) і призводить до неекстенсивної статистики\n\\[\nS_q = -k\\frac{1-\\sum_{i=1}^{W}p_{i}^{q}}{1-q},\n\\tag{10.3}\\]\nде \\(p_i\\) — ймовірності, пов’язані з мікроскопічними конфігураціями, \\(W\\) — їх загальне число, \\(q\\) — дійсне число, і \\(k\\) — постійна Больцмана. Значення \\(q\\) є мірою неекстенсивності системи. При цьому, \\(q=1\\) відповідає стандартній статистиці BG. Вираз (Рівняння 10.3) модифікує \\(S_{BG}\\) (\\(\\lim q\\to 1, S_q = S_{BG}\\)), як основу можливого узагальнення статистичної механіки BG. Значення ентропійного індексу \\(q\\) для конкретної системи повинно визначатися апріорі з мікроскопічної динаміки.\nЗ часу своєї появи ентропія (Рівняння 10.3) стала джерелом кількох важливих результатів як у фундаментальній, так і в прикладній фізиці, а також в інших наукових областях, таких як біологія, хімія, економіка, геофізика та медицина.\n\n\n10.1.2 Неекстенсивна ентропія і триплет Тсалліса\nСистеми, що характеризуються статистичною механікою Больцмана-Гіббса, мають такі характеристики: (i) їх функції розподілу для енергій пропорційні експоненціальній функції в присутності термостата; (ii) Вони мають сильну чутливість до початкових умов, яка з часом зростає в геометричній прогресії (хаос), характеризуючись позитивним максимальним показником Ляпунова; (iii) Вони, як правило, представляють для основних макроскопічних величин експоненціальний розпад з певним часом релаксації. Іншими словами, ці три способи поведінки описуються експоненціальними функціями (тобто \\(q=1\\)). Однак встановлено, що для систем, які можна вивчати в рамках неекстенсивної статистичної механіки, функція щільності ймовірності енергії (пов’язана зі стаціонарністю або рівновагою), чутливість до початкових умов та релаксація описуються трьома ентропійними індексами \\(q_{stat}, q_{sens}, q_{rel}\\), які отримали назву триплета Тсалліса, або \\(q\\)-триплета Тсалліса.\nНеекстенсивна статистична теорія математично базується на нелінійному рівнянні:\n\\[\n\\frac{dy}{dx} = y^{q},\n\\tag{10.4}\\]\nрозв’язком якого є \\(q\\)-експоненціальна функція, що визначається як\n\\[\n\\exp_q(x) = \\begin{cases}\n    \\left( 1+(1-q)x \\right)^{\\frac{1}{1-q}}, & \\text{якщо} \\; 1+(1-q)x &gt; 0.\\\\\n    0, & \\text{якщо} \\; 1+(1-q)x \\leq 0.\n\\end{cases}\n\\tag{10.5}\\]\nДля \\(q\\to1\\) \\(q\\)-гаусіан відповідає звичайному розподілу Гауса.\nРозв’язок рівняння (Рівняння 10.4) можна реалізувати трьома різними способами, включеними до \\(q\\)-триплету Тсалліса: (\\(q_{sens}, q_{stat}, q_{rel}\\)). Ці величини характеризують три фізичні процеси, які узагальнені тут, тоді як значення \\(q\\)-триплету характеризують атракторний набір динаміки у фазовому просторі динаміки, і вони можуть змінюватися, коли динаміка системи притягується до іншого набору атракторів.\nДля неекстенсивної системи величина \\(q\\)-індексу залежить від оцінюваних властивостей динаміки і фазового простору системи. Для динамічних систем оцінюється \\(q\\)–триплет, що відображає три властивості системи (Рис. 10.1). Індекс \\(q_{stat}\\) оцінюється на основі рівноважної моделі рангового розподілу з використанням методів нелінійного оцінювання. Цей індекс є параметром області атракції системи. Індекс \\(q_{sens}\\) відображає чутливість системи до початкових умов та виробництво ентропії і визначається за мультифрактальним спектром. Релаксаційний індекс \\(q_{rel}\\) оцінюється на основі автокореляції і характеризує процеси дифузії.\n\n\n\nРис. 10.1: У міру еволюції динаміки системи з метою максимізації \\(q\\)-ентропії вона продукує \\(q\\)-ентропію. Ми можемо розрізнити три різні періоди часу. Перший період відповідає виробництву ентропії через параметр \\(q_{sens}\\) \\(q\\)-триплету Тсалліса. Другий період відповідає певному процесу релаксації через параметр \\(q_{rel}\\) \\(q\\)-триплету Тсалліса. Нарешті, оскільки система функціонує в стаціонарному стані з максимізованою \\(q\\)-ентропією, вона виявляє коливання через параметр \\(q_{stat}\\) \\(q\\)-триплету Тсалліса\n\n\n\n10.1.2.1 Стаціонарність \\(q=q_{stat}\\)\nЗначення \\(q\\) для стаціонарного стану отримують із функції розподілу прибутковостей, що в свою чергу отримується шляхом підгонки \\(q\\)-Гаусіана:\n\\[\nP_q(\\beta, x) = \\frac{\\sqrt{\\beta}}{C_q}\\exp(-\\beta rx^2)\n\\tag{10.6}\\]\nдля емпірично побудованої гістограми \\({p(x_i)|i=1,...,N}\\) та різних значень \\(\\beta\\). Значення \\(\\beta\\) підбирається шляхом мінімізації \\(\\sum_i \\left[ P_{q_{stat}}(\\beta, x_i) - p(x_i) \\right]^2\\). В залежності від значення \\(q\\), \\(C_q\\) може приймати наступні види:\n\\[\nC_q = \\begin{cases}\n    \\frac{2\\sqrt{\\pi}\\Gamma\\left( \\frac{1}{1-q} \\right)}{(3-q)\\sqrt{1-q}\\Gamma\\left( \\frac{3-q}{2(1-q)} \\right)}, & \\text{якщо} \\; -\\infty&lt;q&lt;1.\\\\\n    \\sqrt{\\pi}, & \\text{якщо} \\; q=1.\\\\\n    \\frac{\\sqrt{\\pi}\\Gamma\\left( \\frac{3-q}{2(q-1)} \\right)}{\\sqrt{q-1}\\Gamma\\left( \\frac{1}{q-1} \\right)}, & \\text{якщо} \\; 1&lt;q&lt;3.\n\\end{cases}\n\\tag{10.7}\\]\nДля оцінки динаміки значення \\(q\\) будується графік залежності \\(\\ln_q[p(x)]\\) від \\(x^2\\) для вибраного інтервалу \\(q\\) (наприклад, від 1 до 5), що забезпечує найкраще лінійне наближення, яке оцінюється за максимальним коефіцієнтом детермінації \\(R^2\\). Зрозуміло, що значення \\(p(x)\\) стають помітно негаусівськими вздовж хвостів, і замість цього можуть бути описані степеневим законом.\n\n\n10.1.2.2 Релаксація \\(q=q_{rel}\\)\nВідповідне \\(q\\)-значення для релаксаційного процесу знаходиться з коефіцієнта автокореляції\n\\[\nC(\\tau) = \\frac{\\sum_{t}|g_{t+\\tau}|\\cdot|g_t|}{\\sum_{t}|g_t|^2}.\n\\tag{10.8}\\]\nДля статистики BG така кореляція має спадати експоненціально. Той самий алгоритм, що й для \\(q_{stat}\\), необхідно проробити на графіку залежності \\(\\ln_{q}[C(\\tau)]\\) від \\(\\tau\\) щоб визначити, який набір \\(q\\) найкраще лінеаризує емпіричні дані.\n\n\n10.1.2.3 Чутливість до початкових умов \\(q=q_{sens}\\)\nВиробництво ентропії пов’язане із загальним характером атракторної множини. Цей атрактор може бути описаний мультифрактальністю, а також чутливістю до початкових умов. Чутливість до початкових умов можна виразити як:\n\\[\n\\frac{d\\xi}{dt} = \\lambda_1\\xi + (\\lambda_q - \\lambda_1)\\xi^q,\n\\tag{10.9}\\]\nде \\(\\xi\\) — відхилення траєкторії у фазовому просторі: \\(\\xi \\equiv \\lim_{\\delta \\to 0} \\left[ \\delta(t)/\\delta(0) \\right]\\), і \\(\\delta(t)\\) — це відстань між сусідніми траєкторіями через час \\(t\\). Розв’язок рівняння (Рівняння 10.9) представлений у вигляді:\n\\[\n\\xi = \\left[ 1 - \\frac{\\lambda_{q_{sens}}}{\\lambda_{1}} + \\frac{\\lambda_{q_{sens}}}{\\lambda_{1}}\\exp{((1-q_{sens})\\lambda_{1}t)} \\right]^{\\frac{1}{1-q_{sens}}}.\n\\tag{10.10}\\]\nСпочатку було висловлено гіпотезу, а згодом доведено для часових рядів неекстенсивних систем різної природи, що має місце таке співвідношення:\n\\[\n\\frac{1}{1-q_{sens}} = \\frac{1}{\\alpha_{min}} - \\frac{1}{\\alpha_{max}},\n\\tag{10.11}\\]\nде \\(\\alpha_{min}\\) та \\(\\alpha_{max}\\) — відповідно мінімальні та максимальні значення \\(\\alpha\\) відповідного мультифрактального спектру \\(f(\\alpha)\\).\nСпектр мультифрактальності в свою чергу випливає з процедури мультифрактального аналізу детрендованих флуктуацій (МФ-АДФ), що дозволяє розрахувати показник Херста для різних часових масштабів."
  },
  {
    "objectID": "lab_10.html#хід-роботи",
    "href": "lab_10.html#хід-роботи",
    "title": "10  Лабораторна робота № 10",
    "section": "10.2 Хід роботи",
    "text": "10.2 Хід роботи\nРозглянемо як можна застосовувати зазначені показники в якості індикаторів кризових станів.\nСпочатку імпортуємо необхідні бібліотеки:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nimport neurokit2 as nk\nimport fathon\nfrom fathon import fathonUtils as fu\nimport scipy\nfrom scipy.stats import norm\nfrom scipy.special import gamma\nimport statsmodels.api as sm\nfrom scipy.optimize import curve_fit\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nта визначимо необхідні функції для подальшої роботи:\n\n# q-експоненціальна функція\ndef np_exp_q(x, q=1):\n    if q==1:\n        return np.exp(x)\n    else:\n        return (1+(1-q)*x)**(1/(1-q))\n\n# q-логарифм\ndef np_log_q(x, q=1):\n    if q==1:\n        return np.log(x)\n    else: \n        return x**(1-q)-1/(1-q)\n\n# значення для обчислення q-гаусіана\ndef C_q(q=1.0):\n    if q==1:\n        return np.sqrt(np.pi)\n    elif q&lt;1:\n        return 2*np.sqrt(np.pi)*gamma(1/(1-q))/(3-q)*np.sqrt(1-q)*gamma((3-q)/(2*(1-q)))\n    elif q&gt;1:\n        return (np.sqrt(np.pi)*gamma((3-q))/(2*(q-1)))/(np.sqrt(q-1)*gamma(1/(q-1)))\n\n# функція щільності q-гаусіана для обчислення q_stat\ndef G_q(r, beta, q):\n    return np.sqrt(beta)/C_q(q) * np_exp_q(-beta*r, q)\n\n\n# функція автокореляцій для обчислення q_rel\ndef acf(x, maxlag):\n\n    n = len(x)\n    a = (x - x.mean()) / (x.std() * n)\n    b = (x - x.mean()) / x.std()\n\n    cor = np.correlate(a, b, mode=\"full\")\n    acf = cor[n:n+maxlag+1]\n    lags = np.arange(maxlag + 1)\n\n    return acf, lags\n\n# рункція релаксацій для обчислення q_rel\ndef rel_func(x, q, tau):\n    return np_exp_q(-x/tau, q)\n\n\n# функція для обчислення прибутковостей ряду чи його стандартизації\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n# функція для побудови парних графіків\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nУ цій роботі розглянемо динаміку неекстенсивних показників на прикладі фондового індексу S&P 500, але дивитимемось на ряд, починаючи з 2016 року. Для отримання значень індексу скористаємось бібліотекою yfinance.\n\nsymbol = '^DJI'          # Символ індексу\nstart = \"2016-01-01\"     # Дата початку зчитування даних\nend = \"2023-12-31\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 10.2: Динаміка щоденних змін фондового індексу Доу Джонса\n\n\n\n\n\n10.2.1 Розрахунок показника \\(q_{stat}\\)\n\n10.2.1.1 Побудова \\(q\\)-гаусіана для всього ряду\n\nq_stat_time_ser = time_ser.copy()\nret_type = 4 # визначення типу ряду для його перетворення\nq_stat_time_ser = transformation(q_stat_time_ser, ret_type)\n\nhist, bin_edg = np.histogram(q_stat_time_ser, bins=250, density=True)\n\nmu, std = norm.fit(q_stat_time_ser)\nx = np.linspace(q_stat_time_ser.min(), q_stat_time_ser.max(), len(bin_edg[1:]))\np = norm.pdf(x, mu, std)\n\nxval = bin_edg[1:]**2\nyval = hist\n\npopt, pcov = curve_fit(G_q, xdata=xval, ydata=yval, bounds=([0.0, 0.0], [np.inf, 3.0]))\n\n\nplt.plot(bin_edg[1:], hist, 'o', label=r\"$P_{емпіричний}$\")\nplt.plot(x, p, 'o', label=\"Гаус\")\nplt.plot(x, G_q(x**2, popt[0], popt[1]), label=r\"$q$-гаусіан\")\nplt.yscale('log')\nplt.xlabel(\"x\")\nplt.ylabel(r\"$\\log{P(\\beta, x)}$\")\nplt.legend()\nplt.show(); \n\n\n\n\nРис. 10.3: Функція розподілу нормалізованих прибутковостей для Доу Джонса в порівнянні з розподілом Гауса та \\(q\\)-гаусіаном\n\n\n\n\n\n\n10.2.1.2 Розрахунок \\(q_{stat}\\) у віконній процедурі\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser)\n\nq_stats = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n\n    hist_fragm, bin_edg_fragm = np.histogram(fragm, bins=100, density=True)\n\n    xval = bin_edg_fragm[1:]**2\n    yval = hist_fragm\n\n    popt, pcov = curve_fit(G_q, xdata=xval, ydata=yval, bounds=([0.01, 1.0], [np.inf, 5.0]))\n    q_stat = popt[1]\n\n    \n    q_stats.append(q_stat)\n\n100%|██████████| 1762/1762 [00:11&lt;00:00, 158.40it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"q_stat_name={symbol}_window={window}_step={tstep}_rettype={ret_type}.txt\"\n\nnp.savetxt(name, q_stats)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника q_stat в легенді рисунку \nlabel_q_stat = r'$q_{stat}$'  \n\n# назва рисунку\nfile_name = f\"q_stat_name={symbol}_window={window}_step={tstep}_rettype={ret_type}\"\n\n# колір показника\ncolor = 'brown'  \n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          q_stats, \n          ylabel, \n          label_q_stat,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\nРис. 10.4: Порівняльна динаміка коливань ціни індексу Доу Джонса та показника \\(q_{stat}\\)\n\n\n\n\n\n\n\n10.2.2 Розрахунок показника \\(q_{rel}\\)\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nmax_lag = 100\n\nlength = len(time_ser)\n\nq_rels = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n\n    autocor, lags = acf(x=fragm, maxlag=max_lag)\n    lags = lags\n    autocor = autocor\n    \n    popt, pcov = curve_fit(rel_func, xdata=lags[1:], ydata=autocor[1:], bounds=(1, [np.inf, 10]))\n    q_rel = popt[0]\n    \n    q_rels.append(q_rel)\n\n100%|██████████| 1762/1762 [00:08&lt;00:00, 215.14it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"q_rel_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_maxlag={max_lag}.txt\"\n\nnp.savetxt(name, q_rels)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника q_rel в легенді рисунку \nlabel_q_rel = r'$q_{rel}$'  \n\n# назва рисунку\nfile_name = f\"q_rel_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_maxlag={max_lag}\"\n\n# колір показника\ncolor = 'red'  \n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          q_rels, \n          ylabel, \n          label_q_rel,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\nРис. 10.5: Порівняльна динаміка коливань ціни індексу Доу Джонса та показника \\(q_{rel}\\)\n\n\n\n\n\n\n10.2.3 Розрахунок показника \\(q_{sens}\\)\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nrev = True         # Чи повторювати розрахунок ф-ції флуктуацій з кінця\naccumulate = False # Повторна акумуляція детрендованого ряду для роботи із сильно антиколрельованими рядами\n\nq_min = -5         # мінімальне значення q\nq_max = 5          # максимальне значення q\nq_inc = 1          # крок збільшення q\n\nwin_beg = 10       # Початкова ширина сегменту\nwin_end = window-1 # Кінцева ширина сегменту\n\nlength = len(time_ser)\n\nq = np.arange(q_min, q_max+q_inc, q_inc)\nq = np.round_(q, decimals = 1)\n\norder = 1          # порядок поліному для детрендування (MF-DFA)\n\nq_sens_values = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n\n    if accumulate == True:\n        fragm = np.cumsum(fragm-np.mean(fragm))\n\n    a = fu.toAggregated(fragm)\n\n    pymfdfa = fathon.MFDFA(a)\n\n    wins = fu.linRangeByStep(win_beg, win_end)\n\n    n, F = pymfdfa.computeFlucVec(wins, q, revSeg=rev, polOrd=order)\n    list_H, list_H_intercept = pymfdfa.fitFlucVec()\n\n    if accumulate == True:\n        list_H = list_H - 1\n    \n    # розрахунок значень tau(q)\n    tau = q * list_H - 1\n\n    # розрахунок значень сингулярності\n    alpha = np.gradient(tau, q, edge_order=2)\n\n    # максимальне значення сингулярності\n    maximal_alpha = alpha.max()\n\n    # мінімальне значення сингулярності\n    minimal_alpha = alpha.min()\n\n    # розрахунок q_sens\n    q_sens = (maximal_alpha-minimal_alpha-maximal_alpha*minimal_alpha)/(maximal_alpha-minimal_alpha)\n\n    q_sens_values.append(q_sens)\n\n100%|██████████| 1762/1762 [01:27&lt;00:00, 20.25it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"q_sens_name={symbol}_ret={ret_type}_qmin={q_min}_qmax={q_max}_qinc={q_inc}_wind={window}_step={tstep}.txt\"\n\nnp.savetxt(name, q_sens_values)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника q_rel в легенді рисунку \nlabel_q_sens = r'$q_{sens}$'  \n\n# назва рисунку\nfile_name = f\"q_sens_name={symbol}_ret={ret_type}_qmin={q_min}_qmax={q_max}_qinc={q_inc}_wind={window}_step={tstep}\"\n\n# колір показника\ncolor = 'green'  \n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          q_sens_values, \n          ylabel, \n          label_q_sens,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\nРис. 10.6: Порівняльна динаміка коливань ціни індексу Доу Джонса та показника \\(q_{sens}\\)\n\n\n\n\n\n\n10.2.4 Розрахунок ентропії Тсалліса\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser)\n\ntsallis_en = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n\n    p, be = np.histogram(fragm,               # розраховуємо щільність ймовірностей\n                        bins='auto', \n                        density=True)  \n    r = be[1:] - be[:-1]                      # знаходимо dx\n    P = p * r                                 # представляємо ймовірність як f(x)*dx\n    P = P[P!=0]                               # фільтруємо по всім ненульовим ймовірностям\n    \n    tsen, _ = nk.entropy_tsallis(freq=P, \n                                 q=1, \n                                 base=np.exp(1))\n    tsen /= np.log(len(P))\n    \n    tsallis_en.append(tsen)\n\n100%|██████████| 1762/1762 [00:00&lt;00:00, 1933.70it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"tsen_name={symbol}_ret={ret_type}_wind={window}_step={tstep}.txt\"\n\nnp.savetxt(name, tsallis_en)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення ентропії Тсалліса в легенді рисунку \nlabel_ts_en = r'$TsEn$'  \n\n# назва рисунку\nfile_name = f\"tsen_name={symbol}_ret={ret_type}_wind={window}_step={tstep}\"\n\n# колір показника\ncolor = 'purple'  \n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          tsallis_en, \n          ylabel, \n          label_ts_en,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\nРис. 10.7: Порівняльна динаміка коливань ціни індексу Доу Джонса та ентропії Тсалліса$"
  },
  {
    "objectID": "lab_11.html#теоретичні-відомості",
    "href": "lab_11.html#теоретичні-відомості",
    "title": "11  Лабораторна робота № 11",
    "section": "11.1 Теоретичні відомості",
    "text": "11.1 Теоретичні відомості\nНезворотність часу є фундаментальною властивістю нерівноважних дисипативних систем, і її втрата може свідчити про розвиток деструктивних процесів.\nЗ огляду на статистичні властивості досліджуваного сигналу, його еволюцію можна було б назвати незворотною, якби була відсутня інваріантність, тобто був би отриманий той же сигнал, якби ми виміряли його в протилежному напрямку. Функція \\(f\\) може бути застосована для знаходження характеристик, які відрізняються прямою і зворотною версіями, тобто часові ряди незворотні, якщо \\(f(X^d) \\neq f(X^r)\\). Основна ідея цього визначення полягає в тому, що немає ніяких обмежень на \\(f(\\cdot )\\).\nПередбачається, що стаціонарний процес \\(X\\) називається статистично зворотним у часі, якщо розподіл ймовірностей прямої та зворотної систем приблизно однаковий. Незворотність часових рядів вказує на наявність нелінійних залежностей (пам’яті) в динаміці системи, далекій від рівноваги, включаючи негауссові випадкові процеси та дисипативний хаос.\n\n11.1.1 Незворотність на основі діаграм Пуанкаре\nДіаграма Пуанкаре для часового ряду являє собою графік, на осі \\(x\\) якого розташовані значення для поточного часу \\(t\\), а на осі \\(y\\) — його наступні значення в часі \\(t+\\tau\\). Усі наступні значення, які рівні один одному (\\(x(t) = x(t+\\tau)\\)), розташовані на лінії ідентичності (line of identity, LI). Інтервали, що представляють зростаючу тендецію, відмічені вище LI (\\(x(t)&lt;x(t+\\tau)\\)), тоді як спадна тенденція характеризуватиметься скупченням точок нижче LI (\\(x(t)&gt;x(t+\\tau)\\)). Оцінюючи асиметрію точок на діаграмі, ми можемо вивести різні кількісні показники незворотності (асиметрії) досліджуваних систем.\nІндекс Гузіка (GIx)\nGIx можна визначити як відношення відстаней точок вище LI до відстаней усіх точок на діаграмі:\n\\[\nGIx = \\frac{\\sum_{i=1}^{a} \\left( D_{i}^{+} \\right)^{2}}{\\sum_{i=1}^{m} \\left( D_{i} \\right)^{2} },\n\\]\nде \\(a = C(P_{i}^{+})\\) позначає кількість точок над LI; \\(m = C(P_{i}^{+}) + C(P_{i}^{-})\\) позначає кількість точок на графіку Пуанкаре; \\(D_{i}^{+}\\) це відстань від точки над LI до самої LI. Відстань точки до LI можна визначити як\n\\[\nD_{i} = \\frac{|x(i+\\tau) - x(i)|}{\\sqrt{2}}.\n\\]\nІндекс Порти\nІндекс Порти (PIx) визначається як кількість точок нижче LI, поділена на загальну кількість точок на графіку Пуанкаре, за винятком тих, що знаходяться на LI:\n\\[\nPIx = \\frac{b}{m},\n\\]\nде \\(b = C(P_{i}^{-})\\) кількість точок нижче LI.\nІндекс Кошти\nІндекс Кошти бере до уваги кількість інкриментів (\\(x(i+1)-x(i) &gt; 0\\)) та декриментів (\\(x(i+1)-x(i) &lt; 0\\)). Вони представляються симетричними, якщо рівні один одному. Даний індекс розраховується для двовимірної мультимасштабної площини (\\(x(i), x(i+L)\\)), де новий крос-гранульований ряд \\(y_{\\tau}(i) = x(i+L)-x(i)\\) для \\(1 \\leq i \\leq N-\\tau\\) відображає асиметрію інкриментів та декриментів ряду, і індекс незворотності для діапазону масштабів \\(\\tau\\) визначається наступним виразом:\n\\[\nCIx_{\\tau} = \\frac{\\sum_{y_{\\tau}&lt;0} H[y_{\\tau}] - \\sum_{y_{\\tau}&gt;0} H[y_{\\tau}]}{N-\\tau}.\n\\]\nУзагальнений індекс Кошти для діапазону мастабів \\(\\tau\\) може бути визначений як\n\\[\nCIx = \\frac{1}{L} \\sum_{\\tau=1}^{L} |CIx_{\\tau}|,\n\\]\nде \\(L\\) — це максимальний масштаб.\nІндекс Ейлера\nОпираючись на асиметрію розподілу точок нижче та вище LI, Ейлер запропонував індекс асиметрії:\n\\[\nEIx = \\frac{\\sum_{i=1}^{N-1} \\left[ x(i)-x(i+\\tau) \\right]^{3}}{\\left[ \\sum_{i=1}^{N-1} \\left[ x(i)-x(i+\\tau) \\right]^{2} \\right]^{\\frac{3}{2}}}.\n\\]\nЗначне відхилення \\(EIx\\) від 0 вказує на асиметрію системи. Якщо \\(EIx&gt;0\\), розподіл точок на діаграмі Пуанкаре значно зміщений у сторону вище LI. Зворотня ситуація спостерігається для \\(EIx&lt;0\\). Для \\(EIx \\approx 0\\) досліджувані сегменти представляються зворотніми в часі.\nІндекс площі\nІндекс площі (AIx) визначається як сукупна площа секторів, що сформовані точками над LI поділена на сукупну площу секторів, що відповідають усім точкам на графіку Пуанкаре (крім тих, що розташовані точно на LI). Площа сектора, що відповідає певній точці \\(P_{i}\\) на графіку Пуанкаре, обчислюється як\n\\[\nS_{i} = \\frac{1}{2} \\times R\\theta_{i} \\times r^{2},\n\\]\nде \\(r\\) — це радіус сектора; \\(R\\theta_{i} = \\theta_{LI} - \\theta_{i}\\); \\(\\theta_{LI}\\) — це фазовий кут, і \\(\\theta_{i} = \\arctan{\\left[ \\frac{x(i+\\tau)}{x(i)} \\right]}\\), що визначає фазовий кут \\(i\\)-ої точки. Далі, \\(AIx\\) визначається за наступною формулою:\n\\[\nAIx = \\frac{\\sum_{i=1}^{a}|S_{i}|}{\\sum_{i=1}^{m}|S_{i}|}.\n\\]\nІндекс кута нахилу\nНа додачу до представлених вище мір, було запропоновано розраховувати незворотність сигналу з відношення кутів нахилу точок над LI до нахилу всіх точок на діаграмі:\n\\[\nSIx = \\frac{\\sum_{i=1}^{a}|R\\theta_{i}|}{\\sum_{i=1}^{m}|R\\theta_{i}|}.\n\\]\n\n\n11.1.2 Методи складних мереж\nГрафи видимості (VG) базуються на простому відображенні часових рядів у мережеву область, використовуючи локальну опуклість скалярно-позначених часових рядів, де кожне спостереження є вершиною в складній мережі. Дві вершини і пов’язані ребром, якщо для всіх вершин застосовується наступна умова:\n\\[\nx_{k} &lt; x_{j} + \\left( x_{i} - x_{j} \\right) \\frac{t_{j}-t_{k}}{t_{j}-t_{i}}.\n\\]\nМатрицю суміжності (\\(A_{ij}\\)) представленого ненаправленого та незваженого VG можна представити як:\n\\[\nA_{ij}^{VG} = A_{ji}^{VG} = \\prod_{k=i+1}^{j-1} H \\left( x_{k} &lt; x_{j} + \\left( x_{i} - x_{j} \\right) \\frac{t_{j}-t_{k}}{t_{j}-t_{i}} \\right),\n\\]\nде \\(H( \\cdot )\\) — це функція Гевісайда.\nГраф горизонтальної видимості (HVG) є спрощеною версією цього алгоритму. Для досліджуваного часового ряду набори вершин VG і HVG однакові, тоді як набір ребер HVG відображає взаємну горизонтальну видимість двох спостережень \\(x_{i}\\) та \\(x_{j}\\). Тобто можна побудувати ребро \\((i,j)\\), якщо \\(x_{k} &lt; \\min(x_{i}, x_{j})\\) для всіх \\(k\\) при \\(t_{i} &lt; t_{k} &lt; t_{j}\\) так що\n\\[\nA_{ij}^{VG} = A_{ji}^{VG} = \\prod_{k=i+1}^{j-1} H \\left( x_{i} - x_{k} \\right) H \\left( x_{j} - x_{k} \\right).\n\\]\nVG і HVG фіксують по суті одні й ті ж властивості досліджуваної системи, оскільки HVG є підграфом VG з тим же набором вершин, але володіє тільки підмножиною ребер VG. Зверніть увагу, що VG інваріантний щодо суперпозиції лінійних трендів, тоді як HVG — ні.\nОскільки визначення VGs та HVGs чітко враховує часовий порядок спостережень, напрямок часу нерозривно пов’язаний з отриманою структурою мережі. Щоб врахувати цей факт, ми визначаємо набір нових статистичних мережевих показників на основі двох простих характеристик вершин:\n\nОскільки кількість ребер інцидентних вершині \\(i\\) можна визначити як \\(k_{i}^{r} = \\sum_{j} A_{ij}\\), для (H)VG ми можемо переписати дану кількісну характеристику для вершини в час \\(t_{i}\\) відносно її минулих та майбутніх вершин:\n\n\\[\nk_{i}^{r} = \\sum_{j&lt;i} A_{ij} \\quad \\mathrm{and} \\quad k_{i}^{a} \\sum_{j&gt;i} A_{ij},\n\\]\nде \\(k_{i} = k_{i}^{r} + k_{i}^{a}\\), і \\(k_{i}^{r}\\) та \\(k_{i}^{a}\\) сприймаються як вхідні (минулі) та вихідні (майбутні) вершини.\n\nЛокальний коефіцієнт кластеризації \\(C_{i} = \\left( \\begin{matrix} k_{i}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j,k} A_{ij}A_{jk}A_{ki}\\) інша властивість старшного порядку структурного сусідства вершини \\(i\\). Для дослідження незворотності, ми можемо переписати дані характеристики наступним чином:\n\n\\[\nC_{i}^{r} = \\left( \\begin{matrix} k_{i}^{r}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j&lt;i,k&lt;i} A_{ij}A_{jk}A_{ki} \\quad \\textrm{and} \\quad C_{i}^{a} = \\left( \\begin{matrix} k_{i}^{a}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j&gt;i,k&gt;i} A_{ij}A_{jk}A_{ki}.\n\\]\nЯкщо уявити нашу систему зворотною в часі, ми припускаємо, що розподілу ймовірностей прямих і зворотних за часом характеристик повинні бути однаковими. Для незворотних процесів ми очікуємо виявити статистичну нееквівалентність. Ця нееквівалентність буде визначатися через дивергенцію Кульбака-Лейблера:\n\\[\nD_{KL}(p||q) = \\sum_{i=1}^{N} p(x_{i}) \\cdot \\log{\\left[ \\frac{p(x_{i})}{q(x_{i})} \\right]},\n\\]\nде \\(p(\\cdot)\\) відповідатиме розподілу вхідних характеристикам, а \\(q(\\cdot)\\) відповідатиме зворотнім.\n\n\n11.1.3 Незворотність на основі пермутаційних шаблонів\nІдея аналізу пермутаційних шаблонів (PP — permutation patterns) спочатку була запропонована Бандтом і Помпе, щоб надати дослідникам простий та ефективний інструмент для характеристики складності динаміки реальних систем. Він уникає порогу амплітуди і замість цього має справу з порядковими шаблонами перестановок. Їх частоти дозволяють відрізнити детерміновані процеси від абсолютно випадкових. Розрахунки PP припускають, що часовий ряд розбивається на пересічні підвектори довжини \\(d_{E}\\):\n\\[\n\\vec{X}(i) = \\left\\{ x(i), x(i+\\tau), ... , x(i+[d_{E}-1]\\tau) \\right\\},\n\\]\nде часова затримка \\(\\tau\\) відповідає часу розділення між елементами.\nПісля цього кожен вектор представляється у вигляді порядкового шаблону \\(\\pi = \\{ r_0, r_1, ... , r_{d_{E}-1} \\}\\), що має задовільняти наступній умові:\n\\[\nx(i+r_0) \\leq x(i+r_1) \\leq ... \\leq x(i+r_{d_{E}-1}).\n\\]\nЦікава для нас міра незворотності часу на основі PP може бути отримана шляхом врахування їх відносної частоти як для початкового, так і для оберненого часового ряду. Відповідно, якщо обидва типи мають приблизно однакові розподіли ймовірностей своїх патернів, часові ряди представляються зворотними, а для іншого випадку робиться протилежний висновок.\nРізницю між розподілами прямих часових рядів (\\(P^{d}\\)) та зворотних (\\(P^{r}\\)) можна оцінити за допомогою дивергенції Кульбака-Лейблера."
  },
  {
    "objectID": "lab_11.html#хід-роботи",
    "href": "lab_11.html#хід-роботи",
    "title": "11  Лабораторна робота № 11",
    "section": "11.2 Хід роботи",
    "text": "11.2 Хід роботи\n\n11.2.1 Підключення необхідних бібліотек\n\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport scienceplots\nimport pandas as pd\nimport yfinance as yf\nimport networkx as nx\nfrom collections import defaultdict, Counter\nfrom ordpy import ordinal_distribution\nfrom tqdm import tqdm\nfrom scipy.spatial import distance\nfrom ts2vg import NaturalVG, HorizontalVG\n\n%matplotlib inline\n\n\n\n11.2.2 Встановлення параметрів для побудови графіків\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),          # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,              # товщина ліній\n    'axes.titlesize': 'small',         # розмір титулки над рисунком\n    'font.family': \"Times New Roman\",  # сімейство стилів підписів \n    'font.serif': [\"Times\"],           # стиль підпису\n    'mathtext.fontset': \"dejavuserif\", # стиль математичних виразів \n    'savefig.dpi': 300                 # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\n\n\n11.2.3 Визначення функції для побудови рекурентного графа\n\ndef recurrence_net(time_ser, rec_thr, dim, tau, dist_type='euclidien'):\n    time_series = nk.complexity_embedding(time_ser, dimension=dim, delay=tau)\n    rp = (distance.cdist(time_series, time_series, dist_type) &lt;= rec_thr).astype(int)\n    adj_matrix_RN = rp\n    np.fill_diagonal(adj_matrix_RN, 0)\n\n    rec_nw = nx.from_numpy_matrix(adj_matrix_RN)\n    \n    return rec_nw\n\ndef node_positions_recurrence_net(ts, xs):\n    return {i: (xs[i], ts[i]) for i in range(len(ts))}\n\n\n\n11.2.4 Визначення функції для розрахунку індексу Кошти\n\ndef Costa_1(time_ser, taus):\n    Cst = []\n    for tau in taus:\n        fragm_Costa = np.array([time_ser[tau:], time_ser[:-tau]])\n        DiffCosta = np.diff(fragm_Costa,axis=0)\n        IncCosta = np.sum(DiffCosta&gt;0)\n        DecCosta = np.sum(DiffCosta&lt;0)\n        C = (IncCosta-DecCosta)/(len(time_ser)-tau)\n        Cst.append(C)\n    Costa = np.mean(np.abs(Cst))\n    return Costa\n\n\n\n11.2.5 Оголошення функцій для підрахунку показників незворотності\n\n11.2.5.1 Пермутаційна незворотність\n\ndef PermIrrever(time_ser, d_e, tau, delta=1e-10, distance_irr=\"kullback\"):\n\n    rev_arr = np.flip(time_ser)\n    \n    # отримання розподілу порядкових шаблонів для вихідного ряду\n    _, dist_dir = ordinal_distribution(time_ser, \n                                        dx=d_e, \n                                        taux=tau, \n                                        return_missing=True)\n    \n    # отримання розподілу порядкових шаблонів для зворотнього ряду\n    _, dist_rev = ordinal_distribution(rev_arr,\n                                        dx=d_e,\n                                        taux=tau,\n                                        return_missing=True)\n    \n    if distance_irr == \"kullback\":\n        KLD_perm = dist_dir * np.log((dist_dir + delta) / (dist_rev + delta))\n        return np.sum(KLD_perm)   \n    else:\n        return distance.jensenshannon(dist_dir + delta, dist_rev + delta)\n\n\n\n11.2.5.2 Функція для підрахунку ймовірностей\n\ndef calc_prob_dist(p, q):\n\n    p_cnt, q_cnt = dict(Counter(p)), dict(Counter(q))\n    p_sum = sum(p_cnt.values())\n    p_dist = {k: v / p_sum for k, v in p_cnt.items()}\n    q_sum = sum(q_cnt.values())\n    q_cnt = {k: v / q_sum for k, v in q_cnt.items()}\n    q_dist = defaultdict(lambda: 0)\n    q_dist.update(q_cnt)\n\n    for k in p_dist.keys():\n        if k not in q_dist.keys():\n            q_dist[k] = 0.0\n\n    for k in q_dist.keys():\n        if k not in p_dist.keys():\n            p_dist[k] = 0.0\n\n    return p_dist, q_dist\n\n\n\n11.2.5.3 Дивергеція Кульбака-Лейблера\n\ndef KLD(p_dist, q_dist, delta=1e-10):\n\n    div_list = [\n        p_proba * np.log((p_proba + delta) / (q_dist[k] + delta)) for k, p_proba in p_dist.items()\n    ]\n    \n    kld = np.sum(np.array(div_list))\n\n    return kld\n\n\n\n11.2.5.4 Дивергенція Йенсена-Шеннона\n\ndef JS(p_dist, q_dist, delta=1e-10):\n\n    m_dist = {k: 0.5*(p_dist[k]+q_dist[k]) for k in p_dist.keys()}\n\n    js = 0.5*KLD(p_dist, m_dist) + 0.5*KLD(q_dist, m_dist)\n\n    return js\n\n\n\n11.2.5.5 Графо-динамічна незворотність\n\ndef GraphIrrever(fragm, \n                 graph_type='classic', \n                 delta=1e-10, \n                 d_e_rec=3, \n                 tau_rec=1, \n                 eps_rec=0.1, \n                 dist_rec='chebyshev', \n                 distance_irr='kullback'):\n    \n    # будуємо граф \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n    elif graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n    else:\n        g = recurrence_net(fragm, \n                           rec_thr=eps_rec*np.abs(np.std(fragm)), \n                           dim=d_e_rec, \n                           tau=tau_rec, \n                           dist_type=dist_rec)\n    \n    # розраховуємо вхідні та вихідні характеристики \n    adjacency_mat = g.adjacency_matrix()\n    ret_deg, adv_deg = GetDegree(adjacency_mat)\n    ret_clust, adv_clust = GetLocalClusteringCoefficient(adjacency_mat, ret_deg, adv_deg)\n\n    # знаходимо розподіл імовірностей\n    ret_deg_probs, adv_deg_probs = calc_prob_dist(ret_deg, adv_deg)\n    ret_clust_probs, adv_clust_probs = calc_prob_dist(ret_clust, adv_clust)\n\n    # розраховуємо асиметрію (незворотність) за допомогою Кульбака-Лейблера\n    if distance_irr == \"kullback\":\n        distance_deg = KLD(ret_deg_probs, adv_deg_probs, delta)\n        distance_clust = KLD(ret_clust_probs, adv_clust_probs, delta)\n\n    # розраховуємо асиметрію (незворотність) за допомогою Йенсена-Шеннона\n    if distance_irr == \"shannon\":\n        distance_deg = JS(ret_deg_probs, adv_deg_probs, delta)\n        distance_clust = JS(ret_clust_probs, adv_clust_probs, delta)\n      \n    return distance_deg, distance_clust\n\n\n\n11.2.5.6 Функції для отримання ступеня вершини та локальної кластеризації\nПроцедура знаходження ступеню зв’язків та локальної кластеризації кожної вершини є доволі громіздкою. Для прискорення розрахунків відповідних процедур скористаємось бібліотекою numba. Numba — це швидкий компілятор для Python, який найкраще працює з кодом, що використовує масиви, функції та цикли NumPy. Найпоширеніший спосіб використання Numba — це колекція декораторів, які можна застосувати до ваших функцій, щоб доручити Numba їх компілювати. Коли здійснюється виклик функції, прикрашеної Numba, вона компілюється у машинний код “just-in-time” для виконання, і весь або частина вашого коду може згодом виконуватися зі швидкістю власного машинного коду!\nВстановити її можна в наступний спосіб:\n\n!pip install numba==0.56.4\n\nNumba надає декілька утиліт для генерації коду, але центральною функцією є декоратор numba.jit(). За допомогою цього декоратора ви можете позначити функцію для оптимізації JIT-компілятором Numba. Різні режими виклику викликають різні варіанти компіляції та поведінки. Імпортуємо відповідний декоратор з бібліотеки numba:\n\nfrom numba import jit\n\n\n@jit(nopython=True, nogil=True) \ndef GetDegree(AM):\n    numNodes = AM.shape[0]\n    retarded_degree = np.zeros((numNodes))\n    advanced_degree = np.zeros((numNodes))\n     \n    for i in range(numNodes):\n        retarded_degree[i] = AM[i, :i].sum()\n\n    for i in range(numNodes):\n        advanced_degree[i] = AM[i, i:].sum()\n        \n    return retarded_degree, advanced_degree\n\n\n@jit(nopython=True, nogil=True) \ndef GetLocalClusteringCoefficient(AM, ret_deg, adv_deg):\n    \n    numNodes = AM.shape[0]\n    retardedCC = np.zeros( (numNodes) )\n    advancedCC = np.zeros( (numNodes) )\n    ret_norm = ret_deg * (ret_deg - 1) / 2\n    adv_norm = adv_deg * (adv_deg - 1) / 2\n    \n    for i in range(numNodes):\n        if ret_norm[i] != 0: \n            counter = 0\n            \n            for j in range(i):\n                for k in range(j): \n                    if AM[i, j] == 1 and AM[j, k] == 1 and AM[k, i] == 1: \n                        counter += 1\n                        \n            retardedCC[i] = counter / ret_norm[i]\n    \n    for i in range(numNodes-2):\n        if adv_norm[i] != 0: \n            counter = 0\n            \n            for j in range(i+1, numNodes):\n                for k in range(i+1, j): \n                    if AM[i, j] == 1 and AM[j, k] == 1 and AM[k, i] == 1: \n                        counter += 1\n                        \n            advancedCC[i] = counter / adv_norm[i]\n                 \n                \n    return retardedCC, advancedCC\n    \n\n\n\n\n11.2.6 Завантажуємо дані з сайту Yahoo! Finance\n\nsymbol = '^RUT'                       # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\ndate_in_num = mdates.date2num(time_ser.index)\n\nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\ndate_in_num = time_ser.index\n\n\n\n\n11.2.7 Виводимо досліджувані ряди\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 11.1: Динаміка щоденних змін індексу Russell 2000\n\n\n\n\nКористуючись тими методами, що ми розглянули в попередній лабораторній роботі, побудуємо діаграму Пуанкаре та граф нашого часового ряду. Але перш за все, для діаграми Пункаре, треба знайти стандартизовані прибутковості. Для цього оголосимо функцію transformation(), що прийматиме на вхід часовий сигнал, тип ряду, і повертатиме його перетворення:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n\n11.2.8 Встановлення параметрів для розрахунків\n\nwindow = 500    # розмір ковзного вікна\ntstep = 1       # часовий крок\n\nret_type = 1    # тип ряду: \n                # 1 - вихідний, \n                # 2 - детрендований\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований вихідний часовий ряд\n\n# параметри для мір асиметрії\ntau_assym = 1                 # часова затримка для діаграми Пуанкаре\ntau_Costa_begin = 1           # початковий часовий масштаб для індексу Кошти\ntau_Costa_end = 20            # кінцевий часовий масштаб для індексу Кошти\ntaus_Costa = np.arange(tau_Costa_begin, tau_Costa_end+1) # формуємо масив масштабів\n                \nlength = len(time_ser)\n\n\n\n11.2.9 Виводимо діаграму Пуанкаре та розраховуємо міри на її основі\n\nfor_puank = time_ser.copy()\n\nfor_puank = transformation(for_puank, ret_type)\n\n\nfig, ax1 = plt.subplots(1, 1)\n\nax1.scatter(for_puank[:-tau_assym],for_puank[tau_assym:], marker=\"X\", s=180, c=\"g\")\n\nlow_x, high_x = ax1.get_xlim()\nlow_y, high_y = ax1.get_ylim()\nax1.axline([low_x, low_y], [high_x, high_y])\n\nax1.set_aspect('equal', 'box')\nax1.set_xlabel(r'$g(t)$')\nax1.set_ylabel(r'$g(t+\\tau)$') \nax1.set_xlim(left=low_x, right=high_x)\nax1.set_ylim(bottom=low_y, top=high_y)\nplt.locator_params(axis='y', nbins=7)\n\nplt.savefig(f\"Poincare_plot_{symbol}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\nВиходячи з даної діаграми, можна зазначити, що для прибутковостей індексу Russell 2000 спостерігається асиметрія у сторону зростаючих флуктуацій ряду.\n\n\n11.2.10 Побудова показників незворотності із використанням ковзного вікна\nВизначаємо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nОголошуємо масиви для збереження результатів:\n\nPIx = []\nGIx = []\nSIx = []\nAIx = []\nEIx = []\nCIx = []\n\nРозраховуємо відповідні міри у віконній процедурі:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент ряду \n    \n    fragm = transformation(fragm, ret_type)\n    \n    Temp_fragm = np.array([fragm[:-tau_assym], fragm[tau_assym:]])\n    \n    T2   = np.transpose(np.arctan(Temp_fragm[1,:]/Temp_fragm[0,:])*180/np.pi)\n    Dup  = abs(np.diff(Temp_fragm[:,T2&gt;45],axis=0))\n    Dtot = abs(np.diff(Temp_fragm[:,T2!=45],axis=0))\n    Sup  = np.sum(abs(T2[T2&gt;45]-45))\n    Stot = np.sum(abs(T2[T2!=45]-45))\n    Aup  = np.sum(abs(np.transpose(((T2[T2&gt;45]-45))*np.sqrt(np.sum(Temp_fragm[:,T2&gt;45]**2,axis=0)))))\n    Atot = np.sum(abs(np.transpose(((T2[T2!=45]-45))*np.sqrt(np.sum(Temp_fragm[:,T2!=45]**2,axis=0)))))\n    Ethird = np.sum(np.transpose(Temp_fragm[0,:]-Temp_fragm[1,:])**3)\n    Etot = (np.sum(np.transpose(Temp_fragm[0,:]-Temp_fragm[1,:])**2))**(3/2)\n\n    \n    Porta = sum(T2&lt;45)/sum(T2!=45)\n    Gudzik = np.sum(Dup**2)/np.sum(Dtot**2)\n    Slope = Sup/Stot\n    Area = Aup/Atot\n    Eiler = Ethird/Etot\n    Costa = Costa_1(fragm, taus_Costa)\n    \n    PIx.append(Porta)\n    GIx.append(Gudzik)\n    SIx.append(Slope)\n    AIx.append(Area)\n    EIx.append(Eiler)\n    CIx.append(Costa)\n\n100%|██████████| 8605/8605 [00:30&lt;00:00, 283.27it/s]\n\n\nЗберігаємо значення до .txt файлів\n\nnp.savetxt(f\"Porta_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", PIx)\nnp.savetxt(f\"Gudzik_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", GIx)\nnp.savetxt(f\"Slope_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", SIx)\nnp.savetxt(f\"Area_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", AIx)\nnp.savetxt(f\"Eiler_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", EIx)\nnp.savetxt(f\"Costa_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", CIx)\n\n\n\n11.2.11 Візуалізація показників на основі діаграми Пуанкаре\n\n11.2.11.1 Індекс Порти\n\nmeasure_label = r\"$PIx$\"\nfile_name = f\"PIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          PIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРис. 11.2: Динаміка індексу Russell 2000 та індексу Порти\n\n\n\n\n\n\n11.2.11.2 Індекс Гузіка\n\nmeasure_label = r\"$GIx$\"\nfile_name = f\"GIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          GIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРис. 11.3: Динаміка індексу Russell 2000 та індексу Гузіка\n\n\n\n\n\n\n11.2.11.3 Індекс кута нахилу\n\nmeasure_label = r\"$SIx$\"\nfile_name = f\"SIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРис. 11.4: Динаміка індексу Russell 2000 та індексу кута нахилу\n\n\n\n\n\n\n11.2.11.4 Індекс площі секторів\n\nmeasure_label = r\"$AIx$\"\nfile_name = f\"AIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          AIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРис. 11.5: Динаміка індексу Russell 2000 та індексу площі секторів\n\n\n\n\n\n\n11.2.11.5 Індекс Ейлера\n\nmeasure_label = r\"$EIx$\"\nfile_name = f\"EIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          EIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРис. 11.6: Динаміка індексу Russell 2000 та індексу Ейлера\n\n\n\n\n\n\n11.2.11.6 Індекс Кошти\n\nmeasure_label = r\"$CIx$\"\nfile_name = f\"CIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          CIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\nРис. 11.7: Динаміка індексу Russell 2000 та індексу Кошти\n\n\n\n\n\n\n\n11.2.12 Побудова графу досліджуваного ряду\n\ngraph_type = 'classic'    # тип графу: classic, horizontal, recurrent\n\n# параметри для рекурентного графу\nd_e_rec = 3 # розмірність вкладень\ntau_rec = 1 # часова затримка\neps_rec = 1.3 # радіус\ndist_rec = 'chebyshev' # відстань між траєкторіями: \n                       # canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, \n                       # ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, \n                       # ‘jensenshannon’, ‘kulsinski’, ‘kulczynski1’, ‘mahalanobis’, \n                       # ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, \n                       # ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n\n\nindex_begin = 2000  # початковий індекс для графу\nindex_end = 4000    # кінцевий індекс для графу\n\nret_type = 1        # вид ряду\n\n\nfor_graph_plot = time_ser.copy()\n\nfor_graph_plot = transformation(for_graph_plot, ret_type)\n\ndate = date_in_num[index_begin:index_end] # вилучаємо необхідні по індексам дати\n\n# будуємо граф у залежності від типу графа\nif graph_type == 'classic':\n    g = NaturalVG(directed=None).build(for_graph_plot[index_begin:index_end], xs=date)\n    pos = g.node_positions()\n    nxg = g.as_networkx()\nelif graph_type == 'horizontal':\n    g = HorizontalVG(directed=None).build(for_graph_plot[index_begin:index_end], xs=date)\n    pos = g.node_positions()\n    nxg = g.as_networkx()\nelse:\n    g = recurrence_net(for_graph_plot[index_begin:index_end], \n                       rec_thr=eps_rec * np.abs(np.std(for_graph_plot[index_begin:index_end])), \n                       dim=d_e_rec, \n                       tau=tau_rec, \n                       dist_type=dist_rec)\n    \n    pos = node_positions_recurrence_net(for_graph_plot[index_begin:index_end], date)\n    nxg = g\n    \n    \n# встановлення параметрів для побудови графів\ngraph_plot_options = {\n    'with_labels': False,\n    'node_size': 2,\n    'node_color': [(0, 0, 0, 1)],\n    'edge_color': [(0, 0, 0, 0.15)],\n}\n\nВиводимо зв’язки видимості:\n\nfig, ax = plt.subplots(1, 1)\n\nnx.draw_networkx(nxg, ax=ax, pos=pos, **graph_plot_options)\nax.tick_params(bottom=True, labelbottom=True)\nax.plot(time_ser.index[index_begin:index_end], \n        for_graph_plot[index_begin:index_end], \n        label=fr\"{symbol}\")\n\nax.set_title('Visibility Connections', fontsize=22)\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{symbol}\")\nax.set_xlim(time_ser.index[index_begin:index_end][0],\n            time_ser.index[index_begin:index_end][-1])\nax.legend(loc='upper right')\n\nplt.savefig(f\"Time_ser_connections_symbol={symbol}_ \\\n    idx_beg={index_begin}_idx_end={index_end}_sertype={ret_type}_ \\\n    network_type={graph_type}.jpg\", bbox_inches=\"tight\")\n\nplt.show(); \n\n\n\n\nВиходячи з графу взятого нами фрагменту видно, що крах поблизу 2000-го року характеризується високою концентрацією вузлів. Це вказує на високий ступінь довготривалої пам’яті для кризових явищ фондового ринку, що в свою чергу впливає і на їх незворотність.\nТепер розглянемо сам граф:\n\npos = nx.spring_layout(nxg, k=0.15, iterations=100)\n# знаходимо вузол близький до центру графа (0.5,0.5)\ndmin = 1\nncenter = 0\nfor n in pos: \n    x, y = pos[n]\n    d = (x - 0.5)**2 + (y - 0.5)**2\n    if d &lt; dmin:\n        ncenter = n\n        dmin = d\n\n# розфарбовуємо в залежності від ступеня вершини\n\np = dict(nx.degree(nxg))\nfig, ax2 = plt.subplots(1, 1)\nax2.set_title('Graph representation')\nnx.draw_networkx_edges(nxg, ax=ax2, pos=pos, nodelist=[ncenter], alpha=0.4,width=0.1)\nnx.draw_networkx_nodes(nxg, ax=ax2, pos=pos, nodelist=list(p.keys()),\n                       node_size=10, edgecolors='r', linewidths=0.01,\n                       node_color=list(p.values()),\n                       cmap=plt.cm.Blues_r)\n        \nvmin = np.asarray(list(p.values())).min()\nvmax = np.asarray(list(p.values())).max()\n\nsm = plt.cm.ScalarMappable(cmap=plt.cm.Blues_r, norm=plt.Normalize(vmin=vmin, vmax=vmax))\ncb = plt.colorbar(sm, ax=ax2)\ncb.set_label('degree')\n\nplt.savefig(f\"Graph_representation_symbol={symbol}_ \\\n            idx_beg={index_begin}_idx_end={index_end} \\\n            _sertype={ret_type}_network_type={graph_type}.jpg\", bbox_inches=\"tight\")\nplt.show(); \n\n\n\n\n\n\n11.2.13 Побудова показників незворотності на основі пермутаційних шаблонів та графів\nІніціалізуємо масиви для збереження результатів розрахунків:\n\nwindow = 500      # ширина вікна\ntstep = 1         # часовий крок\n\nret_type = 1      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\n# параметри для рекурентного графу\nd_e_rec = 3            # розмірність вкладень\ntau_rec = 1            # часова затримка\neps_rec = 1.3          # радіус\n\ndist_rec = 'chebyshev' # відстань між траєкторіями: \n                       # canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, \n                       # ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, \n                       # ‘jensenshannon’, ‘kulsinski’, ‘kulczynski1’, ‘mahalanobis’, \n                       # ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, \n                       # ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n\n# параметри для мір незворотності\nd_e_perm = 3              # розмірність вкладень для пермутаційних патернів \ntau_perm  = 1             # часова затримка для пермутаційних патернів\ndistance_irr = 'kullback' # відстань між розподілами: kullback, shannon\ngraph_type = 'classic'    # тип графу: classic, horizontal, recurrent\n\nlength = len(time_ser.values)  # довжина самого ряду\n\nDegree = []\nClust = []\nPerm = []\n\nРозпочинаємо процедуру рухомого вікна:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент ряду\n\n    fragm = transformation(fragm, ret_type)  # виконуємо перетворення\n        \n    deg, clust = GraphIrrever(fragm, \n                            graph_type=graph_type, \n                            delta=1e-10, \n                            d_e_rec=d_e_rec, \n                            tau_rec=tau_rec, \n                            eps_rec=eps_rec, \n                            dist_rec=dist_rec, \n                            distance_irr=distance_irr)  \n    \n    perm = PermIrrever(fragm, \n                        d_e=d_e_perm, \n                        tau=tau_perm, \n                        delta=1e-10, \n                        distance_irr=distance_irr)\n      \n    Degree.append(deg)\n    Clust.append(clust)\n    Perm.append(perm)\n\n100%|██████████| 8605/8605 [02:01&lt;00:00, 71.09it/s]\n\n\nЗберігаємо результати до .txt файлів\n\nnp.savetxt(f\"{distance_irr}_deg_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_graph_type={graph_type}.txt\", Degree)\nnp.savetxt(f\"{distance_irr}_clust_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_graph_type={graph_type}.txt\", Clust)\nnp.savetxt(f\"{distance_irr}_perm_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_d_e={d_e_perm}_tau={tau_perm}.txt\", Perm)\n\n\n\n11.2.14 Візуалізація показників на основі графів та пермутаційних шаблонів\n\n11.2.14.1 Ступінь незворотності на основі ступеня вершини\n\nmeasure_label = r\"$Dist_{deg}$\"\nfile_name = f\"Degree_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            _graph_type={graph_type}_dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Degree, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\nРис. 11.8: Динаміка індексу Russell 2000 та показника незворотності на основі ступеня вершини\n\n\n\n\n\n\n11.2.14.2 Ступінь незворотності на основі показника локальної кластеризації\n\nmeasure_label = r\"$Dist_{clust}$\"\nfile_name = f\"Clust_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            _graph_type={graph_type}_dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Clust, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\nРис. 11.9: Динаміка індексу Russell 2000 та показника незворотності на основі локальної кластеризації\n\n\n\n\n\n\n11.2.14.3 Ступінь незворотності на основі пермутаційних шаблонів\n\nmeasure_label = r\"$Dist_{perm}$\"\nfile_name = f\"Perm_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            d_e={d_e_perm}_tau={tau_perm} \\\n            _dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Perm, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\nРис. 11.10: Динаміка індексу Russell 2000 та показника незворотності на основі пермутаційних шаблонів"
  },
  {
    "objectID": "lab_11.html#висновок",
    "href": "lab_11.html#висновок",
    "title": "11  Лабораторна робота № 11",
    "section": "11.3 Висновок",
    "text": "11.3 Висновок\nУ даній роботі було розглянуто показники незворотності (асиметрії) системи на основі діаграм Пуанкаре, графу видимості та пермутаційних шаблонів. Було продемонстровано побудову діграми Пуанкаре та зв’язків видимості як для всього ряду, так і для деяких із його фрагментів. Видно, що значення на діаграмі Пуанкаре характеризуються розподілом точок, що виходять за межі нормального Гаусового розподілу. Для графу видимості видно, що кризові стани характеризуються значною концентрацією зв’язків, що є довгостроковими. Таким чином, поставало актуальним проводити розрахунок показників незворотності графового типу для вихідного ряду.\nПоказники на основі діаграми Пуанкаре демонструють зріст або спад у кризові періоди, що вказує на зростання асиметрії у даний період часу. Дані показники можуть слугувати в якості індикаторів кризових явищ.\nВиходячи з 3 показників незворотності, що представлені вище, видно, що дані показники починають зростати в передкризовий період, вказуючи на стартовий період хаосу. Найгірше серед них себе поводить \\(Dist_{deg}\\). Найращим чином \\(Dist_{clust}\\) та \\(Dist_{perm}\\). Їх і варто використовувати в якості індикаторів-передвісників крахів."
  },
  {
    "objectID": "lab_12.html#теоретичні-відомості",
    "href": "lab_12.html#теоретичні-відомості",
    "title": "12  Лабораторна робота № 12",
    "section": "12.1 Теоретичні відомості",
    "text": "12.1 Теоретичні відомості\n\n12.1.1 Імпортуємо необхідні бібліотеки\n\n\n\n\n\n\nУвага\n\n\n\nПерш за все, для подальшої роботи, нам необхідно буде встановити бібліотеку pylevy. Встановити її можна з відповідного GitHub репозиторію. Ми у свою чергу завантажимо її напряму через команду pip install слідуючим чином:\n\n\n\n!pip install git+https://github.com/josemiotto/pylevy.git\n\nПісля встановлення необхідного модулю можна приступати до подальшої роботи.\n\nimport numpy as np                 # бібліотека для роботи з масивами чисел\nimport matplotlib.pyplot as plt    # бібліотека для побудови графіків\nimport yfinance as yf              # бібліотека для зчитування фінансових даних з Yahoo Finance\nimport levy                        # бібліотека для роботи з альфа-стабільним розподілом Леві\nimport pandas as pd                # бібліотека для фільтрації даних та їх обробки\nimport scienceplots\nfrom scipy.stats import norm, laplace  # бібліотека для побудови теоретичного розподілу Гауса\n                                       # та Лапласа\nfrom tqdm import tqdm                  # бібліотека для виводу шкали завантаження\n\n%matplotlib inline\n\n\n\n12.1.2 Виконуємо налаштування рисунків\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 14,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\n\n\n12.1.3 Зчитування даних\n\ndf = pd.read_csv('databases\\^spx_d.csv')\ndf.set_index('Date', inplace=True)\ndf.index = pd.to_datetime(df.index)\ndf = df[df.index &gt;= '1950-01-01']\n\n\n\n12.1.4 Важкі хвости та Чорні понеділки\nСпочатку ми розглянемо розподіл щоденних прибутковостей індексу S&P 500, починаючи з 1950 року. У клітинці нижче буде показано Pandas dataframe df, що містить дані OHLC з файлу (ці дані були попередньо завантажені в клітинці вище).\n\ndf\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n1950-01-03\n16.66\n16.66\n16.66\n16.66\n7.000000e+05\n\n\n1950-01-04\n16.85\n16.85\n16.85\n16.85\n1.050000e+06\n\n\n1950-01-05\n16.93\n16.93\n16.93\n16.93\n1.416667e+06\n\n\n1950-01-06\n16.98\n16.98\n16.98\n16.98\n1.116667e+06\n\n\n1950-01-07\n17.09\n17.09\n17.09\n17.09\n1.116667e+06\n\n\n...\n...\n...\n...\n...\n...\n\n\n2023-04-10\n4085.20\n4109.50\n4072.55\n4109.11\n1.951642e+09\n\n\n2023-04-11\n4110.29\n4124.26\n4102.61\n4108.94\n2.000949e+09\n\n\n2023-04-12\n4121.72\n4134.37\n4086.94\n4091.95\n2.249009e+09\n\n\n2023-04-13\n4100.04\n4150.26\n4099.40\n4146.22\n2.198579e+09\n\n\n2023-04-14\n4140.11\n4163.19\n4113.20\n4137.64\n2.088609e+09\n\n\n\n\n18527 rows × 5 columns\n\n\n\nЦей історичний ряд щоденних цін містить ~ 18527 торгових днів. Замість того, щоб дивитися безпосередньо на ціни, ми розглянемо щоденні log-прибутковості індексу S&P 500. Пам’ятайте, що логарифмічна прибутковість \\(r_t\\) визначається як логарифм відношення між цінами закриття \\(p_t\\) у теперішній момент часу та попередній, \\(p_{t-1}\\):\n\\[\nr_t = \\log(\\frac{p_t}{p_{t-1}})\n\\]\nЛогарифмічні прибутковості характеризуються симетрією, тобто якщо в один день індекс втрачає \\(r_t = -0.1\\), а на наступний день індекс набирає \\(r_{t+1} = +0.1\\), індекс знову досягає початкової ціни. Зі звичайними прибутковостями, спочатку втративши \\(10\\%\\), а потім знову набравши \\(10\\%\\), ви не повернетесь до початкової ціни. Ми зберігаємо прибутковості в новому фреймі даних lr:\n\nlr = np.log(df['Close']).diff().dropna()\n\n\nlr.plot()\nplt.show();\n\n\n\n\nВикористовуючи вбудовані функції Pandas та Numpy, тепер ми можемо подивитись на найгірше щоденнє значення прибутковостей. Ми виявляємо, що найгірше значення становило \\(-0.229\\), що відповідає \\(-22.9\\%\\), і що це сталося в понеділок, 19 жовтня 1987 року — так званий Чорний понеділок.\n\n# найгірший щоденні прибутковості від закриття до закриття\nlr.min()\n\n-0.2289972265656708\n\n\n\n# відповідні арифметичні прибутковості\nnp.exp(lr.min()) - 1\n\n-0.20466926070038938\n\n\n\n# індекс найгіршого значення\nnp.argmin(lr)\n\n9583\n\n\n\ndf.iloc[9583:9585]\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n1987-10-16\n298.08\n298.92\n281.52\n282.70\n188055556.0\n\n\n1987-10-19\n282.70\n282.70\n224.83\n224.84\n335722222.0\n\n\n\n\n\n\n\nДалі ми будуємо графік вибіркового розподілу щоденних логарифмічних прибутковостей, що демонструє нам, як часто ми спостерігаємо щоденні прибутковості певного знака та величини. Зверніть увагу, що ми встановили логарифмічне масштабування для вісі Оy, щоб мати краще представлення про хвости розподілу, тобто дуже великі, але рідкісні негативні та позитивні прибутковості — крахи та підйоми:\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.yscale('log')\nplt.ylabel('Розп. ймовірностей')\nplt.xlabel('Щоденні лог-прибутковості')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.011, 99])\nplt.annotate('Чорний понеділок (1987)', [-0.226, 0.02], xytext=[-0.215, 0.15],\n             arrowprops=dict(facecolor='0.4', width=1, headwidth=6, headlength=6),\n             ha='left', va='bottom');\nplt.show();\n\n\n\n\nЯк ми можемо описати погіршення розподілу прибутковості? За допомогою логарифмічного масштабування осі Oy, щоб експоненціальний спад виглядав лінійно, тому ми могли би запідозрити, що при малих абсолютних значеннях прибутковості розподіл зменшується експоненціально, але в міру збільшення абсолютних значень прибутковостей ми все більше і більше відхиляємося від початкового нахилу. Насправді, ми можемо показати, що ні розповсюдженний розподіл Гауса (також званий нормальним розподілом), ні розподіл Лапласа з його експоненціальним спадом з обох сторін точно не можуть змоделювати розподіл прибутковостей S&P 500. Як розподіл Гауса, так і розподіл Лапласа недооцінюють ймовірність екстремальних подій — підйомів та крахів:\n\n# підганяємо розподіл Гауса та Лапласа під дані\nnorm_loc, norm_scale = norm.fit(lr)\nlaplace_loc, laplace_scale = laplace.fit(lr)\n\n# генеруємо значення (x,y) з використанням отриманих параметрів розподілів\nx_fit = np.linspace(-0.25, 0.125, 1000)\ny_fit_norm = norm(loc=norm_loc, scale=norm_scale).pdf(x_fit)\ny_fit_laplace = laplace(loc=laplace_loc, scale=laplace_scale).pdf(x_fit)\n\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.plot(x_fit, y_fit_laplace, lw=3, c='C3', alpha=0.8, label='Розподіл Лапласа')\nplt.plot(x_fit, y_fit_norm, lw=3, c='0.2', alpha=0.8, label='Розподіл Гауса')\nplt.yscale('log')\nplt.ylabel('Розп. ймовірностей')\nplt.xlabel('Щоденні лог-прибутковості')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.011, 99])\nplt.legend()\n\nplt.annotate('Чорний понеділок (1987)', [-0.226, 0.02], xytext=[-0.215, 0.15],\n             arrowprops=dict(facecolor='0.4', width=1, headwidth=6, headlength=6),\n             ha='left', va='bottom');\n\nplt.show();\n\n\n\n\n\n\n12.1.5 Відхилення в 23 сигма\nЯк ми можемо бачити вище, як розподіл Гауса, так і розподіл Лапласа різко недооцінюють ймовірність дуже волатильної динаміки! Часто зазначають, що трапилась “3-сигма подія на ринку” або щось подібне, тому що багатьом людям подобається оцінювати індивідуальні прибутковості по стандартному відхиленню (зване “сигмою”) підібраного гаусового розподілу. Однак, якщо Розподіл Гауса взагалі не відповідає розподілу вибірки, як у нашому випадку, оцінка краху в одиницях “сигма” може ввести в оману! Щоб показати це, ми спочатку перевіримо, на скільки сигм ринок змістився в Чорний понеділок 1987 року:\n\nnp.abs(np.min(lr))/np.std(lr, ddof=1)\n\n22.969393251617504\n\n\nВиходячи з розподілу по Гауса, Чорний понеділок був би подією з “23 сигмами”! Тепер Розподіл Гауса чітко говорить нам, як часто X-сигма подія має відбуватися:\n\n1-сигма: прибл. 1 з 3 днів\n2-сигма: прибл. 1 з 22 днів\n3-сигма: прибл. 1 з 370 днів\n\n…\nМи можемо розширити цю таблицю, попрацювавши з функцією кумулятивного розподілу, тісно пов’язана з функцією виживання, і перемістивши обчислення в логарифмічний простір для чисельної стабільності, як демонструють наступні комірки коду:\n\n# у середньому, як багато днів між 1-сигма подіями?\n1/(2*(1. - norm.cdf(1.)))\n\n3.1514871875343764\n\n\n\n# у середньому, як багато днів між 2-сигма подіями?\n1/(2*(1. - norm.cdf(2.)))\n\n21.97789450799283\n\n\n\n# у середньому, як багато днів між 3-сигма подіями?\n1/(2*(1. - norm.cdf(3.)))\n\n370.3983473449564\n\n\n\n# те саме, що й вище, але із використанням функції виживання\n1/(2*norm.sf(3.))\n\n370.3983473449592\n\n\n\n# те саме, що й вище, але із використанням логарифмічної функції функції виживання для чисельної стабільності\nnp.exp(-norm.logsf(3.) - np.log(2.))\n\n370.398347344959\n\n\n\n# те саме, що й вище, але із використанням логарифму з основою 10 для замість натурального логарифму для кращої інтерпретації\n10**(-norm.logsf(3.)/np.log(10) - np.log10(2.))\n\n370.3983473449588\n\n\n\n# як багато днів (log10 значення) між 23-сигма подіями, в середньому?\n(-norm.logsf(23.)/np.log(10) - np.log10(2.))\n\n116.33149536636726\n\n\nЩоб отримати додаткову інформацію про частоту екстремальних прибутковостей, ми можемо навіть створити невеликий інтерактивний віджет за допомогою повзунка, який вказує кількість “сигм”, а текст поруч із ним показує середню кількість днів між двома такими екстремальними подіями:\n\nfrom ipywidgets import IntSlider, interact\n\ndef years_between(X):\n    log10_days = (-norm.logsf(X)/np.log(10) - np.log10(2.))\n    days = int(10**log10_days)\n    return f'У сереньому, між двома X-сигма подіями, варто очікувати {days} дні(в), (день).'\n\ninteract(years_between, X=IntSlider(min=1, max=23, step=1, value=1));\n\n\n\n\n\nnp.log10(float(214533622638557983431869220329015643677794364342592664885632499326649056583365324378194575664549945880382591666749440))\n\n116.33149536636726\n\n\nВи бачите, що при переміщенні повзунка вправо кількість днів або років між двома наступними “X-сигма” подіями швидко наростає. Наша таблиця тепер виглядає наступним чином:\n\n1-сигма: прибл. 1 з 3 днів\n2-сигма: прибл. 1 з 22 днів\n3-сигма: прибл. 1 з 370 днів\n\n…\n\n23-сигма: приблизно 1 з \\(10^{116}\\) днів (!!!)\n\nВнесемо ясність: виходячи з розподілу Гауса, щоденні втрати Чорної п’ятниці 1987 року повинні бути подією, яку ми очікуємо раз на \\(10^{116}\\) днів. Це число в значній мірі незбагненно велике. Якби S&P 500 почав торгуватися відразу після народження Всесвіту, це становило б лише \\(10^{13}\\) торгових днів. Через \\(10^{116}\\) днів усі зірки у Всесвіті давно згорять, навіть усі чорні діри випаруються і Всесвіт стане темним і порожнім місцем. Тож або ми повинні бути дуже щасливі, що єдина очікувана Чорна п’ятниця в історії та майбутньому Всесвіту позаду, або ви дійсно ніколи не повинні використовувати Розподіл Гауса для моделювання прибутковості акцій! Незалежно від ринку та деталей, якщо хтось говорить про події з 10 сигмами або 23 сигмами, він, безумовно, використовує неправильну модель, оскільки шанс спостерігати таку подію в нашому житті незначний.\nОтже, виникає питання: Як ми справляємось із такими екстремальними значеннями? Чи слід позначати їх як викиди або артефакти, щоб наші фінансові моделі краще описували більшість значень? Звичайно, ні, оскільки результат наших інвестицій може критично залежати не від більшості менших прибутковостей, а саме від таких екстремальних подій! Далі ми познайомимося з підходом, який може пояснити та екстраполювати за межі екстремальних подій, таких як Чорний понеділок. Але ми також побачимо, що не всі способи врахування важких хвостів розподілів працюють добре, оскільки деякі моделі, такі як GARCH може заколисати нас помилковим почуттям безпеки, коли ми завжди беремо до уваги минулі екстремальні події, але завжди дивуємося новим.\n\n\n12.1.6 Статистика степеневого розподілу\nТепер, коли ми знаємо, що розподіл Гауса не є хорошим вибором, який тип розподілу насправді може описати частоту екстремальних подій, які струшують S&P 500? Частота екстремально позитивні та негативні прибутковості асиметричні для більшості фінансових активів, оскільки крахи, як правило, більш сильні, ніж підйоми. Ось чому нижче ми зосередимося на лівому (негативному) хвості розподілу прибутковостей.\nОскільки нас цікавлять лише екстремальні події, ми розглянемо лише ті прибутковості, які менше \\(-0.03\\) (приблизно три стандартних відхилення від середнього значення). Нижче ми візуалізуємо ці екстремально негативні показники прибутковостей в логарифмічному масштабі (тут ми використовуємо абсолютні значення негативних прибутковостей). Як вісь Oy (яка показує частоту логарифмічних прибутковостей), так і вісь Ox (яка показує величину логарифмічних прибутковостей) масштабуються логарифмічно:\n\n# хвіст починається приблизно при трьох стандартних відхиленнях від середнього\nnp.mean(lr) - 3*np.std(lr)\n\n-0.029610514264246313\n\n\n\nneg_lr = -lr[lr &lt; 0]\n\nplt.figure()\nhist = plt.hist(neg_lr, bins=np.logspace(np.log10(0.03),np.log10(0.3), 20), density=True)\nplt.yscale(\"log\")\nplt.xscale(\"log\")\nplt.ylabel('Розп. ймов.')\nplt.xlabel('Щоденні негативні абсолютні прибутковості')\n\nplt.ylim([0.1, 110])\nplt.xlim([0.03, 0.3])\nplt.xticks([0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3], [0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3])\nplt.show();\n\n\n\n\nЯк ми бачимо, ця гістограма екстремальних абсолютних прибутків зменшується приблизно лінійно при логарифмічному масштабуванні. Щоразу, коли ви виявляєте пряму лінію на логарифмічному графіку, це вказує на так званий степеневий розподіл. Степеневе співвідношення ймовірності спостереження великих абсолютних логарифмічних прибутковостей задається у вигляді:\n\\[\np(|r_t|) = c \\cdot |r_t|^{-\\alpha}\n\\]\nКонстанта \\(c\\) на даний момент нас не надто турбує, вона просто гарантує, що ліва частина рівняння належним чином нормалізована, але показник \\(\\alpha\\) представляє для найбільший інтерес, оскільки він говорить нам, як працює степеневий закон. Якщо ви знаєте частоту даних абсолютних логарифмічних прибутковостей, то степеневий закон підкаже вам, у скільки разів менш імовірними були б логарифмічні прибутковості подвоєного розміру:\n\\[\np(2 \\cdot |r_t|) = c \\cdot (2 \\cdot |r_t|)^{-\\alpha} = 2^{-\\alpha} \\cdot c \\cdot |r_t|^{-\\alpha} = 2^{-\\alpha} \\cdot p(|r_t|)\n\\]\nабо простіше:\n\\[\n\\frac{p(2 \\cdot |r_t|)}{p(|r_t|)} = \\frac{1}{2^{\\alpha}}\n\\]\nДля \\(\\alpha = 0\\) всі прибутковості однаково ймовірні, для \\(\\alpha = 2\\) подвоєння прибутковостей робить їх у 4 рази менш імовірними. Оскільки це правило подвоєння вгору або вниз не залежить від значення самого \\(\\left| r_t \\right|\\), але працює для всіх значень \\(\\left| r_t \\right|\\). Степеневе співвідношення також називаються “безмасштабним” або масштабо-інваріантним.\nЯк щодо показника \\(\\alpha\\) для наших надзвичайно негативних логарифмічних прибутковостей S&P 500? Ми можемо легко оцінити його за даними, але є деякі тонкощі у правильному розбитті вищезазначеної гістограми, які можуть вплинути на нашу оцінку (див., наприклад, White et al. (2008)). Щоб обійти ці проблеми з розбиттям, ми можемо замість цього оцінити степенний показник кумулятивної гістограми логарифмічних прибутковостей, а потім відняти одиницю від отриманого нахилу:\n\n# підганяємо лінію регресії до кумулятивної гістограми (log10 для ймовірності) vs. лог-прибутковості (log10)\nsorted_neg_lr = np.sort(neg_lr.values)\ncumulative_probability = np.linspace(1, 0, len(sorted_neg_lr)+1)[:-1]\n\nx_min = 0.03\nmask = sorted_neg_lr &gt;= x_min\nm, b = np.polyfit(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]), 1)\nalpha = -(m - 1)\n\nplt.figure()\nplt.plot(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]))\nplt.scatter(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]), label='sample data')\n\nx_fit = np.linspace(np.log10(x_min), np.log10(1.1*np.max(neg_lr)), 100)\ny_fit = m*x_fit + b\n\nplt.plot(x_fit, y_fit, label='степенева підгонка')\nplt.title(f'розрахований степеневий показник $\\\\alpha={alpha:.2f}$')\nplt.xlabel('лог-прибутковості (log10-scaled)')\nplt.ylabel('кумулятивна гістограма (log10-scaled)')\nplt.show();\n\n\n\n\nВиходячи з цієї оцінки, ми отримуємо степеневої показник, рівний \\(\\alpha \\approx 3.7\\). Якщо ми побудуємо гістограму наших надзвичайно негативних логарифмічних прибутковостей, то побачимо, що вона добре відображає зниження ймовірності, оскільки масштаб збоїв збільшується. Однак цей степеневий показник все ще недооцінює ймовірність настання чергового “Чорного понеділка”, оскільки лінія відповідності не ідеально підходить до цієї крайньої точки. Якби ми захотіли створити ще більш консервативну модель екстремальних подій, нам потрібно було б вручну ще більше зменшити значення \\(\\alpha\\), щоб врахувати більшу ймовірність настання чорних понеділків за рахунок втрати точності підгонки для менших збоїв. Це перший раз, коли ми можемо побачити, як одна точка даних впливає на наші рішення щодо моделювання. На даний момент ми будемо довіряти оцінці параметра і погодимося з оцінкою \\(\\alpha=3.7\\).\n\nneg_lr = -lr[lr &lt; 0]\n\nplt.figure()\nhist = plt.hist(neg_lr, bins=np.logspace(np.log10(0.03),np.log10(0.3), 20), density=True)\nplt.yscale(\"log\")\nplt.xscale(\"log\")\nplt.ylabel('Розп. ймов.')\nplt.xlabel('Щоденні негативні абсолютні прибутковості')\nplt.ylim([0.1, 110]);\nplt.xlim([0.03, 0.3])\nplt.xticks([0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3], [0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3]);\n\nx_fit = np.logspace(np.log10(0.03),np.log10(0.3), 100)\ny_fit = 0.00022*(x_fit**-3.7)\n\nplt.plot(x_fit, y_fit, lw=3)\nplt.show();\n\n\n\n\nМи можемо використовувати цей степеневий закон, щоб екстраполювати, як часто ми очікуємо настання чорного понеділка (або “події з 23 сигмами”) у довгостроковій перспективі. Виходячи з наявних у нас даних, ми можемо оцінити, що ймовірність спостереження зниження на \\(-0.03\\) або гірше становить близько \\(1.6\\%\\). Слідуючи нашій степеневій кривій, зниження чорного понеділка на \\(-0.229\\), таким чином, відбувалося б із частотою \\(\\left(\\frac{0.229}{0.03}\\right)^{-3.7} \\approx \\frac{1}{1845}\\). Так само часто, як зниження на \\(-0.03\\), яке ми спостерігаємо приблизно раз на три місяці. Це означає, що, виходячи з нашої степеневої кривої, очікується, що чорні понеділки, подібні до 1987 року, траплятимуться приблизно раз на 450 років (див. код нижче).\n\n# як часто ринок знижується більш ніж на -0.03\n(neg_lr &gt; 0.03).mean()\n\n0.016091417910447763\n\n\n\n# у середньому, як багато днів між цима трьома подіями\n1./(neg_lr &gt; 0.03).mean()\n\n62.144927536231876\n\n\n\n# у скільки разів менше відбувається подій \"Чорного понеділка\" в порівнянні зі зниженням на -0.03?\n(np.max(neg_lr)/0.03)**3.7\n\n1845.1139014672306\n\n\n\nnp.max(neg_lr)\n\n0.2289972265656708\n\n\n\n# скільки років пройшло між двома подіями \"Чорного понеділка\",\n# виходячи з припущення, що подія -0.03 відбувається один раз у 63 дня?\n(1./((1/62)*1/1845))/252\n\n453.92857142857144\n\n\nЯкби ми включили більше даних, що містять інші приклади екстремальних подій, наприклад, з Великої депресії, ця оцінка може стати ще меншою. Аналогічно, якби ми скоригували степеневий показник так, щоб він був більш консервативним, ніж передбачає наш простий метод оцінки, ми також отримали б коротший період. Хоча 460 років все ще є дуже великим періодом, це набагато більш реалістична оцінка порівняно з \\(10^{114}\\) роками, які ми отримали, використовуючи розподіл Гауса. Однією з переваг степеневої моделі є її здатність до екстраполяції: використовуючи степеневу оцінку \\(\\alpha\\) та частоту менших прибутковостей, щодо яких ми маємо достовірні дані, ми можемо оцінити частоту майбутніх серйозних збоїв, щодо яких ми ще не отримали жодних даних. Ще про одну перевагу оцінки \\(\\alpha\\) ми розглянемо далі.\n\n\n\n\n\n\nПРИМІТКА\n\n\n\nНе існує такого поняття, як “подія з 10 сигмами”! Справжня подія з 10 сигмами настільки неймовірно рідкісна, що пережити її протягом нашого життя — мізерно мало. Екстремальні події, безумовно, трапляються, але вони не можуть бути описані за допомогою “сигм”, їх потрібно враховувати і екстраполювати з використанням степеневих законів. Завжди будьте обережні, коли аналітики виправдовують себе за те, що вони не були готові до “події з 10 сигмами”, оскільки це вказує на те, що їх моделі ризиків вкрай недосконалі.\n\n\n\n\n12.1.7 Статистичні моменти під впливом важких хвостів\nЧому все це так важливо? Степенневий показник \\(\\alpha\\) насправді багато говорить нам про стабільність і збіжність моментів розподілу ймовірностей, і це, в свою чергу, має значення для деяких найбільш часто використовуваних моделей у фінансах. Перші чотири центральні моменти — це:\n\nСереднє: очікуване значення розподілу\nДисперсія: Квадрат стандартного відхилення, який часто використовується для оцінки волатильності на основі розподілу прибутковості.\nАсиметрія: вимірює зміщення розподілу. Розподіл прибутковостей, як правило, має негативну асиметрію, оскільки збої є більш стрімкими, ніж підйоми.\nЕксцес: вимірює тяжкість хвостів розподілу. Розподіли прибутковостей зазвичай мають ексцес більший ніж у розподілі Гауса, тобто екстремальні події відбуваються частіше, ніж очікувалося в гаусовій моделі.\n\nЕксцес Гаусового розподілу дорівнює 3 (використовуючи визначення Пірсона, з визначенням Фішера надлишкового ексцесу це 0), але наша вибірка щоденних логарифмічних прибутковостей S&P 500 має ексцес \\(28.6\\), що знову вказує на те, що екстремальні події набагато більш вірогідні, що суперечить Гаусовому розподілу.\n\n# обчислюємо ексцес лог-прибутковостей\nfrom scipy.stats import kurtosis\n\nkurtosis(lr, fisher=False, bias=False)\n\n28.610931957080663\n\n\nТепер ми дійшли до того моменту, коли ми могли б запитати: наскільки сильно одна точка впливає на нашу оцінку ексцесу? Якщо ми перерахуємо ексцес всіх щоденних прибутків S&P 500 з 1950 року і виключимо тільки чорний понеділок 1987 року, ми отримаємо значення \\(14.3\\), майже половину від значення, яке ми отримуємо, коли використовуємо всі значення! Видалення подальших найгірших днів, звичайно, ще більше зменшує ексцес, але ефект значно менший.\n\n# обчислюємо ексцес лог-прибутковостей після видалення Чорного понеділка 1987\nkurtosis(lr[lr &gt; np.min(lr)], fisher=False, bias=False)\n\n14.3083633314701\n\n\n\nlr_sorted = lr.sort_values()\nkurt = [kurtosis(lr_sorted.iloc[i:], fisher=False, bias=False) for i in range(10)]\n\nplt.figure()\nplt.plot(np.arange(10), kurt, c='C3', lw=2, zorder=2)\nplt.scatter(np.arange(10), kurt, s=120, lw=0.75, edgecolor='k', facecolor='C3', zorder=3)\nplt.grid()\nplt.xlabel('кіль-ть виключених найгірших днів')\nplt.ylabel('Ексцес щоденних лог-прибутковостей')\nplt.ylim([0, 32])\nplt.show();\n\n\n\n\nБільш радикальний погляд на цю проблему досягається, якщо ми будуємо графік ексцесу в покроковому режимі, тобто для кожного дня ми будуємо розрахунковий ексцес, використовуючи всі минулі логарифмічні дані до цього моменту часу. Як ви можете бачити нижче, до “Чорного понеділка” ексцес ніколи не перевищував \\(15\\), проте навіть через 35 років після “Чорного понеділка” ексцес ще не “оговтався” від цієї події і залишається на позначці 30. Чи означає це, що ми повинні просто ігнорувати Чорний понеділок як викид і продовжувати використовувати нашу “чисту” оцінку в \\(14.6\\)? Ні, точно ні! Як ми побачимо незабаром, ми скоріше повинні запитати себе, чи є сенс у тому, щоб оцінювати ексцес!\n\nkurt = [kurtosis(lr.iloc[:i], fisher=False, bias=False) for i in range(1000, len(lr))]\n\n\nplt.figure()\nplt.plot(lr.index[1000:], kurt, c='C3', lw=3, zorder=2)\nplt.ylabel('Ексцес щоденних лог-прибутковостей')\nplt.show();\n\n\n\n\nТой факт, що одна точка даних може настільки сильно змінити нашу оцінку ексцесу, і, схоже, вона більше не сходиться, коли додаються нові точки даних, є чітким свідченням того, що ексцес базового розподілу ймовірностей насправді нескінченний! Проблема полягає в тому, що для кінцевої вибірки даних (а всі набори даних скінченні) ми завжди зможемо оцінити скінчене значення ексцесу. scipy.stats.kurtosis не може повернути \\(\\infty\\), він завжди повідомлятиме про скінченний ексцес вибірки. Але ця кінцева оцінка не допоможе нам нічого сказати про майбутню поведінку ринку, оскільки ексцес продовжуватиме переходити до ще більших значень до наступного чорного понеділка, іншого чорного вівторка … у більш-менш віддаленому майбутньому.\nОднак опис частоти екстремальних подій згідно степеневого закону, який ми представили вище, може допомогти нам вирішити, чи є ексцес кінцевим, так що, переглянувши достатню кількість точок даних, ми зможемо вирішити, чи слід прийняти той факт, що ексцес не піддається оцінці і справді нескінченний. Щоб побачити це, нам потрібно ввести ще один розподіл ймовірностей, t-розподіл Стьюдента. Це колоколообразний симетричний розподіл зі степеневими хвостами. Його додатковий параметр \\(\\nu\\) визначає показник степеневого закону \\(\\alpha=(\\nu+1)\\). Нижче ми підганяємо t-розподіл до наших логарифмічних прибутковостей S&P 500, зберігаючи при цьому степеневий показник \\(\\alpha=3.7\\) (відповідний \\(\\nu = 2.7\\)), який ми підганяли раніше:\n\nfrom scipy.stats import t\n\nfit_params = t.fit(lr, fix_df=2.7)\n\nx_fit = np.linspace(-0.25, 0.125, 1000)\ny_fit_t = t(*fit_params).pdf(x_fit)\n\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.plot(x_fit, y_fit_t, lw=3, c='C3', alpha=0.8, label=\"Підігнаний t-розподіл\")\nplt.yscale('log')\nplt.ylabel('Розп. ймов.')\nplt.xlabel('Щоденні лог-прибутковості')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.0011, 99])\nplt.legend()\n\nplt.title(f'Підігнаний степеневий показник: $\\\\alpha = {(fit_params[0]+1):.2f}$')\nplt.show();\n\n\n\n\nЗвичайно, можна поставити під сумнів, чи дійсно t-розподіл Стьюдента добре відповідає нашим логарифмічним прибутковостям, оскільки він переоцінює частоту абсолютних прибутковостей і все ще недооцінює ймовірність настання Чорного понеділка. На даний момент ми ігноруємо ці деталі та зосереджуємось на ексцесі t-розподілу Стьюдента та зв’язку зі степеневим показником \\(\\alpha\\):\n\n\\(\\text{Kurt} = \\frac{6}{\\nu-4} + 3~\\) для \\(~\\nu &gt; 4~\\) або \\(~\\alpha &gt; 5\\)\n\\(\\text{Kurt} = \\infty~~~~~~~~~~\\,\\) для \\(~2 &lt; \\nu \\leq 4~\\) або \\(~3 &lt; \\alpha \\leq 5\\)\nінакше невизначено\n\nЦе підтверджує наше початкове припущення про те, що при степеневому показнику, рівному \\(\\alpha \\approx 3.7\\), ексцес дійсно нескінченний, і немає сенсу оцінювати його по скінченій вибірці, оскільки оціночне значення буде зберігатися тільки до наступної екстремальної події, яка підштовхне його ще вище. Ми можемо легко змоделювати цей ефект, витягуючи випадкові вибірки з t-розподілів Стьюдента з різними степеневими показниками, які відповідають трьом режимам, зазначеним вище. Нижче ми витягуємо 100 000 випадкових значень з t-розподілів з \\(\\alpha = 2\\), \\(\\alpha = 3.7\\), \\(\\alpha = 6\\), а потім обчислюємо ексцес поетапно, як і раніше, щоб побачити, як еволюціонує передбачуваний ексцес вибірки з часом у трьох різних випадках:\n\nnp.random.seed(1)\nsamples_alpha_20 = t(1.0, 0., 1.).rvs(100000)\nsamples_alpha_37 = t(2.7, 0., 1.).rvs(100000)\nsamples_alpha_60 = t(5.0, 0., 1.).rvs(100000)\n\n\nm = np.arange(1000, 100000, 50)\nkurt_alpha_20 = [kurtosis(samples_alpha_20[:i], fisher=False, bias=False) for i in m]\nkurt_alpha_37 = [kurtosis(samples_alpha_37[:i], fisher=False, bias=False) for i in m]\nkurt_alpha_60 = [kurtosis(samples_alpha_60[:i], fisher=False, bias=False) for i in m]\n\n\nplt.figure()\n\nplt.subplot(311)\nplt.plot(m, kurt_alpha_60, c='C2', lw=3, zorder=2)\nplt.axhline(6/(5-4) + 3, lw=1, ls='--', c='k')\nplt.ylabel('Ексцес')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\\\alpha=6$')\n\nplt.subplot(312)\nplt.plot(m, kurt_alpha_37, c='C1', lw=3, zorder=2)\nplt.ylabel('Ексцес')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\\\alpha=3.7$')\n\nplt.subplot(313)\nplt.plot(m, kurt_alpha_20, c='C3', lw=3, zorder=2)\nplt.ylabel('Ексцес')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\\\alpha=2$')\n\nplt.tight_layout()\nplt.show();\n\n\n\n\nУ випадку \\(\\alpha=6\\) (\\(\\nu=5\\)) ексцес скінченний і повинен приймати значення \\(\\frac{6}{\\nu - 4}+3=9\\). Хоча ми бачимо деякі коливання, оціночне значення ексцесу повільно наближається до справжнього значення. Для прикладу, який відповідає степеневому показнику індексу S&P 500 з \\(\\alpha = 3.7\\), моделювання показує таку ж поведінку, що і для реальних даних: збіжність не може бути виявлена, скоріше кожна екстремальна подія збільшує ексцес; при нескінченних вибірках ми би досягли нескінченного ексцесу. У третьому випадку з \\(\\alpha = 2\\) ексцес t-розподілу Стьюдента навіть не визначений належним чином, і ми можемо побачити інший тип поведінки моделювання: зараз більшість точок даних збільшують оцінку ексцесу, а не лише кілька окремих екстремальних зразків. При такому повільно спадаючому статечному законі екстремальні значення зустрічаються повсюдно, що призводить до збільшення ексцесу при моделюванні.\n\n\n\n\n\n\nПримітка\n\n\n\nНавіть якщо певні властивості розподілу ймовірностей нескінченні, ми завжди знайдемо кінцеві вибіркові оцінки при перегляді даного набору даних просто тому, що набір даних містить лише кінцеву кількість точок. Однак, якщо теорія степеневого закону чітко говорить нам, що певна властивість нескінченна, ми не повинні ніде використовувати кінцеві оцінки і ми не повинні використовувати моделі, які вимагають, щоб ця властивість була кінцевою! Якщо ми все одно це зробимо, це в кращому випадку буде працювати лише до наступної екстремальної події, яка анулює наші минулі оцінки і — у випадку фінансових моделей — може збанкрутувати нас!\n\n\n\n\n12.1.8 Як ексцес дестабілізує розрахунки волатильності\nОскільки ексцес збільшується — або стає нескінченним — це також впливає на нижчі моменти, найголовніше, на дисперсію. Квадратний корінь дисперсії — це стандартне відхилення, або волатильність, як це називається у фінансах. Волатильність є розповсюдженним параметром при оптимізації портфеля і практично в кожній фінансовій моделі, оскільки в кінцевому підсумку вона стала найбільш часто використовуваним показником “ризику”. Щоб побачити, як ексцес впливає на оцінку волатильності, ми проводимо простий експеримент Монте-Карло: ми витягуємо щоденну прибутковість за 50 років з розподілу ймовірностей за нашим вибором (ми будемо використовувати Гаусовий і t-розподіл Стьюдента) і обчислюємо оцінку волатильності, використовуючи вибіркове стандартне відхилення. Ми робимо це не тільки один раз, але і 10000 разів, кожен раз, коли ми розраховуємо нову гіпотетичну щоденну прибутковість за 50 років і обчислюємо відповідну оцінку волатильності. Звичайно, не всі ці оцінки будуть однаковими, але коливатимуться навколо справжньої волатильності, оскільки ми маємо лише кінцеву кількість даних. Оскільки ми знаємо справжній розподіл ймовірностей, який ми використовуємо для моделювання, ми також знаємо справжню волатильність. Щоб побачити, наскільки точно ми можемо оцінити волатильність на основі даних за 50 років, ми обчислюємо відносну похибку між оцінками волатильності та справжньою волатильністю.\n\nn_sim = 10000\n\ntrue_var = 1.\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(norm(0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:08&lt;00:00, 1125.24it/s]\n\n\n0.5039568642168273\n\n\n\nn_sim = 10000\n\nalpha = 6.0\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:13&lt;00:00, 728.17it/s]\n\n\n0.9616583563963519\n\n\n\nn_sim = 10000\n\nalpha = 4.0\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:14&lt;00:00, 700.76it/s]\n\n\n3.8082666425995932\n\n\n\nn_sim = 10000\n\nalpha = 3.7\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:14&lt;00:00, 709.36it/s]\n\n\n6.115969986022528\n\n\n\nn_sim = 10000\n\nalpha = 3.1\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:13&lt;00:00, 718.23it/s]\n\n\n38.47707859009605\n\n\nМи отримуємо наступні відносні похибки для різних розподілів та їх параметрів: - Гаусовий розподіл; відн. похиб. \\(\\approx 0.5\\%\\) - t-розподіл Стьюдента з \\(\\alpha=6.0\\) (\\(\\nu=5.0\\)); відн. похиб. \\(\\approx 1.0\\%\\) - t-розподіл Стьюдента з \\(\\alpha=4.0\\) (\\(\\nu=3.0\\)); відн. похиб. \\(\\approx 3.8\\%\\) - t-розподіл Стьюдента з \\(\\alpha=3.7\\) (\\(\\nu=2.7\\)); відн. похиб. \\(\\approx 6.1\\%\\)\nПри степеневому показнику \\(\\alpha= 3.7\\) (що відповідає \\(\\nu = 2.7\\); як ми підрахували для S&P 500, щоб пояснити Чорний понеділок), щоденна прибутковість за 50 років все ще призводить до помилки оцінки волатильності більш ніж на \\(6%\\)! Зверніть увагу, що ця велика помилка оцінки зростатиме ще швидше, коли вона наближається до \\(\\alpha = 3.0\\), оскільки дисперсія стає нескінченною для t-розподілу Стьюдента при \\(\\alpha = 3.0\\) (\\(\\nu = 2.0\\)). Виходячи з t-розподілу прибутковостей S&P 500, похибка оцінки волатильності зросла в 12 разів (!) порівняно з тим, що ми очікували б, припускаючи, що прибутковості розподілені за Гаусом.\n\n\n\n\n\n\nПримітка\n\n\n\nЧим важчі хвости розподілу прибутковостей, тим більше точок даних необхідно для досягнення необхідної достовірності оцінок певних параметрів. Пам’ятайте про це щоразу, коли ви намагаєтеся оцінити ризик інвестування у відносно новий фінансовий актив, який має дані лише за кілька років.\n\n\n\n\n12.1.9 Наслідки для моделювання та прогнозування\nТой факт, що більш високі моменти розподілу прибутковості можуть бути нескінченними і що оцінка нижчих моментів, таких як дисперсія, дестабілізується за наявності “товстих хвостів”, має суттєві наслідки для деяких найбільш використовуваних моделей у фінансах. Ми коротко розглянемо дві з них, щоб проілюструвати проблему.\n\n\n12.1.10 Марковіц і стабільність волатильності\nУ 1954 році Гаррі Марковіц отримав ступінь доктора економічних наук за роботу з теорії фінансових портфелів. Його робота стала наріжним каменем сучасної портфельної теорії і тоді це було настільки новим, що під час захисту докторської дисертації Мілтон Фрідман стверджував, що ця робота насправді не стосується галузі економіки (як пояснив Марковіц у своїй Нобелівській лекції у 1990 році). Марковіц стверджував, що при заданому рівні волатильності існує набір вагових коефіцієнтів розподілу активів у портфелі, які дають максимальну надлишкову прибутковість портфеля (порівняно з безризиковим активом). Якщо ви переглянете діапазон значень волатильності та обчислите максимальну віддачу портфеля, отримавши оптимальні ваги розподілу, Ви отримаєте набір ефективних портфелів, який також називають межею ефективності. Одна точка на цій межі в багатьох випадках представляє особливий інтерес, — це точка максимуму коефіцієнта Шарпа.\nКоефіцієнт Шарпа — це часто критикуваний, але також часто використовуваний показник для оцінки ефективності портфеля або фінансових активів загалом, таких як гедж-фонди або ETF. Він визначається як:\n\\[\nS = \\frac{\\mu - \\mu_{\\text{rf}}}{\\sigma}.\n\\]\nТут \\(\\mu\\) позначає річну прибутковість портфеля, \\(\\mu_{\\text{rf}}\\) позначає річну безризикову прибутковість, а \\(\\sigma\\) позначає річну волатильність портфеля. Припускаючи нульову безризикову ставку (\\(\\mu_{\\text{rf}} = 0\\)), коефіцієнт Шарпа можна інтерпретувати як співвідношення сигнал/шум, відомий у фізиці та інженерії: максимізуючи коефіцієнт Шарпа, ми знаходимо найкращий компромісне значення, що максимізує наші прибутковості (сигнал) та мінімізує волатильності (шум).\nЯкщо наш портфель складається з \\(n\\) фінансових активів з очікуваною прибутковістю \\(\\vec{\\mu} = (\\mu_{1},...,\\mu_{n})\\), з коваріаційною матрицею \\(\\Sigma\\), яка містить інформацію про волатильність і кореляції між активами, і з вектором ваги розподілу \\(\\vec{w} = (w_1,...,w_n)\\) (нормалізовано до одиниці: \\(\\sum_{i=1}^n w_i = 1\\)), тоді коефіцієнт Шарпа у контексті сучасної теорії портфеля можна обчислити наступним чином:\n\\[\nS(\\vec{w}) = \\frac{\\vec{w}\\cdot\\vec{\\mu}}{\\sqrt{\\vec{w}\\Sigma\\vec{w}^T}}.\n\\]\nЗверніть увагу, що з цього моменту ми завжди припускаємо, що \\(\\mu_{\\text{rf}} = 0\\).\nЩоб максимізувати прибутковість відносно волатильності та отримати портфель з максимальним коефіцієнтом Шарпа, ми змінюємо ваги розподілу, поки не знайдемо набір ваг \\(\\vec{w}^*\\) з максимальним коефіцієнтом Шарпа:\n\\[\n\\vec{w}^* = \\text{argmax}_{\\vec{w}} ~ S(\\vec{w})\n\\]\nОсновне припущення, що лежить в основі сучасної теорії портфеля, полягає в тому, що об’єднаний розподіл логарифмічних прибутковостей всіх активів у портфелі може бути повністю охарактеризований мультиваріативним розподілом Гауса. Після попередніх частин цієї лабораторної ми дуже добре розуміємо цю помилку, і сучасна теорія портфеля по праву зазнала жорсткої критики за відсутність врахування “важких хвостів”, наприклад, з боку Нассіма Талеба:\n\n\n\n\n\n\nНассім Талеб у Чорний лебідь: Про (не)ймовірне в реальному житті (ст. 277)\n\n\n\nПісля краху фондового ринку (у 1987 році) вони нагородили двох теоретиків, Гаррі Марковіца та Вільяма Шарпа, які побудували прекрасні платонівські моделі на гаусовій основі, сприяючи тому, що називається сучасною теорією портфеля. Простіше кажучи, якщо ви видалите їх гаусові припущення і розглядаєте ціни як масштабо-інваріантні, у вас залишиться гаряче повітря. Нобелівський комітет міг би протестувати моделі Шарпа і Марковіца — вони працюють як шарлатанські ліки, що продаються в Інтернеті, — але ніхто в Стокгольмі, схоже, про це не подумав.\n\n\nТут ми просто хочемо проілюструвати наслідки припущення гаусовости, змоделювавши прибутковість двох гіпотетичних акцій і спробувавши знайти ваги розподілу по максимуму Шарпа. Як ми побачимо, великі хвости в модельованих розподілах прибутковостей вносять істотну невизначеність в розрахункові ваги розподілу — прямий наслідок нестабільності дисперсії.\nСпочатку ми генеруємо випадкові числа з Гаусового розподілу, достатнього для моделювання щоденної прибутковості двох гіпотетичних акцій за 10 років. Ми присвоюємо їм різну позитивну очікувану прибутковість і волатильність. Для простоти ми припускаємо, що кореляція між двома активами є нульовою:\n\nnp.random.seed(1234)\nn_years = 10\n\nlog_ret_1 = 0.0025*norm.rvs(size=n_years*252) + 0.0005\nlog_ret_2 = 0.0050*norm.rvs(size=n_years*252) + 0.0010\n\nplt.figure()\nplt.plot(np.exp(np.cumsum(log_ret_1)), label='Акція 1')\nplt.plot(np.exp(np.cumsum(log_ret_2)), label='Акція 2')\n\nplt.ylabel('Ціна акції')\nplt.xlabel('Торговий день')\nplt.legend()\nplt.yscale('log')\nplt.show();\n\n\n\n\nЗвичайно, насправді ми не знаємо справжньої очікуваної прибутковості або значень волатильності кореляцій між активами. Таким чином, ми повинні оцінити очікувану прибутковість за середнім значенням вибірки та матрицю коваріації за матрицею коваріації вибірки. Зверніть увагу, що існують більш складні способи оцінки цих параметрів на основі історичних даних, таких як оцінки усадки для коваріаційної матриці або моделі розподілу Блека-Літтермана. Ці методи, безумовно, покращують вибіркові оцінки, але загальний недолік сучасної теорії портфеля через наявність “важких хвостів” залишається. Щоб вивчити та протестувати різні методи оптимізації портфеля, рекомендуємо розглянути бібліотеку PyPortfolioOpt, що представляє собою добре задокументований пакет Python, який реалізує як основні, так і складні методи оптимізації портфеля та дозволяє легко тестувати ці методи на реальних даних. Поки ми дотримуємося вибіркових оцінок (і використовуємо коефіцієнт 252 — середню кількість торгових днів на рік для отримання очікуваної прибутковості в річному обчисленні і коваріації за щоденними оцінками):\n\nexp_returns = np.array([np.mean(log_ret_1), np.mean(log_ret_2)])*252\ncov_matrix = np.cov([log_ret_1, log_ret_2])*252\n\nДалі ми визначаємо функцію, яка приймає ваги розподілу кандидатів, розрахункову очікувану прибутковість і коваріаційну матрицю і виводить коефіцієнт Шарпа портфеля:\n\ndef sharpe(weights, exp_returns, cov_matrix):\n    return weights.dot(exp_returns) / np.sqrt(weights.dot(cov_matrix).dot(weights.T))\n\nsharpe(np.array([0.5, 0.5]), exp_returns, cov_matrix)\n\n4.810679451759804\n\n\nЗамість того, щоб використовувати реальний оптимізатор для пошуку ваг розподілу, які максимізують коефіцієнт Шарпа, ми просто переглядаємо всі можливі комбінації ваг з інтервалом \\(1\\%\\). Звичайно, це розумний підхід лише для двох активів, і він швидко стає нерозв’язним, використовуючи більше двох активів.\n\ncandidate_weights = np.array([np.linspace(0, 1, 101), 1. - np.linspace(0, 1, 101)]).T\ncandidate_weights[:5]\n\narray([[0.  , 1.  ],\n       [0.01, 0.99],\n       [0.02, 0.98],\n       [0.03, 0.97],\n       [0.04, 0.96]])\n\n\nТепер ми можемо перебрати всі наші ваги-кандидати, обчислити коефіцієнт Шарпа портфеля для всіх з них, а потім вибрати оптимальний набір ваг.\n\ncandidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\nopt_weights = candidate_weights[np.argmax(candidate_sharpe)]\nopt_weights\n\narray([0.7, 0.3])\n\n\nЩоб отримати уявлення про те, наскільки точно ми можемо оцінити ваги розподілу на основі гіпотетичної щоденної прибутковості за 10 років, ми зараз повторюємо цей експеримент, повторно генеруємо нові випадкові прибутковості, повторно оцінюємо очікувані прибутковості та матрицю коваріації та знаходимо оптимальні ваги. Ми робимо це 10000 разів. Зверніть увагу, що ми реєструємо лише першу вагу розподілу \\(w_1\\) для кожного запуску без втрати інформації, оскільки друга вага безпосередньо випливає з нормалізації \\(w_2 = 1 - w_1\\).\n\nnp.random.seed(1234)\nopt_weight_gaussian = []\n\nfor i in tqdm(range(10000)):\n    log_ret_1 = 0.0025*norm.rvs(size=n_years*252) + 0.0005\n    log_ret_2 = 0.0050*norm.rvs(size=n_years*252) + 0.0010\n\n    exp_returns = [np.mean(log_ret_1), np.mean(log_ret_2)]\n    cov_matrix = np.cov([log_ret_1, log_ret_2])\n\n    candidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\n    opt_weights = candidate_weights[np.argmax(candidate_sharpe)]\n\n    opt_weight_gaussian.append(opt_weights[0])\n\n100%|██████████| 10000/10000 [00:07&lt;00:00, 1358.21it/s]\n\n\nНарешті, ми можемо побудувати гістограму всіх 10000 значень, які ми отримуємо для \\(w_1\\)\n\nplt.figure()\nplt.hist(opt_weight_gaussian, 10, density=True);\nplt.xlim([0, 1])\nplt.xlabel('Вага $w_1$ для портфеля з максимальним коеф. Шарпа')\nplt.ylabel('Розп. ймов.')\nplt.show();\n\n\n\n\nЯк ми бачимо, розподіл для першої акції коливається приблизно від \\(60%\\) до \\(70%\\) на основі історичних даних за 10 років. Ми можемо додатково оцінити це кількісно, обчисливши 5-й і 95-й процентилі всіх значень \\(w_1\\). Таким чином, з упевненістю в \\(90%\\) ми отримаємо розподіл між \\(61%\\) і \\(72%\\):\n\nnp.percentile(opt_weight_gaussian, 5), np.percentile(opt_weight_gaussian, 95)\n\n(0.61, 0.72)\n\n\nТепер давайте повторимо цей експеримент, але цього разу ми замінимо розподіл Гауса на розподіл Стьюдента зі степеневим показником \\(\\alpha=3.7\\) (що відповідає кількості ступенів свободи \\(\\nu=2.7\\)). Ми залишаємо очікувану прибутковість і волатильність (виміряні стандартним відхиленням розподілу) точно такими ж. Зверніть увагу, що стандартне відхилення стандартного t-розподілу визначається як \\(\\sqrt{\\frac{\\nu}{\\nu-2}}\\) в нашому випадку, тому ми повинні внести поправку на цей коефіцієнт маштабування при вилученні випадкових значень зі стандартного t-розподілу. Нижче ви можете побачити пряме порівняння розподілів Гауса, використаних у наведеному вище моделюванні, та t-розподілів із відповідним середнім значенням та стандартним відхиленням.\n\nscale_correction = np.sqrt(2.7/(2.7-2.))\n\n\nx = np.linspace(-0.025, 0.025, 10000)\n\ny1_norm = norm(loc=0.0005, scale=0.0025).pdf(x)\ny1_t = t(loc=0.0005, scale=0.0025/scale_correction, df=2.7).pdf(x)\n\ny2_norm = norm(loc=0.001, scale=0.005).pdf(x)\ny2_t = t(loc=0.001, scale=0.005/scale_correction, df=2.7).pdf(x)\n\n\nplt.figure()\n\nplt.subplot(121)\n\nplt.fill_between(x, 0., y1_norm, facecolor='C0', alpha=0.7)\nplt.fill_between(x, 0., y1_t, facecolor='C3', alpha=0.5)\nplt.ylim([0, 300])\nplt.ylabel('Розп. ймов.')\nplt.title('Акція 1')\n\nplt.subplot(122)\nplt.fill_between(x, 0., y2_norm, facecolor='C0', alpha=0.7, label='Гаус')\nplt.fill_between(x, 0., y2_t, facecolor='C3', alpha=0.5, label=\"Стьюдент\")\nplt.ylim([0, 300])\nplt.ylabel('Розп. ймов.')\nplt.title('Акція 2')\nplt.legend()\n\nplt.tight_layout()\nplt.show(); \n\n\n\n\nЯк ми можемо легко бачити, узгоджені t-розподіли Стьюдента насправді дають менші значення прибутковостей. Однак вони також дають набагато більш екстремальні значення, але ми не можемо легко побачити це, оскільки значення щільності ймовірності в хвостах дуже малі! Якщо ми перейдемо до логарифмічного масштабування вісі Oy, картина стане більш чіткою:\n\nplt.figure()\n\nplt.subplot(121)\n\nplt.plot(x, y1_norm, c='C0', lw=3)\nplt.plot(x, y1_t, c='C3', lw=3)\nplt.ylabel('Розп. ймов.')\nplt.title('Акція 1')\nplt.yscale('log')\n\nplt.subplot(122)\nplt.plot(x, y2_norm, c='C0', lw=3, label='Гаус')\nplt.plot(x, y2_t, c='C3', lw=3, label=\"Стьюдент\")\nplt.ylabel('Розп. ймов.')\nplt.title('Акція 2')\nplt.yscale('log')\nplt.legend()\n\nplt.tight_layout()\nplt.show();\n\n\n\n\nТепер ми чітко бачимо, що, наприклад, ймовірність щоденних логарифмічних прибутковостей \\(-0.02\\) для акції 1 в \\(10^{10}\\) разів вища за умови t-розподілу порівняно з відповідним розподілом Гауса! Завжди дивіться на розподіли з логарифмічною шкалою щільності ймовірності, щоб отримати чітке уявлення про хвости. Використовуючи наші узгоджені t-розподіли, ми діємо, як і раніше, і моделюємо гіпотетичні 10-річні історичні показники для наших двох некорельованих акцій. Нижче ви можете побачити приклад. Тільки при уважному розгляді ви можете помітити, що ці криві сукупної прибутковості насправді трохи більш нерівні в порівнянні з тими, які були побудовані вище з гаусовими прибутковостями. Але різниця невловима, якщо дивитися на таку “наближену” діаграму десятирічного періоду:\n\nnp.random.seed(1234)\nn_years = 10\n\nlog_ret_1 = (0.0025/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0005\nlog_ret_2 = (0.0050/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0010\n\nplt.figure()\nplt.plot(np.exp(np.cumsum(log_ret_1)), label='Акція 1')\nplt.plot(np.exp(np.cumsum(log_ret_2)), label='Акція 2')\n\nplt.ylabel('Ціна акції')\nplt.xlabel('Торговий день')\nplt.legend()\nplt.yscale('log')\nplt.show();\n\n\n\n\nНарешті, щоб візуалізувати вплив t-розподілу на оцінку ваги розподілу, ми повторюємо наш експеримент Монте-Карло з моделювання 10000 різних 10-річних історичних значень та оцінюємо ваги розподілу в кожному з цих 10000 випадків. Потім ми будуємо графік розподілу \\(w_1\\) і порівнюємо цей розподіл з розподілом, отриманим в результаті моделювання на основі Гауса.\n\nnp.random.seed(1234)\nopt_weight_t = []\n\nfor i in tqdm(range(10000)):\n    log_ret_1 = (0.0025/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0005\n    log_ret_2 = (0.0050/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0010\n\n    exp_returns = [np.mean(log_ret_1), np.mean(log_ret_2)]\n    cov_matrix = np.cov([log_ret_1, log_ret_2])\n\n    candidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\n    opt_weights = candidate_weights[np.argmax(candidate_sharpe)]\n\n    opt_weight_t.append(opt_weights[0])\n\n100%|██████████| 10000/10000 [00:09&lt;00:00, 1037.43it/s]\n\n\n\nplt.figure()\nplt.hist(opt_weight_gaussian, 10, alpha=0.8, density=True, label='Гаусові прибутковості')\nplt.hist(opt_weight_t, 20, alpha=0.8, density=True, label=\"Прибутковості Стьюдента\")\nplt.xlim([0, 1])\nplt.legend()\nplt.xlabel('Вага $w_1$ для портфеля з максимальним коеф. Шарпа')\nplt.ylabel('Розп. ймов.')\nplt.show();\n\n\n\n\n\nnp.percentile(opt_weight_t, 5), np.percentile(opt_weight_t, 95)\n\n(0.54, 0.77)\n\n\nЯк ми можемо бачити, оптимальна вага розподілу \\(w_1\\) тепер коливається між \\(54\\%\\) і \\(77\\%\\), що приблизно в 2 рази перевищує ширину надійного інтервалу для оптимізації портфеля, яка дотримувалася припущення про гаусовість! Навіть при узгодженому стандартному відхиленні гаусового і t-розподілу, нескінченний ексцес t-розподілу дестабілізує вибіркову дисперсію наших змодельованих прибутковостей і, таким чином, знижує точність наших оціночних ваг розподілу. Хоча цей приклад, безумовно, не розкриває всіх слабкостей сучасної теорії портфеля, він допомагає нам побачити, як досить абстрактні ефекти, такі як нестабільність дисперсії через надмірний ексцес, в кінцевому результаті псують широко використовувані фінансові моделі. Використання модельованої прибутковості, заснованої на гаусовому розподілі порівняно з розподілом з великим хвостом, може допомогти виявити обмеження фінансових моделей або знайти мінімальний обсяг даних, необхідний для досягнення певної достовірності, перш ніж нова модель буде фактично запущена у виробництво!\n\n\n\n\n\n\nПримітка\n\n\n\nВажкі хвости розподілу прибутковостей можуть сильно вплинути на результати усталених математичних моделей у фінансах, або збільшуючи невизначеність параметрів (як у сучасній теорії портфеля), або приводячи до збоїв моделей поза вибірки через нескінченно великі моменти. У фінансах краще екстраполювати частоту екстремальних подій і готуватися до них за допомогою конкретних інструментів геджування, ніж намагатися передбачити майбутню прибутковість за допомогою моделей часових рядів!\n\n\n\n\n12.1.11 Розподіл Леві\nАльфа-стабільний розподіл є узагальненням розподілу Гауса, яке цінується за те, що воно тягне за собою жирні хвости. З цієї причини він широко використовується при обробці сигналів, наприклад, у медицині чи фінансах.\nЗагальний клас стабільних розподілів був введений і отримав цю назву французьким математиком Полем Леві на початку 1920-х років.\nРаніше ця тема привертала лише помірну увагу провідних експертів, хоча були і ентузіасти, з яких можна згадати російського математика Олександра Яковича Хінчіна. Натхненням для Леві стало бажання узагальнити відому центральну граничну теорему, згідно з якою будь-який розподіл ймовірностей з кінцевою дисперсією збігається до гауссового розподілу.\nСтабільні розподіли мають три виняткові властивості, які можна коротко підсумувати, заявляючи, що вони:\n\nінваріантні при додаванні;\nмають власну область збіжності;\nдозволяють канонічну форму характеристичної функції.\n\n\n12.1.11.1 Інваріантність при додавані\nВипадкова величина \\(X\\) підпорядковується стабільному розподілу \\(P(x) = \\text{Prob}\\{X \\leq x\\}\\) якщо для будь-якого \\(n \\geq 2\\) існують додатнє значення \\(c_{n}\\) та дійсне значення \\(d_{n}\\) такі, що\n\\[\nX_1 + X_2 + ... + X_n \\stackrel{d}{=} c_{n}X + d_{n},\n\\]\nде \\(X_1, X_2, ..., X_n\\) характеризуються як незалежні, ідентично розподілені випадкові величини. Також \\(\\stackrel{d}{=}\\) позначає рівність розподілів, тобто, випадкові величини з обох сторін мають однаковий розподіл ймовірностей.\nЗагалом, сума незалежних, ідентично розподілених випадкових величин результує у випадкову величину з іншим розподілом. Однак, для випадкових величин, що характеризуються стабільним розподілом, сума ідентично розподілених випадкових величин до величини такого самого розподілу. У цьому випадку результуюча випадкова величина (розподіл) може відрізнятися від попередніх величин характерним масштабом (\\(c_{n}\\)) та зміщенням (\\(d_{n}\\)). Якщо \\(d_{n} = 0\\), розподіл називається строго стабільним.\nВідомо, що нормована константа \\(c_{n}\\) має вид\n\\[\nc_{n} = n^{1/\\alpha} \\, \\text{with} \\, 0 &lt; \\alpha \\leq 2.\n\\]\nПараметр \\(\\alpha\\) має назву характеристична експонента або індекс стабільності стабільного розподілу.\nПопередня теорема має альтернативну версію, що включає в суму лише дві випадкові величини. Випадкова величина \\(X\\) підпорядковується стабільному розподілу, якщо для будь-яких позитивних значень \\(A\\) та \\(B\\) існує позитивне число \\(C\\) та дійсне число \\(D\\) такі, що\n\\[\nA X_1 + B X_2 \\stackrel{d}{=} C X + D,\n\\]\nде \\(X_1\\) та \\(X_2\\) незалежні копії \\(X\\). Тоді існує значення \\(\\alpha \\in (0, 2]\\) при яких значення \\(C\\) задовільняє рівність \\(C^{\\alpha} = A^{\\alpha} + B^{\\alpha}\\).\nДля строго стабільних розподілів \\(D = 0\\). Це означає, що всі лінійні комбінації випадкових незалежних, ідентично розподілених випадкових величин, що підкоряються строго стабільному розподілу, результують у випадкову величину з одним і тим же типом розподілу.\nСтабільний розподіл вважається симетричним, якщо випадкова величина \\(-X\\) має такий самий тип розподілу. Симетричний стабільний розподіл обов’язково строго стабільний.\nОскільки аналітичний вираз функції щільності ймовірностей для стабільного розподілу невідома, за винятком кількох членів стабільного сімейства, більшість традиційних методів математичної статистики не можуть бути використані. Відбовідними вийнятками є\n\nГаусовий розподіл \\(S_2(0,\\mu, \\sigma) = \\mathcal{N}(\\mu, 2\\sigma^2)\\). Гаусовий розподіл є спеціальним випадком стабільного розподілу при \\(\\alpha = 2\\) так що \\(\\mathcal{N}(\\mu, \\sigma) = S(2,0,\\mu, \\frac{\\sigma}{\\sqrt{2}})\\), де \\(\\mu\\) позначає середнє значення нормального розподілу, а \\(\\sigma\\) — це стандартне відхилення. Функція щільності ймовірностей має вид\n\n\\[\n\\frac{1}{\\sigma\\sqrt{2\\pi}}\\text{exp}^{-(x-\\mu)^{2}/2\\sigma^{2}}.\n\\]\n\nРозподіл Коші. Розподіл Коші — це ще одне представлення стабільного розподілу при \\(\\alpha = 1\\) та \\(\\beta = 0\\) такими, що \\(Cauchy(\\delta, \\gamma) = S_1(1,0,\\gamma,\\delta)\\), де \\(\\gamma\\) — це параметр масштабування, а \\(\\delta\\) — це параметр зсуву розподілу Коші. Функція щільності ймовірностей представлена як:\n\n\\[\n\\frac{\\gamma}{\\pi((x-\\delta)^2 + \\gamma^2)}, \\, -\\infty &lt; x &lt; \\infty.\n\\]\n\nРозподіл Леві також є вийнятком із класу стабільних розподілів, де \\(\\alpha = 0.5\\) і \\(\\beta = 1\\). Іншими словами, \\(Levy(\\delta, \\gamma) = S_{1/2}(0.5, 1, \\gamma, \\delta)\\). Функція щільності ймовірностей має вид\n\n\\[\n\\sqrt{\\frac{\\gamma}{2\\pi}}\\frac{1}{(x-\\delta)^{3/2}}\\exp{\\left[ \\frac{-\\gamma}{2(x-\\delta)} \\right]}, \\, \\delta &lt; x &lt; \\infty.\n\\]\nЯкщо \\(X \\sim S_{1/2}(0.5, 1, \\gamma, \\delta)\\), тоді для \\(x &gt; 0\\)\n\\[\nP(X \\leq x) = 2 \\left( 1 - \\phi \\left( \\sqrt{\\frac{\\gamma}{x}} \\right) \\right),\n\\]\nде \\(\\phi\\) позначає кумулятивну функцію нормального розподілу.\n\n\n12.1.11.2 Область збіжності\nІнше (еквівалентне) визначення стверджує, що стабільні розподіли — це єдині розподіли, які можна отримати при границі нормалізованих сум незалежних, ідентично розподілених випадкових величин. Кажуть, що випадкова величина \\(X\\) має область збіжності, тобто якщо існує послідовність незалежних, ідентично розподілених випадкових величин \\(Y_1, Y_2, ...\\) і послідовності позитивних чисел \\({\\gamma_n}\\) і дійсних чисел \\({\\delta_n}\\) таких, що\n\\[  \n\\frac{Y_1 + Y_2 + ... + Y_n}{\\gamma_n} + \\delta_n \\stackrel{d}{\\Rightarrow} X.  \n\\]\nКоли \\(X\\) це гаусова випадкова величина, а \\(Y_i's\\) є незалежними, ідентично розподіленими випадковими величинами з визначенною дисперсією, тоді рівняння вище є твердженням звичайної центральної граничної теореми. Область збіжності \\(X\\) вважається нормальною коли \\(\\gamma_n = n^{1/\\alpha}\\).\n\n\n12.1.11.3 Канонічні представлення характеристичної функції\nЧотири параметри використовуються для опису випадкової величини, що слідує за стабільним розподілом: \\(X \\sim S(\\alpha, \\beta, \\mu, \\gamma)\\). Параметр \\(\\alpha \\in (0, 2]\\) — це той, який нас найбільше зацікавить. Цей параметр визначає товщину хвостів. Параметр \\(\\beta \\in [-1, 1]\\) є параметром асиметрії. Останні два параметри позначають розташування \\((\\mu \\in \\Re)\\) і масштаб \\((\\gamma &gt; 0)\\) розподілу. Альфа-стабільний розподіл немає жодного аналітичного виразу для щільності ймовірності \\(X\\), але ми можемо охарактеризувати його характеристичною функцією:\n\\[\n    \\begin{split}\n    \\phi(t) &= E\\left[\\exp(itX)\\right] \\\\\n            &=\n        \\begin{cases}\n            \\exp\\left(i \\mu t - \\gamma^{\\alpha}|t|^{\\alpha} \\left[1-i\\beta\\text{sign}(t)\\tan{\\frac{\\pi\\alpha}{2}}\\right]\\right) & \\text{if} \\, \\alpha \\neq 1 \\\\ \\exp\\left(i \\mu t - \\gamma|t| \\left[1+i\\beta\\text{sign}(t)\\frac{2}{\\pi}\\log{|t|}\\right]\\right) & \\text{if} \\, \\alpha = 1.\n        \\end{cases}\n    \\end{split}\n\\]\nМи могли б використовувати перетворення Фур’є, щоб отримати функцію щільності розподілу ймовірностей з характеристичної функції:\n\\[\nf(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^{+\\infty} \\phi(t) \\cdot \\exp(-itX) dt.\n\\]\nАле наведена вище параметризація не є повністю задовільною, оскільки функція щільності розподілу ймовірностей не є безперервною, зокрема, при \\(\\alpha = 1\\). Дійсно, коли \\(\\beta &gt; 0\\), щільність розподілу зміщується вправо, коли \\(\\alpha &lt; 1\\) і вліво, коли \\(\\alpha &gt; 1\\), зі зсувом в сторону \\(+\\infty\\) (відповідно \\(-\\infty\\)), коли \\(\\alpha\\) прагне до 1. Таким чином, для прикладного аналізу даних та інтерпретації коефіцієнтів слід уникати такої параметризації.\nІснує багато параметризацій для стабільних законів, і ці різні параметризації викликали велику плутанину. Різноманітність параметризацій обумовлена поєднанням історичної еволюції плюс численними проблемами, які були проаналізовані за допомогою спеціалізованих форм стабільного розподілу. Є вагомі причини використовувати різні параметризації в різних ситуаціях. Якщо в пріорітеті чисельні розрахунки, робота із даними, то краще використовувати одну параметризацію. Якщо бажані прості алгебраїчні властивості розподілу або аналітичні властивості строго стійких законів, то краще розглянути декілька параметризацій. Нолан запропонував використовувати параметризацію Золотарьова (M), яку також часто позначають як \\(S^{0}\\). Характеристична функція, що відповідає \\(X \\sim S^{0}(\\alpha, \\beta, \\mu_{0}, \\gamma)\\), дорівнює:\n\\[\n    \\begin{split}\n    \\phi(t) &= E\\left[\\exp(itX)\\right] \\\\\n            &=\n        \\begin{cases}\n            \\exp\\left(i \\mu_{0} t - \\gamma^{\\alpha}|t|^{\\alpha} \\left[1+i\\beta\\text{sign}(t)\\tan{\\frac{\\pi\\alpha}{2}}\\left( \\gamma^{1-\\alpha}|t|^{1-\\alpha}-1 \\right)\\right]\\right) & \\text{if} \\, \\alpha \\neq 1 \\\\ \\exp\\left(i \\mu_{0} t - \\gamma|t| \\left[1+i\\beta\\text{sign}(t)\\frac{2}{\\pi}\\left(\\log{|t|} + \\log{\\gamma}\\right) \\right]\\right) & \\text{if} \\, \\alpha = 1.\n        \\end{cases}\n    \\end{split}\n\\]\nЦя альтернативна параметризація недалека від зазначеної напочатку. Єдина відмінність стосується параметра \\(\\mu\\), який у даній параметризації коригує зсув для значень \\(\\alpha\\) близьких до 1:\n\\[\n\\mu_{0} = \\begin{cases} \\mu + \\beta\\gamma\\tan{\\frac{\\pi\\alpha}{2}} & \\text{if} \\, \\alpha \\neq 1 \\\\ \\mu + \\beta\\frac{2}{\\pi}\\gamma\\log{\\gamma} & \\text{if} \\, \\alpha = 1. \\end{cases}\n\\]\n\n\n12.1.11.4 Метод розрахунку параметрів \\(\\alpha\\)-стабільного розподілу\nЧислені методи, такі як метод Маккаллоха, заснований на квантилях, і метод оцінки максимальної правдоподібності були розроблені в результаті відсутності аналітичних рішень. Припустимо, що \\(\\text{X} = (X_1, ... , X_T)\\) вектор, що складається з \\(T\\) незалежних ідентично розподілених випадкових величин із розподілу Парето, і також \\(x \\sim S_{\\alpha}(\\alpha, \\beta, \\delta, \\gamma)\\). Визначивши \\(\\theta = (\\alpha, \\beta, \\delta, \\gamma)\\), Митник, Доганоглу та Ченайо розробили алогритм максимальної правдоподібності і показали, що \\(\\theta\\) можна розрахувати, максимізуючи функцію логарифмічної правдоподібності\n\\[\nl(\\theta, x) = \\sum_{i=1}^{T}\\log{f(x_{i}, \\theta)}.\n\\]\nДюмушель застосував метод максимальної правдоподібності до стабільного розподілу і визначив функцію правдоподібності наступним чином:\n\\[\nL(\\theta) = \\prod_{k=1}^{n}S_{\\alpha,\\beta} \\left( \\frac{X_{k} - \\delta}{\\gamma} \\right) \\Big/ \\gamma,\n\\]\nде \\(\\theta = (\\alpha, \\beta, \\delta, \\gamma)\\) опираючись на \\(x = (x_1, ... , x_n)\\) для розміру вибірки \\(n\\)."
  },
  {
    "objectID": "lab_12.html#хід-роботи",
    "href": "lab_12.html#хід-роботи",
    "title": "12  Лабораторна робота № 12",
    "section": "12.2 Хід роботи",
    "text": "12.2 Хід роботи\nЗчитуємо дані з Yahoo Finance:\n\n# встановлення назви індексу\nsymbol = \"^BSESN\" \n\n# встановлення діапазону з яким будемо працювати\nstart = \"1980-01-01\"\nend = \"2022-11-07\"\n\n# завантаження даних з Yahoo\ndata = yf.download(symbol, start, end)\ntime_ser = data['Adj Close'].copy()\n\n# підпис по вісі Ох \nxlabel = 'time, days'\n\n# підпис по вісі Оу\nylabel = symbol                       \n\n# збереження результату в текстовий документ \nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо.\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо графік досліджуваного ряду\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРис. 12.1: Динаміка щоденних змін фондового індексу BSESN\n\n\n\n\n\n12.2.1 Побудова розподілу Леві та розрахунок параметрів для всього ряду\nДля приведення ряду до стандартизованого вигляду або прибутковостей визначимо функцію transformations():\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nДля побудови в парі певного індикатора та досліджуваного ряду визначимо функцію plot_pair:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nДалі виконаємо приведення ряду до прибутковостей:\n\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд \n\nfor_levy = transformation(time_ser)\n\nПідганяємо розподіл Леві та Гауса для порівняння:\n\nparams = levy.fit_levy(for_levy)\nmean, std = norm.fit(for_levy)\n\nОтримуємо параметри розподілу Леві у відповідності до однієї із параметризацій, що пропонує пакет levy:\n\nalpha, beta, mu, sigma = params[0].get('1')\n\nБудуємо теоретичні та емпіричні розподіли:\n\nxmin = for_levy.min()\nxmax = for_levy.max()\n\nx = np.linspace(xmin, xmax, len(for_levy))\npdf = levy.levy(x, alpha, beta, mu, sigma)\npdf_norm = norm.pdf(x, mean, std)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 8))\n\nfig.suptitle(fr'Теоретичні та емпіричні $\\alpha$-стабільні розподіли для {symbol}', fontsize=20)\n\nax[0].hist(for_levy, bins=50, density=True, alpha=0.6, color='b')\nax[0].plot(x, pdf, 'k')\nax[0].plot(x, pdf_norm, 'r')\nax[0].set_yscale('log')\nax[0].set_xlabel(r'$x$')\nax[0].set_ylabel(r'$f_{\\alpha}(x), \\, \\mathrm{ePDF}$')\n\nax[1].hist(for_levy, bins=50, density=True, alpha=0.6, color='g')\nax[1].plot(x, pdf, 'k')\nax[1].plot(x, pdf_norm, 'r')\nax[1].set_xlabel(r'$x$')\nax[1].set_ylabel(r'$f_{\\alpha}(x), \\, \\mathrm{ePDF}$')\n\n\nplt.savefig(f\"Теоретичні та емпіричні альфа стабільні розподіли для {symbol}.jpg\")\n\nfig.tight_layout()\nplt.show();\n\n\n\n\nРис. 12.2: Теоретичні та емпіричні альфа-стабільні функції щільності ймовірностей\n\n\n\n\nВиводимо параметри розподілу Леві для заданого індексу\n\nprint(fr\"Параматери alpha = {alpha:.2f}, beta = {beta:.2f}, mu = {mu:.2f}, sigma = {sigma:.2f}\")\n\nПараматери alpha = 1.63, beta = -0.11, mu = -0.01, sigma = 0.53\n\n\nДля досліджуваного індексу бачимо, що параметр \\(\\alpha &lt; 2.0\\) та \\(\\beta &lt; 0\\), що вказує на відхилення розподілу даного індексу від нормального. Тобто, для даного ряду переважаючими є кризові явища, на що вказують важкі хвости розподілу. Із порівняльного аналізу Гауссового та Леві розподілів бачимо, що хвости нормального розподілу значно недооцінюють ймовірність появи кризових явищ чого, наприклад, не скажешь про альфа-стабільний розподіл. Взявши логарифм значень ймовірності по осі \\(Oy\\) ми можемо спостерігати, що, наприклад, недооцінка негативних прибутковостей Гаусовим розподілом, у порівнянні з альфа-стабільним, складає \\(\\approx 10^{15}\\) порядків. Для позитивних прибутковостей, що перевищують значення \\(+10\\sigma\\) недооцінка Гауссовим розподілом складає \\(\\approx 10^{27}\\) порядків. Теоретичне значення альфа-стабільного розподілу достатньо точно враховує важкі хвости емпіричного розподілу, що також виражається високим ексцесом розподілу. Також варто зазначити, що коефіцієнт асиметрії \\(\\beta\\) вказує на невеличке зміщення розподілу в ліву сторону, що також демонструє переважання кризових явищ.\n\n\n12.2.2 Дослідження поведінки альфа-стабільного розподілу Леві\n\nx = np.arange(-5, 5, .01)\nbeta_1 = 0\nmu = 0 \nsigm = 1 \n\nbeta_2 = 1.0\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 10))\n\nax[0][0].plot(x, levy.levy(x, 0.5, beta_1, mu, sigm), label = r\"$ \\alpha=0.5 $\")\nax[0][0].plot(x, levy.levy(x, 0.75, beta_1, mu, sigm), label = r\"$ \\alpha=0.75 $\")\nax[0][0].plot(x, levy.levy(x, 1.0, beta_1, mu, sigm), label = r\"$ \\alpha=1.0 $\")\nax[0][0].plot(x, levy.levy(x, 1.25, beta_1, mu, sigm), label = r\"$ \\alpha=1.25 $\")\nax[0][0].plot(x, levy.levy(x, 1.5, beta_1, mu, sigm), label = r\"$ \\alpha=1.5 $\")\n\nax[0][0].set_title(r\"Симетричні $\\alpha$-стабільні розподіли, $ \\beta = 0 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[0][0].legend(fontsize=20)\n\nax[0][0].set_xlabel(r\"$ x $\")\nax[0][0].set_ylabel(r\"$ f_{\\alpha}(x) $\")\n\n\nax[0][1].plot(x, levy.levy(x, 0.5, beta_2, mu, sigm), label = r\"$ \\alpha=0.5 $\")\nax[0][1].plot(x, levy.levy(x, 0.75, beta_2, mu, sigm), label = r\"$ \\alpha=0.75 $\")\nax[0][1].plot(x, levy.levy(x, 1.0, beta_2, mu, sigm), label = r\"$ \\alpha=1.0 $\")\nax[0][1].plot(x, levy.levy(x, 1.25, beta_2, mu, sigm), label = r\"$ \\alpha=1.25 $\")\nax[0][1].plot(x, levy.levy(x, 1.5, beta_2, mu, sigm), label = r\"$ \\alpha=1.5 $\")\n\nax[0][1].set_title(r\"Зміщенні $\\alpha$-стабільні розподіли, $ \\beta = 1 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[0][1].legend(fontsize=20)\n\nax[0][1].set_xlabel(r\"$ x $\")\nax[0][1].set_ylabel(r\"$ f_{\\alpha}(x) $\")\n\n\nax[1][0].plot(x, levy.levy(x, 0.5, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=0.5 $\")\nax[1][0].plot(x, levy.levy(x, 0.75, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=0.75 $\")\nax[1][0].plot(x, levy.levy(x, 1.0, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.0 $\")\nax[1][0].plot(x, levy.levy(x, 1.25, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.25 $\")\nax[1][0].plot(x, levy.levy(x, 1.5, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.5 $\")\n\nax[1][0].set_title(r\"Симетричні $\\alpha$-стабільні розподіли, $ \\beta = 0 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[1][0].legend(fontsize=20, loc=\"lower right\")\n\nax[1][0].set_xlabel(r\"$ x $\")\nax[1][0].set_ylabel(r\"$ F_{\\alpha}(x) $\")\n\nax[1][1].plot(x, levy.levy(x, 0.5, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=0.5 $\")\nax[1][1].plot(x, levy.levy(x, 0.75, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=0.75 $\")\nax[1][1].plot(x, levy.levy(x, 1.0, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.0 $\")\nax[1][1].plot(x, levy.levy(x, 1.25, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.25 $\")\nax[1][1].plot(x, levy.levy(x, 1.5, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.5 $\")\n\nax[1][1].set_title(r\"Зміщенні $\\alpha$-стабільні розподіли, $\\beta = 1$, $\\mu = 0$, $\\sigma = 1$\", y=1.03, fontsize=20)\n\nax[1][1].legend(fontsize=20, loc=\"lower right\")\n\nax[1][1].set_xlabel(r\"$ x $\")\nax[1][1].set_ylabel(r\"$ F_{\\alpha}(x) $\")\n\nfig.tight_layout()\nplt.show();\n\n\n\n\nРис. 12.3: Залежність функції щільності ймовірностей альфа-стабільного розподілу Леві та кумулятивної функції щільності від різних значень параметрів розподілу\n\n\n\n\n\n\n12.2.3 Віконна процедура\nУ даному блоці оберемо тип ряду для якого і виконуватимуться розрахунки. Перед нами представлено 6 варіантів представлення часового ряду. Виконуватимемо подальші обчислення для стандартизованих прибутковостей, оскільки згідно багатьом роботам було показано, що розподіл прибутковостей тих же самих фінансових активів (фондових індексів, валютних, криптовалют, тощо) виходить за межі нормального Гауссового розподілу. Покажемо це у даному ноутбуці та застосуємо альфа-стабільний розподіл Леві для кращого моделювання складних систем та передчасної ідентифікації кризових явищ.\n\nwindow = 250    # ширина ковзного вікна\ntstep = 5       # крок\n\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n        \nlength = len(time_ser)\n\nalpha = []\nbeta = []\nmu = []\nsigma = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # здійснюємо підгонку розподілу під значення ряду\n    params = levy.fit_levy(fragm)\n    \n    # отримуємо параметри розподілу\n    a, b, m, s = params[0].get('0')\n    \n    alpha.append(a)\n    beta.append(b)\n    mu.append(m)\n    sigma.append(s)\n\n100%|██████████| 1199/1199 [05:58&lt;00:00,  3.34it/s]\n\n\nЗберігаємо абсолютні значення у текстовому документі:\n\nnp.savetxt(f\"alpha_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", alpha)\nnp.savetxt(f\"beta_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", beta)\nnp.savetxt(f\"mu_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", mu)\nnp.savetxt(f\"sigma_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", sigma)\n\n\n\n12.2.4 Динаміка показника стабільності \\(\\alpha\\)\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nmeasure_label = r'$\\alpha$'\nfile_name = f\"alpha_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alpha, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРис. 12.4: Динаміка фондового індексу BSESN та показника стабільності \\(\\alpha\\)\n\n\n\n\nПараметр \\(\\alpha\\) (індекс стабільності хвостів розподілу) починає спадати у (перед)кризовий період, що робить його індикатором(-передвісником) кризових явищ. Під час криз у розподілі прибутковостей зростає ексцес, а самі хвости стають важчими, на що даний показник реагує передчасно.\n\n\n12.2.5 Динаміка показника асиметрії \\(\\beta\\)\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nmeasure_label = r'$\\beta$'\nfile_name = f\"beta_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          beta, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРис. 12.5: Динаміка фондового індексу BSESN та показника асиметрії \\(\\beta\\)\n\n\n\n\nДинаміка даної міри виглядає набагато хаотичніше у порівнянні з індексом стабільності. Для представлених результатів можна зробити наступний висновок: у передкризовий період даний показник має зростати, вказуючи на значну правосторонню асиметрію розподілу прибутковостей (переважання позитивних прибутковостей). Для кризового періоду цей показник має спадати, вказуючи на домінацію негативних прибутковостей (лівостороння асиметрія розподілу). Даний показник важко розглядати у якості надійного індикатора, оскільки його коливання представляються значними навіть при незначних падіннях представленого фондового індексу.\n\n\n12.2.6 Динаміка параметра зміщення \\(\\mu\\)\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nmeasure_label = r'$\\mu$'\nfile_name = f\"mu_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          mu, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРис. 12.6: Динаміка фондового індексу BSESN та показника зміщення \\(\\mu\\)\n\n\n\n\nПоказник розташування \\(\\mu\\) потроху спадає у кризовий період, демонструючи зміщення розподілу в сторону негативних прибутковостей. Тим не менш, цей розподіл представляєть настільки ж хаотичним як і показник асиметрії \\(\\beta\\).\n\n\n12.2.7 Динаміка параметра масштабу \\(\\sigma\\)\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nmeasure_label = r'$\\sigma$'\nfile_name = f\"sigma_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          sigma, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРис. 12.7: Динаміка фондового індексу BSESN та показника масштабу \\(\\sigma\\)\n\n\n\n\nІз представлених результатів видно, що даний показник спадає у (перед)кризовий період, вказуючи на зменшення масштабу (форми) альфа-стабільного розподілу Леві.\nОтже, з усіх 4-ох показників, показник стабільності \\(\\alpha\\) є найкращим для ідентифікації кризових явищ та побудови надійних стратегій ризик-менеджменту."
  },
  {
    "objectID": "lab_12.html#висновок",
    "href": "lab_12.html#висновок",
    "title": "12  Лабораторна робота № 12",
    "section": "12.3 Висновок",
    "text": "12.3 Висновок\nСтабільні розподіли — захоплюючий і плідний об’єкт досліджень в теорії ймовірностей; більше того, в даний час вони мають велику цінність при моделюванні складних процесів у фізиці, астрономії, економіці, теорії комунікацій, тощо.\nУ даній роботі було представлено теоретичні та чисельні обгрунтування в сторону альфа-стабільного розподілу Леві в якості практичної моделі для кращого розуміння та передбачення кризових явищ у складних системах.\nТут ми представили розрахунки як для усього ряду, так і для його підфрагментів, використовуючи алгоритм ковзного вікна. Виходячи з усього ряду прибутковостей, видно, що хвости їх розподілу далеко виходять за межі Гаусового. Найкраще емпіричний розподіл прибутковостей збігається саме з теоретичним альфа-стабільним розподілом Леві.\nВикористовуючи алгоритм сковзного вікна, ми побачили, що параметри альфа-стабільного розподілу змінюються з часом.\n\n\n\n\n\n\nЛітература для подальшого вивчення степеневих розподілів та важких хвостів\n\n\n\nNassim Taleb’s Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications (вільно доступна за посиланням)."
  },
  {
    "objectID": "lab_13.html#теоретичні-відомості",
    "href": "lab_13.html#теоретичні-відомості",
    "title": "13  Лабораторна робота № 13",
    "section": "13.1 Теоретичні відомості",
    "text": "13.1 Теоретичні відомості\nДля сучасних складних систем характерна нерегулярність зв’язків і висока чисельність елементів, яка може досягати десятків і сотень тисяч. Таким системам та їх мережним моделям, які володіють нетривіальними топологічними властивостями, найбільше відповідає термін “комплексні”. Комплексною мережею вважається система, яка\n\nскладається з великої кількості компонентів;\nдопускає «далекосяжні» зв’язки між компонентами;\nволодіє великомасштабною (у тому числі просторово-часовою) мінливістю.\n\nДана мережа є графом з досить великою кількістю вузлів різної природи, що характеризуються багатовимірним кортежем ознак і динамічно мінливими зв’язками; розподіл ознак вузлів і характеристик зв’язків може бути описаний ймовірнісною моделлю (багатомірним розподілом).\nОсновною причиною підвищення актуальності розробок у області теорії і практики комплексних мереж є результати сучасних досліджень реальних комп’ютерних, біологічних і соціальних мереж. Властивості багатьох реальних мереж істотно відрізняються від властивостей класичних випадкових графів з рівноймовірними зв’язками між вузлами, які донедавна розглядалися в якості їх базисного математичного модельного прототипу, і тому побудову їх моделей було запропоновано здійснювати з використанням зв’язних структур і степеневих розподілів.\nУ теорії комплексних мереж виділяють три основні напрямки:\n\nдослідження статистичних властивостей, які характеризують поведінку мереж;\nстворення моделей мереж;\nпрогнозування поведінки при зміні структурних властивостей мереж.\n\nКомплексні мережі використовуються для моделювання об’єктів і систем, дослідження яких іншими способами (за допомогою спостереження або активного експерименту) недоцільні або неможливі. Комп’ютерні мережі відносяться до мереж, які постійно ростуть і розвиваються. Серед факторів, що впливають на зростання мережі в першу чергу необхідно відзначити розмір або протяжність локальної мережі, яка визначається відстанню між найвіддаленішими станціями, при якій в нормальному режимі роботи вузлів чітко розпізнаються колізії, і кількість об’єднаних у мережу комп’ютерів. Для Інтернет-мереж цей розмір називається діаметром мережі і складає приблизно 1 км відстані, що дозволяє отримати високу швидкість зв’язку та максимально можливий рівень сервісу. При зростанні мережі збільшується кількість колізій, різко падає її корисна пропускна здатність і швидкодія передавання сигналу. Обмеження мережі за довжиною є передумовою вибору структури мережі, розбиття її на окремі частини (сегменти), появи додаткових серверів з новою мережею зв’язків, проблеми генеруються в контексті технологій так званої “останньої милі”. Спостерігається динаміка зростання мережі, своєрідна кластеризація, сервери виступають центрами утворених кластерів, відбувається просторове позиціонування компонент мережі у вигляді чітких ієрархічних структур.\nМережа розглядається як множина сегментів, кожен з яких закінчується точкою розгалуження або кінцевої вершиною мережі. Вершинами мережі є сервери, комутатори й кінцеві користувачі, загальну кількість яких позначимо \\(N\\). Локальні комп’ютерні мережі є об’єктними прототипами графових структур і тому для їх дослідження застосовують методи теорії графів.\nМоделювання мереж із використанням апарата теорії графів є важливим напрямком досліджень дискретної математики. В останні роки зросла зацікавленість дослідників до складних мереж з великою кількістю вузлів, зокрема до комп’ютерних мереж, структура яких нерегулярна, складна і динамічно розвивається в часі. Для таких мереж доводиться генерувати стохастичні графи з величезною кількістю вершин.\nУ загальному вигляді модель комп’ютерної мережі являє собою випадковий граф, закон взаєморозміщення ребер і вершин для якого задається розподілом ймовірностей.\nУ даний час найпоширенішими є два основних підходи до моделювання складних мереж:\n\nвипадкові Пуассонівські графи та узагальнені випадкові графи;\nмодель “тісного світу” Ватса і Строґатса та її узагальнення, еволюційна модель зростання мережі Барабаші й Альберт.\n\nПерший передбачає генерацію випадкового графа із заздалегідь відомою кількістю вершин і заданими ймовірнісними властивостями. Його ще називають графом Ердоша-Рені зі сталою кількістю вершин \\(N\\). Розподіл ступенів вузлів \\(k\\) для цього графа визначається формулою Пуассона \\(P(k) = \\exp^{-\\left\\langle k \\right\\rangle} \\left\\langle k \\right\\rangle^k / k!\\). Побудова графа здійснюється генеруванням, коли до \\(N\\) відокремлених вершин послідовно додаються ребра, що з’єднують випадковим чином довільні пари вершин. Початково граф складатиметься із сукупності малих вершин, які в процесі генерування з часом розростаються до гігантського кластера зв’язаних між собою вершин, число яких є скінченною частиною загальної кількості \\(N\\). При генерації постійно зростає ймовірність зв’язування вершин, яка досягає з часом деякого критичного значення. В результаті процесу, який має характер фазового переходу, граф спонтанно розростається до гігантського кластера вершин, пов’язаних між собою, що нагадує конденсацію краплі води в перенасиченій парі.\nМодель Ваттса-Строґаца є комп’ютерною моделлю тісного світу. Її побудова зводиться до наступного: розглядається одновимірний, замкнений у кільце, періодичний ланцюг, який складається із \\(N\\) вершин. Спочатку кожну вершину з’єднують з іншими сусідніми, які знаходяться від неї на відстані, не більшій за \\(k\\), а потім кожне ребро з певною ймовірністю \\(m\\) перез’єднується з довільною вершиною, що призводить до трансформації регулярного ланцюга у граф тісного світу (Рис. 13.1). Оскільки в цій моделі кількість ребер є сталою, а ймовірності реалізації графів — різні, то вона зводиться до канонічного ансамблю графів і описує реально існуючі мережі, топологія яких не є ані цілком регулярною, ані цілком випадковою.\n\n\n\nРис. 13.1: Трансформація регулярного ланцюга у граф тісного світу і далі у випадковий граф\n\n\nБільшість реальних графів підпорядковуються степеневому закону розподілу \\(P(k)\\). Ці графи побудови мереж описуються моделлю переважного приєднання Барабаші-Альберт. Через далекоглядні взаємодії у системи не існує масштабу зміни характерних величин. Ріст і переважне приєднання є основними механізмами побудови безмасштабних (масштабно-інваріантних) мереж.\nНехай вузол \\(i\\) має \\(k_i\\) зв’язків і він може бути приєднаним (зв’язаним) до інших вузлів $k_i. Ймовірність приєднання нового вузла до вузла \\(i\\) залежить від ступеня \\(k_i\\) вузла \\(i\\). Величину \\(W(k_i) = k_i/\\sum_{j}k_j\\) називають переважним приєднанням (preferential attachment). Не всі вузли мають однакову кількість зв’язків, тому вони характеризуються функцією розподілу \\(P(k)\\), що визначає ймовірність того, що випадково вибраний вузол має \\(k\\) зв’язків. Для комплексних мереж функція \\(P(k)\\) відрізняється від розподілу Пуассона для випадкових графів. Для переважної більшості комплексних мереж спостерігається степенева залежність \\(P(k)\\propto k^{-\\gamma}\\).\nУ попередній роботі ми вже ввели деякі з мір складності. Зараз же зробимо це систематично і покажемо, яким чином у межах єдиного алгоритму розрахувати і проаналізувати основні спектральні і топологічні властивості найпростіших графів. Для аналізу мережі досліджують характеристики окремих вузлів (локальні), характеристики мережі в цілому (глобальні), та характеристики мережних підструктур. Числові показники деяких глобальних характеристик мережі можуть бути представлені у вигляді аналітичних узагальнень її локальних характеристик (наприклад — найменше, найбільше, середнє значення локального показника, взяте по всім вузлам). Окрім того, що глобальна характеристика може бути представлена у формі одного числа, це також може бути представлення у вигляді розподілу значень локальної характеристики вузлів по всій мережі.\n\n13.1.1 NetworkX\nДля аналізу складних мереж і їх спектральних і топологічних характеристик можна скористатися такою бібліотекою як NetworkX.\nNetworkX дозволяє моделювати, аналізувати та візуалізувати мережі різної природи та складності. Пакет надає класи для представлення декількох типів мереж та реалізацію багатьох алгоритмів, що використовуються в мережевій науці. NetworkX відносно простий у встановленні та використанні і має багато вбудованих функцій, тому він ідеально підходить для вивчення мережевої науки і виконання аналізу малих і середніх мереж.\nNetworkX є безкоштовним програмним забезпеченням з відкритим вихідним кодом. Це означає, що вихідний код доступний для читання, модифікації та розповсюдження (за певних умов). Сам код доступний за адресою https://github.com/networkx/networkx. NetworkX був написаний спільнотою з десятків дописувачів. Якщо у вас є ідея щодо нової функції або способу покращення програмного забезпечення, ви можете самостійно її запрограмувати й поділитися нею зі спільнотою.\n\n13.1.1.1 Встановлюємо NetworkX\nДля встановлення даної бібліотеки можна скористатися наступною командою:\n\n!pip install networkx\n\nДалі можемо імпортувати відповідні бібліотеки:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt # для візуалізації графіків\nimport numpy as np              # для роботи з матрицями\n\nПам’ятайте, що оператори import знаходяться у верхній частині вашого коду, вказуючи Python завантажити зовнішній модуль. У цьому випадку ми хочемо завантажити NetworkX, але дамо йому короткий псевдонім nx, оскільки нам доведеться вводити його неодноразово, звідси й інструкція as.\nДавайте перевіримо встановлену версію NetworkX. Ми хочемо переконатися, що не використовуємо застарілий пакет.\n\nnx.__version__\n\n'3.1'\n\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 14,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\n\n\n\n13.1.2 Типи мереж\nМережі, представлені в цій лабораторній, поки що мають лише найнеобхідніше. Ці мережі називаються простими мережами, тому що вони прості. У NetworkX прості мережі представлені класом Graph. Назва Graph походить від терміну, що використовується в математиці для опису мереж. Ви можете подумати, що він означає малюнок або креслення, але в даному випадку він просто означає мережу. Математики часто використовують повсякденні слова в дуже специфічними способами, які дуже відрізняються від їх повсякденних значень, наприклад наприклад, “граф”, “пучок” або “кільце”.\n\n13.1.2.1 Простий граф (ненаправлений та незважений)\n\n# \"звичайний\" граф є неорієнтованим\nG = nx.Graph()\n\n# дайте кожній вершині \"ім'я\", яке у цьому випадку є літерою.\nG.add_node('a')\n\n# метод add_nodes_from дозволяє додавати вузли з послідовності, у цьому випадку зі списку\nnodes_to_add = ['b', 'c', 'd']\nG.add_nodes_from(nodes_to_add)\n\n# додаємо ребро з 'a' в 'b'\n# оскільки граф неорієнтовний, то порядок не має значення\nG.add_edge('a', 'b')\n\n# так само як і add_nodes_from, ми можемо додавати ребра з послідовності\n# ребра повинні бути задані як 2-кортежі\nedges_to_add = [('a', 'c'), ('b', 'c'), ('c', 'd')]\nG.add_edges_from(edges_to_add)\n\n# будуємо граф\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\nІснує багато необов’язкових аргументів для функції draw, щоб налаштувати зовнішній вигляд.\n\nnx.draw_networkx(G,\n        with_labels=True,\n        node_color='blue',\n        node_size=1600,\n        font_color='white',\n        font_size=16,\n        )\n\n\n\n\n\n\n13.1.2.2 Зважена мережа\nПовертаючись до випадку неорієнтованих мереж, іноді не всі ребра є рівними. Наприклад, у мережі, що представляє міську систему водопостачання, ребра можуть представляти серію труб, якими вода транспортується з одного місця в інше. Деякі з них можуть мати більшу пропускну здатність, ніж інші. Коли ребра можуть мати різну міцність, мережа називається зваженою, а міцність кількісно вимірюється числом, яке називається вагою. Зваженими можуть бути як орієнтовані, так і неорієнтовані мережі. При візуалізації мережі вагу ребер часто вказують, змінюючи товщину або непрозорість ребра. Ваги ребер можна використовувати для для представлення різних типів атрибутів.\n\n# зважена мережа\nG_weighted = nx.Graph()\n\nG_weighted.add_edge(\"A\",\"B\",weight=6)\nG_weighted.add_edge(\"A\",\"D\",weight=3)\nG_weighted.add_edge(\"A\",\"C\",weight=0.5)\nG_weighted.add_edge(\"B\",\"D\",weight=1)\n\nnx.draw_networkx(G_weighted, with_labels=True)\n\n\n\n\n\n\n13.1.2.3 Направлений граф\nІноді буває корисно додавати трохи більше деталей до мережі. Ребра, які ми бачили попередньо, не враховують звідки одна вершина прямує або куди; Вони просто з’єднують два вузли, тому їх називають симетричними або неорієнтованими.\nУявіть собі мережу, яка являє собою систему доріг (ребер) і перехресть (вузлів). A мережа з ненаправленими ребрами була б гарним представленням, доки ви не натрапили на вулицю з одностороннім рухом. Ненаправлене ребро припускає, що ви можете рухатися в будь-якому напрямку однаково. Хоча в реальності напрям руху по дорожній смузі матиме значення навіть для вашого життя.\nКоли напрямок має значення, мережа називається орієнтованою (направленою). В направленій мережі кожне ребро має вузол-джерело і вузол-приймач. Як правило, ребро представляє якийсь потік, наприклад, трафік, від джерела до цілі. Але що, якщо не всі з’єднання є односторонніми? Легко! Двосторонні з’єднання створюються шляхом поєднання двох спрямованих ребер, що йдуть в протилежних напрямках. У спрямованих мережах ребра зображуються стрілками, що вказують на ціль.\n\n# направлений граф\nG_di = nx.DiGraph()\n\nG_di.add_edge(\"A\",\"B\",weight=1)\nG_di.add_edge(\"A\",\"D\",weight=3)\nG_di.add_edge(\"A\",\"C\",weight=1)\nG_di.add_edge(\"B\",\"D\",weight=2)\n\n# створити словник позицій для вузлів\npos = nx.spring_layout(G_di)\n\n# будуємо граф\nnx.draw_networkx_nodes(G_di, pos)\nnx.draw_networkx_edges(G_di, pos)\nnx.draw_networkx_labels(G_di, pos)\n\n# створити словник міток ребер\nedge_labels = {(u, v): d['weight'] for u, v, d in G_di.edges(data=True)}\n\n# створення міток для ребер\nnx.draw_networkx_edge_labels(G_di, pos, edge_labels=edge_labels)\n\n# побудова рисунка\nplt.show()\n\n\n\n\nОб’єкт граф має деякі властивості та методи, які надають дані про весь граф.\n\n# Cписок усіх вузлів\nG_di.nodes()\n\nNodeView(('A', 'B', 'D', 'C'))\n\n\n\n# Список усіх ребер  \nG_di.edges()\n\nOutEdgeView([('A', 'B'), ('A', 'D'), ('A', 'C'), ('B', 'D')])\n\n\n\nG_di.edges(data=True) # триплет зі словником (третім йде вага ребра)\n\nOutEdgeDataView([('A', 'B', {'weight': 1}), ('A', 'D', {'weight': 3}), ('A', 'C', {'weight': 1}), ('B', 'D', {'weight': 2})])\n\n\n\nG_di.edges[\"A\",\"B\"] # виводимо вагу, вказуючи цікаві для нас вузли напряму. У результаті отримуємо словник\n\n{'weight': 1}\n\n\n\nG_di.edges[\"A\",\"C\"][\"weight\"] # виводимо вагу, вказуючи цікаві для нас вузли напряму. У результаті отримуємо скаляр\n\n1\n\n\n\npos # виводимо словник координат розташувань вузлів на графіку\n\n{'A': array([-0.03682774,  0.03785267]),\n 'B': array([-0.34916457, -0.14735806]),\n 'D': array([-0.61400769,  0.06413737]),\n 'C': array([1.        , 0.04536802])}\n\n\nОб’єкти NodeView та EdgeView мають ітератори, тому ми можемо використовувати їх у циклах for:\n\nfor node in G_di.nodes:\n    print(node)\n\nA\nB\nD\nC\n\n\n\nfor edge in G_di.edges:\n    print(edge)\n\n('A', 'B')\n('A', 'D')\n('A', 'C')\n('B', 'D')\n\n\nЗверніть увагу, що ребра подано у вигляді 2-кортежів, так само, як ми їх ввели.\nМи можемо отримати кількість вершин та ребер у графі за допомогою методів number_of_.\n\nG_di.number_of_nodes()\n\n4\n\n\n\nG_di.number_of_edges()\n\n4\n\n\nДеякі методи роботи з графами приймають ребро або вершину як аргумент. Вони надають властивості графа для даного ребра або вершини. Наприклад, метод .neighbors() повертає вершини, пов’язані з даною вершиною:\n\n# список сусідів вершини 'A'\nG_di.neighbors('A')\n\n&lt;dict_keyiterator at 0x24424f180e0&gt;\n\n\nЗ міркувань продуктивності багато методів для роботи з графами повертають ітератори замість списків. Їх зручно використовувати у циклах:\n\nfor neighbor in G_di.neighbors('A'):\n    print(neighbor)\n\nB\nD\nC\n\n\nі ви завжди можете використати конструктор list для створення списку з ітератора:\n\nlist(G_di.neighbors('A'))\n\n['B', 'D', 'C']\n\n\nЗверніть увагу на асиметрію в методах роботи з ребрами, таких як has_edge():\n\nG_di.has_edge('A', 'B')\n\nTrue\n\n\n\nG_di.has_edge('B', 'A')\n\nFalse\n\n\nЗамість симетричного зв’язку “сусіди”, вузли в орієнтованих графах мають попередників (successors або “in-neighbours”) і наступників (predecessors або “out-neighbours”):\n\nprint('Попередники вершини B:', list(G_di.successors('B')))\n\nprint('Наступники вершини B:', list(G_di.predecessors('B')))\n\nПопередники вершини B: ['D']\nНаступники вершини B: ['A']\n\n\nСпрямовані графи мають вхідні степені вершини (in-degree) та вихідні степені вершини (out-degree), які показують кількість ребер, що ведуть до та від даної вершини, відповідно:\n\nG_di.in_degree('A') # у вершину А не входить жодна вершина (шлях)\n\n0\n\n\n\nG_di.out_degree('A') # з вершини А виходять 3 вершини (шляхи)\n\n3\n\n\nУ NetworkX існує декілька алгоритмів компонування, які можуть бути використані для розміщення вузлів графа у візуалізації, в тому числі\n\nnx.spring_layout(): Цей алгоритм використовує примусовий підхід до розміщення вершин. Вузли, які з’єднані ребрами, притягуються один до одного, тоді як вузли, які не з’єднані, відштовхуються. Алгоритм намагається мінімізувати енергію системи, регулюючи положення вузлів.\nnx.circular_layout(): Цей алгоритм розміщує вузли рівномірно по колу.\nnx.spectral_layout(): Цей алгоритм використовує власні вектори матриці суміжності графа для розміщення вершин. Власні вектори використовуються для проектування вершин у простір нижчої розмірності, а положення вершин потім визначаються шляхом оптимізації функції вартості.\nnx.random_layout(): Цей алгоритм розміщує вершини випадковим чином у заданій обмежувальній області.\nnx.shell_layout(): Цей алгоритм розміщує вершини у вигляді концентричних кіл або оболонок, причому вершини в одній і тій же оболонці мають однакову відстань до центру.\nnx.kamada_kawai_layout(): Цей алгоритм використовує ітераційний оптимізаційний підхід для розміщення вузлів. Алгоритм намагається мінімізувати навантаження на систему, змінюючи положення вузлів.\nnx.fruchterman_reingold_layout(): Цей алгоритм є варіацією алгоритму nx.spring_layout(), і також використовує силовий підхід до розміщення вузлів.\n\nКожен алгоритм компонування має свої сильні та слабкі сторони, і вибір найкращого з них залежить від характеристик графа та цілей візуалізації. NetworkX дозволяє легко застосовувати ці алгоритми до ваших графів і створювати візуалізації, які допоможуть вам зрозуміти і передати структуру мережі.\n\n\n13.1.2.4 Знакова мережа\n\nSigned_G = nx.Graph()\n\n# додаємо ребра\nSigned_G.add_edge(\"A\",\"B\",sign=\"*\")\nSigned_G.add_edge(\"A\",\"C\",sign=\"-\")\nSigned_G.add_edge(\"A\",\"d\",sign=\"+\")\n\n# вибір алгоритму компонування\npos = nx.random_layout(Signed_G)\n\n# створити словник кольорів ребер на основі знаку кожного ребра\nedge_colors = {'+': 'green', '-': 'red', \"*\":\"black\"}\ncolors = [edge_colors[Signed_G[u][v]['sign']] for u, v in Signed_G.edges()]\n\n# створити словник стилів ребер на основі знаку кожного ребра\nedge_styles = {'+': 'solid', '-': 'dashed', \"*\":\"dashed\"}\nstyles = [edge_styles[Signed_G[u][v]['sign']] for u, v in Signed_G.edges()]\n\n# будуємо граф з кольоровими та стилізованими ребрами\nnx.draw_networkx_nodes(Signed_G, pos, node_color='blue')\nnx.draw_networkx_edges(Signed_G, pos, edge_color=colors, style=styles)\nnx.draw_networkx_labels(Signed_G, pos)\n\n# будуємо рисунок\nplt.show()\n\n\n\n\n\n\n13.1.2.5 Мультиграф\nМультиграф — це тип графа в NetworkX, який допускає декілька ребер між парою вузлів. Іншими словами, MultiGraph може мати паралельні ребра, в той час як стандартний Graph може мати лише одне ребро між будь-якою парою вузлів.\nПростіше кажучи, Мультиграф — це мережа, в якій декілька ребер можуть з’єднувати одні й ті ж вузли.\n\nMulti_G = nx.MultiGraph()\n\nMulti_G.add_edge(\"A\",\"B\",relation=\"family\",weight=1)\nMulti_G.add_edge(\"A\",\"C\",relation=\"family\",weight=2)\nMulti_G.add_edge(\"A\",\"B\",relation=\"Work\",weight=3)\nMulti_G.add_edge(\"D\",\"B\",relation=\"Work\",weight=1)\nMulti_G.add_edge(\"B\",\"E\",relation=\"Friend\",weight=2)\n\nnx.draw_networkx(Multi_G, with_labels=True)\n\n\n\n\n\n# краща візуалізація \n\n# компонуємо\npos = nx.spring_layout(Multi_G)\n\n# будуємо вузли\nnx.draw_networkx_nodes(Multi_G, pos, node_color='lightblue', node_size=500)\n\n# будуємо ребра\nedge_labels = {}\nfor u, v, d in Multi_G.edges(data=True):\n    if (u, v) in edge_labels:\n        edge_labels[(u, v)] += \"\\n\" + d[\"relation\"] + \": \" + str(d[\"weight\"])\n    else:\n        edge_labels[(u, v)] = d[\"relation\"] + \": \" + str(d[\"weight\"])\n\nnx.draw_networkx_edge_labels(Multi_G, pos, edge_labels=edge_labels)\nnx.draw_networkx_edges(Multi_G, pos, width=1, alpha=0.7)\n\n# будуємо мітки\nnx.draw_networkx_labels(Multi_G, pos, font_size=10, font_family=\"sans-serif\")\n\n{'A': Text(-0.3953994670990945, -0.17881537379881896, 'A'),\n 'B': Text(0.12667156095841894, 0.1072655052856365, 'B'),\n 'C': Text(-1.0, -0.5003629204344762, 'C'),\n 'D': Text(0.9892555664513475, -0.20408521348504768, 'D'),\n 'E': Text(0.27947233968932794, 0.7759980024327062, 'E')}\n\n\n\n\n\n\n# ребра\nlist(Multi_G.edges())\n\n[('A', 'B'), ('A', 'B'), ('A', 'C'), ('B', 'D'), ('B', 'E')]\n\n\n\n# G.edges(data=True)\nlist(Multi_G.edges(data=True))\n\n[('A', 'B', {'relation': 'family', 'weight': 1}),\n ('A', 'B', {'relation': 'Work', 'weight': 3}),\n ('A', 'C', {'relation': 'family', 'weight': 2}),\n ('B', 'D', {'relation': 'Work', 'weight': 1}),\n ('B', 'E', {'relation': 'Friend', 'weight': 2})]\n\n\n\n# конкретно перелічуючи ребра\nlist(Multi_G.edges(data=\"relation\"))\n\n[('A', 'B', 'family'),\n ('A', 'B', 'Work'),\n ('A', 'C', 'family'),\n ('B', 'D', 'Work'),\n ('B', 'E', 'Friend')]\n\n\n\n# певне ребро \nMulti_G.edges[\"A\",\"B\"] # помилка\n\nValueError: not enough values to unpack (expected 3, got 2)\n\n\n\n#натомість\n# атрибути в мультиграфі\ndict(Multi_G[\"A\"][\"B\"])\n\n{0: {'relation': 'family', 'weight': 1}, 1: {'relation': 'Work', 'weight': 3}}\n\n\n\nlist(Multi_G.edges(\"B\"))\n\n[('B', 'A'), ('B', 'A'), ('B', 'D'), ('B', 'E')]\n\n\n\nMulti_G[\"A\"][\"B\"][0][\"relation\"]\n\n'family'\n\n\n\n\n13.1.2.6 Двочастковий (bipartite) граф\nДвочастковий граф — це тип графа, в якому вершини можна розбити на дві непересічні множини так, що всі ребра з’єднують вершину з однієї множини з вершиною в іншій множині. Іншими словами, не існує ребер, які з’єднують вершини всередині однієї множини.\nДвочасткові графи корисні для моделювання відносин між двома різними типами об’єктів, наприклад, покупцями і продавцями на ринку, або акторами і фільмами в кіноіндустрії.\nУ NetworkX ви можете створювати і маніпулювати двосторонніми графами за допомогою модуля bipartite, який надає різні функції і алгоритми для двосторонніх графів. Крім того, існує декілька методів візуалізації, які можна використовувати для відображення двосторонніх графів, наприклад, двосторонній макет, який розташовує вузли у два окремі рядки.\nПриклад акціонерів та акцій:\n\nfrom networkx.algorithms import bipartite\n\n# список акціонерів\nstockholders = ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Harry', 'Ivy', 'John']\n\n# перелік акцій\nstocks = ['AAPL', 'GOOG', 'TSLA', 'AMZN', 'FB', 'MSFT', 'NVDA', 'PYPL', 'NFLX', 'TWTR']\n\n# створити двочастковий граф\nB = nx.Graph()\n\n# додавання вузлів зі списку\nB.add_nodes_from(stockholders, bipartite=0)\nB.add_nodes_from(stocks, bipartite=1)\n\n# додавання ребер випадковим чином\nimport random\nwhile not nx.is_connected(B):\n    B.add_edge(random.choice(stockholders), random.choice(stocks))\n\n# будуємо двочастковий граф\npos = nx.bipartite_layout(B, stockholders)\nnx.draw_networkx(B, pos, with_labels=True)\n\n\n\n\n\n# 2 набори двочасткових графів\nbipartite.sets(B)\n\n({'Alice',\n  'Bob',\n  'Charlie',\n  'David',\n  'Eve',\n  'Frank',\n  'Grace',\n  'Harry',\n  'Ivy',\n  'John'},\n {'AAPL',\n  'AMZN',\n  'FB',\n  'GOOG',\n  'MSFT',\n  'NFLX',\n  'NVDA',\n  'PYPL',\n  'TSLA',\n  'TWTR'})\n\n\nХоча двочасткові графи корисні для представлення повної структури зв’язків “багато-до-багатьох”, іноді простіше працювати зі стандартними односторонніми мережами. Це може бути у випадку, якщо аналіз фокусується на певному типі вузлів, або якщо необхідна методика доступна лише для односторонніх (одномодальних) мереж, або якщо методика доступна лише для одномодових мереж, або якщо мережа зв’язків має занадто багато вузлів для чіткої візуалізації. На щастя, можна створити одномодові мережі з з мережі зв’язків за допомогою процесу, який називається “проекція”. І, як ви могли б очікувати, NetworkX спрощує цей процес.\nОдномодові мережі, побудовані з мереж зв’язків, називаються мережами спільної приналежності, тому що вузли з’єднуються ребрами, якщо вони мають спільні зв’язки. Існує кілька типів проекцій, які використовуються для створення спільної приналежності, але всі вони обертаються навколо однієї і тієї ж ідеї: з’єднання вузлів зі спільним сусідом у вихідній мережі приналежності. Найпростіша можлива проекція — це незважена проекція, яка створює незважене ребро між вузлами з одним або декількома спільними сусідами. У коді наступний код використовує функцію projected_graph() для проектування мережі акціонерів, що мають спільні акції компаній:\n\n# Лівобічний граф (акціонери) \n\n# акціонери, які мають спільні акції, пов'язані між собою\n\n# Лівобічний граф\n\nP = bipartite.projected_graph(B, bipartite.sets(B)[0])\nnx.draw_networkx(P, with_labels=True,node_size=10)\n\n\n\n\nУ такий самий спосіб ми можемо побудувати мережу акцій:\n\n# Правобічний граф \nP = bipartite.projected_graph(B,bipartite.sets(B)[1])\nnx.draw_networkx(P, with_labels=True,node_size=10)\n\n\n\n\n\n# Зважений лівобічний граф\n# як багато спільного \nP = bipartite.weighted_projected_graph(B, bipartite.sets(B)[0])\nnx.draw_networkx(P, with_labels=True,node_size=10)\n\n\n\n\n\n# краща візуалізація\nP = bipartite.weighted_projected_graph(B,bipartite.sets(B)[0])\npos = nx.circular_layout(P)\n\n# будуємо граф \nnx.draw_networkx_nodes(P, pos)\nnx.draw_networkx_edges(P, pos)\nnx.draw_networkx_labels(P, pos)\n\n# створюємо словник міток ребер\nedge_labels = {(u, v): d['weight'] for u, v, d in P.edges(data=True)}\n\n# будуємо мітки для ребер \nnx.draw_networkx_edge_labels(P, pos, edge_labels=edge_labels);\n\n\n\n\n\n\n\n13.1.3 Імпортуємо інформацію про мережу\n\n13.1.3.1 Імпортуємо дані з file.txt та GEXF\nЩоб імпортувати інформацію про мережу до NetworkX, ви можете скористатися однією з декількох функцій, залежно від формату ваших даних. Ось кілька прикладів:\n\nІмпорт з файлу списку граней:\n\nПрипустимо, у вас є файл списку граней, що мають наступне представлення:\nA B\nA C\nB D\nC D\nD E\nВи можете імпортувати цей файл у граф NetworkX за допомогою функції read_edgelist наступним чином:\n\nG = nx.read_edgelist('databases\\lab_13\\Sample1.txt')\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\n\nІмпорт з файлу матриці суміжності:\n\nПрипустимо, що у вас є файл матриці суміжності у наступному форматі:\n0 1 1 0 0\n1 0 0 1 0\n1 0 0 1 1\n0 1 1 0 1\n0 0 1 1 0\nВи можете імпортувати цей файл у граф NetworkX за допомогою функції from_numpy_matrix наступним чином:\n\nadj_matrix = np.loadtxt('databases\\lab_13\\Sample2.txt')\n\nG = nx.Graph()\n\n# додаємо ребра\nfor i in range(adj_matrix.shape[0]):\n    for j in range(adj_matrix.shape[1]):\n        if adj_matrix[i][j] == 1:\n            G.add_edge(i, j)\n\nnx.draw_networkx(G,with_labels=True)\n\n\n\n\n\nІмпорт з файлу GEXF:\n\nЯкщо у вас є файл мережі у форматі GEXF, який є популярним форматом для обміну даних про графи між різними програмними пакетами. Ви можете імпортувати його у графік NetworkX за допомогою функції read_gexf наступним чином:\nПростий граф у форматі GEXF:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;gexf xmlns=\"http://www.gexf.net/1.3\" version=\"1.3\"&gt;\n  &lt;meta lastmodifieddate=\"2022-10-01\"&gt;\n    &lt;creator&gt;NetworkX&lt;/creator&gt;\n    &lt;description&gt;An example graph in GEXF format&lt;/description&gt;\n  &lt;/meta&gt;\n  &lt;graph mode=\"static\" defaultedgetype=\"undirected\"&gt;\n    &lt;nodes&gt;\n      &lt;node id=\"0\" label=\"Node 0\"/&gt;\n      &lt;node id=\"1\" label=\"Node 1\"/&gt;\n      &lt;node id=\"2\" label=\"Node 2\"/&gt;\n    &lt;/nodes&gt;\n    &lt;edges&gt;\n      &lt;edge id=\"0\" source=\"0\" target=\"1\"/&gt;\n      &lt;edge id=\"1\" source=\"1\" target=\"2\"/&gt;\n      &lt;edge id=\"2\" source=\"2\" target=\"0\"/&gt;\n    &lt;/edges&gt;\n  &lt;/graph&gt;\n&lt;/gexf&gt;\n\nG = nx.read_gexf('databases\\lab_13\\\\basic.gexf')\nnx.draw_networkx(G,with_labels=True)\n\n\n\n\n\n# Зберігаємо графік у форматі GEXF\nnx.write_gexf(G, 'databases\\lab_13\\Sample3.gexf')\n\n\n\n13.1.3.2 Матриця суміжності\n\nG_mat = np.array([[0, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n                  [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n                  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                  [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n                  [0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n                  [1, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n                  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n                  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n                  [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n                  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])\n\nG_mat\n\narray([[0, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n       [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n       [1, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])\n\n\nперетворення матриці суміжності у граф за допомогою nx.Graph:\n\nG = nx.Graph(G_mat)\nnx.draw_networkx(G)\n\n\n\n\n\n\n\n13.1.4 Графостатистичні показники\n\n13.1.4.1 Степінь вершини\nНезалежно від того, чи представляють вузли людей, місця, комп’ютери або атоми, розташування вузла в структурі мережі тісно пов’язане з роллю, яку він відіграє в загальній системі. Різні структури уможливлюють різні ролі. Отже, кількісно оцінюючи структурні властивості вузла, можна зрозуміти роль, яку відіграє цей вузол. Числові міри, які характеризують мережеві властивості вузла, називаються мірами центральності. Центральність часто вводять як міру важливості, але є багато способів, у які вузол може бути важливим. Наприклад, однією з найпростіших мір центральності є степенева центральність (degree centrality). Степенева центральність вузла — це просто кількість сусідів, які наявні у вузла (в орієнтованій мережі існують як степеневі, так і нестепеневі центральні вузли). У соціальній мережі степенева центральність є мірою популярності, і може бути хорошим способом здогадатися, хто влаштовує найкращі вечірки, хто має найбільшу кількість публікацій або хто є монополістом на ринку праці. Степенева центральність — це досить елементарний приклад, але далі будуть представлені більш складні міри, які часто використовуються в науці складних мереж. Кожна міра центральності кількісно оцінює різний тип важливості і може бути корисною для відповідей на різні типи питань.\nПоказник степеневої центральності тісно пов’язаний із такою мірою як степінь вершини в мережі (node degree), яка визначає кількість ребер, з якими з’єднана конкретна досліджувана вершина. У мережі з \\(N\\) вершин і \\(M\\) ребер, ступінь \\(k_i\\) вершини \\(i\\) визначається як:\n\\[ k_i = \\sum_{j=1}^M A_{ij} \\]\nде \\(A\\) — матриця суміжності мережі, де \\(A_{ij} = 1\\), якщо існує ребро, що з’єднує вершини \\(i\\) та \\(j\\), і \\(A_{ij} = 0\\) в іншому випадку.\nОколицею вершини \\(i\\) називається множина вершин, які безпосередньо з’єднані з \\(i\\) ребром. Околиця \\(i\\) позначається як \\(N_i\\) і визначається як:\n\\[ N_i = \\{j \\mid A_{ij} = 1\\} \\]\nде \\(A\\) - матриця суміжності мережі, де \\(A_{ij} = 1\\), якщо існує ребро, що з’єднує вершини \\(i\\) та \\(j\\), і \\(A_{ij} = 0\\) в іншому випадку.\n\nG_karate = nx.karate_club_graph()\n\n\nnx.draw_networkx(G_karate)\n\n\n\n\n\nnode = 2\nneighborhood = list(nx.neighbors(G_karate, node))\nneighborhood\n\n[0, 1, 3, 7, 8, 9, 13, 27, 28, 32]\n\n\n\n# ступінь = кількість сусідів \nlen(neighborhood)\n\n10\n\n\n\n# ступінь вершини\nG_karate.degree(node)\n\n10\n\n\n\n# Усі ступені вершини\ndict(G_karate.degree)\n\n{0: 16,\n 1: 9,\n 2: 10,\n 3: 6,\n 4: 3,\n 5: 4,\n 6: 4,\n 7: 4,\n 8: 5,\n 9: 2,\n 10: 3,\n 11: 1,\n 12: 2,\n 13: 5,\n 14: 2,\n 15: 2,\n 16: 2,\n 17: 2,\n 18: 2,\n 19: 3,\n 20: 2,\n 21: 2,\n 22: 2,\n 23: 5,\n 24: 3,\n 25: 3,\n 26: 2,\n 27: 4,\n 28: 3,\n 29: 4,\n 30: 4,\n 31: 6,\n 32: 12,\n 33: 17}\n\n\n\nplt.hist(sorted(dict(G_karate.degree).values()))\nplt.show();\n\n\n\n\n\n\n13.1.4.2 Тріадичне закриття\nМіра, представлена в цьому розділі, стосується зв’язків між сусідами вузла, а не самого вузла. Часто буває корисно розглянути, чи мають сусіди вузла тенденцію бути пов’язаними один з одним. У соціальній мережі це питання зводиться до того, щоб запитати, чи товариш вашого товариша є і вашим товаришом одночасно. Ця властивість відома як транзитивність. Результатом таких стосунків є трикутники: три вузли, пов’язані між собою. Тенденція до виникнення таких трикутників називається кластеризацією. Сильна кластеризація часто свідчить про надійність і надлишковість мережі — якщо один ребро зникає, шлях все ще існує через два інших. Кластеризація вимірюється за допомогою коефіцієнта локальної кластеризації, який визначає тенденцію вузлів об’єднуватись у тріади. Глобальний коефіцієнт кластеризації представляє середнє значення по всім локальним кластеризаціям, що були визначені для кожного вузла мережі.\n\n13.1.4.2.1 Коефіцієнт кластеризації\nКоефіцієнт кластеризації вершини \\(i\\) задається формулою:\n\\[C_i = \\frac{1}{k_i(k_i - 1)}\\sum_{j,k}A_{ij}A_{jk}A_{ki}\\]\nде \\(k_i=\\sum_{j}A_{ij}\\) — кількість ребер, що входять у вершину \\(i\\); \\(A\\) позначає матрицю суміжності.\n\nnx.draw_networkx(G)\n\n\n\n\n\n# локальна кластеризація \nnx.clustering(G, 2)\n\n0\n\n\n\n# список кластеризацій\nnx.clustering(G)\n\n{0: 0.16666666666666666,\n 1: 0.3333333333333333,\n 2: 0,\n 3: 0.3333333333333333,\n 4: 0,\n 5: 0,\n 6: 0,\n 7: 0,\n 8: 0,\n 9: 0}\n\n\n\n\n13.1.4.2.2 Глобальний коефіцієнт кластеризації\nБагато спостережуваних соціальних мереж є більш кластеризованими, ніж це могло б виникнути випадковим чином\nКоефіцієнт кластеризації мережі є середнім значенням коефіцієнтів кластеризації всіх вузлів:\n\\[C = \\frac{1}{N}\\sum_{i=1}^{N} C_i\\]\nде \\(N\\) - загальна кількість вузлів у мережі.\n\n# середній ступінь кластеризації\nnx.average_clustering(G)\n\n0.08333333333333333\n\n\n\n\n13.1.4.2.3 Транзитивність\nТранзитивність — це властивість мережі, яка вимірює ймовірність того, що якщо два вузли мережі мають спільного сусіда, то вони також будуть безпосередньо з’єднані один з одним. Іншими словами, вона вимірює тенденцію до утворення “трикутників” у мережі.\nФормально транзитивність мережі визначається як відношення кількості трикутників у мережі до кількості з’єднаних трійок вузлів (тобто трійок вузлів, які безпосередньо з’єднані один з одним або мають спільного сусіда). У математичній нотації транзитивність мережі позначається як:\n\\[\nT = \\frac{\\sum_{i,k,j=1}^{N}A_{ik}A_{kj}A_{ji}}{\\sum_{i,k,j=1}^{N}A_{ik}A_{ji}}.\n\\]\nВисока транзитивність вказує на те, що вузли в мережі мають тенденцію до утворення трикутних кластерів або спільнот, тоді як низька транзитивність вказує на те, що мережа є більш випадковою або децентралізованою структурою. Транзитивність тісно пов’язана з поняттям коефіцієнта кластеризації, який вимірює схильність вузлів до утворення локальних кластерів або спільнот.\n\n#транзитивність\n#transitivity зважує вершини з великим ступенем вершини\nnx.transitivity(G)\n\n0.15789473684210525\n\n\n\n\n\n13.1.4.3 Шлях\nШлях між двома вузлами \\(A\\) та \\(B\\) у мережі - це послідовність вузлів \\(A, X_1, X_2, ..., X_n, B\\) та послідовність ребер \\((A, X_1), (X_1, X_2), ..., (X_n, B)\\), де кожен вузол та ребро у послідовності є суміжним з попереднім та наступним вузлом або ребром у послідовності.\nДовжина шляху — це кількість ребер у ньому. Шлях довжиною 1 — це ребро між двома вершинами, шлях довжиною 2 — послідовність з двох ребер і трьох вершин, і так далі. Найкоротший шлях між двома вершинами — це шлях мінімальної довжини, який їх з’єднує.\n\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\n\n# згенерувати усі прості шляхи між вершинами 1 та 3\npaths = nx.all_simple_paths(G, source=1, target=3)\n\n# перетворити генератор у список\nPath_List = [path for path in paths]\n\nprint(\"Список шляхів:\", Path_List)\n\nСписок шляхів: [[1, 0, 3], [1, 0, 5, 4, 3], [1, 3]]\n\n\n\nPath1 = Path_List[0]\n# перевірити, чи є шлях простим у графі\nis_valid = nx.is_simple_path(G, Path1) # Простий шлях - це шлях, який не містить жодної вершини, що повторюється.\nprint(\"Чи є шлях простим?\", is_valid)\n\nЧи є шлях простим? True\n\n\n\n# хибний приклад\nnx.is_simple_path(G, [0,8,5])\n\nFalse\n\n\n\n# формуємо список ребер, що формують шлях\nedge_list = [(Path1[i], Path1[i+1]) for i in range(len(Path1)-1)] # len(Path1)-1 = довжина шляху\nedge_list\n\n[(1, 0), (0, 3)]\n\n\n\n# обчислюємо вагу шляху\nweight = sum(G[u][v]['weight'] for u, v in edge_list if 'weight' in G[u][v])\nprint(\"Вага шляху:\", weight)\n\nВага шляху: 2\n\n\n\n# Зважена мережа\nP = bipartite.weighted_projected_graph(B,bipartite.sets(B)[0])\npos = nx.circular_layout(P)\n\n# будуємо граф\nnx.draw_networkx_nodes(P, pos)\nnx.draw_networkx_edges(P, pos)\nnx.draw_networkx_labels(P, pos)\n\n# створюємо словник міток ребер\nedge_labels = {(u, v): d['weight'] for u, v, d in P.edges(data=True)}\n\n# будуємо мітки для ребер \nnx.draw_networkx_edge_labels(P, pos, edge_labels=edge_labels);\n\n\n\n\n\n13.1.4.3.1 Цикл\n\n# Знаходимо всі цикли в графі\ncycles = nx.simple_cycles(G)\nlist(cycles)\n\n[[0, 1, 3], [0, 1, 3, 4, 5], [0, 3, 4, 5]]\n\n\n\n\n13.1.4.3.2 Геодезична лінія\nГеодезичний шлях між двома вузлами \\(A\\) і \\(B\\) в мережі — це найкоротший шлях, який їх з’єднує. Іншими словами, це шлях з мінімальною кількістю ребер, які потрібно пройти, щоб дістатися з вузла \\(A\\) до вузла \\(B\\). Довжина геодезичного шляху — це кількість ребер у цьому шляху.\n\n# геодезичний шлях = найкоротший шлях \nnx.shortest_path(G,1,2)\n\n[1, 0, 2]\n\n\n\n# обчислити найкоротший шлях між двома вузлами\npath = nx.shortest_path(G, source=1, target=3)\n\n# обчислити відповідні ребра шляху\nedges = [(path[i], path[i+1]) for i in range(len(path)-1)]\n\n# будуємо граф та шлях \npos = nx.circular_layout(G)\nnx.draw_networkx(G, pos, with_labels=True)\nnx.draw_networkx_edges(G, pos, edgelist=edges, edge_color='r', width=3);\n\n\n\n\n\n# геодезична довжина \nnx.shortest_path_length(G, 1, 2)\n\n2\n\n\nПошук геодезичного шляху від вузла i до кожного іншого вузла є обчислювально складним, тому нам потрібен ефективний алгоритм для цього.\nТут ми використовуємо пошук в ширину\n\n# алгоритм пошуку в ширину\nT = nx.bfs_tree(G, 1)\nnx.draw_networkx(T, with_labels=True)\nlist(T.edges())\n\n[(1, 0), (1, 3), (1, 6), (0, 2), (0, 5), (3, 4), (5, 8), (4, 7), (8, 9)]\n\n\n\n\n\n\n# усі найкоротші шляхи \nnx.shortest_path_length(G, 1) # виводимо словник \n\n{1: 0, 0: 1, 3: 1, 6: 1, 2: 2, 5: 2, 4: 2, 8: 3, 7: 3, 9: 4}\n\n\n\n# середній найкоротший шлях \nnx.average_shortest_path_length(G)\n\n2.4\n\n\n\n\n13.1.4.3.3 Зв’язні компоненти\nУ простій мережі вище ми бачимо, що для кожної пари вершин можна знайти шлях, який їх з’єднує. Це і є визначенням зв’язного графа. Ми можемо перевірити цю властивість для заданого графа:\n\nnx.is_connected(G)\n\nTrue\n\n\nНе кожен граф зв’язний:\n\nG_test = nx.Graph()\n\nnx.add_cycle(G_test, (1,2,3))\nG_test.add_edge(4, 5)\n\nnx.draw_networkx(G_test, with_labels=True)\n\n\n\n\n\nnx.is_connected(G_test)\n\nFalse\n\n\nА NetworkX видасть помилку, якщо ви запитаєте шлях між вузлами, якого не існує:\n\nnx.has_path(G_test, 3, 5)\n\nFalse\n\n\n\nnx.shortest_path(G_test, 3, 5)\n\nNetworkXNoPath: No path between 3 and 5.\n\n\nВізуально ми можемо ідентифікувати дві пов’язані компоненти на нашому графі. Давайте перевіримо це:\n\nnx.number_connected_components(G_test)\n\n2\n\n\nФункція nx.connected_components() отримує граф і повертає список наборів імен вершин, по одному такому набору для кожної зв’язної компоненти. Перевірте, чи відповідають дві множини у наступному списку двом зв’язним компонентам на рисунку графа вище:\n\nlist(nx.connected_components(G_test))\n\n[{1, 2, 3}, {4, 5}]\n\n\nЯкщо ви не знайомі з множинами у Python, це колекції елементів без дублікатів. Вони корисні для збору імен вузлів, оскільки імена вузлів повинні бути унікальними. Як і у випадку з іншими колекціями, ми можемо отримати кількість елементів у множині за допомогою функції len:\n\ncomponents = list(nx.connected_components(G_test))\nlen(components[0])\n\n3\n\n\nНас часто цікавить найбільша зв’язна компонента, яку іноді називають ядром мережі. Ми можемо скористатися вбудованою функцією max у Python, щоб отримати найбільший зв’язну компоненту. За замовчуванням функція max у Python сортує дані у лексикографічному (тобто алфавітному) порядку, що не є корисним у даному випадку. Ми хочемо отримати максимальний зв’язаний компонент при сортуванні в порядку його розміру, тому ми передаємо len як ключову функцію:\n\nmax(nx.connected_components(G_test), key=len)\n\n{1, 2, 3}\n\n\nХоча часто достатньо мати лише список назв вершин, іноді нам потрібен власне підграф, що містить найбільш зв’язну вершину. Один із способів отримати її — передати список назв вершин у функцію G.subgraph():\n\ncore_nodes = max(nx.connected_components(G_test), key=len)\ncore = G.subgraph(core_nodes)\n\nnx.draw_networkx(core, with_labels=True)\n\n\n\n\nТі з вас, хто використовує завершення написання коду за допомогою табуляції, також помітять функцію nx.connected_component_subgraphs(). Її також можна використати для отримання основного підграфа, але представлений метод є більш ефективним, якщо вас цікавить лише найбільший зв’язна компонента.\n\n\n13.1.4.3.4 Направлені шляхи та компоненти\nДавайте поширимо ці ідеї про шляхи та зв’язні компоненти на орієнтовані графи.\n\nD = nx.DiGraph()\nD.add_edges_from([\n    (1,2),\n    (2,3),\n    (3,2), (3,4), (3,5),\n    (4,2), (4,5), (4,6),\n    (5,6),\n    (6,4),\n])\n\nnx.draw_networkx(D, with_labels=True)\n\n\n\n\nМи знаємо, що в орієнтованому графі ребро з довільної вершини \\(u\\) до довільної вершини \\(v\\) не говорить про те, що існує ребро з \\(v\\) до \\(u\\). Тобто, для направленого графу ми спостерігатимемо асиметрію шляхів. Зверніть увагу, що цей граф має шлях від 1 до 4, але не у зворотному напрямку.\n\nnx.has_path(D, 1, 4)\n\nTrue\n\n\n\nnx.has_path(D, 4, 1)\n\nFalse\n\n\nІнші функції NetworkX, що працюють зі шляхами, також враховують цю асиметрію:\n\nnx.shortest_path(D, 2, 5)\n\n[2, 3, 5]\n\n\n\nnx.shortest_path(D, 5, 2)\n\n[5, 6, 4, 2]\n\n\nОскільки немає ребра з 5 в 3, найкоротший шлях з 5 в 2 не може просто пройти назад по найкоротшому шляху з 2 в 5 — він повинен пройти довшим шляхом через вузли 6 і 4.\nНаправлені мережі мають два типи зв’язності. Сильно зв’язні означають, що між кожною парою вузлів існує спрямований шлях, тобто з будь-якого вузла ми можемо дістатися до будь-якого іншого вузла, дотримуючись спрямованості ребер. Уявіть собі автомобілі на мережі вулиць з одностороннім рухом: вони не можуть їхати проти потоку транспорту.\n\nnx.is_strongly_connected(D)\n\nFalse\n\n\nСлабка зв’язність говорить про те, що між кожною парою вузлів існує шлях, незалежно від напрямку. Подумайте про пішоходів у мережі вулиць з одностороннім рухом: вони ходять по тротуарах, тому їх не хвилює напрямок руху.\n\nnx.is_weakly_connected(D)\n\nTrue\n\n\nЯкщо мережа сильно зв’язана, вона також є і слабко зв’язаною. Зворотне не завжди вірно, як видно з цього прикладу.\nФункція is_connected для неорієнтованих графів видасть помилку, якщо задано орієнтований граф.\n\n# Це призведе до помилки\nnx.is_connected(D)\n\nNetworkXNotImplemented: not implemented for directed type\n\n\nУ випадку направленого графа замість nx.connected_components тепер маємо nx.weak_connected_components та nx.strong_connected_components:\n\nlist(nx.weakly_connected_components(D))\n\n[{1, 2, 3, 4, 5, 6}]\n\n\n\nlist(nx.strongly_connected_components(D))\n\n[{2, 3, 4, 5, 6}, {1}]\n\n\n\n\n\n13.1.4.4 Ексцентриситет\nЕксцентриситет вершини \\(u\\) в мережі — це максимальна відстань між \\(u\\) та будь-якою іншою вершиною мережі. Іншими словами, це максимальна довжина найкоротшого шляху між \\(u\\) та будь-якою іншою вершиною. Ексцентриситет мережі — це максимальний ексцентриситет будь-якого вузла мережі.\n\n# ексцентриситет\n# найбільша відстань між n та всіма іншими вершинами:\nnx.eccentricity(G)\n\n{0: 3, 1: 4, 2: 4, 3: 4, 4: 3, 5: 3, 6: 5, 7: 4, 8: 4, 9: 5}\n\n\n\n# діаметр: max Ексцентриситет між двома вузлами у всій мережі (max max)\nnx.diameter(G)\n\n5\n\n\n\n# Діаметр - максимальний ексцентриситет\nmax(nx.eccentricity(G).values())\n\n5\n\n\n\n# радіус: min Ексцентриситет між двома вузлами у всій мережі (min max)\nnx.radius(G)\n\n3\n\n\n\n# радіус - мінімальний ексцентриситет\nmin(nx.eccentricity(G).values())\n\n3\n\n\n\n# периферія\n# Ексцентриситет=діаметр\nnx.periphery(G)\n\n[6, 9]\n\n\n\n# центр графа: Ексцентриситет = радіус\nnx.center(G)\n\n[0, 4, 5]\n\n\n\n\n13.1.4.5 Центральність\nНезалежно від того, чи представляють вузли людей, місця, комп’ютери або атоми, розташування вузла в структурі мережі тісно пов’язане з роллю, яку він відіграє в загальній системі. Різні структури уможливлюють різні ролі. Отже, кількісно оцінюючи структурні властивості вузла, можна зрозуміти роль, яку відіграє цей вузол. Числові міри, які характеризують мережеві властивості вузла, називаються мірами центральності. Центральність часто вводять як міру важливості, але є багато способів, у які вузол може бути важливим. Наприклад, однією з найпростіших мір центральності є степенева центральності. Степенева центральність вузла — це просто кількість сусідів, яких він має (у спрямованій мережі є як степеневі, так і нестепеневі сусіди). У соціальній мережі степенева центральність є мірою популярності, і може бути хорошим способом здогадатися, хто влаштовує найкращі вечірки. Степенева центральності є досить примітивним прикладом, але в наступних лабораторних будуть представлені більш складні показники, які часто використовуються в мережевій науці. Кожна міра центральності кількісно вимірює різний тип важливості і може бути корисною для відповідей на різні типи питань.\n\n13.1.4.5.1 Степенева центральність — ненаправлені графи\nСтепенева центральність — це міра важливості вузла в мережі, що базується на кількості зв’язків, які він має з іншими вузлами. Степеневу центральність вершини \\(i\\) можна обчислити як:\n\\[C_D(i) = \\frac{k_i}{n-1}\\]\nде \\(k_i\\) — степінь вершини \\(i\\), тобто кількість ребер, інцидентних вершині, а \\(n\\) — загальна кількість вершин у мережі. Знаменник \\(n-1\\) використовується для того, щоб врахувати той факт, що вершина не може бути з’єднана сама з собою.\nСтепенева центральність вузла коливається від 0 до 1, причому більше значення вказує на те, що вузол є більш центральним у мережі. Вузли з високою степеневою центральності, як правило, добре пов’язані з іншими вузлами, і їх видалення з мережі може мати значний вплив на її зв’язність.\nРозглянемо деякі показники на прикладі графу карате-клубу.\n\n\n\n\n\n\nВідомості про граф карате-клубу\n\n\n\nГраф карате-клубу — це соціальна мережа, що представляє дружбу між 34 членами карате-клубу, як це спостерігав Вейн В. Захарі у 1977 році. Кожна вершина графа представляє члена клубу, а кожне ребро — дружбу між двома членами. Граф має 34 вершини та 78 ребер.\nКарате-клуб є відомим прикладом аналізу соціальних мереж і використовувався для вивчення різних властивостей мережі, таких як структура спільноти і міра центральності. Граф характеризується розколом клубу на дві фракції, очолювані інструкторами клубів: вершина 1 та вершина 34. Цей розкол був спричинений суперечкою між двома лідерами, яка врешті-решт призвела до утворення двох окремих клубів карате.\n\n\n\n# Карате-клуб\nG_karate = nx.karate_club_graph()\nG_karate = nx.convert_node_labels_to_integers(G_karate, first_label=1)\nnx.draw_networkx(G_karate, with_labels=True)\n\n\n\n\n\n# Встановіть положення вузлів за допомогою конструктора Камада-Каваї\npos = nx.kamada_kawai_layout(G_karate)\n\n# Будуємо граф з червоними вузлами для вузла 0 (інструктор клубу) і вузла 33 (член клубу): тепер це 1 і 34.\nred_nodes = [1, 34]\nnode_colors = ['red' if node in red_nodes else 'blue' for node in G_karate.nodes()]\nnx.draw_networkx_nodes(G_karate, pos, node_color=node_colors)\nnx.draw_networkx_edges(G_karate, pos)\n\n# Будуємо мітки для вузлів\nnx.draw_networkx_labels(G_karate, pos);\n\n\n\n\n\n# степеневі центральності\ndegCent = nx.degree_centrality(G_karate)\ndegCent\n\n{1: 0.48484848484848486,\n 2: 0.2727272727272727,\n 3: 0.30303030303030304,\n 4: 0.18181818181818182,\n 5: 0.09090909090909091,\n 6: 0.12121212121212122,\n 7: 0.12121212121212122,\n 8: 0.12121212121212122,\n 9: 0.15151515151515152,\n 10: 0.06060606060606061,\n 11: 0.09090909090909091,\n 12: 0.030303030303030304,\n 13: 0.06060606060606061,\n 14: 0.15151515151515152,\n 15: 0.06060606060606061,\n 16: 0.06060606060606061,\n 17: 0.06060606060606061,\n 18: 0.06060606060606061,\n 19: 0.06060606060606061,\n 20: 0.09090909090909091,\n 21: 0.06060606060606061,\n 22: 0.06060606060606061,\n 23: 0.06060606060606061,\n 24: 0.15151515151515152,\n 25: 0.09090909090909091,\n 26: 0.09090909090909091,\n 27: 0.06060606060606061,\n 28: 0.12121212121212122,\n 29: 0.09090909090909091,\n 30: 0.12121212121212122,\n 31: 0.12121212121212122,\n 32: 0.18181818181818182,\n 33: 0.36363636363636365,\n 34: 0.5151515151515151}\n\n\n\n# сортування за степеневою центральністю\nsorted_degcent = {k: v for k, v in sorted(degCent.items(), key=lambda item: item[1], reverse=True)}\nsorted_degcent\n\n{34: 0.5151515151515151,\n 1: 0.48484848484848486,\n 33: 0.36363636363636365,\n 3: 0.30303030303030304,\n 2: 0.2727272727272727,\n 4: 0.18181818181818182,\n 32: 0.18181818181818182,\n 9: 0.15151515151515152,\n 14: 0.15151515151515152,\n 24: 0.15151515151515152,\n 6: 0.12121212121212122,\n 7: 0.12121212121212122,\n 8: 0.12121212121212122,\n 28: 0.12121212121212122,\n 30: 0.12121212121212122,\n 31: 0.12121212121212122,\n 5: 0.09090909090909091,\n 11: 0.09090909090909091,\n 20: 0.09090909090909091,\n 25: 0.09090909090909091,\n 26: 0.09090909090909091,\n 29: 0.09090909090909091,\n 10: 0.06060606060606061,\n 13: 0.06060606060606061,\n 15: 0.06060606060606061,\n 16: 0.06060606060606061,\n 17: 0.06060606060606061,\n 18: 0.06060606060606061,\n 19: 0.06060606060606061,\n 21: 0.06060606060606061,\n 22: 0.06060606060606061,\n 23: 0.06060606060606061,\n 27: 0.06060606060606061,\n 12: 0.030303030303030304}\n\n\n\n# степенева центральність вузла\n\ndegCent[34]\n\n0.5151515151515151\n\n\n\n# намалювати мережу з розмірами вершин на основі їх степеневої центральності\n\n# створити список розмірів вершин на основі степеневої центральності\nnode_sizes = [10000*v*v for v in degCent.values()]\n\n# будуємо граф\nnx.draw_networkx(G_karate, with_labels=True, node_size=node_sizes,pos=nx.spring_layout(G_karate))\n\n\n\n\n\n# кольори на основі ступеневої центральності\nnode_colors = [v for v in degCent.values()]\n\n# будуємо граф\nnx.draw_networkx(G_karate, with_labels=True, node_size=node_sizes,pos=nx.spring_layout(G_karate), node_color=node_colors, cmap=plt.cm.PuBu)\n\n# PuBu розшифровується як \"Pu\" (фіолетовий) - \"Bu\" (синій), \n# і це послідовна карта кольорів, яка варіюється від світло-фіолетового до темно-синього.\n\n\n\n\n\n\n13.1.4.5.2 Степенева центральність — направлені графи\n\n# направлений граф \nG = nx.DiGraph()\n\nG.add_edge(\"A\",\"B\")\nG.add_edge(\"A\",\"D\")\nG.add_edge(\"A\",\"C\")\nG.add_edge(\"B\",\"D\")\n\n# будуємо вузли з мітками\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\n\n# вхідний ступінь вершини\nindegCent = nx.in_degree_centrality(G)\nindegCent\n\n{'A': 0.0,\n 'B': 0.3333333333333333,\n 'D': 0.6666666666666666,\n 'C': 0.3333333333333333}\n\n\n\n# вихідний\noutdegCent = nx.out_degree_centrality(G)\noutdegCent\n\n{'A': 1.0, 'B': 0.3333333333333333, 'D': 0.0, 'C': 0.0}\n\n\n\n# конкретна вершина\noutdegCent[\"A\"]\n\n1.0\n\n\n\n\n13.1.4.5.3 Ступінь близькості\nМіра, відома як ступінь близькості, є однією з найстаріших мір центральності, що використовується в мережевій науці, запропонована соціологом Алексом Бавеласом у 1950 році. Близькість визначається як зворотна величина до віддаленості. Що таке віддаленість? Більш зрозуміло, віддаленість вузла — це сума відстаней між цим вузлом і всіма іншими вузлами. Отже, вузол з високою центральністю близькості знаходиться буквально поруч з іншими вузлами. Центральність вузла вимірює, наскільки швидко він може поширювати інформацію або вплив по всій мережі, оскільки вузли з меншою середньою відстанню до всіх інших вузлів можуть спілкуватися більш ефективно. Крім того, вузли з високим показником центральності часто розташовані в центрі мережі, і їх видалення може мати значний вплив на зв’язність мережі.\nСтупінь близькості вузла \\(i\\) можна обчислити як:\n\\[C_C(i) = \\frac{1}{\\sum\\limits_{j \\neq i} d_{ij}}\\]\nде \\(d_{ij}\\) - найкоротша відстань між вузлами \\(i\\) та \\(j\\). Ступінь близькості вузла коливається від 0 до 1, причому більше значення вказує на меншу середню відстань до всіх інших вузлів мережі.\nУ наступному прикладі використовується функція NetworkX closeness_centrality() для обчислення значень центральності для мережі карате клубу та відображення 10 найближчих один до одного каратистів:\n\ncloseness = nx.closeness_centrality(G_karate)\nsorted(closeness.items(), key=lambda x:x[1], reverse=True)[:10]\n\n[(1, 0.5689655172413793),\n (3, 0.559322033898305),\n (34, 0.55),\n (32, 0.5409836065573771),\n (9, 0.515625),\n (14, 0.515625),\n (33, 0.515625),\n (20, 0.5),\n (2, 0.4852941176470588),\n (4, 0.4647887323943662)]\n\n\n\n# намалювати мережу з розмірами вершин на основі їх ступеня впливовості\n\n# створити список розмірів вершин на основі ступеня впливовості\nnode_sizes = [3000*v*v for v in closeness.values()]\n\n# кольори на основі ступеневої центральності\nnode_colors = [v for v in closeness.values()]\n\n# будуємо граф\nnx.draw_networkx(G_karate, \n                 with_labels=True, \n                 node_size=node_sizes, \n                 pos=nx.spring_layout(G_karate), \n                 node_color=node_colors, \n                 cmap=plt.get_cmap('plasma'))\n\n\n\n\n\n\n13.1.4.5.4 Ступінь посередництва\nУ популярній дитячій грі “Телефон” один гравець починає з того, що шепоче повідомлення іншому, той шепоче це повідомлення іншому і так далі. Врешті-решт, останній гравець промовляє повідомлення вголос. Як правило, фінальне повідомлення не має нічого спільного з початковим. Так, повідомлення, яке починалося як “слідуй за фанковою течією”, може закінчитися як “якби кожна свиняча відбивна була ідеальною, у нас не було б хот-догів”. Кожного разу, коли повідомлення передається від людини до людини, воно може змінюватися, можливо, через те, що його неправильно почули, а можливо, через те, що його навмисно змінили. У більш складних соціальних мережах, таких як організації та громадські рухи, особи, які з’єднують різні частини мережі, мають найбільші можливості фільтрувати, посилювати та змінювати інформацію. Таких людей називають брокерами, а ребра, що з’єднують віддалені частини мережі, — мостами. Важливість таких вузлів і ребер не обмежується соціальними мережами. У потокових мережах — таких як залізниці, водопроводи та телекомунікаційні системи — вузли що з’єднують віддалені частини мережі, можуть діяти як вузькі місця, обмежуючи обсяг потоку. Виявлення таких вузьких місць дає змогу збільшити їхню пропускну здатність і захистити їх від збоїв та атак. Мости і брокери важливі, тому що вони знаходяться між різними частинами мережі. Відповідно, тип центральності, який використовується для визначення мостів і брокерів називається cтупенем посередництва.\nСтупінь посередництва — це міра того, наскільки вузол лежить на найкоротших шляхах між іншими вузлами мережі. Ступінь посередництва для вузла \\(i\\) можна обчислити як:\n\\[C_B(i) = \\sum\\limits_{s \\neq i \\neq t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}\\]\nде \\(s\\) і \\(t\\) — два вузли мережі, \\(\\sigma_{st}\\) — загальна кількість найкоротших шляхів між \\(s\\) і \\(t\\), а \\(\\sigma_{st}(i)\\) — кількість найкоротших шляхів між \\(s\\) і \\(t\\), які проходять через вузол \\(i\\).\nСтупінь посередництва змінюється від 0 до 1, причому більше значення вказує на більшу кількість найкоротших шляхів, що проходять через вершину. Вузли з високим значенням центральності часто розташовані на “мостах” між різними кластерами або спільнотами в мережі, і їх видалення може мати значний вплив на зв’язність мережі.\nСтупінь посередництва базується на припущенні, що чим більше найкоротших шляхів проходить через вершину (або ребро), тим більше вона виступає в ролі брокера (або моста). Для ступеня посередництва знаходять найкоротші шляхи між кожною парою вузлів. Значення ступеня посередництва для вузла або ребра — це просто кількість цих шляхів, що проходять через нього. Тут є кілька застережень. По-перше, за домовленістю, шляхи не вважаються такими, що проходять через їхні кінцеві точки і не враховуються при визначенні посередництва між ними. Крім того, вам може бути цікаво, що станеться, якщо є два найкоротших шляхи однакової довжини. У цьому випадку кожен шлях робить 1/2 внеску в посередництво своїх вершин/ребер (або 1/3, якщо є 3 шляхи, і так далі).\nНа наступній діаграмі показано приклад мережі та розраховані значення посередництва для кожної вершини та ребра. Для кожної пари вершин показано найкоротший шлях (за винятком тривіальних шляхів довжиною 1). Посередництво вузла — це сума шляхів, які проходять через цей вузол. Посередництво ребра — це кількість нетривіальних шляхів, які проходять через це ребро, плюс 1 для самого ребра:\n\n\n\nРис. 13.2: Усі нетривіальні найкоротші шляхи у прикладі мережі та отримані центри посередництва\n\n\nСтупінь посередництва між вузлами легко обчислюється в NetworkX за допомогою функції betweenness_centrality(). Ця функція повертає словник, який зіставляє позначення вузлів зі значеннями посередництва. Якщо аргумент normalized має значення True (за замовчуванням), значення ступеня посередництва ділиться на кількість пар вузлів, що може бути корисним для порівняння значень посередництва, що мають різні масштаби. Якщо аргумент endpoints має значення True (за замовчуванням False), то кінцеві точки шляху будуть включені в розрахунок посередництва.\n\nbtwnCent = nx.betweenness_centrality(G_karate, endpoints = False)\nsorted(btwnCent.items(), key=lambda x:x[1], reverse=True)[:10]\n\n[(1, 0.43763528138528146),\n (34, 0.30407497594997596),\n (33, 0.145247113997114),\n (3, 0.14365680615680618),\n (32, 0.13827561327561325),\n (9, 0.05592682780182781),\n (2, 0.053936688311688304),\n (14, 0.04586339586339586),\n (20, 0.03247504810004811),\n (6, 0.02998737373737374)]\n\n\n\n# намалювати мережу з розмірами вершин на основі їх ступеня посередництва\n\n# створити список розмірів вершин на основі ступеня посередництва\nnode_sizes = [10000*v*v for v in btwnCent.values()]\n\n# кольори на основі ступеня посередництва\nnode_colors = [v for v in btwnCent.values()]\n\n# будуємо граф\nnx.draw_networkx(G_karate, with_labels=True, \n        node_size=node_sizes, \n        pos=nx.spring_layout(G_karate), \n        node_color=node_colors, \n        cmap=plt.get_cmap('viridis'))\n\n\n\n\nВидно, що високим рівнем посередництва характеризуються вершини 1, 34 і 33. Високий рівень посередництва між ними свідчить про те, що ці особи є важливими інформаційними посередниками в клубі карате. Можливо, вони є найбільш вправними каратистами.\nЦентральність посередництва для ребер — це міра того, наскільки ребро лежить на найкоротших шляхах між іншими ребрами в мережі. Посередництво ребра \\(e\\) можна обчислити наступним чином:\n\\[C_B(e) = \\sum_{s \\neq e \\neq t} \\frac{\\sigma_{st}(e)}{\\sigma_{st}}\\]\nде \\(s\\) і \\(t\\) — дві вершини мережі, \\(\\sigma_{st}\\) — загальна кількість найкоротших шляхів між \\(s\\) і \\(t\\), а \\(\\sigma_{st}(e)\\) — кількість найкоротших шляхів між \\(s\\) і \\(t\\), які проходять через ребро \\(e\\).\nСтупінь посередництва ребра змінюється від 0 до 1, причому більше значення вказує на більшу кількість найкоротших шляхів, які проходять через ребро. Ребра з високою посередництвом часто розташовані на “мостах” між різними кластерами або спільнотами в мережі, і їх видалення може мати значний вплив на зв’язність мережі.\n\nbtwnCent_edge = nx.edge_betweenness_centrality(G_karate, normalized=True)\nsorted(btwnCent_edge.items(), key=lambda x:x[1], reverse=True)[:10] \n\n[((1, 32), 0.1272599949070537),\n ((1, 7), 0.07813428401663695),\n ((1, 6), 0.07813428401663694),\n ((1, 3), 0.0777876807288572),\n ((1, 9), 0.07423959482783014),\n ((3, 33), 0.06898678663384543),\n ((14, 34), 0.06782389723566191),\n ((20, 34), 0.05938233879410351),\n ((1, 12), 0.058823529411764705),\n ((27, 34), 0.0542908072319837)]\n\n\n\n# намалювати мережу з розмірами вершин на основі ступеня посередництва їх ребер\n\n# кольори ребер на основі ступеня посередництва ребер\nedge_colors = [v for v in btwnCent_edge.values()]\nedge_widths = [v*100 for v in btwnCent_edge.values()]\n\n# будуємо граф\nnx.draw_networkx(G_karate, \n        with_labels=True, \n        pos=nx.spring_layout(G_karate), \n        edge_color=edge_colors, \n        cmap=plt.get_cmap('winter'), \n        width=edge_widths)\n\n\n\n\nЯкщо розглядати, наприклад, топ 3 ребер із найбільшим ступенем посередництва, ми побачимо, що, як правило, найкраща комунікація проходить у тренера з учнями під номерами 32, 7, 6, 3 тощо.\n\n\n13.1.4.5.5 Ступінь впливовості\nУявіть, що у вас є важливе повідомлення, яке потрібно донести до цілої групи (наприклад, до вашого роботодавця або школи), але ви можете передати його лише одній людині. Кому б ви це сказали? Ви б хотіли знайти когось, хто має хороші зв’язки з усією мережею. Ви можете спробувати звернутися до людини з найвищою степеневою центральністю (найбільшою кількістю друзів). Недоліком такого підходу є те, що її друзі можуть бути не дуже добре пов’язані з рештою мережі. Наприклад, у гіпотетичній компанії директор з продажу на східному узбережжі може знати найбільше людей, але не знати, як зв’язатися з іншими відділами чи регіонами. Замість нього краще знайти когось, хто має тісні зв’язки з іншими людьми, які мають тісні зв’язки, наприклад, генерального директора (або, що більш ймовірно, його помічника). Таких людей іноді називають хабами, тому що, подібно до центру колеса зі спицями, вони з’єднують між собою багато різних точок. Цю концепцію високозв’язних хабів добре відображає показник, який називається ступенем впливовості.\nСтупінь впливовості вершини \\(i\\) можна визначити через головний власний вектор матриці суміжності \\(\\mathbf{A}\\) мережі:\n\\[\\mathbf{Av} = \\lambda \\mathbf{v}\\]\nде \\(\\mathbf{v}\\) — власний вектор, що відповідає найбільшому власному значенню \\(\\lambda\\). Ступінь впливовості вершини \\(i\\) задається \\(i\\)-им елементом \\(\\mathbf{v}\\).\nСтупінь впливовості вузла коливається від 0 до 1, причому більше значення вказує на більшу важливість вузла та його сусідів у мережі. Вузли з високим ступенем впливовості часто розташовані в центрі мережі і добре пов’язані з іншими сильно пов’язаними вузлами, і їх видалення може мати значний вплив на зв’язність мережі.\n\neigenvector_centrality = nx.eigenvector_centrality_numpy(G_karate)\nsorted(eigenvector_centrality.items(), key=lambda x:x[1], reverse=True)[:10]\n\n[(34, 0.3733634702914835),\n (1, 0.355491444524566),\n (3, 0.31719250448643166),\n (33, 0.30864421979104795),\n (2, 0.26595991955249154),\n (9, 0.2274039071254003),\n (14, 0.22647272014248124),\n (4, 0.21117972037789015),\n (32, 0.19103384140654378),\n (31, 0.17475830231435313)]\n\n\n\n# намалювати мережу з розмірами вершин на основі їх ступеня впливовості\n\n# створити список розмірів вершин на основі ступеня впливовості\nnode_sizes = [10000*v*v for v in eigenvector_centrality.values()]\n\n# кольори на основі степеневої впливовості\nnode_colors = [v for v in eigenvector_centrality.values()]\n\n# будуємо граф\nnx.draw_networkx(G_karate, \n        with_labels=True, \n        node_size=node_sizes, \n        pos=nx.spring_layout(G_karate), \n        node_color=node_colors, \n        cmap=plt.get_cmap('Purples'))\n\n\n\n\n\n\n\n\n13.1.5 Широкомасштабний опис мереж\nШирокомасштабні структури можуть сильно відрізнятися від мережі до мережі. Ці відмінності часто вказують на різні типи мереж (наприклад, соціальні та технологічні). Широкомасштабні структури також можуть мати важливі наслідки для функціональних властивостей, таких як як стійкість до збоїв і атак. Розглянемо аналіз структурних показників для мереж різних типів.\nЯк ви вже могли переконатися на прикладі графу карате-клубу, NetworkX надає декілька вбудованих наборів мережевих даних, які можна використовувати для тестування та експериментів. Ці набори даних доступні в самій бібліотеці NetworkX і можуть бути завантажені за допомогою функцій, які починаються з префікса nx., за яким слідує назва набору даних.\nОсь кілька прикладів вбудованих мережевих наборів даних у NetworkX:\n\nnx.karate_club_graph() - повертає мережу Zachary’s Karate Club, соціальну мережу карате-клубу, де кожен вузол представляє члена клубу, а кожне ребро представляє дружні стосунки між членами.\nnx.les_miserables_graph() - Повертає мережу персонажів роману Віктора Гюго “Знедолені”, де кожен вузол представляє персонажа роману, а кожне ребро представляє спільну появу двох персонажів у главі.\nnx.davis_southern_women_graph() - Повертає мережу соціальних взаємодій між жінками у містечку на півдні США у 1930-х роках, де кожен вузол представляє жінку, а кожне ребро - соціальні стосунки між двома жінками.\n\nЦе лише кілька прикладів вбудованих мережевих наборів даних в NetworkX. Ви можете знайти більше інформації про доступні набори даних та їх використання в документації NetworkX.\n\n# генеруємо першу мережу\nG_karate = nx.karate_club_graph()\nmr_hi = 0\njohn_a = 33\n\n# генеруємо другу мережу \nG_novel = nx.les_miserables_graph()\n\n# генеруємо третю мережу \nG_woman = nx.davis_southern_women_graph()\n\nНаступний код візуалізує три приклади мереж:\n\nfig, ax = plt.subplots(1, 3, figsize=(10, 5))\n\nax[0].set_title(\"Карате\")\nnx.draw_networkx(G_karate, node_size=0, with_labels=False, ax=ax[0])\n\nax[1].set_title(\"Роман\")\nnx.draw_networkx(G_novel, node_size=0, with_labels=False, ax=ax[1])\n\nax[2].set_title(\"Жінки\")\nnx.draw_networkx(G_woman, node_size=0, with_labels=False, ax=ax[2])\n\nplt.tight_layout()\n\n\n\n\n\n13.1.5.1 Діаметр і найкоротший шлях\nМережі можуть бути охарактеризовані відповідно до розподілу довжини найкоротшого шляху. Наведена нижче функція будує гістограму всіх найкоротших шляхів у мережі:\n\ndef path_length_histogram(G, title=None):\n    # знаходимо довжини шляхів\n    length_source_target = dict(nx.shortest_path_length(G))\n    # конвертуємо словник словників до звичайного списку\n    all_shortest = sum([\n    list(length_target.values())\n    for length_target\n    in length_source_target.values()],\n    [])\n    # розраховуємо цілочисельні біни\n    high = max(all_shortest)\n    bins = [-0.5 + i for i in range(high + 2)]\n    # будуємо гістограму\n    plt.hist(all_shortest, bins=bins, rwidth=0.8)\n    plt.title(title)\n    plt.xlabel(\"Відстань\")\n    plt.ylabel(\"Підрахунок\")\n\nТепер давайте порівняємо розподіл довжин шляхів для трьох мереж:\n\n# Створюємо рисунок\nplt.figure(figsize=(10, 5))\n# Будуємо гістограми найкоротших шляхів\nplt.subplot(1, 3, 1)\npath_length_histogram(G_karate, title=\"Карате\")\nplt.subplot(1, 3, 2)\npath_length_histogram(G_novel, title=\"Роман\")\nplt.subplot(1, 3, 3)\npath_length_histogram(G_woman, title=\"Жінки\")\n\nplt.tight_layout()\n\n\n\n\nУсі три графи мають достатньо малі найкоротші шляхи. Соціальні мережі, як правило, мають короткі шляхи, відомий як феномен малого світу.\nХоча розподіл повної довжини шляху є інформативним, він є дещо громіздким, тому корисно використовувати агреговані показники. Однією з таких мір є середня довжина найкоротшого шляху, яку можна обчислити наступним чином:\n\nprint(\"Середній найкоротший шлях для карате-клубу: \", nx.average_shortest_path_length(G_karate))\n\nprint(\"Середній найкоротший шлях для роману: \", nx.average_shortest_path_length(G_novel))\n\nprint(\"Середній найкоротший шлях для жінок: \", nx.average_shortest_path_length(G_woman))\n\nСередній найкоротший шлях для карате-клубу:  2.408199643493761\nСередній найкоротший шлях для роману:  2.6411483253588517\nСередній найкоротший шлях для жінок:  2.306451612903226\n\n\n\n\n\n\n\n\nПредупреждение\n\n\n\nУ роз’єднаній мережі, де мережа може бути розділена на дві або або більше компонентів без ребра між ними, середня довжина шляху стає нескінченною. Цю проблему можна вирішити кількома способами, наприклад використання гармонічного, а не арифметичного середнього, або усереднення середнього значення найкоротших шляхів у межах кожної зв’язної компоненти. Який метод є доречним, залежить від типу мережі, що аналізується.\n\n\nКрім того, розмір мережі може бути охарактеризований найбільшою довжиною шляху довжиною, яка називається діаметром. Діаметри трьох прикладів мереж можна знайти за допомогою функції diameter():\n\nprint(\"Діаметр для карате-клубу: \", nx.diameter(G_karate))\n\nprint(\"Діаметр для роману: \", nx.diameter(G_novel))\n\nprint(\"Діаметр для жінок: \", nx.diameter(G_woman))\n\nДіаметр для карате-клубу:  5\nДіаметр для роману:  5\nДіаметр для жінок:  4\n\n\nЯк ми можемо бачити результати доволі схожі на попередні. На відміну від середньої довжини найкоротшого шляху, діаметр залежить лише від одного шляху. Як наслідок, один викид може значно збільшити діаметр. Однак у такому разі діаметр може бути гарним показником найгіршої довжини шляху.\n\n\n13.1.5.2 Вимірювання стійкості мережі\nСтійкість — це здатність системи протистояти збоям і атакам. Наприклад, в електромережі стійкість означає продовження подачі електроенергії, коли лінія електропередач або генератор вийшли з ладу. У дорожньому русі це може означати можливість перенаправляти автомобілі, коли вулиця перекрита через аварію.\nСтійкість — це фундаментальна властивість мережі, оскільки вона зазвичай досягається за допомогою резервних шляхів. Коли один шлях більше не доступний, інші все ще можуть бути використані.\nНайпростішим (і найгрубішим) показником стійкості є щільність мережі: частка можливих ребер, які існують. Чим більше ребер у мережі, тим більше надлишкових шляхів існує між її вузлами. Наступний код використовує функцію density() для обчислення цього значення:\n\nprint(\"Щільність для карате-клубу: \", nx.density(G_karate))\n\nprint(\"Щільність для роману: \", nx.density(G_novel))\n\nprint(\"Щільність для жінок: \", nx.density(G_woman))\n\nЩільність для карате-клубу:  0.13903743315508021\nЩільність для роману:  0.08680792891319207\nЩільність для жінок:  0.17943548387096775\n\n\nМережа зазвичай вважається розрідженою, якщо кількість ребер близька до \\(N\\) (кількість вузлів), і щільною, якщо кількість ребер близька до \\(N^2\\).\nМожна бачити, що найбільш стійкою (щільною) серед усіх трьох графів є мережа жінок.\n\n\n13.1.5.3 Найменші розрізи\nБільш складні показники відмовостійкості базуються на концепції найменших розрізів. Найменший розріз або min-cut — це кількість вузлів (або ребер), які потрібно видалити, щоб розділити мережу на дві незв’язані частини. Найменші розрізи можна знайти або між двома конкретними вузлами, або над усіма парами вузлів.\nУ NetworkX найменший розріз між двома вузлами знаходять за допомогою функції minimum_st_node_cut(). Зауважте, що ця функція знаходиться у пакеті connectivity і має бути імпортована окремо на додачу до базового пакету networkx. Наступний код знаходить мінімальну довжину шляху між містером Хі та Джоном А. у мережі карате-клубу:\n\nimport networkx.algorithms.connectivity as nxcon\nnxcon.minimum_st_node_cut(G_karate, mr_hi, john_a)\n\n{2, 8, 13, 19, 30, 31}\n\n\nПопередній результат говорить про те, що вузли 2, 8, 12, 19, 30, 31 потрібно видалити, щоб розділити мережу на дві половини, одна з яких містить містера Хі, а інша міститиме Джона А.\nАналогічно, найменший розріз ребер може бути знайдений наступним чином:\n\nnxcon.minimum_st_edge_cut(G_karate, mr_hi, john_a)\n\n{(0, 8),\n (0, 31),\n (1, 30),\n (2, 8),\n (2, 27),\n (2, 28),\n (2, 32),\n (9, 33),\n (13, 33),\n (19, 33)}\n\n\nЯкщо вам потрібно знати лише розмір найменшого розрізу, ви можете скористатися функціями node_connectivity() або edge_connectivity() у базовому пакеті networkx. У наступному прикладі обчислюються ці значення для мережі карате-клубу:\n\nnx.node_connectivity(G_karate, mr_hi, john_a)\n\n6\n\n\n\nnx.edge_connectivity(G_karate, mr_hi, john_a)\n\n10\n\n\n\n\n13.1.5.4 Зв’язність\nНайменші розрізи можуть бути використані для визначення показників зв’язності для всієї мережі. Ці міри дуже корисні для кількісної оцінки стійкості мережі.\nЗв’язність вузлів — це найменший мінімальний розріз між усіма парами вузлів. Зв’язність ребер визначається аналогічно. Фактичні значення розрізів між вузлами та ребрами можна знайти за допомогою пакету connection:\n\nnxcon.minimum_node_cut(G_karate)\n\n{0}\n\n\n\nnxcon.minimum_edge_cut(G_karate)\n\n{(11, 0)}\n\n\nЗв’язність можна обчислити за допомогою функцій node_connectivity() та edge_connectivity(), не вказуючи вихідні та цільові вузли. У наступному прикладі обчислюється зв’язність вузлів для трьох прикладів мереж:\n\nnx.node_connectivity(G_karate)\n\n1\n\n\n\nnx.node_connectivity(G_novel)\n\n1\n\n\n\nnx.node_connectivity(G_woman)\n\n2\n\n\nЗдається, що всі ці мережі, окрім мережі жінок, можна роз’єднати, видаливши лише один вузок. Для мережі жінок потребується видалити два вузли.\nПопередня міра зв’язності знаходить розмір найменшого мінімального розрізу, але його видалення не вплине на всі шляхи в мережі. Після видалення вузла або ребра мережа буде розділена, але в кожній половині вузли все ще будуть з’єднані один з одним.\nКращий показник надійності можна знайти, усереднивши зв’язність по всіх вузлах або ребрах за допомогою функцій average_node_connectivity() і average_edge_connectivity(). Зауважте, що обчислення цих значень може зайняти багато часу, навіть для невеликих мереж. Наступний код обчислює середню зв’язність вузлів для досліджуваних мереж:\n\nprint(\"Середня зв'язність для карате-клубу: \", nx.average_node_connectivity(G_karate))\n\nprint(\"Середня зв'язність для роману: \", nx.average_node_connectivity(G_novel))\n\nprint(\"Середня зв'язність для жінок: \", nx.average_node_connectivity(G_woman))\n\nСередня зв'язність для карате-клубу:  2.2174688057040997\nСередня зв'язність для роману:  2.2624743677375254\nСередня зв'язність для жінок:  3.7399193548387095\n\n\nМережа каратистів та персонажів роману доволі подібні один до одного по зв’язності, але мережа жінок представляється найбільш стійкою або, іншими словами, організованою.\n\n\n13.1.5.5 Централізація та нерівномірність\nМережі також можна класифікувати за ступенем централізації — наскільки вони зосереджені в одному або декількох вузлах. Нерівномірний розподіл є більш централізованим. Наприклад, найбільш централізованою мережею є мережа, всі вузли якої під’єднані до одного вузла-хабу. Наступний код будує гістограми ступенів впливовості для кожної з мереж:\n\n# Функція для побудови гістограми\ndef centrality_histogram(x, title=None):\n    plt.hist(x, density=True)\n    plt.title(title)\n    plt.xlabel(\"Впливовість\")\n    plt.ylabel(\"Підрахунок\")\n\n\n# Створення рисунку\nplt.figure(figsize=(10, 5))\n# Розрахунок центральностей для кожного графу\nplt.subplot(1, 3, 1)\ncentrality_histogram(\nnx.eigenvector_centrality(G_karate).values(), title=\"Карате\")\nplt.subplot(1, 3, 2)\ncentrality_histogram(\nnx.eigenvector_centrality(G_novel).values(),\ntitle=\"Роман\")\nplt.subplot(1, 3, 3)\ncentrality_histogram(\nnx.eigenvector_centrality(G_woman).values(), title=\"Жінки\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nЗ представлених гістограм видно, що найвищі значення впливовості приходять на мережу жінок. Найгіршою за впливовістю предстає мережа персонажів роману.\nВиміряти нерівномірність набору значень можна за допомогою ентропії Шеннона. Концепцію ми вже розглядали в попередніх лабораторних, але достатньо знати, що чим більш рівномірно розподіленим є набір чисел, тим вища його ентропія. Наступна функція повертає ентропію списку чисел:\n\nimport math\ndef entropy(x):\n    # Нормалізація\n    total = sum(x)\n    x = [xi / total for xi in x]\n    H = sum([-xi * math.log2(xi) for xi in x])\n    return H\n\nОбчислення ентропії ступенів впливовості у кожній з мереж дає наступний результат:\n\nprint(\"Ентропія ступенів впливовості для карате-клубу: \", \n      entropy(nx.eigenvector_centrality(G_karate).values()))\n\nprint(\"Ентропія ступенів впливовості для роману: \", \n      entropy(nx.eigenvector_centrality(G_novel).values()))\n\nprint(\"Ентропія ступенів впливовості для жінок: \", \n      entropy(nx.eigenvector_centrality(G_woman).values()))\n\nЕнтропія ступенів впливовості для карате-клубу:  4.842401948329853\nЕнтропія ступенів впливовості для роману:  5.52075429881287\nЕнтропія ступенів впливовості для жінок:  4.858808158743919\n\n\nНайбільш рівномірно розподіленою в даному випадку представляється мережа персонажів роману. Мережі карате-клубу та жінок мають трохи вищий ступінь централізації.\nУ соціальних мережах не всі стосунки є рівними. Ви можете підписати заявку на кредит для свого брата чи сестри, але навряд чи для сажотруса вашого стоматолога стоматолога. У соціології міцність стосунків вимірюється поняттям міцність зв’язності. У цьому контексті зв’язність — це певний вид міжособистісних стосунків, а міцність — це будь-яка міра того, наскільки інтенсивними чи інтимними є ці стосунки (зв’язності).\nУ 1973 році соціолог Марк Грановеттер описав важливість слабких зв’язків для зближення різних спільнот. Якщо всі зв’язки всередині спільноти сильні, то будь-які зв’язки між спільнотами мають бути слабкими. Він назвав це явище силою слабких зв’язків. З’єднуючи різні спільноти, слабкі зв’язки дають змогу знаходити інформацію з віддалених частин мережі. Але як виміряти силу зв’язностей?\n\n\n13.1.5.6 Сила зв’язності\nУ мережі карате-клубів немає ніякої додаткової інформації про міцність ребер, але є відповідні властивості цих ребер, які можна обчислити, наприклад, сила зв’язності. Сила зв’язності зростає зі збільшенням кількості сусідів, які мають спільні вершини. Це мотивовано спостереженням, що близькі друзі, як правило, мають більше спільних друзів, і це часто може дати уявлення про структуру соціальної мережі. Наступний код обчислює силу зв’язку, використовуючи метод neighbors() для пошуку сусідів вузлів та множини Python для обчислення кількості спільних сусідів:\n\ndef tie_strength(G, v, w):\n    # Отримуємо сусідів вершин v та w у G\n    v_neighbors = set(G.neighbors(v))\n    w_neighbors = set(G.neighbors(w))\n    # Повернути розмір заданої зв'язності\n    return 1 + len(v_neighbors & w_neighbors)\n\nТут ми визначили міцність зв’язку як кількість спільних сусідів плюс один. Чому плюс один? Нульова вага умовно означає відсутність ребра, тому без додаткової одиниці ребра між вершинами, які не мають спільних сусідів, не вважатимуться ребрами.\n\n\n\n\n\n\nУведомление\n\n\n\nТут можна поцікаватись, чи існує теоретичне пояснення цього “додаткового друга”. Якщо у вас з другом немає спільних друзів, є один з його друзів, з яким ви досить добре знайомі — це ви самі. Отже, додаткову одиницю можна інтерпретувати як таку, що вказує на зв’язність ребра самого із собою.\n\n\n\nG = nx.karate_club_graph()\n\n# Надаємо інформацію про те, хто в якому клубі \n# опинився після розділення клубу\nmember_club = [\n    0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n    0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n    1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n    1, 1, 1, 1]\n\nnx.set_node_attributes(G, dict(enumerate(member_club)), 'club')\n\n# Знаходимо внутрішні та зовнішні ребра\nfor v, w in G.edges:\n # Перебираємо пари вершин\n # Встановлюємо 'True', якщо вершини в одному кластері (клубі)\n if G.nodes[v][\"club\"] == G.nodes[w][\"club\"]:\n    G.edges[v, w][\"internal\"] = True\n else:\n    G.edges[v, w][\"internal\"] = False\n\n# Внутріші - каратисти знаходяться в одному клубі й підтримують зв'язок\ninternal = [e for e in G.edges if G.edges[e][\"internal\"]]\n\n# Зовнішні - каратисти в різних клубах, але продовжують підтримувати зв'язок\nexternal = [e for e in G.edges if ~G.edges[e][\"internal\"]]\n\nНаступний код обчислює силу зв’язності кожного ребра і зберігає її в змінну strength:\n\nstrength = dict(\n ((v,w), tie_strength(G, v, w))\n for v, w in G.edges())\n\n\n\n13.1.5.7 Мостовий проліт\nМіцність зв’язків також можна оцінити кількісно, розглядаючи ефект видалення ребра з мережі. Вузли, з’єднані ребром, завжди знаходяться на відстані в 1 крок один від одного (у незваженій мережі). Але якщо це ребро видалити, його кінцеві точки можуть знаходитись на відстані в 2 кроки, і навіть до зовсім не з’єднаних між собою. Цю концепцію відображає мостовий проліт — відстань між кінцевими точками ребра, якщо це ребро видалити. Ребра з великим прольотом з’єднують віддалені частини мережі, тому їх можна вважати слабкими зв’язками, незважаючи на те, що вони відіграють важливу роль.\nНаступний код обчислює довжину кожного ребра в мережі карате-клубу:\n\ndef bridge_span(G):\n    # Отримуємо список ребер\n    edges = G.edges()\n    # Створюємо копію графа\n    G = nx.Graph(G)\n    # Створюємо словник для збереження результату\n    result = dict()\n    for v, w in edges:\n        # Тимчасово видаляємо ребро \n        G.remove_edge(v, w)\n        # Знаходимо нову відстань між двома вузлами \n        # після видалення ребра\n        try:\n            d = nx.shortest_path_length(G, v, w)\n            result[(v, w)] = d\n        except nx.NetworkXNoPath:\n            result[(v, w)] = float('inf')\n        # Відновлюємо ребро\n        G.add_edge(v, w)\n    return result\n\n\nspan = bridge_span(G)\n\n\n\n13.1.5.8 Порівняння міцності та прольоту\nРозглянемо 10 найміцніших і 10 найслабших ребер у мережі карате-клубів. Наступний код виводить ці ребра:\n\n# Упорядковуємо ребра за силою зв'язності\nordered_edges = sorted(strength.items(), key=lambda x: x[1])\nprint('Ребро\\t Міцність\\t Проліт\\t Внутрішній зв\\'язок')\n# Виводимо 10 найміцніших\nfor e, edge_strength in ordered_edges[:10]:\n    print('{:10}{}\\t\\t{}\\t{}'.format(\n    str(e), edge_strength, span[e], G.edges[e]['internal']))\nprint('...')\n# Виводимо 10 найслабших\nfor e, edge_strength in ordered_edges[-10:]:\n    print('{:10}{}\\t\\t{}\\t{}'.format(\n    str(e), edge_strength, span[e], G.edges[e]['internal']))\n\nРебро    Міцність    Проліт  Внутрішній зв'язок\n(0, 11)   1     inf True\n(0, 31)   1     3   False\n(1, 30)   1     3   False\n(2, 9)    1     3   False\n(2, 27)   1     3   False\n(2, 28)   1     3   False\n(9, 33)   1     3   True\n(13, 33)  1     3   False\n(19, 33)  1     3   False\n(23, 25)  1     3   True\n...\n(8, 32)   4     2   True\n(23, 33)  4     2   True\n(29, 33)  4     2   True\n(1, 2)    5     2   True\n(1, 3)    5     2   True\n(2, 3)    5     2   True\n(0, 2)    6     2   True\n(0, 3)    6     2   True\n(0, 1)    8     2   True\n(32, 33)  11        2   True\n\n\nРезультат вище показує, що ребра з низькою міцністю і великим прольотом, як правило, є зовнішніми, з’єднуючи членів клубу, які розкололися на різні клуби-відколи. З іншого боку, ребра з високою міцністю і малим прольотом є внутрішніми, вони з’єднують членів клубу, які залишилися разом після розколу.\n\n\n13.1.5.9 Спектральні міри складності\nУ теорії графів і комбінаториці є багато теорем, при доказі яких застосовуються спектри графів, хоча вони і не зустрічаються у формулюванні теорем. Отже, використання спектрів грає роль досить важливого методу, який називається спектральним.\nСпектром графа \\(G\\) називається множина власних значень матриці, що відповідає даному графу. Відомі декілька підходів встановлення зв’язку між графом \\(G\\) та його спектром. Для випадку регулярних графів (якими є графи часових рядів фондових індексів) можна показати, що різні види спектрів еквівалентні, тобто містять однакову кількість інформації про структуру графу \\(G\\).\nМи вже згадували, що одним із способів представлення графа у вигляді матриці є матриця суміжності. Матриця Лапласа (Laplacian matrix) \\(L\\) — також є одним видів подання графа. Вона може бути використана для розрахунку кількості остовних дерев для графа. Для знаходження матриці Кірхгофа використовують формулу: \\(L=D-A\\), де \\(D\\) — діагональна матриця:\n\\[\nd_{ij} = \\begin{cases}\n            d_i, & i=j,\\\\\n            0, & i \\neq j,\n        \\end{cases}\n\\]\nде \\(d_i\\) — ступінь відповідної вершини графа. Отже,\n\\[\nl_{ij} = \\begin{cases}\n    d_i, & i=j, \\\\\n    -1, & i \\neq j \\, \\text{і} \\, v_i \\, \\text{суміжна з} \\, v_j, \\\\\n    0 & \\text{в іншому випадку}.\n\\end{cases}\n\\]\nAlgebraic connectivity (алгебраїчна зв’язність графу) — друге найменше власне значення матриці Лапласа. Це власне значення більше 0, тоді і тільки тоді, коли граф зв’язний. Величина цього значення відображає, наскільки добре граф пов’язує ці компоненти, і була використана при аналізі надійності та синхронізації мереж. Бібліотека NetworkX містить метод algebraic_connectivity() для обчислення даного показника. Бібліотека також надає змогу розрахувати нормалізовану матрицю Лапласа. Сенс нормалізації полягає в тому, що вершина з великим степенем вершини, яку також називають важкою вершиною, призводить до того, що в матриці Лапласа з’являється великий діагональний елемент, який домінує у властивостях матриці. Нормалізація спрямована на те, щоб зробити вплив таких вершин більш рівним впливу інших вершин, шляхом ділення елементів матриці Лапласа на степені вершин. Щоб уникнути ділення на нуль, ізольовані вершини з нульовими степенями виключаються з процесу нормалізації.\n\nprint(\"Алгебраїчна зв'язність для карате-клубу: \", nx.algebraic_connectivity(G_karate, normalized=True, method='tracemin_lu'))\n\nprint(\"Алгебраїчна зв'язність для роману: \", nx.algebraic_connectivity(G_novel, normalized=True, method='tracemin_lu'))\n\nprint(\"Алгебраїчна зв'язність для жінок: \", nx.algebraic_connectivity(G_woman, normalized=True, method='tracemin_lu'))\n\nАлгебраїчна зв'язність для карате-клубу:  0.11007419200657868\nАлгебраїчна зв'язність для роману:  0.06737737553000267\nАлгебраїчна зв'язність для жінок:  0.20797214796917626\n\n\nМожемо бачити, що найбільш зв’язним у даному випадку представляється саме граф жінок. Тобто спілкування та кооперація між ними залишається найбільш тісною.\nGraph energy (енергія графу) — це сума абсолютних значень власних значень матриці суміжності графу. Нехай \\(G\\) є граф з \\(n\\) вершинами. Передбачається, що \\(G\\) — простий, тобто він не містить петлі чи паралельних ребер. Нехай \\(A\\) — матриця суміжності графу \\(G\\) і \\(\\lambda_i\\), \\(i=1,...,n\\) — власні значення матриці \\(A\\). Тоді енергія графу визначається як:\n\\[\nE(G) = \\sum_{i=1}^{n}\\left| \\lambda_i \\right|.\n\\]\nВбудованого методу в NetworkX для визначення енергії графу немає, але ми доволі запросто можемо розрахувати спектр власних значень матриці суміжності, а потім скористатися формулою вище. Власні значення матриці \\(A\\) можна знайти за допомогою методу adjacency_spectrum(). Далі визначимо наступну функцію для розрахунку енергії графу:\n\ndef graph_energy(G): \n\n    adj_spectrum = nx.adjacency_spectrum(G) # спектр власних значень матриці суміжності\n    graph_en = np.sum(np.abs(adj_spectrum))\n\n    return graph_en\n\nТепер розрахуємо енергію для кожного досліджуваного графа:\n\nprint(\"Енергія графу карате-клубу: \", graph_energy(G_karate))\n\nprint(\"Енергія графу роману: \", graph_energy(G_novel))\n\nprint(\"Енергія графу жінок: \", graph_energy(G_woman))\n\nЕнергія графу карате-клубу:  153.22817810462595\nЕнергія графу роману:  460.4651813130984\nЕнергія графу жінок:  51.820121985616545\n\n\nНайвище значення енергії графу вказує на найвищу складність мережі або на найвищий ступінь централізованості деяких вузлів. Для наших графів видно, що найвища енергія приходить саме граф персонажів роману. Тобто, тут є декілька персонажів на які приходить найбільша кількість зв’язків (діалогів) у порівнянні з іншими персонажами.\nСпектральний розрив (spectral gap) — різниця між найбільшим і другим за величиною власного значення, надає інформацію про те, як швидко досягається синхронний стан. Можемо визначити й прорахувати наступну функцію:\n\ndef spectral_gap(G):\n    adj_spectrum = nx.adjacency_spectrum(G)\n    sorted_adj_spectrum = np.sort(adj_spectrum.real)\n    spec_gap = sorted_adj_spectrum[-1] - sorted_adj_spectrum[-2]\n\n    return spec_gap\n\nprint(\"Спектральний розрив для карате-клубу: \", spectral_gap(G_karate))\n\nprint(\"Спектральний розрив для роману: \", spectral_gap(G_novel))\n\nprint(\"Спектральний розрив для жінок: \", spectral_gap(G_woman))\n\nСпектральний розрив для карате-клубу:  4.5812458234061815\nСпектральний розрив для роману:  16.25810678675367\nСпектральний розрив для жінок:  2.3618098280049002\n\n\nСпектральний радіус є найбільшим за модулем власним значенням:\n\\[\nr(A) = \\max_{\\lambda \\in Spec(A)} \\left| \\lambda \\right|,\n\\]\nде \\(Spec(A)\\) — спектр власних значень матриці суміжності. Для розрахунків визначимо наступну функцію:\n\ndef spectral_radius(G): \n    adj_spectrum = nx.adjacency_spectrum(G).real\n    spec_rad = np.max(np.abs(adj_spectrum))\n\n    return spec_rad\n\nprint(\"Спектральний радіус для карате-клубу: \", spectral_radius(G_karate))\n\nprint(\"Спектральний радіус для роману: \", spectral_radius(G_novel))\n\nprint(\"Спектральний радіус для жінок: \", spectral_radius(G_woman))\n\nСпектральний радіус для карате-клубу:  21.68756590395423\nСпектральний радіус для роману:  65.02628035526055\nСпектральний радіус для жінок:  6.741908124910328\n\n\nСпектральний момент. Для визначення \\(k\\)-ого спектрального моменту використовують матрицю суміжності. Визначимо її наступним чином:\n\\[\nm_k(A)=\\frac{1}{n}\\sum_{i=1}^{n}\\lambda_{i}^{k},\n\\]\nде \\(\\lambda_i\\) — власні значення матриці суміжності \\(A\\), \\(n\\) — вершини графу \\(G\\). Значення \\(k\\) у нашому випадку випадку буде дорівнювати 3. Тобто, будемо обчислювати спектральний момент 3-го порядку. Визначимо наступну функцію для розрахунку даного показника:\n\ndef spectral_moment(G):\n\n    adj_spectrum = nx.adjacency_spectrum(G).real\n    spec_mom_3 = np.mean(adj_spectrum ** 3)\n\n    return spec_mom_3\n\nprint(\"Спектральний момент для карате-клубу: \", spectral_moment(G_karate))\n\nprint(\"Спектральний момент для роману: \", spectral_moment(G_novel))\n\nprint(\"Спектральний момент для жінок: \", spectral_moment(G_woman))\n\nСпектральний момент для карате-клубу:  321.3529411764726\nСпектральний момент для роману:  4325.688311688315\nСпектральний момент для жінок:  9.703349235223868e-14\n\n\nОстанні показники говорять по те, що персонажі роману характеризуються найвищим ступенем складності в порівнянні з іншими графами. Ми показали, що достатня кількість вузлів має досить невисокий найкоротший шлях, але може мати гіршу щільність зв’язності вузлів або рівнорозподіленності ступеня впливовості.\n\n\n13.1.5.10 Проблема малого світу\nУ 1967 році соціальні психологи Джеффрі Треверс і Стенлі Мілґрем надіслали листи групам людей у Вічіті, штат Канзас, та Омасі, штат Небраска. Вони також обрали одну цільову особу в штаті Массачусетс. Кожному отримувачу листа було доручено переслати його знайомому, який, найімовірніше, знав цільову людину. Багато листів дійшли до адресата, і дослідники змогли з’ясувати, скільки кроків було зроблено для цього. Середня кількість кроків становила шість, звідси і поширена фраза “шість ступенів відокремлення”.\n\n\n13.1.5.11 Кільцеві мережі\nЯк правило, більшість знайомих людини — це люди, які живуть у тій самій місцевості. Якби кожна людина була знайома лише з тими, хто живе поруч, то можна було б очікувати, що для того, щоб надіслати повідомлення з Канзасу до Массачусетсу, знадобилося б більше шести стрибків, оскільки кожен стрибок міг би подолати лише невелику відстань. Таку мережу можна змоделювати як кільце: вузли, розташовані по колу, причому кожен вузол з’єднаний з найближчими \\(k/2\\) вузлами з кожного боку. Наступний приклад створює та візуалізує чотирикільце за допомогою функції watts_strogatz_graph() про яку ми ще поговоримо.\n\nG_small_ring = nx.watts_strogatz_graph(16, 4, 0)\npos = nx.circular_layout(G_small_ring)\nnx.draw_networkx(G_small_ring, pos=pos, with_labels=False)\n\n\n\n\nЩоб з’єднати два вузли в попередньому прикладі, потрібно пройти по краю кола, пропускаючи щонайбільше кожен другий вузол. Навіть у цій дуже маленькій мережі типова мережева відстань є досить великою порівняно з шістьма градусами, які знайшли Треверс і Мілграм.\nНаступний код знаходить середній найкоротший шлях і середню кластеризацію в більш реалістичному 10-кільці з 4000 вузлів:\n\nG_ring = nx.watts_strogatz_graph(4000, 10, 0)\nnx.average_shortest_path_length(G_ring)\n\n200.45011252813202\n\n\n\nnx.average_clustering(G_ring)\n\n0.6666666666666546\n\n\nЦя мережа має в середньому 200 кроків розділення, що набагато більше, ніж шість! Вона також має досить великий середній коефіцієнт кластеризації 0.67, що показує, що сусіди вузла мають тенденцію бути пов’язаними один з одним.\n\n\n13.1.5.12 Випадкові мережі\nЩоб дослідити цю таємницю, розглянемо інший тип мережі. У цій мережі ми починаємо з \\(k\\)-кільця, але випадковим чином переставляємо кінцеві точки кожного ребра. В результаті отримаємо мережу з тією ж кількістю вузлів і ребер, але з випадковою структурою, що демонструється наступним чином:\n\nG_small_random = nx.watts_strogatz_graph(16, 4, 1)\npos = nx.circular_layout(G_small_random)\nnx.draw_networkx(G_small_random, pos=pos, with_labels=False)\n\n\n\n\nТепер давайте розглянемо властивості перев’язаного 10-кільця з 4000 вузлів:\n\nG_random = nx.watts_strogatz_graph(4000, 10, 1)\nnx.average_shortest_path_length(G_random)\n\n3.866869092273068\n\n\n\nnx.average_clustering(G_random)\n\n0.0023133463049678235\n\n\nСередній найкоротший шлях дуже близький до реальної соціальної мережі, але середня кластеризація тепер майже 0. Поки що моделі, які ми бачили, досягають коротких шляхів або високої кластеризації, але не того й іншого разом.\n\n\n13.1.5.13 Мережа Воттса-Строгаца\nПроблема малого світу полягає в тому, як люди, що живуть на великій відстані один від одного, можуть бути пов’язані короткими шляхами, навіть якщо їхні зв’язки зв’язки є локальними. Дункан Воттс і Стівен Строгац розробили клас мереж для пояснення такої поведінки. Мережі починаються як \\(k\\)-кільця: вузли, розміщені по колу, кожен з яких з’єднаний з найближчими \\(k\\) сусідами. Потім, з ймовірністю \\(p\\), ребра кожного вузла перев’язуються з іншим випадково обраним вузлом. Ці перестановки створюють короткі шляхи по всій мережі. Навіть невелика кількість коротких шляхів значно скорочує відстані між вузлами мережі, вирішуючи проблему малого світу. Фактично, це саме те, що робить функція watts_strogatz_graph(), яку ми використовували, а третій параметр задає частку ребер, які потрібно перев’язати. Наступний код обчислює середній найкоротший шлях і середню кластеризацію для діапазону ймовірностей перев’язування:\n\npath = []\nclustering = []\n# Пробуємо список імовірностей перев'язування\np = [10**(x) for x in range(-6, 1)]\nfor p_i in p:\n    path_i = []\n    clustering_i =[]\n    # Створюємо 10 моделей для кожної ймовірності\n    for n in range(10):\n        G = nx.watts_strogatz_graph(1000, 10, p_i)\n        path_i.append(nx.average_shortest_path_length(G))\n        clustering_i.append(nx.average_clustering(G))\n    # Усереднюємо показники для кожного значення p_i\n    path.append(sum(path_i) / len(path_i))\n    clustering.append(sum(clustering_i) / len(clustering_i))\n\nРезультати наступного коду зберігаються у списках path та clustering. Використовуючи функцію semilogx() з matplotlib.pyplot, наступний код візуалізує, як ці значення змінюються при зміні ймовірності перев’язування від 0 до 1:\n\nplt.figure()\nfor spine in ax.spines.values():\n    spine.set_visible(True)\nplt.semilogx(p, [x / path[0] for x in path], label=r'$L_{mean} / L_0$')\nplt.semilogx(p, [x / clustering[0] for x in clustering], label=r'$C_{mean} / C_0$')\nplt.tick_params(axis='both', which='major', labelsize=16)\nplt.xlabel('Ймовірність перев\\'язування p', fontsize=16)\nplt.legend(fontsize=16)\nplt.show();\n\nAttributeError: 'numpy.ndarray' object has no attribute 'spines'\n\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\nУведомление\n\n\n\nФункція semilogx() використовує логарифмічний масштаб для осі x. Часто в мережевих науках значення змінюються так швидко, що візуалізація їх на лінійній шкалі призводить до того, що початкове значення миттєво падає до кінцевого. Функція semilogx() може виявити поведінку, яка може бути втрачена при використанні лінійної осі. Пакет pyplot також пропонує функції semilogy() і loglog() для використання інших логарифмічних осей.\n\n\nЯк ми вже бачили, зі збільшенням кількості перев’язувань, як середня кластеризація, так і середній найкоротший шлях зменшуються. Однак цікава річ відбувається при проміжних значеннях. Довжина шляху стає коротшою при дуже низьких значеннях перев’язування, в той час як зменшення кластеризації відбувається лише при більших значеннях перев’язування. Іншими словами, перев’язування дуже малої частки ребер створює “мости”, які з’єднують віддалені частини мережі і різко скорочують середній найкоротший шлях, не змінюючи при цьому кластеризацію. Можна сказати, що найкращий тип мереж це той, що зберігає як частку впорядкованості, так і частку випадковості.\nДалі можемо подивитись, як виглядає мережа Воттса й Строгаца при наступних імовірностях: \\(p=0\\), \\(p=0.1\\) та \\(p=1\\).\n\nplt.figure(figsize=(12, 5))\nfor i, p in enumerate([0.0, 0.1, 1.0]):\n    # Генеруємо граф\n    G = nx.watts_strogatz_graph(12, 6, p)\n    # Будуємо рисунок\n    plt.subplot(1, 3, i + 1)\n    pos = nx.circular_layout(G)\n    nx.draw_networkx(G, pos=pos)\n    plt.title(\"p = {:0.1f}\".format(p))\n    \n\n\n\n\nУ деяких випадках перев’язування може призвести до того, що дві компоненти в мережі Воттса-Строгаца будуть роз’єднані. Роз’єднана мережа може бути непотрібним ускладненням. Мережа Ньюмана-Воттса-Строгаца — це варіант, який гарантує, що отримана мережа буде зв’язною. Вона схожа на оригінальну версію, але залишає копію оригінального ребра на місці кожного ребра, що перев’язується. Такі мережі можна створювати за допомогою функції newman_watts_strogatz_graph(), як показано нижче:\n\nplt.figure(figsize=(12, 5))\nfor i, p in enumerate([0.0, 0.1, 1.0]):\n    \n    G = nx.newman_watts_strogatz_graph(12, 6, p)\n\n    plt.subplot(1, 3, i + 1)\n    pos = nx.circular_layout(G)\n    nx.draw_networkx(G, pos=pos)\n    plt.title(\"p = {:0.1f}\".format(p))\n\n\n\n\n\n\n13.1.5.14 Степеневі закони та переважне приєднання\nВід інтернету до поїздок в аеропорт, багато мереж характеризуються кількома вузлами з великою кількістю зв’язків і багатьма вузлами з дуже малою кількістю зв’язків. Такі мережі характеризуються важкими хвостами, тому що при побудові гістограми степенів вузлів, вузли з високим рівнем зв’язності утворюють хвіст.\nІснує багато способів генерування мереж з важким хвостом, але одним з найпоширеніших є модель переважного приєднання Барабаші-Альберта. Модель переважного приєднання імітує процеси, в яких багаті стають багатшими. Кожного разу, коли додається новий вузол, він випадковим чином з’єднується з існуючими вузлами, причому більш вірогідним є з’єднання з вузлами високого ступеня.\nУ NetworkX функція barabasi_albert_graph(), яка генерує мережі переважного приєднання. У наступному коді показано приклад такої мережі з 35 вузлами:\n\nG_preferential_35 = nx.barabasi_albert_graph(35, 1)\npos = nx.spring_layout(G_preferential_35, k=0.1)\nnx.draw_networkx(G_preferential_35, pos)\n\n\n\n\nСтруктура мережі переважного приєднання ще більш очевидна при більшій кількістю вузлів. У наступному прикладі використовується 1000 вузлів:\n\nG_preferential_1000 = nx.barabasi_albert_graph(1000, 1)\npos = nx.spring_layout(G_preferential_1000)\nnx.draw_networkx(G_preferential_1000, pos, node_size=0, with_labels=False)\n\n\n\n\nВажкі хвости цих мереж можна побачити, побудувавши їхні степеневі розподіли. Наступна функція будує розподіл степенів мережі:\n\ndef plot_degree_hist(G, title):\n    \"\"\"Функція для побудови розподілу степенів вершин мережі\"\"\"\n    plt.hist(dict(nx.degree(G)).values(), bins=range(1, 11))\n    plt.xlabel('Степінь')\n    plt.ylabel('Щільність')\n    plt.title(title)\n\nВикористовуючи цю функцію, наступний код візуалізує розподіл степенів для 35-вузлової та 1000-вузлових мереж переважного приєднання:\n\nplt.figure(figsize=(10, 5))\nax = plt.subplot(1,2,1)\nplot_degree_hist(G_preferential_35, '35 вузлів')\nfor spine in ax.spines.values():\n    spine.set_visible(True)\nax = plt.subplot(1,2,2)\nfor spine in ax.spines.values():\n    spine.set_visible(True)\nplot_degree_hist(G_preferential_1000, '1000 вузлів')\nplt.tight_layout()\nplt.show()\n\n\n\n\nМережі з переважним приєднанням мають одну цікаву властивість: вони масштабоінваріантні. Розподіл степенів у масштабоінваріантних мережах підпорядковується степеневому закону, що призводить до схожої структури на різних масштабах. Один із способів побачити це — порівняти попередні гістограми. Незважаючи на дуже різні масштаби, вони мають схожу форму. Розподіл степенів вершин можна описати степеневою функцією виду:\n\\[P(k) \\propto k^{-\\gamma}\\]\nде \\(k\\) — степінь вузла, \\(P(k)\\) — ймовірність того, що вузол має степінь \\(k\\), і \\(\\gamma\\) — показник степеневого закону. Показник \\(\\gamma\\) зазвичай знаходиться в діапазоні від 2 до 3 для більшості реальних мереж.\nРозподіл степенів степеневого закону має важливі наслідки для структури та функцій мереж. Наприклад, мережі зі степеневим розподілом часто є більш надійними і стійкими до випадкових збоїв, але більш вразливими до цілеспрямованих атак на вузли з високим степенем."
  },
  {
    "objectID": "lab_13.html#хід-роботи",
    "href": "lab_13.html#хід-роботи",
    "title": "13  Лабораторна робота № 13",
    "section": "13.2 Хід роботи",
    "text": "13.2 Хід роботи\nТепер давайте проведемо порівняльний аналіз графів різної складності з використанням деяких із зазначених показників. За допомогою бібліотеки NetworkX розглянемо наступні типи графів:\n\nлінійний граф — path_graph();\nциклічний граф — cycle_graph();\nграф-зірка — star_graph();\nграф Ердеша-Реньї — erdos_renyi_graph();\nграф малого світу — watts_strogatz_graph();\nграф переважного приєднання — barabasi_albert_graph().\n\nВізуалізуємо кожен із зазначених графів:\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 12))\n\n# лінія \naxes[0, 0].set_title('Лінія')\nline_graph = nx.path_graph(100)\npos_line_graph = nx.spring_layout(line_graph, k=0.15, iterations=100)\nnx.draw_networkx(line_graph, pos=pos_line_graph, node_size=10, with_labels=False, ax=axes[0, 0])\n\n# коло \naxes[0, 1].set_title('Коло')\ncycle_graph = nx.cycle_graph(100)\npos_cycle_graph = nx.circular_layout(cycle_graph)\nnx.draw_networkx(cycle_graph, pos=pos_cycle_graph, node_size=10, with_labels=False, ax=axes[0, 1])\n\n# зірка\naxes[1, 0].set_title('Зірка')\nstar_graph = nx.star_graph(100)\npos_star_graph = nx.spring_layout(star_graph, k=0.15, iterations=100)\nnx.draw_networkx(star_graph, pos=pos_star_graph, node_size=10, with_labels=False, ax=axes[1, 0])\n\n# Ердеша-Реньї\naxes[1, 1].set_title('Ердеш-Реньї')\nerdos_renyi_graph = nx.erdos_renyi_graph(100, 0.01)\npos_erdos_renyi_graph = nx.circular_layout(erdos_renyi_graph)\nnx.draw_networkx(erdos_renyi_graph, pos=pos_erdos_renyi_graph, node_size=10, with_labels=False, ax=axes[1, 1])\n\n# Малий світ \naxes[2, 0].set_title('Малий світ')\nsmall_world_graph = nx.watts_strogatz_graph(100, 30, 0.01, seed=32)\npos_small_world_graph = nx.spring_layout(small_world_graph, k=0.15, iterations=100)\nnx.draw_networkx(small_world_graph, pos=pos_small_world_graph, node_size=10, with_labels=False, ax=axes[2, 0])\n\n# Переважне приєднання \naxes[2, 1].set_title('Переважне приєднання')\nbarabasi_albert_graph = nx.barabasi_albert_graph(100, 30, seed=32)\npos_barabasi_albert_graph = nx.spring_layout(barabasi_albert_graph, k=0.15, iterations=100)\nnx.draw_networkx(barabasi_albert_graph, pos=pos_barabasi_albert_graph, node_size=10, with_labels=False, ax=axes[2, 1])\n\nplt.show();\n\n\n\n\nВізуалізація для подальшого аналізу графів\n\n\n\n\nКожен із даних графів може різнитись як за своєю спектральною структурою, так і, очевидно, топологічною: деякі можуть мати вищий ступінь кластеризації, або ступеня вершини, або посередництва тощо. Розглянемо як ранжується ступінь складності кожного графа за досліджуваними нами показниками.\nСпочатку збережемо кожен із побудованих графів до одного масиву для ітеративного проведення розрахунків по кожному з них:\n\ngraphs = [line_graph, cycle_graph, star_graph, erdos_renyi_graph, small_world_graph, barabasi_albert_graph] # графи\nlabels = ['Лінія', 'Коло', 'Зірка', 'Ердеш-Реньї', 'Малий світ', 'Переважне приєднання']                    # їх мітки\ncolors = ['b', 'purple', 'red', 'green', 'pink', 'black']\nlinestyles = ['-', '-', '--', '--', ':', '-']\nmarkers = ['d', 'v', '*', 's', 'H', 'o']\n\n\n13.2.1 Спектральні міри складності\nТепер виконаємо розрахунки спектральних мір складності для кожного графу:\n\nalgebraic_connect_vals = np.zeros(6) \nenergy_vals = np.zeros(6)\nspec_gap_vals = np.zeros(6)\nspec_mom_vals = np.zeros(6)\n\nfor i, graph in enumerate(graphs):\n    algebraic_connect_vals[i] = nx.algebraic_connectivity(graph, normalized=False, method='tracemin_lu')\n    energy_vals[i] = graph_energy(graph)\n    spec_gap_vals[i] = spectral_gap(graph)\n    spec_mom_vals[i] = spectral_moment(graph)\n\nnum_rep = 30\nalgebraic_connect_vals = np.repeat(algebraic_connect_vals[:, np.newaxis], num_rep, axis=1)\nenergy_vals = np.repeat(energy_vals[:, np.newaxis], num_rep, axis=1)\nspec_gap_vals = np.repeat(spec_gap_vals[:, np.newaxis], num_rep, axis=1)\nspec_mom_vals = np.repeat(spec_mom_vals[:, np.newaxis], num_rep, axis=1)\n\nВиведемо результат:\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 8))\n\n# Зв'язність\naxes[0, 0].set_title('Алгебраїчна зв\\'язність')\nfor i in range(len(graphs)):\n    axes[0, 0].plot(algebraic_connect_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[0, 0].legend(fontsize=10)\n\n\n# Енергія \naxes[0, 1].set_title('Енергія графу')\nfor i in range(len(graphs)):\n    axes[0, 1].plot(energy_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[0, 1].legend(fontsize=10)\n\n# Розрив\naxes[1, 0].set_title('Спектральний розрив')\nfor i in range(len(graphs)):\n    axes[1, 0].plot(spec_gap_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[1, 0].legend(fontsize=10)\n\n# Момент \naxes[1, 1].set_title('Спектральний момент')\nfor i in range(len(graphs)):\n    axes[1, 1].plot(spec_mom_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[1, 1].legend(fontsize=10)\n\nplt.show(); \n\n\n\n\nРис. 13.3: Спектральні властивості канонічних і модельних графів алгебраїчної зв’язності, енергії графу, спектрального розриву та спектрального моменту\n\n\n\n\nНа рисунку (Рис. 13.3) можна побачити наступне:\n\nпо-перше, усі спектральні показники залишаються найбільшими саме для графу переважного приєднання, що представляється найбільш складним серед усіх інших графів;\nпо друге, згідно динаміці спектральних показників, найпростішими серед усіх графів є граф лінії, зірки та Ердеша-Реньї. Для лінії зберігається зв’язок тільки між парами послідовних вершин. Для зірки зберігається зв’язок усіх вершин із центром, але самі вони не пов’язані один із одним.\nпо третє, граф малого світу залишається другим по складності майже по всім показникам окрім спектрального розриву. Спектральний розрив говорить, що граф зірки є трохи складнішим за малий світ. Це може бути обумовлене тим, що для зірки ми спостерігаємо достатньо високий ступінь централізації.\n\n\n\n13.2.2 Топологічні міри\nРозрахуємо для досліджуваних графів топологічні міри складності. В якості прикладу розглянемо такі міри як\n\nмаксимальний ступінь вершини (\\(d_{max}\\));\nглобальний коефіцієнт кластеризації (\\(C\\));\nсередній ступінь посередництва (\\(B_{mean}\\));\nсередня довжина найкоротшого шляху (\\(L_{mean}\\)).\n\n\nmax_degree_vals = np.zeros(6) \nglobal_clust_vals = np.zeros(6)\nmean_betweenness_vals = np.zeros(6)\nmean_path_vals = np.zeros(6)\n\nfor i, graph in enumerate(graphs):\n    max_degree_vals[i] = max(dict(graph.degree()).values())\n    global_clust_vals[i] = nx.average_clustering(graph)\n    mean_betweenness_vals[i] = np.mean(list(nx.betweenness_centrality(graph).values()))\n    mean_path_vals[i] = np.mean([nx.average_shortest_path_length(C) for C in \n                                 (graph.subgraph(c).copy() for c in nx.connected_components(graph))])\n\nnum_rep = 30\nmax_degree_vals = np.repeat(max_degree_vals[:, np.newaxis], num_rep, axis=1)\nglobal_clust_vals = np.repeat(global_clust_vals[:, np.newaxis], num_rep, axis=1)\nmean_betweenness_vals = np.repeat(mean_betweenness_vals[:, np.newaxis], num_rep, axis=1)\nmean_path_vals = np.repeat(mean_path_vals[:, np.newaxis], num_rep, axis=1)\n\nВиводимо результат:\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 8))\n\naxes[0, 0].set_title('Макс. ступінь вершини')\n\nfor i in range(len(graphs)):\n    axes[0, 0].plot(max_degree_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[0, 0].legend(fontsize=10)\n\n# Енергія \naxes[0, 1].set_title('Глобальний коефіцієнт кластеризації')\nfor i in range(len(graphs)):\n    axes[0, 1].plot(global_clust_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[0, 1].legend(fontsize=10)\n\n# Розрив\naxes[1, 0].set_title('Середній ступінь посередництва')\nfor i in range(len(graphs)):\n    axes[1, 0].plot(mean_betweenness_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[1, 0].legend(fontsize=10)\n\n# Момент \naxes[1, 1].set_title('Середня довжина найкоротшого шляху')\nfor i in range(len(graphs)):\n    axes[1, 1].plot(mean_path_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[1, 1].legend(fontsize=10)\n\nplt.show(); \n\n\n\n\nРис. 13.4: Топологічні міри складності канонічних і модельних графів максимального значення ступеня вершини, глобального коефіцієнту кластеризації, середнього ступеня посередництва та середньої довжини найкоротшого шляху\n\n\n\n\nНа рисунку (Рис. 13.4) можна побачити наступне:\n\nпо-перше, найбільшим максимальним ступенем вершини характеризується саме граф-зірка, центро якої з’єднаний абсолютно з усіма вершинами мережі. Другим по ступеню концентрованності можна поставити граф переважного приєднання, що, як ми вже зазначали, є найкращим представлення реальних соціальних систем. До найпростіших можна віднести графи лінії, кола та Ердеша-Реньї.\nпо-друге, глобальний коефіцієнт кластеризації вказує на те, що найвищий ступінь кластеризації спостерігається саме для графу малого світу. Закономірно за ним іде граф переважного приєднання. Найпростішими знову виявляються графи Ердеша-Реньї, лінії, кола та, цього разу, зірки. Для зірки навіть візуально видно, що всі вершини мають тенденцію слідувати тільки за однією конкретною.\nпо-третє, середній ступінь посередництва є найнижчим для зірки, графу Ердеша-Реньї, малого світу та переважного приєднання. Для цих мереж передача інформації від одного вузла до іншого не займає значну частку часу. Для лінії та кола від одного кінця графу до іншого може знадобитися досить великий проміжок часу для передачі інформації. Схожа ситуація спостерігається й для середньої довжини найкоротшого шляху, оскільки міра посередництва на пряму залежить від значення найкоротшого шляху від одного вузла до іншого."
  },
  {
    "objectID": "appa.html",
    "href": "appa.html",
    "title": "Додаток A — Інструкція зі встановлення Anaconda Navigator",
    "section": "",
    "text": "Відвідайте сторінку Anaconda за наступним посиланням. Перейшовши, ви маєте побачити наступне зображення:\n\n\n\n\n\n\n\nНатискаємо на кнопку Download:\n\n\n\n\n\n\n\nЯкщо ви набиратимете назву anaconda у пошуковому рядку, тоді потрібно буде перейти за наступним посиланням, що має з’явитися найпершим:\n\n\n\n\n\n\nПерейшовши по виділеному посиланню, ви маєте побачити наступну сторінку:\n\n\n\n\n\nТак само як і в пункті 2, потрібно натиснути на кнопку Download, щоб розпочати встановлення.\n\nЗ’явиться вікно наступного виду, що запропонує зберегти встановлювальний файл там, де це потрібно:\n\n\n\n\n\n\n\nНатискаємо на клавішу Next:\n\n\n\n\n\n\n\nУважно читаємо ліцензійну угоду та натискаємо клавішу I Agree, якщо угода вас задовольняє або вам потрібно скласти залік/іспит:\n\n\n\n\n\n\n\nДалі, якщо ви не перебуваєте у команді розробників, і ви єдина людина, хто буде користуватися Anaconda — Just me ваш вибір:\n\n\n\n\n\n\nНатискаємо Next.\n\nНа наступному кроці нам пропонується обрати папку, до якої буде встановлено дистрибутив Anaconda. Бажано, щоб шлях до папки не містив кирилиці. Можна залишити шлях за замовчуванням. Ми натиснемо на Browse. Далі, обираємо зручний для нас диск та натискаємо Создать папку. Таким чином ми створимо в нашому диску папку з назвою anaconda до якої і буде завантажено Anaconda.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nНаступним кроком буде обрання середовища змінних (the environment variables).\nЯкщо ви встановлюєте Python вперше, відмітимо Add Anaconda to my PATH environment variable. Окрім всього, це дасть вам можливість використовувати Anaconda в командному рядку (або Git bash, cmder, powershell і т.д.).\nЯкщо ви вже маєте Python на своєму комп’ютері, тоді прапорець не варто відмічати. Ви матимете запускати Anaconda Navigator або Anaconda Command Prompt (розташований в меню Пуск, у розділі Anaconda) коли вам потрібно буде запускати Anaconda (ви завжди матимете можливість додати Anaconda до свого шляху пізніше, якщо не встановите цей прапорець).\nНа представленій локальній машині нам не треба відмічати прапорець.\n\n\n\n\n\n\n\nРекомендований підхід\n\n\n\n\n\n\n\nАльтернативний\n\n\n\n\n\n\nНатискаємо Install і чекаємо завершення:\n\n\n\n\n\n\n\n\n\n\n\n\nНатискаємо Next аж до вікна з подякою за встановлення Anaconda і після натискаємо Finish:\n\n\n\n\n\n\n\nНа наступному кроці потрібно перейти в меню Пуск і знайти папку під назвою Anaconda:\n\n\n\n\n\n\n\nРозгорнувши папку можна побачити цікаві для нас іконки:\n\n\n\n\n\n\nНа зазначеній вище фотографії можна бачити декілька ярликів Anaconda Navigator, Anaconda Powershell Promt, Anaconda Promt та Jupyter Notebook з різними найменуваннями anaconda та anaconda3. Це тому, що на представленій локальній машині попередньо було встановлено Anaconda, але на іншому локальному диску. Якщо ви встановлюєте Anaconda вперше чи перевстановлюєте, тоді ви матимете у 2 рази менше ярликів.\n\nНатиснувши на Anaconda Navigator, потрапляємо до середовища, що пропонує різноманітні інструменти для аналізу даних. Для подальшої роботи нам знадобиться лише Jupyter Notebook:\n\n\n\n\n\n\nНатискаємо на Launch та потрапляємо до середовища Jupyter (кореневої папки до якої було встановлено anaconda):\n\n\n\n\n\nУ верхньому лівому кутку буде знаходитись значок New. Спочатку натискаємо на нього і потім на Python 3. Має створитися відповідний Notebook у якому можна писати код:\n\n\nОкрім цього, в меню пуск можна натиснути на значок Jupyter Notebook та одразу перейти до роботи:\n\n\n\n\n\n\n\nНатиснувши на Anaconda Powershell Promt або Anaconda Promt можна перейти до командного рядка представленого дистрибутивом Anaconda. З його допомогою можна докачати необхідні модулі або запустити необхідний інструмент, що представляє Anaconda. Запустимо Jupyter Notebook за допомогою команди jupyter notebook:"
  },
  {
    "objectID": "appb.html#коментарі-коду",
    "href": "appb.html#коментарі-коду",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.1 Коментарі коду",
    "text": "B.1 Коментарі коду\nКоментар — це примітка, зроблена програмістом у вихідному коді програми. Його мета — прояснити вихідний код і полегшити людям відстеження того, що відбувається. Все, що міститься в коментарі, зазвичай ігнорується при фактичному запуску коду, що робить коментарі корисними для включення пояснень і міркувань, а також для видалення певних рядків коду, в яких ви можете бути не впевнені. Коментарі в Python створюються за допомогою символу решітуи (# вставити текст тут). Включення # у рядок коду коментує все, що слідує за ним.\n\n# print(\"Привіт усім\")\n# Це коментар\n# Ці рядки коду не змінять жодних значень\n# все, що слідує за першим #, не виконується як код\n\nВи можете побачити текст, укладений у потрійні лапки (\"\"\" вставте текст тут \"\"\"). Такий синтаксис представлятиме багаторядкове коментування, але це не зовсім точно. Це особливий тип string (тип даних, який ми розглянемо), який називаєтьсяdocstring, який використовується для пояснення призначення функції.\n\n\"\"\" This is a special string \"\"\"\n\n' This is a special string '\n\n\nПереконайтеся, що Ви прочитали коментарі в кожній комірці коду (якщо вони там є). Вони нададуть більш детальні пояснення того, що відбувається в режимі реального часу, коли ви переглядаєте кожен рядок коду."
  },
  {
    "objectID": "appb.html#змінна",
    "href": "appb.html#змінна",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.2 Змінна",
    "text": "B.2 Змінна\nЗмінні надають імена для значень. Якщо ви хочете зберегти значення для подальшого або повторного використання, ви присвоюєте значенню ім’я, зберігаючи вміст у змінній. Змінні в програмуванні працюють принципово аналогічно змінним в алгебрі, але в Python вони можуть приймати різні типи даних.\nОсновними типами змінних, які ми розглянемо в цьому розділі, є цілі числа, числа з плаваючою комою, логічні значення та рядки.\nЦіле число у програмуванні - це те саме, що і в математиці, кругле число без значень після десяткової коми. Ми використовуємо вбудовану функцію print() тут для відображення значень наших змінних, а також їх типів!\n\nmy_integer = 50\nprint(my_integer, type(my_integer))\n\n50 &lt;class 'int'&gt;\n\n\nЗмінні, незалежно від типу, призначаються за допомогою одного знака рівності (=). Змінні чутливі до регістру, тому будь-які зміни в зміні заголовних літер імені змінної будуть посилатися повністю на іншу змінну.\n\none = 1\nprint(one)\n\n1\n\n\nЧисло з плаваючою комою або float - це вигадлива назва дійсного числа (знову ж таки, як у математиці). Щоб визначити float, нам потрібно або включити десяткову крапку, або вказати, що значення є float.\n\nmy_float = 1.0\nprint(my_float, type(my_float))\nmy_float = float(1)\nprint(my_float, type(my_float))\n\n1.0 &lt;class 'float'&gt;\n1.0 &lt;class 'float'&gt;\n\n\nЗмінна типу float не округлятиме число, яке ви в ній зберігаєте, тоді як змінна типу integer округлятиме. Це робить floats більш придатними для математичних обчислень, де потрібно більше, ніж просто цілі числа.\nЗверніть увагу, що оскільки ми використовували функцію float(), щоб змусити число рахуватися float, ми можемо використовувати функцію int(), щоб змусити число представлятися в типі int.\n\nmy_int = int(3.14159)\nprint(my_int, type(my_int))\n\n3 &lt;class 'int'&gt;\n\n\nФункція int() також усіче будь-які цифри, які число може містити після десяткової коми!\nРядки дозволяють включати текст як змінну для роботи. Вони визначаються з використанням або одинарних лапок (’’), або подвійних лапок (““).\n\nmy_string = 'This is a string with single quotes'\nprint(my_string)\nmy_string = \"This is a string with double quotes\"\nprint(my_string)\n\nThis is a string with single quotes\nThis is a string with double quotes\n\n\nОбидва варіанти дозволені, так що ми можемо включити апострофи або лапки в рядок, якщо ми того побажаємо.\n\nmy_string = '\"Jabberwocky\", by Lewis Carroll'\nprint(my_string)\nmy_string = \"'Twas brillig, and the slithy toves / Did gyre and gimble in the wabe;\"\nprint(my_string)\n\n\"Jabberwocky\", by Lewis Carroll\n'Twas brillig, and the slithy toves / Did gyre and gimble in the wabe;\n\n\nЛогічні значення, або bools, - це двійкові типи змінних. bool може приймати лише одне з двох значень, це True або False. У цій ідеї істинних значень є набагато більше, коли мова заходить про програмування, про що ми розповімо пізніше в розділі Логічні оператори цього зошита.\n\nmy_bool = True\nprint(my_bool, type(my_bool))\n\nTrue &lt;class 'bool'&gt;\n\n\nІснує ще багато типів даних, які ви можете призначити змінними в Python, але це основні з них! Ми розглянемо ще трохи пізніше, коли будемо просуватися по цьому блокноту."
  },
  {
    "objectID": "appb.html#базова-математика",
    "href": "appb.html#базова-математика",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.3 Базова математика",
    "text": "B.3 Базова математика\nPython має ряд вбудованих математичних функцій. Їх можна ще більше розширити, імпортуючи пакет math або включивши будь-яку кількість інших обчислювальних пакетів.\nПідтримуються всі основні арифметичні операції: +, -, /, і *. Ви можете створювати експоненти за допомогою **, а модульна арифметика вводиться за допомогою оператора mod, %.\n\nprint('Addition: ', 2 + 2)\nprint('Subtraction: ', 7 - 4)\nprint('Multiplication: ', 2 * 5)\nprint('Division: ', 10 / 2)\nprint('Exponentiation: ', 3**2)\n\nAddition:  4\nSubtraction:  3\nMultiplication:  10\nDivision:  5.0\nExponentiation:  9\n\n\nЯкщо ви не знайомі з оператором mod, він працює як функція залишку. Якщо ми введемо \\(15 \\ \\% \\  4\\), він поверне залишок після ділення \\(15\\) на \\(4\\).\n\nprint('Частка: ', 15 % 4)\n\nЧастка:  3\n\n\nМатематичні функції також працюють зі змінними!\n\nfirst_integer = 4\nsecond_integer = 5\nprint(first_integer * second_integer)\n\n20\n\n\nПереконайтеся, що ваші змінні є плаваючими, якщо ви хочете, щоб у вашій відповіді були десяткові крапки. Якщо ви виконуєте математику виключно з цілими числами, Ви отримуєте ціле число. Включення будь-якого значення з плаваючою точкою в обчислення зробить результат плаваючим.\n\nfirst_integer = 11\nsecond_integer = 3\nprint(first_integer / second_integer)\n\n3.6666666666666665\n\n\n\nfirst_number = 11.0\nsecond_number = 3.0\nprint(first_number / second_number)\n\n3.6666666666666665\n\n\nPython має кілька вбудованих математичних функцій. Найбільш помітними з них є:\n\nabs()\nround()\nmax()\nmin()\nsum()\n\nУсі ці функції діють так, як ви очікували, враховуючи їх назви. Виклик abs() для числа поверне його абсолютне значення. Функція round() округлить число до вказаної кількості десяткових знаків (значення за замовчуванням дорівнює \\(0\\)). Виклик max() або min() для набору чисел поверне, відповідно, максимальне або мінімальне значення в наборі. Виклик sum() для набору чисел призведе до їх підсумовування. Якщо ви не знайомі з тим, як працюють колекції значень у Python, не хвилюйтеся! Ми детально розглянемо набір в наступному розділі.\nДодаткові математичні функції можуть бути додані разом з пакетом math.\n\nimport math\n\nМатематична бібліотека додає довгий список нових математичних функцій до Python. Не соромтеся ознайомитися з документацією для отримання повного списку та деталей. У ньому полягають деякі математичні константи\n\nprint('Pi: ', math.pi)\nprint(\"Euler's Constant: \", math.e)\n\nPi:  3.141592653589793\nEuler's Constant:  2.718281828459045\n\n\nА також деякі часто використовувані математичні функції\n\nprint('Cosine of pi: ', math.cos(math.pi))\n\nCosine of pi:  -1.0"
  },
  {
    "objectID": "appb.html#колекції",
    "href": "appb.html#колекції",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.4 Колекції",
    "text": "B.4 Колекції\n\nB.4.1 Списки (lists)\nСписок у Python - це впорядкована колекція об’єктів, яка може містити будь-який тип даних. Ми визначаємо список, використовуючи квадратні дужки ([]).\n\nmy_list = [1, 2, 3]\nprint(my_list)\n\n[1, 2, 3]\n\n\nМи також можемо отримати доступ до списку та проіндексувати його за допомогою дужок. Щоб вибрати окремий елемент, просто введіть назву списку, а потім індекс елемента, який ви шукаєте, у фігурних дужках.\n\nprint(my_list[0])\nprint(my_list[2])\n\n1\n3\n\n\nІндексація в Python починається з $ 0$. Якщо у вас є список довжиною \\(n\\), перший елемент списку знаходиться з індексом \\(0\\), другий елемент з індексом \\(1\\), і так далі, і тому подібне. Останній елемент списку матиме індекс \\(n-1\\). Будьте обережні! Спроба отримати доступ до неіснуючого індексу призведе до помилки.\n\nprint('The first, second, and third list elements: ', my_list[0], my_list[1], my_list[2])\nprint('Accessing outside the list bounds causes an error: ', my_list[3])\n\nThe first, second, and third list elements:  1 2 3\n\n\nIndexError: list index out of range\n\n\nМи можемо побачити кількість елементів у списку, викликавши функцію len().\n\nprint(len(my_list))\n\n3\n\n\nМи можемо оновлювати та змінювати список, отримуючи доступ до індексу та призначаючи нове значення.\n\nprint(my_list)\nmy_list[0] = 42\nprint(my_list)\n\n[1, 2, 3]\n[42, 2, 3]\n\n\nЦе принципово відрізняється від того, як обробляються рядки. Список є змінним, що означає, що ви можете змінювати елементи списку без зміни самого списку. Деякі типи даних, такі як рядки, є незмінними, що означає, що ви взагалі не можете їх змінити. Як тільки рядок або інший незмінний тип даних був створений, він не може бути безпосередньо змінений без створення абсолютно нового об’єкта.\n\nmy_string = \"Strings never change\"\nmy_string[0] = 'Z'\n\nTypeError: 'str' object does not support item assignment\n\n\nЯк ми вже говорили раніше, список може містити будь-який тип даних. Таким чином, списки також можуть містити рядки.\n\nmy_list_2 = ['one', 'two', 'three']\nprint(my_list_2)\n\n['one', 'two', 'three']\n\n\nСписки також можуть містити кілька різних типів даних одночасно!\n\nmy_list_3 = [True, 'False', 42]\n\nЯкщо ви хочете об’єднати два списки, їх можна об’єднати символом +.\n\nmy_list_4 = my_list + my_list_2 + my_list_3\nprint(my_list_4)\n\n[42, 2, 3, 'one', 'two', 'three', True, 'False', 42]\n\n\nОкрім доступу до окремих елементів списку, ми можемо отримати доступ до груп елементів за допомогою зрізу.\n\nmy_list = ['friends', 'romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nB.4.1.1 Зріз (slicing)\nМи використовуємо двокрапку (:) для нарізки списків.\n\nprint(my_list[2:4])\n\n['countrymen', 'lend']\n\n\nВикористовуючи :, ми можемо вибрати групу елементів у списку, починаючи з першого вказаного елемента і закінчуючи (але не включаючи) останнім зазначеним елементом.\nМи також можемо вибрати все після певного значення\n\nprint(my_list[1:])\n\n['romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nІ все перед конкретним значенням\n\nprint(my_list[:4])\n\n['friends', 'romans', 'countrymen', 'lend']\n\n\nВикористання негативних чисел буде відлічуватися з кінця індексів, а не з початку. Наприклад, індекс -1 вказує на останній елемент списку.\n\nprint(my_list[-1])\n\nears\n\n\nВи також можете додати третій компонент для нарізки. Замість того, щоб просто вказати першу та кінцеву частини вашого зрізу, ви можете вказати розмір кроку, який ви хочете зробити. Таким чином, замість того, щоб брати кожен окремий елемент, ви можете взяти будь-який інший елемент.\n\nprint(my_list[0:7:2])\n\n['friends', 'countrymen', 'me', 'ears']\n\n\nТут ми вибрали весь список (оскільки 0:7 дасть елементи від 0 до 6), і ми вибрали розмір кроку 2. Отже, це виведе елемент 0, елемент 2, елемент 4 тощо на вибраний елемент списку. Ми можемо пропустити вказаний початок і кінець нашого фрагмента, вказавши лише крок, якщо хочемо.\n\nprint(my_list[::2])\n\n['friends', 'countrymen', 'me', 'ears']\n\n\nСписки неявно вибирають початок і кінець списку, якщо не вказано інше.\n\nprint(my_list[:])\n\n['friends', 'romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nПри негативному розмірі кроку ми можемо навіть перевернути список!\n\nprint(my_list[::-1])\n\n['ears', 'your', 'me', 'lend', 'countrymen', 'romans', 'friends']\n\n\nPython не має власних матриць. Інші пакети, такі як numpy, додають матриці як окремий тип даних, але в базовому Python найкращим способом створення матриці є використання списку списків.\nМи також можемо використовувати вбудовані функції для створення списків. Зокрема, ми розглянемо range() (тому що ми будемо використовувати його пізніше!). Діапазон може приймати кілька різних вхідних даних і поверне список.\n\nb = 10\nmy_list = range(b)\nprint(my_list)\n\nrange(0, 10)\n\n\nПодібно до наших попередніх методів нарізки списків, ми можемо визначити як початок, так і кінець нашого діапазону. Це поверне список, який включає початок і виключає кінець, точно так само, як зріз.\n\na = 0\nb = 10\nmy_list = range(a, b)\nprint(my_list)\n\nrange(0, 10)\n\n\nМи також можемо вказати розмір кроку. Це знову має таку ж поведінку, як і зріз.\n\na = 0\nb = 10\nstep = 2\nmy_list = range(a, b, step)\nprint(my_list)\n\nrange(0, 10, 2)\n\n\n\n\n\nB.4.2 Кортежі (Tuples)\nКортеж - це тип даних, подібний до списку в тому сенсі, що він може містити різні типи даних. Ключова відмінність тут полягає в тому, що кортеж є незмінним. Ми визначаємо кортеж, розділяючи елементи, які ми хочемо включити комами. Зазвичай кортеж укладають в круглі дужки.\n\nmy_tuple = 'I', 'have', 30, 'cats'\nprint(my_tuple)\n\n('I', 'have', 30, 'cats')\n\n\n\nmy_tuple = ('I', 'have', 30, 'cats')\nprint(my_tuple)\n\n('I', 'have', 30, 'cats')\n\n\nЯк згадувалося раніше, кортежі незмінні. Ви не можете змінити будь-яку їх частину, не визначивши новий кортеж.\n\nmy_tuple[3] = 'dogs' # Намагається змінити значення 'cats', що зберігається в кортежі, на 'dogs'\n\nTypeError: 'tuple' object does not support item assignment\n\n\nВи можете нарізати кортежі так само, як ви нарізаєте списки!\n\nprint(my_tuple[1:3])\n\n('have', 30)\n\n\nІ об’єднайте їх так, як Ви б це зробили з рядками!\n\nmy_other_tuple = ('make', 'that', 50)\nprint(my_tuple + my_other_tuple)\n\n('I', 'have', 30, 'cats', 'make', 'that', 50)\n\n\nМи можемо упакувати значення разом, створивши кортеж (як зазначено вище), або ми можемо розпакувати значення з кортежу, витягуючи їх.\n\nstr_1, str_2, int_1 = my_other_tuple\nprint(str_1, str_2, int_1)\n\nmake that 50\n\n\nРозпакування присвоює кожне значення кортежу по порядку кожній змінній у лівій частині знака рівності. Деякі функції, включаючи спеціальні функції, можуть повертати кортежі, тому ми можемо використовувати це, щоб безпосередньо розпакувати їх і отримати доступ до потрібних нам значень.\n\n\nB.4.3 Множини (Sets)\nМножини - це набір невпорядкованих, унікальних елементів. Він працює майже точно так, як ви очікували б від звичайного набору математичних задач, і визначається за допомогою фігурних дужок ({}).\n\nthings_i_like = {'dogs', 7, 'the number 4', 4, 4, 4, 42, 'lizards', 'man I just LOVE the number 4'}\nprint(things_i_like, type(things_i_like))\n\n{'lizards', 'dogs', 'the number 4', 4, 'man I just LOVE the number 4', 7, 42} &lt;class 'set'&gt;\n\n\nЗверніть увагу, як будь-які додаткові екземпляри одного і того ж елемента видаляються в остаточному наборі. Ми також можемо створити множину зі списку, використовуючи функцію set().\n\nanimal_list = ['cats', 'dogs', 'dogs', 'dogs', 'lizards', 'sponges', 'cows', 'bats', 'sponges']\nanimal_set = set(animal_list)\nprint(animal_set) # видаляємо всі дублікати зі списку\n\n{'dogs', 'bats', 'lizards', 'cats', 'sponges', 'cows'}\n\n\nВиклик len() для множини повідомить вам, скільки в ньому елементів.\n\nprint(len(animal_set))\n\n6\n\n\nОскільки множина представляє невпорядковану структуру даних, ми не можемо отримати доступ до окремих елементів за допомогою індексу. Однак ми можемо легко перевірити приналежність (щоб побачити, чи міститься щось у наборі) та використовувати об’єднання та перетини множин за допомогою вбудованих функцій set.\n\n'cats' in animal_set # Тут ми перевіряємо наявність членства, використовуючи ключове слово 'in'.\n\nTrue\n\n\nТут ми перевірили, чи міститься рядок cats у нашому animal_set, і він повернув True, повідомивши нам, що він насправді знаходиться в нашому наборі.\nМи можемо з’єднати множини, використовуючи типові математичні оператори множин, а саме | для об’єднання та & для перетину. Використання | або & поверне саме те, що ви очікували б, якщо Ви знайомі з множинами в математиці.\n\nprint(animal_set | things_i_like) # Ви також можете написати things_i_like / animal_set без будь-якої різниці\n\n{'dogs', 'the number 4', 4, 'bats', 'man I just LOVE the number 4', 7, 42, 'lizards', 'cats', 'sponges', 'cows'}\n\n\nСполучення двох наборів за допомогою | об’єднує множини, видаляючи будь-які повторення, щоб зробити кожен елемент набору унікальним.\n\nprint(animal_set & things_i_like) # Ви також можете написати things_i_like & animal_set без будь-якої різниці\n\n{'lizards', 'dogs'}\n\n\nСполучення двох наборів за допомогою & обчислює перетин обох наборів, повертаючи набір, який містить лише те, що вони мають спільне.\nЯкщо вам цікаво дізнатися більше про вбудовані функції для наборів, не соромтеся ознайомитися з документацією.\n\n\nB.4.4 Словники (Dictionaries)\nЩе однією важливою структурою даних у Python є словник. Словники визначаються за допомогою комбінації фігурних дужок ({}) і двокрапок (:). Фігурні дужки визначають початок і кінець словника, а двокрапки вказують пари ключ-значення. Словник - це, по суті, набір пар ключ-значення. Ключ будь-якого запису повинен бути незмінним типом даних. Це робить кандидатами як рядки, так і кортежі. Ключі можуть бути як додані, так і видалені.\nУ наступному прикладі ми маємо словник, що складається з пар ключ-значення, де ключовим є жанр художньої літератури (рядок), а значенням є список книг (list) у цьому жанрі. Оскільки колекція все ще вважається єдиною сутністю, ми можемо використовувати її для збору декількох змінних або значень в одну пару ключ-значення.\n\nmy_dict = {\"High Fantasy\": [\"Wheel of Time\", \"Lord of the Rings\"],\n           \"Sci-fi\": [\"Book of the New Sun\", \"Neuromancer\", \"Snow Crash\"],\n           \"Weird Fiction\": [\"At the Mountains of Madness\", \"The House on the Borderland\"]}\n\nПісля визначення словника ми можемо отримати доступ до будь-якого окремого значення, вказавши його ключ у дужках.\n\nprint(my_dict[\"Sci-fi\"])\n\n['Book of the New Sun', 'Neuromancer', 'Snow Crash']\n\n\nМи також можемо змінити значення, пов’язане з даним ключем\n\nmy_dict[\"Sci-fi\"] = \"I can't read\"\nprint(my_dict)\n\n{'High Fantasy': ['Wheel of Time', 'Lord of the Rings'], 'Sci-fi': \"I can't read\", 'Weird Fiction': ['At the Mountains of Madness', 'The House on the Borderland']}\n\n\nДодати нову пару ключ-значення так само просто, як і визначити її.\n\nmy_dict[\"Historical Fiction\"] = [\"Pillars of the Earth\"]\nprint(my_dict[\"Historical Fiction\"])\n\n['Pillars of the Earth']\n\n\n\nprint(my_dict)\n\n{'High Fantasy': ['Wheel of Time', 'Lord of the Rings'], 'Sci-fi': \"I can't read\", 'Weird Fiction': ['At the Mountains of Madness', 'The House on the Borderland'], 'Historical Fiction': ['Pillars of the Earth']}"
  },
  {
    "objectID": "appb.html#рядки-strings",
    "href": "appb.html#рядки-strings",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.5 Рядки (Strings)",
    "text": "B.5 Рядки (Strings)\nМи вже знаємо, що рядки зазвичай використовуються для тексту. Ми можемо використовувати вбудовані операції для легкого об’єднання, розділення та форматування рядків, залежно від наших потреб.\nСимвол + вказує на конкатенацію мовою рядків. Це об’єднає два рядки в довший рядок.\n\nfirst_string = '\"Beware the Jabberwock, my son! /The jaws that bite, the claws that catch! /'\nsecond_string = 'Beware the Jubjub bird, and shun /The frumious Bandersnatch!\"/'\nthird_string = first_string + second_string\nprint(third_string)\n\n\"Beware the Jabberwock, my son! /The jaws that bite, the claws that catch! /Beware the Jubjub bird, and shun /The frumious Bandersnatch!\"/\n\n\nРядки також індексуються приблизно так само, як і списки.\n\nmy_string = 'Supercalifragilisticexpialidocious'\nprint('The first letter is: ', my_string[0]) # Uppercase S\nprint('The last letter is: ', my_string[-1]) # lowercase s\nprint('The second to last letter is: ', my_string[-2]) # lowercase u\nprint('The first five characters are: ', my_string[0:5]) # Remember: slicing doesn't include the final element!\nprint('Reverse it!: ', my_string[::-1])\n\nThe first letter is:  S\nThe last letter is:  s\nThe second to last letter is:  u\nThe first five characters are:  Super\nReverse it!:  suoicodilaipxecitsiligarfilacrepuS\n\n\nВбудовані об’єкти та класи часто мають пов’язані з ними спеціальні функції, які називаються методами. Ми отримуємо доступ до цих методів, використовуючи точку (‘.’). Ми детальніше розглянемо об’єкти та пов’язані з ними методи в іншій лекції!\nВикористовуючи рядкові методи, ми можемо підраховувати екземпляри символу або групи символів.\n\nprint('Count of the letter i in Supercalifragilisticexpialidocious: ', my_string.count('i'))\nprint('Count of \"li\" in the same word: ', my_string.count('li'))\n\nCount of the letter i in Supercalifragilisticexpialidocious:  7\nCount of \"li\" in the same word:  3\n\n\nМи також можемо знайти перший екземпляр символу або групи символів у рядку.\n\nprint('The first time i appears is at index: ', my_string.find('i'))\n\nThe first time i appears is at index:  8\n\n\nА також замінити символи в рядку.\n\nprint(\"All i's are now a's: \", my_string.replace('i', 'a'))\n\nAll i's are now a's:  Supercalafragalastacexpaaladocaous\n\n\n\nprint(\"It's raining cats and dogs\".replace('dogs', 'more cats'))\n\nIt's raining cats and more cats\n\n\nІснують також деякі методи, які є унікальними для рядків. Функція upper() перетворює всі символи в рядку в верхній регістр, в той час як lower() перетворює всі символи в рядку в нижній регістр!\n\nmy_string = \"I can't hear you\"\nprint(my_string.upper())\nmy_string = \"I said HELLO\"\nprint(my_string.lower())\n\nI CAN'T HEAR YOU\ni said hello\n\n\n\nB.5.1 Форматування рядків\nВикористовуючи метод format(), ми можемо додавати значення змінних і зазвичай форматувати наші рядки.\n\nmy_string = \"{0} {1}\".format('Marco', 'Polo')\nprint(my_string)\n\nMarco Polo\n\n\n\nmy_string = \"{1} {0}\".format('Marco', 'Polo')\nprint(my_string)\n\nPolo Marco\n\n\nМи використовуємо фігурні дужки ({}) для позначення частин рядка, які будуть заповнені пізніше, і ми використовуємо аргументи функції format() для надання значень для заміни. Цифри у фігурних дужках вказують індекс значення в аргументах format().\nДивіться format() документація для отримання додаткових прикладів.\nЯкщо вам потрібне швидке та брудне форматування, ви можете замість цього використовувати символ%, який називається оператором форматування рядка.\n\nprint('insert %s here' % 'value')\n\ninsert value here\n\n\nСимвол % в основному вказує Python на створення заповнювача. Будь-який символ, що слідує за % (у рядку), вказує, який тип матиме значення, введене в заповнювач. Цей символ називається типом перетворення. Після закриття рядка нам знадобиться ще один %, за яким слідують значення для вставки. У випадку одного значення ви можете просто помістити його туди. Якщо ви вставляєте більше одного значення, вони повинні бути укладені в кортеж.\n\nprint('There are %s cats in my %s' % (13, 'apartment'))\n\nThere are 13 cats in my apartment\n\n\nУ цих прикладах %s вказує, що Python повинен перетворити значення в рядки. Існує кілька типів перетворення, які ви можете використовувати, щоб уточнити форматування. Дивіться форматування рядка для отримання додаткових прикладів та більш повної інформації про використання."
  },
  {
    "objectID": "appb.html#логічні-оператори",
    "href": "appb.html#логічні-оператори",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.6 Логічні оператори",
    "text": "B.6 Логічні оператори\n\nB.6.1 Базова логіка\nЛогічні оператори мають справу з булевими значеннями, як ми коротко розглянули раніше. Якщо ви пам’ятаєте, bool приймає одне з двох значень: True або False (або \\(1\\) або \\(0\\)). Основні логічні твердження, які ми можемо зробити, визначаються за допомогою вбудованих компараторів. Це == (дорівнює), != (не дорівнює), &lt; (Менше), &gt; (Більше), &lt;= (менше або дорівнює) і &gt;= (більше або дорівнює).\n\nprint(5 == 5)\n\nTrue\n\n\n\nprint(5 &gt; 5)\n\nFalse\n\n\nЦі компаратори також працюють у поєднанні зі змінними.\n\nm = 2\nn = 23\nprint(m &lt; n)\n\nTrue\n\n\nМи можемо зв’язати ці компаратори разом, щоб створити більш складні логічні оператори, використовуючи логічні оператори or, and і not.\n\nstatement_1 = 10 &gt; 2\nstatement_2 = 4 &lt;= 6\nprint(\"Statement 1 truth value: {0}\".format(statement_1))\nprint(\"Statement 2 truth value: {0}\".format(statement_2))\nprint(\"Statement 1 and Statement 2: {0}\".format(statement_1 and statement_2))\n\nStatement 1 truth value: True\nStatement 2 truth value: True\nStatement 1 and Statement 2: True\n\n\nОператор or виконує логічне обчислення або. Будь-який компонент, об’єднаний за допомогою або, що є True, представлятиме все твердження як True. Оператор and виводить True, лише якщо всі компоненти разом є True. В іншому випадку він видасть False. Твердження not просто інвертує значення істинності будь-якого наступного за ним твердження. Таким чином, твердження True буде оцінено як False, коли перед ним буде поставлено not. Аналогічно, False твердження стане True, коли перед ним буде стояти not.\nПрипустимо, у нас є два логічні твердження, \\(P\\) і \\(Q\\). Таблиця істинності для основних логічних операторів виглядає наступним чином:\n\n\n\nP\nQ\nnot P\nP and Q\nP or Q\n\n\n\n\nTrue\nTrue\nFalse\nTrue\nTrue\n\n\nFalse\nTrue\nTrue\nFalse\nTrue\n\n\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n\nМи можемо зв’язати кілька логічних операторів разом, використовуючи логічні оператори.\n\nprint(((2 &lt; 3) and (3 &gt; 0)) or ((5 &gt; 6) and not (4 &lt; 2)))\n\nTrue\n\n\nЛогічні твердження можуть бути настільки простими або складними, наскільки нам подобається, залежно від того, що нам потрібно висловити. Оцінюючи наведене вище логічне твердження крок за кроком, ми бачимо, що ми оцінюємо (True and True) or (False and not False). Дана конструкція набуваж вигляду True or (False and True). Згодом стає True or False, і в кінцевому рахунку оцінюється як True.\n\nB.6.1.1 Істинність\nТипи даних у Python мають цікаву характеристику, яка називається істинністю. Це означає, що більшість вбудованих типів будуть оцінюватися як True або False, коли потрібне логічне значення (наприклад, за допомогою оператора if). Як правило, контейнери, такі як рядки, кортежі, словники, списки та множини, повертають True, якщо вони взагалі що-небудь містять, і False, якщо вони нічого не містять.\n\n# Cхоже до того, як працюють float() та int(), book () змушує значення вважатися логічним!\nprint(bool(''))\n\nFalse\n\n\n\nprint(bool('I have character!'))\n\nTrue\n\n\n\nprint(bool([]))\n\nFalse\n\n\n\nprint(bool([1, 2, 3]))\n\nTrue\n\n\nІ так далі, для інших колекцій та контейнерів. None також оцінюється як False. Число 1 еквівалентно True, а число 0 також еквівалентно False в логічному контексті.\n\n\n\nB.6.2 If-оператори\nМи можемо створювати сегменти коду, які виконуються тільки при виконанні набору умов. Ми використовуємо оператори if у поєднанні з логічними операторами для створення розгалужень у нашому коді.\nБлок if вводиться, коли умова вважається True. Якщо умова оцінюється як False, блок if буде просто пропущений, якщо тільки до нього не додається блок else. Умови створюються за допомогою логічних операторів або за допомогою істинності значень у Python. Оператор if визначається двокрапкою і блоком тексту з відступом.\n\nif \"Condition\":\n    print(True)\nelse:\n    print(False)\n\nTrue\n\n\n\ni = 4\nif i == 5:\n    print('The variable i has a value of 5')\n\nОскільки в цьому прикладі i = 4 і оператор if шукає лише те, чи i = 5, оператор print ніколи не буде виконаний. Ми можемо додати оператор else, щоб створити блок коду на випадок надзвичайних ситуацій на випадок, якщо умова в операторі if не буде оцінена як True.\n\ni = 5\nif i == 5:\n    print(\"Усі рядки в цьому блоці з відступом є частиною цього блоку\")\n    print('Змінна i має значення 5')\nelse:\n    print(\"Усі рядки в цьому блоці з відступом є частиною цього блоку\")\n    print('Змінна i не дорівнює 5')\n\nУсі рядки в цьому блоці з відступом є частиною цього блоку\nЗмінна i має значення 5\n\n\nМи можемо реалізувати інші гілки від того самого оператора if, використовуючи elif, скорочення від else if. Ми можемо включати стільки elifсів, скільки захочемо, поки не вичерпаємо всі логічні гілки умови.\n\ni = 1\nif i == 1:\n    print('Змінна i має значення 1')\nelif i == 2:\n    print('Змінна і має значення 2')\nelif i == 3:\n    print('Змінна і має значення 3')\nelse:\n    print(\"Мене не хвилює змінна і\")\n\nМене не хвилює змінна і\n\n\nВи також можете вкласти оператори if в інші оператори if, щоб перевірити наявність додаткових умов.\n\ni = 10\nif i % 2 == 0:\n    if i % 3 == 0:\n        print('і ділиться як на 2, так і на 3!')\n    elif i % 5 == 0:\n        print('і ділиться як на 2, так і на 5!')\n    else:\n        print('i ділиться на 2, але не на 3 або 5!')\nelse:\n    print('Я припускаю, що i - непарне число.')\n\nі ділиться як на 2, так і на 5!\n\n\nПам’ятайте, що ми можемо згрупувати кілька умов разом, використовуючи логічні оператори!\n\ni = 11\nj = 12\nif i &lt; 10 and j &gt; 11:\n    print('{0} менше 10 і {1} більше 11!'.format(i, j))\n\nВи можете використовувати логічні компаратори для порівняння рядків!\n\nmy_string = \"Farthago delenda est\"\nif my_string == \"Carthago delenda est\":\n    print('And so it was! For the glory of Rome!')\nelse:\n    print('War elephants are TERRIFYING. I am staying home.')\n\nWar elephants are TERRIFYING. I am staying home.\n\n\nЯк і у випадку з іншими типами даних, == перевірить, чи дві речі з обох сторін мають однакове значення.\nДеякі вбудовані функції повертають логічне значення, тому їх можна використовувати як умови в операторі if. Користувацькі функції також можуть бути сконструйовані таким чином, щоб вони повертали логічне значення. Це буде розглянуто пізніше в розділі визначення функції!\nКлючове слово in зазвичай використовується для перевірки приналежності значення до іншого значення. Ми можемо перевірити приналежність у контексті оператора if і використовувати його для виведення значення істини.\n\nif 'a' in my_string or 'e' in my_string:\n    print('Those are my favorite vowels!')\n\nThose are my favorite vowels!\n\n\nТут ми використовуємо in, щоб перевірити, чи містить змінна my_string містить якісь конкретні літери. Пізніше ми будемо використовувати in для перебору списків!"
  },
  {
    "objectID": "appb.html#циклічні-структури",
    "href": "appb.html#циклічні-структури",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.7 Циклічні структури",
    "text": "B.7 Циклічні структури\nЦиклічні структури є однією з найважливіших частин програмування. Цикл for і цикл while надають спосіб багаторазового запуску блоку коду повторно. Цикл while буде повторюватися, поки не буде виконана певна умова. Якщо в будь-який момент після ітерації ця умова більше не виконується, цикл завершується. Цикл for буде виконувати ітерацію по послідовності значень і завершиться, коли послідовність закінчиться. Натомість ви можете включити умови в цикл for, щоб вирішити, чи повинен він закінчуватися достроково, чи ви можете просто дозволити йому піти своїм шляхом.\n\ni = 10\nwhile i &gt; 0:\n    i -= 1\n    print('Я зациклений! {0} значень до завершення!'.format(i))\n\nЯ зациклений! 9 значень до завершення!\nЯ зациклений! 8 значень до завершення!\nЯ зациклений! 7 значень до завершення!\nЯ зациклений! 6 значень до завершення!\nЯ зациклений! 5 значень до завершення!\nЯ зациклений! 4 значень до завершення!\nЯ зациклений! 3 значень до завершення!\nЯ зациклений! 2 значень до завершення!\nЯ зациклений! 1 значень до завершення!\nЯ зациклений! 0 значень до завершення!\n\n\nЗа допомогою циклів while нам потрібно переконатися, що щось насправді змінюється від ітерації до ітерації, щоб цикл фактично закінчувався. У цьому випадку ми використовуємо скорочення i -= 1 (скорочення від i = i - 1), так що значення i стає меншим з кожною ітерацією. Врешті-решт i буде зменшено до 0, що призведе до виконання умови False та виходу з циклу.\nЦикл for повторюється задану кількість разів, що визначається при вказівці запису в цикл. У цьому випадку ми повторюємо список, повернутий з range(). Цикл for вибирає значення зі списку по порядку і тимчасово присвоює йому значення i, щоб із цим значенням можна було виконувати операції.\n\nfor i in range(5):\n    print('Я зациклений! Я вже на {0}-ій ітерації!'.format(i + 1))\n\nЯ зациклений! Я вже на 1-ій ітерації!\nЯ зациклений! Я вже на 2-ій ітерації!\nЯ зациклений! Я вже на 3-ій ітерації!\nЯ зациклений! Я вже на 4-ій ітерації!\nЯ зациклений! Я вже на 5-ій ітерації!\n\n\nЗверніть увагу, що в цьому циклі for ми використовуємо ключове слово in. Використання ключового слова in не обмежується перевіркою приналежності, як у прикладі if-конструкцій. Ви можете оброблювати будь-яку колекцію за допомогою циклу for, використовуючи ключове слово in.\nУ цьому наступному прикладі ми переглянемо множину, оскільки хочемо перевірити наявність вмісту та додати до нового набору.\n\nmy_list = {'cats', 'dogs', 'lizards', 'cows', 'bats', 'sponges', 'humans'}\nmammal_list = {'cats', 'dogs', 'cows', 'bats', 'humans'} # перераховані всі ссавці в у світі\nmy_new_list = set()\nfor animal in my_list:\n    if animal in mammal_list:\n        # додаємо будь-яку тварину, що знаходиться і в my_list, і в mammal_list\n        my_new_list.add(animal)\n\nprint(my_new_list)\n\n{'dogs', 'bats', 'cats', 'humans', 'cows'}\n\n\nЄ два твердження, які дуже корисні при роботі як з циклами for, так і з циклами while. Це break і continue. Якщо break трапляється в будь-який момент під час виконання циклу, цикл негайно завершується.\n\ni = 10\nwhile True:\n    if i == 14:\n        break\n    i += 1\n    print(i)\n\n11\n12\n13\n14\n\n\n\nfor i in range(5):\n    if i == 2:\n        break\n    print(i)\n\n0\n1\n\n\nОператор continue вкаже циклу негайно завершити цю ітерацію і перейти до наступної ітерації циклу.\n\ni = 0\nwhile i &lt; 5:\n    i += 1\n    if i == 3:\n        continue\n    print(i)\n\n1\n2\n4\n5\n\n\nЦей цикл пропускає друк числа \\(3\\) через інструкцію continue, яка виконується, коли ми вводимо оператор if. Код ніколи не бачить команди для друку числа \\(3\\), оскільки він уже перейшов до наступної ітерації.\nЗмінна, яку ми використовуємо для ітерації циклу, збереже своє значення при завершенні циклу. Аналогічно, будь-які змінні, визначені в контексті циклу, продовжуватимуть існувати поза ним.\n\nfor i in range(5):\n    loop_string = 'Я виходжу за межі циклу!'\n    print('Я вічний! Я {0} і я існую скрізь!'.format(i))\n\nprint('Моє значення {0}'.format(i))\nprint(loop_string)\n\nЯ вічний! Я 0 і я існую скрізь!\nЯ вічний! Я 1 і я існую скрізь!\nЯ вічний! Я 2 і я існую скрізь!\nЯ вічний! Я 3 і я існую скрізь!\nЯ вічний! Я 4 і я існую скрізь!\nМоє значення 4\nЯ виходжу за межі циклу!\n\n\nМи також можемо виконувати ітерації по словнику!\n\nmy_dict = {'firstname' : 'Inigo', 'lastname' : 'Montoya', 'nemesis' : 'Rugen'}\n\n\nfor key in my_dict:\n    print(key)\n\nfirstname\nlastname\nnemesis\n\n\nЯкщо ми просто перебираємо словник, не роблячи нічого іншого, ми отримуємо лише ключі. Ми можемо або використовувати ключі для отримання значень, як у прикладі:\n\nfor key in my_dict:\n    print(my_dict[key])\n\nInigo\nMontoya\nRugen\n\n\nАбо ми можемо використовувати функцію items(), щоб отримати і ключ, і значення одночасно\n\nfor key, value in my_dict.items():\n    print(key, ':', value)\n\nfirstname : Inigo\nlastname : Montoya\nnemesis : Rugen\n\n\nФункція items створює кортеж з кожної пари ключ-значення, а цикл for розпаковує цей кортеж в ключ, значення при кожному окремому виконанні циклу!"
  },
  {
    "objectID": "appb.html#функції",
    "href": "appb.html#функції",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.8 Функції",
    "text": "B.8 Функції\nФункція-це багаторазовий блок коду, який ви можете викликати повторно для виконання обчислень, виведення даних або дійсно робити все, що завгодно. Це один з ключових аспектів використання мови програмування. Щоб додати до вбудованих функцій у Python, ви можете визначити свої власні!\n\ndef hello_world():\n    \"\"\" Виводить Hello, world! \"\"\"\n    print('Hello, world!')\n\nhello_world()\n\nHello, world!\n\n\n\nfor i in range(5):\n    hello_world()\n\nHello, world!\nHello, world!\nHello, world!\nHello, world!\nHello, world!\n\n\nФункції визначаються за допомогою def, імені функції, списку параметрів та двокрапки. Все, що вказано з відступом нижче двокрапки, буде включено у визначення функції.\nМи можемо змусити наші функції робити все, що ви можете зробити зі звичайним блоком коду. Наприклад, наша функція hello_world() виводить рядок при кожному його виклику. Якщо ми хочемо зберегти значення, обчислене функцією, ми можемо визначити функцію так, щоб вона return потрібне нам значення. Це дуже важлива особливість функцій, оскільки будь-яка змінна, визначена виключно всередині функції, не буде існувати поза нею.\n\ndef see_the_scope():\n    return \"Я тут застряг!\"\n\nprint(see_the_scope())\n\nЯ тут застряг!\n\n\n\na = see_the_scope()\nprint(a)\n\nЯ тут застряг!\n\n\nОбласть змінної - це частина блоку коду, де ця змінна прив’язана до певного значення. Функції в Python мають закриту область дії, що робить можливим прямий доступ до змінних лише всередині цих областей. Якщо ми передамо ці значення оператору return, ми можемо отримати їх із функції.\n\ndef free_the_scope():\n    in_function_string = \"Anything you can do I can do better!\"\n    return in_function_string\nmy_string = free_the_scope()\nprint(my_string)\n\nAnything you can do I can do better!\n\n\nТак само, як ми можемо отримувати значення з функції, ми також можемо розміщувати значення у функції. Ми робимо це, визначаючи нашу функцію з параметрами.\n\ndef multiply_by_five(x):\n    \"\"\" Множимо вхідне значення на 5 \"\"\"\n    return x * 5\n\nn = 4\nprint(n)\nprint(multiply_by_five(n))\n\n4\n20\n\n\nУ цьому прикладі у нас був лише один параметр для нашої функції, x. Ми можемо легко додати додаткові параметри, розділивши всі комою.\n\ndef calculate_area(length, width):\n    \"\"\" Визначаємо площу прямокутника \"\"\"\n    return length * width\n\n\nl = 5\nw = 10\nprint('Area: ', calculate_area(l, w))\nprint('Length: ', l)\nprint('Width: ', w)\n\nArea:  50\nLength:  5\nWidth:  10\n\n\n\ndef calculate_volume(length, width, depth):\n    \"\"\" Визначаємо об'єм прямокутної призми \"\"\"\n    return length * width * depth\n\nЯкщо ми хочемо, ми можемо визначити функцію так, щоб вона приймала довільну кількість параметрів. Ми повідомляємо Python, що хочемо цього, використовуючи зірочку (*).\n\ndef sum_values(*args):\n    sum_val = 0\n    for i in args:\n        sum_val += i\n    return sum_val\n\n\nprint(sum_values(1, 2, 3))\nprint(sum_values(10, 20, 30, 40, 50))\nprint(sum_values(4, 2, 5, 1, 10, 249, 25, 24, 13, 6, 4))\n\n6\n150\n343\n\n\nВи використовуєте *args як параметр для вашої функції - це коли ви не знаєте, скільки значень можна передати в неї, як у випадку з нашою функцією sum. Зірочка в даному випадку - це синтаксис, який повідомляє Python, що ви збираєтеся передати довільну кількість параметрів у свою функцію. Ці параметри зберігаються у вигляді кортежу.\n\ndef test_args(*args):\n    print(type(args))\n\ntest_args(1, 2, 3, 4, 5, 6)\n\n&lt;class 'tuple'&gt;\n\n\nНаші функції можуть повертати будь-який тип даних. Це дозволяє нам легко створювати функції, які перевіряють умови, які ми можемо захотіти відстежувати.\nТут ми визначаємо функцію, яка повертає логічне значення. Ми можемо легко використовувати це в поєднанні з операторами if та іншими ситуаціями, які потребують логічного значення.\n\ndef has_a_vowel(word):\n    \"\"\"\n    Перевіряємо, чи містить слово голосну\n\n    \"\"\"\n    vowel_list = ['a', 'e', 'i', 'o', 'u']\n\n    for vowel in vowel_list:\n        if vowel in word:\n            return True\n\n    return False\n\n\nmy_word = 'catnapping'\nif has_a_vowel(my_word):\n    print('Містить.')\nelse:\n    print('Не містить.')\n\nМістить.\n\n\n\ndef point_maker(x, y):\n    \"\"\" Групує значення x і y в точку, технічно кортеж \"\"\"\n    return x, y\n\nЦя наведена вище функція повертає впорядковану пару вхідних параметрів, збережених як кортеж.\n\na = point_maker(0, 10)\nb = point_maker(5, 3)\ndef calculate_slope(point_a, point_b):\n    \"\"\" Обчислює лінійний нахил між двома точками\"\"\"\n    return (point_b[1] - point_a[1])/(point_b[0] - point_a[0])\nprint(\"Кут нахилу між a і b {0}\".format(calculate_slope(a, b)))\n\nКут нахилу між a і b -1.4"
  },
  {
    "objectID": "appb.html#подальші-кроки",
    "href": "appb.html#подальші-кроки",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.9 Подальші кроки",
    "text": "B.9 Подальші кроки\nЯкщо ви хочете глибше заглибитися в матеріал, тоді зверніться до документації по Python."
  },
  {
    "objectID": "appc.html#комірка-markdown",
    "href": "appc.html#комірка-markdown",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.1 Комірка Markdown",
    "text": "C.1 Комірка Markdown\nЯк можна здогадатися з назви в Markdown осередках можна створювати текст в markdown форматі. Підтримуються різні способи форматування, які можна подивитися за посиланням. Текст, який ви зараз читаєте, також знаходиться в markdown клітинці.\nКрім форматування тексту також підтримується можливість створення математичних формул за допомогою LaTex. Формулу можна вбудувати в текст (наприклад, \\(e^{i\\pi}=-1\\)) або створити в окремому рядку:\n\\[e^x=\\sum_{k=0}^\\infty \\frac{x^k}{k!}\\]\nДля редагування тексту в markdown осередку необхідно два рази клікнути по ній."
  },
  {
    "objectID": "appc.html#комірка-code",
    "href": "appc.html#комірка-code",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.2 Комірка Code",
    "text": "C.2 Комірка Code\nНаступна комірка є Сode осередком і в ній можна писати код і виконувати його. Для виконання коду необхідно натиснути Ctrl + Enter(виконати і залишитися в поточній комірці) або Shift + Enter (виконати і перейти в наступну комірку)\n\nimport numpy as np # імпортуємо бібліотеку\n\nЯкщо останній рядок коду повертає яке-небудь значення, то воно відображається відразу після комірки\n\nnp.random.rand(10) # генеруємо випадкові значення\n\narray([0.68528999, 0.70609617, 0.82579621, 0.38756313, 0.87001097,\n       0.91044051, 0.31743928, 0.27789269, 0.79610991, 0.58255746])"
  },
  {
    "objectID": "appc.html#автодоповнення-та-робота-з-документацією",
    "href": "appc.html#автодоповнення-та-робота-з-документацією",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.3 Автодоповнення та робота з документацією",
    "text": "C.3 Автодоповнення та робота з документацією\nДля автодоповнення можна використовувати клавішу &lt;TAB &gt; після точки або всередині дужки при виклику функції. При цьому вийде список доступних варіантів, які можна вибрати, щоб автоматично доповнити код. Можете спробувати автодоповнення поставивши курсор після np.random.&lt;TAB&gt;.\nВ Jupyter є кілька способів викликати документацію. Перший спосіб це використовувати поєднання клавіш Shift + Tab. Другий спосіб поставити знак ? після необхідного модуля\n\nnp?"
  },
  {
    "objectID": "appc.html#magic-команди",
    "href": "appc.html#magic-команди",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.4 Magic команди",
    "text": "C.4 Magic команди\nJupyter підтримує набір так званих “чарівних” (magic) команд. Це різні корисні команди, які не є частиною Python. Всі ці команди починаються з %.\nМожна безпосередньо завантажити вміст зовнішнього файлу в комірку за допомогою команди %load\n\n# %load code/magic_example.py\ndef square(x): # ініціалізуємо функцію знаходження квадрату вхідного значення\n    \"\"\"\n    Squares given number\n    \"\"\"\n    return x ** 2 # повертаємо значення\n\n\nprint(square(42)) # виводимо результат\n\n1764\n\n\nЗ корисних команд також можна відзначити команду %timeit, яка виконує код багато разів і виводить середній час виконання коду\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n377 µs ± 21.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nСписок усіх magic команд можна подивитися окремою командою %lsmagic."
  },
  {
    "objectID": "appc.html#робота-з-графікою",
    "href": "appc.html#робота-з-графікою",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.5 Робота з графікою",
    "text": "C.5 Робота з графікою\nУ Python є багато бібліотек для візуалізації даних. Багато з них інтегруються з Jupyter і відображають графіки\n\nimport matplotlib.pyplot as plt # імпортуємо бібліотеку\n\n# вбудовуємо вивдені рисунки в юпітеровський ноутбук\n%matplotlib inline\n\nplt.plot([1, 4], [1, 4]) # виводимо лінію по двум точкам\n\n\n\n\nабо декілька графіків\n\nall_data = [np.random.normal(0, std, size=100) for std in range(1, 4)] # генеруємо список значень із нормального розподілу\nlabels = ['x1', 'x2', 'x3'] # ініціалізуємо список міток\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4)) # ініціалізуємо об'єкт матриці рисунків\n\n# прямокутна коробчаста діаграма\nbplot1 = axes[0].boxplot(all_data,\n                         vert=True,  # вертикальне вирівнювання\n                         patch_artist=True,  # заповнити кольором\n                         labels=labels)  # використовується для позначення підписів по осі x\naxes[0].set_title('Прямокутна діаграма') # встановлюємо титулку для першого рисунку\n\n# побудова коробчастої діаграми з виїмкою\nbplot2 = axes[1].boxplot(all_data,\n                         notch=True,\n                         vert=True,  # вертикальне вирівнювання\n                         patch_artist=True,  # заповнити кольором\n                         labels=labels)  # використовується для позначення підписів по осі x\naxes[1].set_title('Прямокутна діаграма з виїмкою') # встановлюємо титулку для другого рисунку\n\n# заповнити кольорами\ncolors = ['pink', 'lightblue', 'lightgreen']\nfor bplot in (bplot1, bplot2):\n    for patch, color in zip(bplot['boxes'], colors):\n        patch.set_facecolor(color)\n\n# додати сітку з горизонтальних ліній\nfor ax in axes:\n    ax.yaxis.grid(True)\n    ax.set_xlabel('Три відокремлений зразки')\n    ax.set_ylabel('Спостережувані значення')\n\n\n\n\nабо тривимірну графіку\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\n\nfig = plt.figure(figsize=(10, 10)) # ініціалізуємо об'єкт рисунок\nax = fig.add_subplot(projection='3d') # додаємо до об'єкту тривимірне представлення\n\n# ініціалізуємо дані\nX = np.arange(-5, 5, 0.25)\nY = np.arange(-5, 5, 0.25)\nX, Y = np.meshgrid(X, Y) # заповнюємо поверхню значеннями по двом осям\nR = np.sqrt(X**2 + Y**2)\nZ = np.sin(R) # ініціалізуємо значення по вісі Oz\n\n# будуємо поверхню\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm) # будуємо тривимірну поверхню по заданим значенням\n\n\nax.set_zlim(-1.01, 1.01) # встановлюємо границі по вісі Oz\nax.zaxis.set_major_locator(LinearLocator(10)) # встановлюємо 10 граничних ліній по вісі Oz\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f')) # визначаємо формат виведення значень"
  },
  {
    "objectID": "appc.html#інші-можливості",
    "href": "appc.html#інші-можливості",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.6 Інші можливості",
    "text": "C.6 Інші можливості\nДля Jupyter Notebook було створено велику кількість плагінів. Наприклад, можна вбудовувати відео з youtube:\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('kjBOesZCoqc')\n\n\n        \n        \n\n\nАбо інтерактивні карти (дана комірка відобразиться тільки якщо у вас встановлений folium. Якщо у вас нічого не відображається, то можете пропустити даний приклад, він далі не знадобиться)\nВстановити необхідну бібліотеку можна через команду pip install назва бібліотеки, яку варто прописати в консолі, як представлено в прикладі нижче:\n\nАбо, як варіант, можна прописати команду прямо в комірці середовища Jupyter Notebook, як представлено в прикладі нижче:\n\n!pip install folium\n\nRequirement already satisfied: folium in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (0.14.0)\nRequirement already satisfied: requests in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from folium) (2.27.1)\nRequirement already satisfied: numpy in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from folium) (1.21.2)\nRequirement already satisfied: branca&gt;=0.6.0 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from folium) (0.6.0)\nRequirement already satisfied: jinja2&gt;=2.9 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from folium) (2.11.3)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from jinja2&gt;=2.9-&gt;folium) (2.0.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from requests-&gt;folium) (2.0.4)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from requests-&gt;folium) (1.26.9)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from requests-&gt;folium) (3.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/admin/opt/anaconda3/lib/python3.9/site-packages (from requests-&gt;folium) (2022.12.7)\n\n\n\nimport folium\nm = folium.Map(zoom_start=12, location=[47.89829743895897, 33.36626740165739])\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nАбо вбудувати будь-який інший шматок HTML за допомогою magic команди %%html. Нижче наведено приклад вбудовування посту з Твіттеру\n\n%%html\n&lt;blockquote class=\"twitter-tweet\" data-lang=\"en\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Replace &quot;AI&quot; with &quot;matrix multiplication &amp; gradient descent&quot; in the calls for &quot;government regulation of AI&quot; to see just how absurd they are&lt;/p&gt;&mdash; Ben Hamner (@benhamner) &lt;a href=\"https://twitter.com/benhamner/status/892136662171504640?ref_src=twsrc%5Etfw\"&gt;July 31, 2017&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\nReplace \"AI\" with \"matrix multiplication & gradient descent\" in the calls for \"government regulation of AI\" to see just how absurd they are— Ben Hamner (@benhamner) July 31, 2017"
  },
  {
    "objectID": "appc.html#гарячі-клавіші",
    "href": "appc.html#гарячі-клавіші",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.7 Гарячі клавіші",
    "text": "C.7 Гарячі клавіші\nБагато дій можна виконати за допомогою гарячих клавіш. Список гарячих клавіш можна знайти в меню Help - Keyboard shortcuts. Нижче наведено список найбільш корисних поєднань:\n\n\n\n\n\n\n\nКлюч\nОписання\n\n\n\n\nEsc\nвийти з режиму редагування та виділити поточну комірку\n\n\nEnter\nперейти в режим редагування комірки поточної комірки\n\n\nCtrl+S, S\nзберегти файл\n\n\nCtrl+Enter\nвиконати код і залишитися в поточній комірці\n\n\nShift + Enter\nвиконати код і перейти в наступну клітинку\n\n\nShift + Tab\nвиводить спливаюче вікно з документацією\n\n\na\nдодати комірку згори (above)\n\n\nb\nдодати комірку знизу (below)\n\n\nc\nскопіювати комірку\n\n\nv\nвставити скопійовану клітинку\n\n\ndd\nвидалити комірку\n\n\nz\nскасування останньої дії"
  }
]