[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Моделювання складних систем у Python",
    "section": "",
    "text": "Передмова\nЦе книга з моделювання складних систем засобами мови програмування Python."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Вступ",
    "section": "",
    "text": "Задекларований перехід економіки України на інноваційний шлях розвитку в умовах обмеженості ресурсів, у першу чергу фінансових, вимагає зосередження їх на найбільш перспективних напрямках, де імовірність досягнення конкурентного успіху є найбільшою. Як свідчить практика, такого роду оцінки доцільно виконувати із застосуванням економіко-математичних моделей. Множинність шляхів вибору моделей і методів актуалізує необхідність розуміння основних тенденцій розвитку наукового знання і ключових досягнень.\nГоворячи мовою синергетики, необхідно виділити такі параметри порядку – провідні змінні, які з плином часу починають визначати динаміку і розвиток складної системи, і підпорядковувати собі її інші параметри, які призведуть до ефективного спрощення складного об’єкта.\nВсе більш очевидною є революція, що почалася в природничих та гуманітарних науках і пов’язана з вивченням феномена самоорганізації і дослідженням мережних структур. Мережна парадигма складності обумовлена великим значенням, яке придбали такі об’єкти, і тим, що на початку ХХІ століття очевидною стала разюча аналогія в топології мережних структур, що виникають завдяки активному використанню інформаційно-телекомунікаційних, гуманітарних, управлінських, військових технологій.\nМережі стали одним з двигунів економіки. У своїй історії людство долало різні бар’єри, створюючи нові матеріали, технології, об’єкти. Однак «мережна економіка» зіткнулася з обмеженнями самої людини – так званим «когнітивним бар’єром». Як показали психологи, людина в змозі активно, творчо взаємодіяти з 5-7 людьми (з рештою опосередковано або стандартно, незалежно від того, скільки у нього друзів у соціальній мережі). Вона може одночасно стежити за 5-7 змінними (незалежно від того, наскільки великий обсяг інформації їй доступний). Приймаючи рішення, вона може зважити 5-7 факторів (скільки б даних у неї не було).\nЗліт нової економіки в США в 1990-х роках, пов’язаний багато в чому з інтернет- компаніями, породив ілюзії, що капіталізація мережних структур пропорційна числу зв’язків між вузлами, тобто квадрату числа вузлів \\(N^2\\). Однак, коли на початку 2000-х років міхур «нової економіки» лопнув (криза «доткомів - .com»), то виявилося, що реальна капіталізація мала зовсім іншу залежність від числа об’єктів, пов’язаних мережею – \\(N\\ln{N}\\). Інакше кажучи, не «всі зв’язуються з усіма», а майже всі взаємодіють з декількома дуже великими вузлами- хабами, які вже тісно пов’язуються між собою. Подібним же чином виявляється влаштована інфраструктура більшості складних систем, незалежно від їх природи.\nЧерговим викликом для нас стала нова індустріальна революція, актуалізована давоським форумом 2016 року. Згідно з опитуванням 800 лідерів технологічних компаній, проведеним спеціально для форуму в Давосі, ключовими драйверами змін стануть хмарні технології, розвиток способів збору і аналізу Big Data, краудсорсінг, шерінгова економіка і біотехнології.\nОчевидно, що революційні вимоги Індустрії 4.0 потребують нових парадигм моделювання соціально-економічних систем. На наш погляд, такою парадигмою може стати мережна парадигма складних систем.\nМережні технології змінили обчислювальну математику, системний аналіз, інформаційні технології. В останні роки було реалізовано кілька грандіозних мережних проектів, в яких поставлена задача вирішувалася завдяки спільним діям сотень тисяч або навіть мільйонів комп’ютерів. Це і криптографічні проблеми, і пошук ліків проти раку, заснований на математичному моделюванні взаємодії різних речовин з клітинами. Це розподілений аналіз даних космічних експериментів та обробка результатів, отриманих на Великому адронному колайдері.\nУ зв’язку з новою мережною парадигмою складності перед фахівцями з моделювання виникають принципово нові, актуальні задачі. Ось, на наш погляд, тільки деякі з них:\n\nдослідження надійності, робастності кіберфізичних мереж відносно випадкових помилок та направлених атак;\nаналіз когнітивних можливостей складних систем;\nмоделювання мультиплексних мереж;\nвплив нанотехнологій на формування наноекономіки та ін.\n\nСьогодні вже зрозуміло, що відповідних інновацій вимагає і система освіти. На початку комп’ютерної ери основну цінність і проблеми становили власне комп’ютери (hardware) і акцент робився на підготовку фахівців з обчислювальної техніки. Потім величезне значення набуло програмне забезпечення (software) і була розпочата підготовка дослідників у цій галузі (computer science) та інженерів-програмістів (computer engineering). У даний час на перший план виходять фахівці з мережних технологій (NetWare). Саме таких фахівців треба починати готувати у провідних національних університетах. За оцінками експертів з Індустрії 4.0 основними трендами освітньої сфери на двадцятирічному горизонті стануть: поширення цінностей мережної культури, прагматизація освіти, автоматизація рутинних інтелектуальних операцій, розвиток індустрії поліпшення когнітивних здібностей, боротьба за таланти, зростання значущості глобальних людських цінностей, уваги до природи і дбайливого поводження з ресурсами.\nМета курсу моделювання складних систем – формування системи теоретичних знань та практичних навичок щодо моделювання структурних і динамічних властивостей систем різної природи як засобу дослідження та управління складними явищами на макро-, мезо- й мікрорівнях. Протягом останніх десяти-п’ятнадцяти років відбулися відчутні зміни в розумінні фундаментальних закономірностей складних систем. Виявилось, що такі складні системи мають універсальні емерджентні властивості, які не знаходять адекватного розуміння у рамках традиційних парадигм. Тому для аналізу все активніше використовуються сучасні методи та моделі фундаментальних наук, які у поєднанні з новітніми досягненнями в галузі інформаційних технологій та досить ємними базами даних (мільйони записів навіть в базах некомерційного призначення) забезпечили значний прогрес у розумінні та квантифікації природи цих систем. Сюди відносяться методи фрактального і мультифрактального аналізу, дослідження рекурентних властивостей динамічних систем, нелінійної динаміки, теорії хаосу і біфуркацій тощо. З’явились нові «кількісні» напрямки економіки: математична економіка, фізична економіка, еконофізика та ін. Деякі з таких моделей, не знайшовши поки що відображення у навчальній літературі, включені до цієї роботи.\nНаукову основу курсу складають теоретичні моделі, математичний апарат, сучасні концепції та парадигми, які визначають підходи до вивчення характеристик складних систем. Курс базується на знаннях, одержаних при вивченні дисциплін математичного циклу, основ теорії систем та системного аналізу, моделювання, фінансового аналізу, макро- і мікроекономіки.\nЗавдання курсу – оволодіння теоретичними знаннями та інструментарієм моделювання складних систем, вивчення підходів до дослідження й аналізу, методів прогнозування їхнього розвитку, управління розвитком та функціонуванням складних систем у різних умовах функціонування.\nПредметом курсу є математичні моделі і методи дослідження складних систем різної природи.\nУ результаті вивчення дисципліни студент повинен:\n\nзнати структурні та динамічні характеристики складної системи; моделі прогнозування характеристик системи; основні методи оцінки якості функціонування; методи оцінки структурних змін; методи дослідження та моделювання складних природних та штучних систем;\nвміти здійснювати класифікацію характеристик складної системи, проводити порівняльний аналіз методів прогнозування; оцінити якість функціонування ієрархічної системи; визначити катастрофічні зміни в системі, які описуються рівняннями динаміки, визначити джерела структурних катастроф;\nдослідити та проаналізувати комплекс моделей складної системи;\nбути ознайомленим з сучасними напрямками розвитку сучасних теорій та парадигм, які використовуються для дослідження якісних характеристик динамічних систем.\n\nСистема контролю якості навчання студентів містить такі заходи:\n\nмодульний контроль;\nпроведення контрольних робіт;\nконтроль теоретичних знань у ході практичних занять;\nвиконання індивідуальних завдань на лабораторних заняттях.\n\nОрганізація самостійної роботи студентів передбачає підготовку до семінарських занять, проведення індивідуальних консультацій, виконання курсової роботи.\nВідсутність альтернативних підручників і методичних посібників з курсу моделювання складних систем спонукала авторів розробити методичні вказівки до виконання практичних робіт з даного курсу. Вони спираються на деяке авторське бачення, певні висновки мають дискусійний характер. Автори будуть вдячні за бачення, спрямоване на покращення змісту та методики викладання дисципліни."
  },
  {
    "objectID": "lab_1.html#теоретичні-відомості",
    "href": "lab_1.html#теоретичні-відомості",
    "title": "2  Лабораторна робота № 1",
    "section": "2.1 Теоретичні відомості",
    "text": "2.1 Теоретичні відомості\n\n2.1.1 Аналіз динаміки прибутків, модулів прибутків та волатильностей\n\nОстаннім часом вчені все більше цікавляться економічними часовими рядами, і відбувається це за кількох причин, зокрема: (1) економічні часові ряди, такі як індекси акцій, курсів валют, залежать від розвитку великої кількості взаємодіючих систем, і є прикладами складних систем, що широко вивчаються у науці; (2) з’явилась велика кількість доступних баз з даними про економічні системи, що містять інформацію з різними часовими шкалами (починаючи з 1 хвилини і закінчуючи 1 роком). Внаслідок цього вже на даний час існує також велика кількість розроблених методів (зокрема, у статистичній фізиці), спрямованих на отримання характеристик цін акцій чи курсів валют, що еволюціонують у часі.\n\n\nДослідження, проведені над часовими рядами, показують, що стохастичний процес, який лежить у основі зміни ціни, характеризується кількома ознаками. Розподіл зміни ціни має виділений хвіст порівняно із Гаусовим розподілом. Функція автокореляції зміни ціни спадає експоненційно з певним характерним часом. Однак, виявляється, що амплітуда зміни ціни, виміряна за абсолютними значеннями чи квадратами цін, показує степеневі кореляції з довго часовою персистентністю аж до кількох місяців, або навіть років. Такі довгочасові залежності краще моделюються з використанням «додаткового процесу», що в економічній літературі часто називається волатильністю. Волатильність змін ціни акції є мірою того, як сильно ринок схильний до флуктуацій, тобто відхилень ціни від попередніх значень.\n\n\nПершим кроком при проведенні аналізу є побудова оцінювача волатильності. Ми будемо отримувати волатильність як локальне середнє модуля зміни ціни.\n\n\nРозуміння статистичних властивостей волатильності має також важливе практичне застосування. Волатильність є інтересом торговців, оскільки визначає ризик і є ключовим входом практично до всіх моделей цін опціонів (вторинного цінного паперу), включаючи і класичну модель Блека-Шоулза. Без задовільних методів оцінювання волатильності трейдерам було б надзвичайно важко визначати ситуації, в яких опціони попадають в недооцінку чи переоцінку.\n\n\n\n2.1.2 Визначення волатильності\n\nТермін волатильність представляє узагальнену міру величини ринкових флуктуацій (відхилень). У літературі існує досить багато визначень волатильності, проте ми будемо використовувати наступне: волатильність є локальним середнім модуля зміни ціни на відповідному часовому інтервалі \\(T\\), що є рухомим параметром нашої оцінки. Для індексу \\(X(t)\\) визначимо зміну ціни \\(G(t)\\) як зміну логарифмів індексів,\n\n\\[\nG(t) = \\ln{X(t+\\Delta t) - \\ln{X(t)}} \\cong \\frac{X(t+\\Delta t) - X(t)}{X(t)}, \\tag{1}\n\\]\n\nде \\(\\Delta t\\) є часовим інтервалом затримки. Величину (1) називають прибутковістю (return). Якщо використовувати границі, то малі зміни \\(X(t)\\) приблизно відповідають змінам, визначеним другою рівністю. Ми лише підраховуємо час роботи ринку, викидаємо ночі, вихідні та свята із набору даних, тобто, вважається, що ринок працює без перерв.\n\n\nМодуль \\(G(t)\\) описує амплітуду флуктуацій. У порівнянні із значеннями \\(G(t)\\) їх модуль не показує глобальних трендів, але великі значення \\(G(t)\\) відповідають крахам та великим миттєвим змінам на ринках.\n\n\nВизначимо волатильність як середнє від \\(G(t)\\) для часових вікон \\(T = n \\cdot \\Delta t\\), тобто\n\n\\[\nV_{T} = \\frac{1}{n}\\sum_{t^{'}=t}^{t+n-1}\\left| G(t^{'}) \\right|, \\tag{2}\n\\]\n\nде \\(n\\) є цілим числом. Таке визначення може бути ще узагальнене заміною \\(G(t)\\) на \\(\\left| G(t) \\right|^{\\gamma}\\), де \\(\\gamma &gt; 1\\) дає більш виражені великі значення \\(G(t)\\), в той час як \\(0 &lt; \\gamma &lt; 1\\) виділяє малі значення \\(G(t)\\).\n\n\nУ цьому визначенні волатильності використовується два параметри, \\(\\Delta t\\) та \\(n\\). Параметр \\(n\\) є шаблонним (чи модельним) часовим інтервалом для даних, а параметр \\(\\Delta t\\) є кроком переміщення часового вікна. Зауважимо, що вказане визначення волатильності має внутрішню помилку, а саме: вибір більшого часового інтервалу \\(T\\) веде до збільшення точності визначення волатильності. Однак, велике значення \\(T\\) також включає погане розбиття часу на інтервали, що веде, у свою чергу, до врахування не всієї прихованої у ряді інформації.\n\n\n\n2.1.3 Визначення кореляцій\n\nДля визначення кореляцій часового ряду використовується функція автокореляції. Саме поняття автокореляції означає кореляцію часового ряду самого з собою (між попередніми та наступними значеннями). Автокореляцію іноді називають послідовною кореляцією, що означає кореляцію між членами ряду чисел, розташованих у певному порядку. Також синонімами цього терміну є лагова кореляція та персистентність. Наприклад, часто зустрічається автокореляція геофізичних процесів, що означає перенесення залишкового процесу на наступні часові проміжки.\n\n\nПозитивно автокорельований часовий ряд часто називають персистентним, що значить існування тенденції слідування великих значень за великими та малих за малими, інакше позитивно корельований часовий ряд можна назвати інертним.\n\n\nВізьмемо \\(N\\) пар спостережень двох змінних \\(x\\) та \\(y\\). Коефіцієнт кореляції між парами \\(x\\) та \\(y\\) визначається як\n\n\\[\n    r = \\frac{\\sum \\left( x_i - \\bar{x} \\right) \\left( y_i - \\bar{y} \\right)}{\\sqrt{\\sum \\left( x_i - \\bar{x} \\right)^{2}} \\sqrt{\\sum \\left( y_i - \\bar{y} \\right)^{2}}}, \\tag{3}\n\\]\nде сума знаходиться по всім \\(N\\) спостереженням.\n\nТаким же чином можна визначати й автокореляцію, або ж кореляцію всередині досліджуваного часового ряду. Для автокореляції першого порядку береться лаг (часова затримка), рівний 1 часовій одиниці. Таким чином, автокореляція першого порядку використовує перші \\(N−1\\) спостережень \\(x_t, t = 1,..., N−1\\), та наступні \\(N−1\\) спостережень \\(x_t , t = 2,..., N\\).\n\n\nКореляція між \\(x_t\\) та \\(x_t + 1\\) визначається наступним чином:\n\n\\[\nr_1 = \\frac{\\sum_{t=1}^{N-1} \\left( x_t - \\bar{x} \\right) \\left( x_{t+1} - \\bar{x} \\right)}{\\sum_{t=1}^{N}\\left( x_t - \\bar{x} \\right)^2}, \\tag{4}\n\\]\nде \\(x\\) - це середнє для досліджуваного періоду.\n\nРівняння (4) може бути узагальнене для отримання кореляції між спостереженнями, розділеними \\(k\\) часовими інтервалами:\n\n\\[\n    r_k = \\frac{\\sum_{t=1}^{N-k} \\left( x_t - \\bar{x} \\right) \\left( x_{t+k} - \\bar{x} \\right)}{\\sum_{t=1}^{N}\\left( x_t - \\bar{x} \\right)^2}. \\tag{4}\n\\]\n\nЗначення \\(r_k\\) називається коефіцієнтом автокореляції з лагом \\(k\\). Графік функції автокореляції як залежності \\(r_k\\) від \\(k\\) також називають корелограмою."
  },
  {
    "objectID": "lab_1.html#хід-роботи",
    "href": "lab_1.html#хід-роботи",
    "title": "2  Лабораторна робота № 1",
    "section": "2.2 Хід роботи",
    "text": "2.2 Хід роботи\nДля подальшої роботи з моделювання складних систем візьмемо з основу бібліотеку yfinance, що дозволяє працювати з даними фінансових ринків засобами мови програмування Python.\n\n\n\n\n\n\nПримітка\n\n\n\nYahoo!, Y!Finance, and Yahoo! finance є зареєстрованими товарними знаками Yahoo, Inc.\nyfinance не є афілійованим, схваленим або перевіреним Yahoo, Inc. Це інструмент з відкритим вихідним кодом, який використовує загальнодоступні API Yahoo, і призначений для дослідницьких та освітніх цілей.\nВи повинні звернутися до умов використання Yahoo! (сюди, сюди і сюди) для отримання детальної інформації про ваші права на використання фактично завантажених даних. Пам’ятайте — фінансовий API Yahoo! призначений лише для особистого використання.\n\n\nДля встановлення бібліотеки yfinance можете скористатися наступною командою:\n\n!pip install yfinance --upgrade --no-cache-dir\n\nГітхаб репозиторій містить більше інформації по самій бібліотеці та помилкам, що можуть виникнути та їх потенційним рішенням.\n\n2.2.1 Вступ до модуля Ticker()\nПерш за все імпортуємо бібліотеку yfinance за допомогою наступної команди:\n\nimport yfinance as yf\n\nМодуль Ticker() дозволяє отримувати ринкові та метадані для цінного паперу, використовуючи Python:\n\nmsft = yf.Ticker(\"MSFT\")\nprint(msft)\n\nyfinance.Ticker object &lt;MSFT&gt;\n\n\nМожна вилучити всю інформацію по досліджуваному індексу:\n\n# отримати інформацію по індексу\nprint(msft.info)\n\n{'address1': 'One Microsoft Way', 'city': 'Redmond', 'state': 'WA', 'zip': '98052-6399', 'country': 'United States', 'phone': '425 882 8080', 'website': 'https://www.microsoft.com', 'industry': 'Software—Infrastructure', 'industryDisp': 'Software—Infrastructure', 'sector': 'Technology', 'sectorDisp': 'Technology', 'longBusinessSummary': 'Microsoft Corporation develops and supports software, services, devices and solutions worldwide. The Productivity and Business Processes segment offers office, exchange, SharePoint, Microsoft Teams, office 365 Security and Compliance, Microsoft viva, and Microsoft 365 copilot; and office consumer services, such as Microsoft 365 consumer subscriptions, Office licensed on-premises, and other office services. This segment also provides LinkedIn; and dynamics business solutions, including Dynamics 365, a set of intelligent, cloud-based applications across ERP, CRM, power apps, and power automate; and on-premises ERP and CRM applications. The Intelligent Cloud segment provides server products and cloud services, such as azure and other cloud services; SQL and windows server, visual studio, system center, and related client access licenses, as well as nuance and GitHub; and enterprise services including enterprise support services, industry solutions, and nuance professional services. The More Personal Computing segment offers Windows, including windows OEM licensing and other non-volume licensing of the Windows operating system; Windows commercial comprising volume licensing of the Windows operating system, windows cloud services, and other Windows commercial offerings; patent licensing; and windows Internet of Things; and devices, such as surface, HoloLens, and PC accessories. Additionally, this segment provides gaming, which includes Xbox hardware and content, and first- and third-party content; Xbox game pass and other subscriptions, cloud gaming, advertising, third-party disc royalties, and other cloud services; and search and news advertising, which includes Bing, Microsoft News and Edge, and third-party affiliates. The company sells its products through OEMs, distributors, and resellers; and directly through digital marketplaces, online, and retail stores. The company was founded in 1975 and is headquartered in Redmond, Washington.', 'fullTimeEmployees': 221000, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Satya  Nadella', 'age': 55, 'title': 'Chairman & CEO', 'yearBorn': 1967, 'fiscalYear': 2022, 'totalPay': 12676750, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Bradford L. Smith LCA', 'age': 63, 'title': 'Pres & Vice Chairman', 'yearBorn': 1959, 'fiscalYear': 2022, 'totalPay': 4655274, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Amy E. Hood', 'age': 50, 'title': 'Exec. VP & CFO', 'yearBorn': 1972, 'fiscalYear': 2022, 'totalPay': 4637915, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Judson  Althoff', 'age': 49, 'title': 'Exec. VP & Chief Commercial Officer', 'yearBorn': 1973, 'fiscalYear': 2022, 'totalPay': 4428268, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Christopher David Young', 'age': 50, 'title': 'Exec. VP of Bus. Devel., Strategy & Ventures', 'yearBorn': 1972, 'fiscalYear': 2022, 'totalPay': 4588876, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Alice L. Jolla', 'age': 55, 'title': 'Corp. VP & Chief Accounting Officer', 'yearBorn': 1967, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. James Kevin Scott', 'age': 50, 'title': 'Exec. VP of AI & CTO', 'yearBorn': 1972, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Brett  Iversen', 'title': 'Gen. Mang. of Investor Relations', 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Frank X. Shaw', 'title': 'Corp. VP for Corp. Communications', 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Christopher C. Capossela', 'age': 52, 'title': 'Exec. VP & Chief Marketing Officer', 'yearBorn': 1970, 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 6, 'boardRisk': 3, 'compensationRisk': 3, 'shareHolderRightsRisk': 2, 'overallRisk': 2, 'governanceEpochDate': 1690848000, 'compensationAsOfEpochDate': 1672444800, 'maxAge': 86400, 'priceHint': 2, 'previousClose': 327.0, 'open': 332.85, 'dayLow': 321.06, 'dayHigh': 332.98, 'regularMarketPreviousClose': 327.0, 'regularMarketOpen': 332.85, 'regularMarketDayLow': 321.06, 'regularMarketDayHigh': 332.98, 'dividendRate': 2.72, 'dividendYield': 0.0083, 'exDividendDate': 1692144000, 'payoutRatio': 0.2748, 'fiveYearAvgDividendYield': 1.06, 'beta': 0.903706, 'trailingPE': 33.263428, 'forwardPE': 25.514263, 'volume': 9366773, 'regularMarketVolume': 9366773, 'averageVolume': 27877593, 'averageVolume10days': 20817470, 'averageDailyVolume10Day': 20817470, 'bid': 321.56, 'ask': 321.64, 'bidSize': 800, 'askSize': 800, 'marketCap': 2392308318208, 'fiftyTwoWeekLow': 213.43, 'fiftyTwoWeekHigh': 366.78, 'priceToSalesTrailing12Months': 11.289, 'fiftyDayAverage': 334.5692, 'twoHundredDayAverage': 285.35226, 'trailingAnnualDividendRate': 2.72, 'trailingAnnualDividendYield': 0.008318043, 'currency': 'USD', 'enterpriseValue': 2397717659648, 'profitMargins': 0.34146, 'floatShares': 7423671316, 'sharesOutstanding': 7429760000, 'sharesShort': 35852374, 'sharesShortPriorMonth': 38002864, 'sharesShortPreviousMonthDate': 1688083200, 'dateShortInterest': 1690761600, 'sharesPercentSharesOut': 0.0047999998, 'heldPercentInsiders': 0.00052, 'heldPercentInstitutions': 0.73212, 'shortRatio': 1.09, 'shortPercentOfFloat': 0.0047999998, 'impliedSharesOutstanding': 7429760000, 'bookValue': 27.748, 'priceToBook': 11.604079, 'lastFiscalYearEnd': 1688083200, 'nextFiscalYearEnd': 1719705600, 'mostRecentQuarter': 1688083200, 'earningsQuarterlyGrowth': 0.2, 'netIncomeToCommon': 72361000960, 'trailingEps': 9.68, 'forwardEps': 12.62, 'pegRatio': 2.04, 'lastSplitFactor': '2:1', 'lastSplitDate': 1045526400, 'enterpriseToRevenue': 11.315, 'enterpriseToEbitda': 23.502, '52WeekChange': 0.17267346, 'SandP52WeekChange': 0.056414247, 'lastDividendValue': 0.68, 'lastDividendDate': 1692144000, 'exchange': 'NMS', 'quoteType': 'EQUITY', 'symbol': 'MSFT', 'underlyingSymbol': 'MSFT', 'shortName': 'Microsoft Corporation', 'longName': 'Microsoft Corporation', 'firstTradeDateEpochUtc': 511108200, 'timeZoneFullName': 'America/New_York', 'timeZoneShortName': 'EDT', 'uuid': 'b004b3ec-de24-385e-b2c1-923f10d3fb62', 'messageBoardId': 'finmb_21835', 'gmtOffSetMilliseconds': -14400000, 'currentPrice': 321.99, 'targetHighPrice': 440.0, 'targetLowPrice': 232.0, 'targetMeanPrice': 387.17, 'targetMedianPrice': 400.0, 'recommendationMean': 1.8, 'recommendationKey': 'buy', 'numberOfAnalystOpinions': 44, 'totalCash': 111256002560, 'totalCashPerShare': 14.974, 'ebitda': 102022995968, 'totalDebt': 79441002496, 'quickRatio': 1.536, 'currentRatio': 1.769, 'totalRevenue': 211914997760, 'debtToEquity': 38.522, 'revenuePerShare': 28.46, 'returnOnAssets': 0.14245, 'returnOnEquity': 0.38824, 'freeCashflow': 47268999168, 'operatingCashflow': 87581999104, 'earningsGrowth': 0.202, 'revenueGrowth': 0.083, 'grossMargins': 0.6892, 'ebitdaMargins': 0.48143002, 'operatingMargins': 0.41772997, 'financialCurrency': 'USD', 'trailingPegRatio': 2.3941}\n\n\nМожна вилучити ринкові значення за максимальний період часу:\n\n# отримати ринкові історичні значення індексу\nmsft.history(period=\"max\")\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n1986-03-13 00:00:00-05:00\n0.055004\n0.063093\n0.055004\n0.060396\n1031788800\n0.0\n0.0\n\n\n1986-03-14 00:00:00-05:00\n0.060396\n0.063632\n0.060396\n0.062553\n308160000\n0.0\n0.0\n\n\n1986-03-17 00:00:00-05:00\n0.062553\n0.064172\n0.062553\n0.063632\n133171200\n0.0\n0.0\n\n\n1986-03-18 00:00:00-05:00\n0.063632\n0.064172\n0.061475\n0.062014\n67766400\n0.0\n0.0\n\n\n1986-03-19 00:00:00-05:00\n0.062014\n0.062553\n0.060396\n0.060936\n47894400\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-08-18 00:00:00-04:00\n314.489990\n318.380005\n311.549988\n316.480011\n24744800\n0.0\n0.0\n\n\n2023-08-21 00:00:00-04:00\n317.929993\n322.769989\n317.040009\n321.880005\n24040000\n0.0\n0.0\n\n\n2023-08-22 00:00:00-04:00\n325.500000\n326.079987\n321.459991\n322.459991\n16102000\n0.0\n0.0\n\n\n2023-08-23 00:00:00-04:00\n323.820007\n329.200012\n323.459991\n327.000000\n21073100\n0.0\n0.0\n\n\n2023-08-24 00:00:00-04:00\n332.850006\n332.980011\n321.059998\n321.989990\n9366773\n0.0\n0.0\n\n\n\n\n9439 rows × 7 columns\n\n\n\nОкрім цього, yfinance дозволяє отримати інформацію по дивідентам та сплітам фінансового індексу:\n\n# показувати дії (дивіденди, спліти)\nmsft.actions\n\n\n\n\n\n\n\n\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n1987-09-21 00:00:00-04:00\n0.00\n2.0\n\n\n1990-04-16 00:00:00-04:00\n0.00\n2.0\n\n\n1991-06-27 00:00:00-04:00\n0.00\n1.5\n\n\n1992-06-15 00:00:00-04:00\n0.00\n1.5\n\n\n1994-05-23 00:00:00-04:00\n0.00\n2.0\n\n\n...\n...\n...\n\n\n2022-08-17 00:00:00-04:00\n0.62\n0.0\n\n\n2022-11-16 00:00:00-05:00\n0.68\n0.0\n\n\n2023-02-15 00:00:00-05:00\n0.68\n0.0\n\n\n2023-05-17 00:00:00-04:00\n0.68\n0.0\n\n\n2023-08-16 00:00:00-04:00\n0.68\n0.0\n\n\n\n\n88 rows × 2 columns\n\n\n\n\n# продемонструвати дивіденти\nmsft.dividends\n\nDate\n2003-02-19 00:00:00-05:00    0.08\n2003-10-15 00:00:00-04:00    0.16\n2004-08-23 00:00:00-04:00    0.08\n2004-11-15 00:00:00-05:00    3.08\n2005-02-15 00:00:00-05:00    0.08\n                             ... \n2022-08-17 00:00:00-04:00    0.62\n2022-11-16 00:00:00-05:00    0.68\n2023-02-15 00:00:00-05:00    0.68\n2023-05-17 00:00:00-04:00    0.68\n2023-08-16 00:00:00-04:00    0.68\nName: Dividends, Length: 79, dtype: float64\n\n\n\n# продемонструвати спліти\nmsft.splits\n\nDate\n1987-09-21 00:00:00-04:00    2.0\n1990-04-16 00:00:00-04:00    2.0\n1991-06-27 00:00:00-04:00    1.5\n1992-06-15 00:00:00-04:00    1.5\n1994-05-23 00:00:00-04:00    2.0\n1996-12-09 00:00:00-05:00    2.0\n1998-02-23 00:00:00-05:00    2.0\n1999-03-29 00:00:00-05:00    2.0\n2003-02-18 00:00:00-05:00    2.0\nName: Stock Splits, dtype: float64\n\n\nДля методу history() доступні наступні аргументи:\n\nperiod: період даних для завантаження (або використовуйте параметр period, або використовуйте start і end). Допустимі періоди: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max.\ninterval: інтервал даних (внутрішньоденні дані не можуть перевищувати 60 днів) Допустимі інтервали 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo.\nstart: Якщо не використовується період — завантажте рядок дати початку у форматі (YYYY-MM-DD) або datetime.\nend: Якщо не використовується період — завантажте рядок дати закінчення (YYYY-MM-DD) або datetime.\nprepost: Включати в результати попередні та пост ринкові дані (За замовчуванням False).\nauto_adjust: Автоматично налаштовувати всі OHLC (ціни відкриття, закриття, найбільшу та найменшу) (За замовчуванням True).\nactions: Завантажувати події дивідендів та дроблення акцій (За замовчуванням True).\n\n\n\n2.2.2 Одночасне вивантаження декількох ринкових активів\nЯк і до цього, ви також можете завантажувати дані для кількох тикерів одночасно.\n\ndata = yf.download(\"SPY AAPL\", \n                   start=\"2017-01-01\", \n                   end=\"2017-04-30\") # вивантажуємо дані, \n                                     # зберігаємо до змінної data\n\ndata.head() # виводимо перші 5 рядків нашого масиву даних \n\n[*********************100%%**********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-03\n27.059305\n201.337372\n29.037500\n225.240005\n29.082500\n225.830002\n28.690001\n223.880005\n28.950001\n225.039993\n115127600\n91366500\n\n\n2017-01-04\n27.029018\n202.535141\n29.004999\n226.580002\n29.127501\n226.750000\n28.937500\n225.610001\n28.962500\n225.619995\n84472400\n78744400\n\n\n2017-01-05\n27.166473\n202.374298\n29.152500\n226.399994\n29.215000\n226.580002\n28.952499\n225.479996\n28.980000\n226.270004\n88774400\n78379000\n\n\n2017-01-06\n27.469334\n203.098343\n29.477501\n227.210007\n29.540001\n227.750000\n29.117500\n225.899994\n29.195000\n226.529999\n127007600\n71559900\n\n\n2017-01-09\n27.720943\n202.427887\n29.747499\n226.460007\n29.857500\n227.070007\n29.485001\n226.419998\n29.487499\n226.910004\n134247600\n46939700\n\n\n\n\n\n\n\nДля отримання конкретно цін закриття індексу SPY, вам варто використовувати наступну команду: data['Close']['SPY'] Але, якщо вам потребується згрупувати дані по їх символу, можна скористатися наступним записом:\n\ndata = yf.download(\"SPY AAPL\", \n                   start=\"2017-01-01\", \n                   end=\"2017-04-30\",\n                   group_by=\"ticker\")\n\n[*********************100%%**********************]  2 of 2 completed\n\n\nТепер для звернення до цін закриття індексу SPY, вам треба використовувати наступний запис: data['SPY']['Close'].\nМетод download() приймає додатковий параметр — threads для швидшої обробки великої кількості фінансових індексів одночасно.\nДля подальшої роботи у даній лабораторній та подальших нас ще цікавитимуть наступні бібліотеки:\n\nmatplotlib: комплексна бібліотека для створення статичних, анімованих та інтерактивних візуалізацій на Python. Matplotlib робить прості речі простими, а складні — можливими.\npandas: програмна бібліотека, написана для мови програмування Python для маніпулювання та аналізу даних. Зокрема, вона пропонує структури даних та операції для маніпулювання числовими таблицями та часовими рядами.\nnumpy: бібліотека, що додає підтримку великих багатовимірних масивів і матриць, а також велику колекцію високорівневих математичних функцій для роботи з цими масивами.\nneurokit2: зручна бібліотека, що забезпечує легкий доступ до розширених процедур обробки біосигналів. Дослідники та клініцисти без глибоких знань програмування або біомедичної обробки сигналів можуть аналізувати фізіологічні дані за допомогою лише двох рядків коду. Перевага даної бібліотеки полягає в тому, що вона надає функціонал, який можна використовувати не лише для біомедичних сигналів, але й для фінансових, фізичних тощо.\n\nВстановити кожну з даних бібліотек можна в наступний спосіб: !pip install *назва бібліотеки*:\n\n!pip install matplotlib pandas numpy neurokit2\n\nІмпортуємо кожну із зазначених бібліотек:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport numpy as np\nimport neurokit2 as nk\n\nДалі нам треба буде визначити стиль рисунків для виведення та збереження. Зробити це можна в наступний спосіб. Для використання подальшого стилю рисунків потребується встановити наступну бібліотеку:\n\n# для встановлення останньої версії (із PyPI)\n!pip install SciencePlots\n\nВиконаємо налаштування стилю наших подальших рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків \n                                      # за замовчуванням\n        \n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nПредставлені налаштування є орієнтовними і можуть змінюватись у ході наступних лабораторних. Ви можете встановлювати власні налаштування. На сайті бібліотеки matplotlib можна ознайомитись з усіма можливими командами.\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи yfinance:\n\nsymbol = 'BTC-USD'       # Символ індексу\nstart = \"2015-09-01\"     # Дата початку зчитування даних\nend = \"2020-03-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиведемо досліджуваний ряд\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)\nax.set_ylabel(ylabel)\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 2.1: Динаміка щоденних змін індексу Біткоїна\n\n\n\n\nВидно, що ряд нестаціонарний, що викликає певні ускладнення для подальшого аналізу. Тому перейдемо до прибутковостей, які вже є стаціонарними, а їх нормалізація стандартним відхиленням дозволяє легко порівнювати їх розподіл з розподілом Гауса.\nПрибутковості розраховуватимуться згідно формулі (1). У Python ми використовуватимемо метод pct_change() для знаходження прибутковостей, що доступний нам завдяки бібліотеці pandas.\nСтандартизовані прибутковості визначаються наступним шляхом:\n\\[\ng(t) = \\frac{G(t) - \\mu}{\\sigma},  \n\\]\nде \\(\\mu\\) відповідає середньому значенню прибутковостей за досліджуваний часовий інтервал, а \\(\\sigma\\) представляє стандартне відхилення.\n\nret = time_ser.copy()      # копіюємо значення вихідного ряду для збереження \n                           # його від змін\n\nret = ret.pct_change()     # знаходимо прибутковості\nret -= ret.mean()          # вилучаємо середнє \nret /= ret.std()           # ділимо на стандартнє відхилення\n\nret = ret.dropna().values  # видаляємо всі можливі нульові значення \n\nВиводимо отриманий результат\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index[1:], ret)           # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Додаємо підпис для вісі Ох\nax.set_ylabel(ylabel + ' прибутковості')   # Додаємо підпис для вісі Оу\nax.axhline(y = 3.0, color = 'r', linestyle = '--')  # Додаємо горизонтальну лінію, що роз-\n                                                    # межує 3 сигма події\nax.axhline(y = -3.0, color = 'r', linestyle = '--') # Додаємо горизонтальну лінію, що роз-\n                                                    # межує -3 сигма події\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'Прибутковості{symbol}.jpg')  # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 2.2: Нормалізовані прибутковості досліджуваного часового ряду\n\n\n\n\nЗверніть увагу, що флуктуації нормалізованих прибутковостей досить часто перевищують величину \\(\\pm 3\\sigma\\), що, як відомо, надзвичайно рідко спостерігається для незалежних подій. Цей факт можна відобразити шляхом порівняння функції розподілу нормалізованих флуктуацій з розподілом Гауса (рис. Рисунок 2.3). Очевидно, що хвости розподілу вихідного ряду містять значні флуктуації, вони досить помітні (часто кажуть “важкі” у порівнянні з самою “головою” розподілу).\nДля побудови нормального розподілу скористаємось бібліотекою scipy. Встановити її можна по аналогії з попередніми бібліотеками.\n\n# Для встановлення останньої версії scipy\n!pip install scipy\n\n\nfrom scipy.stats import norm # імпорт модуля norm для побудови Гаусового розподілу\n\nФункція щільності ймовірності для norm має наступний вид:\n\\[\n    f(x) = \\frac{\\exp{(-x^2/2)}}{\\sqrt{2\\pi}}\n\\]\nдля дійсних значень \\(x\\).\n\nmu, sigma = norm.fit(ret)\n\nx = np.linspace(ret.min(), ret.max(), 10000) # Генеруємо 10000 значень для побудови \n                                             # Гаусового розподілу\np = norm.pdf(x, mu, sigma)                   # Отримання значень функції щільності\n\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(x, p, label='Гаус')                # Додаємо дані до графіку\nax.hist(ret, bins=50,                      # Побудова гістограми для прибутковостей\n        density=True, \n        alpha=0.6, \n        color='g',\n        label='прибутковості '+symbol)\n\nax.legend()                                # Додаємо легенду\nax.set_xlabel(\"g\")                         # Додаємо підпис для вісі Ох\nax.set_ylabel(r\"$f(g)$\")                   # Додаємо підпис для вісі Оу\nax.set_yscale('log')\n\n\nplt.savefig(f'Гаус + прибутковості {symbol}.jpg')       # Зберігаємо графік \nplt.show();                                             # Виводимо графік\n\n\n\n\nРисунок 2.3: Порівняння функцій розподілу нормалізованих прибутковостей з нормальним розподілом\n\n\n\n\nЯк ми можемо бачити, підігнана крива Гауса відхиляється від істинної частоти настання подій, що перевищують \\(\\pm 3\\sigma\\). Отже, ми можемо стверджувати, що наші прибутковості не є незалежними. Підтвердження цьому факту будемо шукати шляхом вивчення кореляційних властивостей нашого часового ряду.\nДля простоти обчислень скористаємось функцією signal_autocor() бібліотеки neurokit2. Виглядає дана функція наступним чином:\nsignal_autocor(signal, lag=None, demean=True, method='auto', show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) - Вектор значень.\nlag (int) — Часовий лаг. Якщо вказано, буде повернуто одне значення автокореляції сигналу з його власним лагом.\ndemean (bool) — Якщо має значення True, від сигналу буде відніматися середнє значення сигналу перед обчисленням автокореляції.\nmethod (str) — Використання \"auto\" запускає scipy.signal.correlate для використання швидшого алгоритму. Інші методи зберігаються з причин застарілості, але не рекомендуються. Інші методи включають \"correlation\" (за допомогою np.correlate()) або \"fft\" (швидке перетворення Фур’є).\nshow (bool) — якщо значення True, побудувати графік автокореляції для всіх значень затримки.\n\nПовертає\n\nr (float) - крос-кореляція сигналу з самим собою на різних часових лагах. Мінімальний часовий лаг дорівнює 0, максимальний часовий лаг дорівнює довжині сигналу. Або значення кореляції на певному часовому лазі, якщо лаг не дорівнює None.\ninfo (dict) - Словник, що містить додаткову інформацію, наприклад, довірчий інтервал.\n\n\n# розрахунок автокореляції\n\nr_init, _ = nk.signal_autocor(time_ser.values, \n                              method='correlation')  # для вихідних значень ряду                                                                    \nr_ret, _ = nk.signal_autocor(ret, \n                             method='correlation')   # для прибутковостей\nr_vol, _ = nk.signal_autocor(np.abs(ret), \n                             method='correlation')   # для модулів прибутковостей\n\nr_range = np.arange(1, len(r_ret) + 1)               # генерація лагів\n\n\nfig, ax = plt.subplots()                    # Створюємо порожній графік\n\nax.plot(r_range, r_init[1:], label=symbol)  # Додаємо дані до графіку\nax.plot(r_range, r_ret, label=r'$g(t)$')                          \nax.plot(r_range, r_vol, label=r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Autocorrelation r\")          # Додаємо підпис для вісі Оу\nax.set_ylim(-1.1, 1.1)                      # Встановлюємо обмеження по вісі Oy\n\nplt.savefig(f'Автокореляції {symbol}.jpg')  # Зберігаємо графік \nplt.show();                                 # Виводимо графік\n\n\n\n\nРисунок 2.4: Зміна з часом парних автокореляційних функцій для вихідного ряду x, нормалізованих прибутковостей g та їх модулів mod(g)\n\n\n\n\nАле, досліджуючи складні системи варто пам’ятати, що їх складність є варіативною. Тому і внутрішні кореляції системи на різних часових лагах також варіюються з плином часу. Із цього випливає, що подальши розрахунки варто виконувати не для всього ряду, а для його фрагментів.\nПодальші розрахунки здійснюватимуться в рамках алгоритму ковзного (рухомого) вікна. Для цього виділятиметься частина часового ряду (вікно), для якої розраховуватимуться міри складності, потім вікно зміщуватиметься разом з часовим рядом на заздалегідь визначену величину, і процедура повторюватиметься до тих пір, поки значення всього ряду не будуть вичерпані. Далі, порівнюючи динаміку фактичного часового ряду і відповідних мір складності, ми матимемо змогу судити про характерні зміни в динаміці міри складності зі зміною досліджуваної системи. Якщо та чи інша міра складності поводиться певним чином для всіх періодів крахів, наприклад, зменшується або збільшується під час передкризовий або передкритичний період, то вона може служити їх індикатором або провісником.\nРозглянемо як поводитиме себе функція автокореляцій та волатильність в рамках алгоритму ковзного вікна.\nСпочатку визначимо параметри\n\nret_type = 4 # вид ряду: \n             # 1 - вихідний, \n             # 2 - детрендований (різниця між теп. значенням та попереднім)\n             # 3 - прибутковості звичайні, \n             # 4 - стандартизовані прибутковості, \n             # 5 - абсолютні значення (волатильності),\n             # 6 - стандартизований вихідний ряд \n\nlength = len(time_ser) # довжина всього ряду\n\nwindow = 250    # довжина вікна - період у межах якого розраховуватимуться наші індикатори\ntstep = 1       # крок зміщення вікна\nvolatility = [] # масив значень волатильностей \nautocorr = []   # масив значень автокореляції при змінній lag\n\nДалі розпочнемо розрахунки. Для відслідковування прогресу зміщення ковзного вікна скористаємось бібліотекою tqdm. Її можна встановити аналогічно попереднім бібліотекам.\n\n!pip install tqdm\n\nІмпортуємо модуль для візуалізації прогресу\n\nfrom tqdm import tqdm\n\nі тепер приступимо до виконання віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # Фрагменти довжиною window  \n                                              # з кроком tstep\n                                              \n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент\n\n                                          # подальшому відбираємо потрібний тип ряду                                         \n    if ret_type == 1:                     # вихідні значення \n        pass\n    elif ret_type == 2:                   # різниці\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:                   # прибутковості\n        fragm = fragm.pct_change()\n    elif ret_type == 4:                   # стандартизовані прибутковості\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:                   # абсолютні значення прибутковостей\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        fragm = fragm.abs()\n    elif ret_type == 6:                   # стандартизований вихідний ряд\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values         # видаляємо зайві нульові значення, якщо є\n    \n    # розрахунок віконної автокореляції\n    r_window, _ = nk.signal_autocor(fragm, method='correlation') \n\n    # розрахунок волатильності по модулям прибутковостей                                     \n    vol_window = np.mean(np.abs(fragm))\n\n    # збереження результатів до масивів\n    volatility.append(vol_window)\n    autocorr.append(r_window[1])\n\n100%|██████████| 1393/1393 [00:01&lt;00:00, 933.40it/s]\n\n\nЗбережемо результати в окремих текстових файлах\n\n# збереження результатів ковзної автокореляції\nnp.savetxt(f\"autocorr_name={symbol}_ \\\n            window={window}_step={tstep}_ \\\n            rettype={ret_type}.txt\", autocorr)\n\n# збереження результатів ковзної волатильності\nnp.savetxt(f\"volatility_name={symbol}_ \\\n            window={window}_step={tstep}_ \\\n            rettype={ret_type}.txt\", volatility)\n\nНарешті порівняємо динаміку вихідного ряду і розрахованих похідних. Для цього врахуємо, що автокореляцію і волатильність ми рахували для рухомого вікна. Результати представлено на рис. Рисунок 2.5.\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.10))\nax4.spines.right.set_position((\"axes\", 1.19))\n\np1, = ax.plot(time_ser.index[window:length:tstep], \n              time_ser.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window+1:length:tstep], \n               ret[window:length:tstep], \"r--\", label=r\"$G(t)$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], \n               autocorr, \"g-\", label=r\"$A$\")\np4, = ax4.plot(time_ser.index[window:length:tstep],\n               volatility, \"m-\", label=r\"$V$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax4.legend(handles=[p1, p2, p3, p4])\n\nplt.savefig(f\"all_name={symbol}_ret={ret_type}_\\\n            wind={window}_step={tstep}.jpg\")\nplt.show();\n\n\n\n\nРисунок 2.5: Динаміка індексу Біткоїна, нормалізованих прибутковостей, віконних автокореляції та волатильності\n\n\n\n\nАналізуючи графік, можна зробити висновок, що у певні моменти спостерігалися стрибки волатильності (як і автокореляції) із поступовим зменшенням її до попереднього рівня, що може бути внаслідок збурень у процесі роботи ринку. Аналіз таких збурень, їх частоти та сили, дозволяє виявляти приховані закономірності роботи ринку."
  },
  {
    "objectID": "lab_1.html#висновок",
    "href": "lab_1.html#висновок",
    "title": "2  Лабораторна робота № 1",
    "section": "2.3 Висновок",
    "text": "2.3 Висновок\nТаким чином, аналіз флуктуацій прибутковостей та волатильностей шляхом побудови функції автокореляції та розподілу ймовірності дозволяє отримати певні висновки, що можуть допомогти в роботі із аналізованими часовими рядами і ринком, з якого взято зазначені часові ряди. Зокрема, у даному випадку, можна давати рекомендації аналітикам, що працюють на фінансових ринках."
  },
  {
    "objectID": "lab_2.html#теоретичні-відомості",
    "href": "lab_2.html#теоретичні-відомості",
    "title": "3  Лабораторна робота № 2",
    "section": "3.1 Теоретичні відомості",
    "text": "3.1 Теоретичні відомості\nДослідження складних систем, як природних, так і штучних, показали, що в їх основі лежать нелінійні процеси, ретельне вивчення яких необхідне для розуміння і моделювання складних систем. У останні десятиліття набір традиційних (лінійних) методик дослідження був істотно розширений нелінійними методами, одержаними з теорії нелінійної динаміки і хаосу; багато досліджень були присвячені оцінці нелінійних характеристик і властивостей процесів, що протікають в природі (скейлінг, фрактальна розмірність). Проте більшість методів нелінійного аналізу вимагає або достатньо довгих, або стаціонарних рядів даних, які досить важко одержати з природи. Більш того, було показано, що дані методи дають задовільні результати для моделей реальних систем, що ідеалізуються. Ці чинники вимагали розробки нових методик нелінійного аналізу даних.\nСтан природних або штучних систем, як правило, змінюється в часі. Вивчення цих, часто складних, процесів — важлива задача в багатьох дисциплінах, дозволяє зрозуміти і описати їх суть, наприклад, для прогнозування стану на деякий час в майбутнє. Метою таких досліджень є знаходження математичних моделей, які б достатньо відповідали реальним процесам і могли б бути використані для розв’язання поставлених задач.\nРозглянемо ідею і коротко теорію рекурентного аналізу, наведемо деякі приклади, розглянемо його можливі області застосування при аналізі і прогнозування складних фінансово-економічних систем.\n\n3.1.1 Фазовий простір та його реконструкція\nСтан системи описується її змінними стану\n\\[\nx^1(t),x^2(t),...,x^d(t)\n\\]\nде верхній індекс — номер змінної. Набір із \\(d\\) змінних стану у момент часу \\(t\\) складає вектор стану \\(\\vec x(t)\\) в \\(d\\)-вимірному фазовому просторі. Даний вектор переміщається в часі в напрямі,визначуваному його вектором швидкості:\n\\[\n\\dot{\\vec x}(t)=\\partial_t\\vec x(t)=\\vec F(t)\n\\]\nПослідовність векторів \\(\\vec x(t)\\) утворює траєкторію у фазовому просторі, причому поле швидкості \\(\\vec F\\) дотичне до цієї траєкторії. Еволюція траєкторії описує динаміку системи і її атрактор. Знаючи \\(\\vec F\\), можна одержати інформацію про стан системи в момент \\(t\\) шляхом інтегрування виразу. Оскільки форма траєкторії дозволяє судити про характер процесу (періодичні або хаотичні процеси мають характерні фазові портрети), то для визначення стану системи не обов’язково проводити інтегрування, достатньо побудувати графічне відображення траєкторії.\nПри дослідженні складних систем часто немає інформації про всі змінні стану, або не все з них можливо виміряти. Як правило, є єдине спостереження, проведене через дискретний часовий інтервал \\(\\Delta t\\). Таким чином, вимірювання записуються у вигляді ряду \\(u_i(t)\\) i , де \\(t=i\\cdot \\Delta t\\). Інтервал \\(\\Delta t\\) може бути постійним, проте це не завжди можливо і створює проблеми для застосування стандартних методів аналізу даних, що вимагають рівномірної шкали спостережень.\nВзаємодії і їх кількість в складних системах такі, що навіть по одній змінній стану можна судити про динаміку всієї системи в цілому (даний факт був встановлений групою американських учених при вивченні турбулентності). Таким чином, еквівалентна фазова траєкторія, що зберігає структури оригінальної фазової траєкторії, може бути відновлена з одного спостереження або часового ряду за теоремою Такенса (Takens) методом часових затримок:\n\\[\n\\widehat{\\vec x}(t)=(u_i,u_{i+\\tau},...,u_{i+(m-1)\\tau})\n\\]\nде \\(m\\) — розмірність вкладення, \\(\\tau\\) — часова затримка (реальна часова затримка визначається як \\(\\tau \\cdot \\Delta t\\)). Топологічні структури відновленої траєкторії зберігаються, якщо \\(m \\geq 2 \\cdot d+1\\), де \\(d\\) — розмірність атрактора. На практиці більшості випадків атрактор може бути відновлений і при \\(m \\leq 2d\\). Затримка, як правило, вибирається апріорно.\nІснує кілька підходів до вибору мінімально достатньої розмірності \\(m\\), крім аналітичного. Високу ефективність показали методи, засновані на концепції фальшивих найближчих точок (false nearest neighbours, FNN). Суть її заключається у тому, що при зменшенні розмірності вкладення відбувається збільшення кількості фальшивих точок, що потрапляють в околицю будь-якої точки фазового простору. Звідси витікає простий метод — визначення кількості FNN як функції від розмірності. Існують і інші методи, засновані на цій концепції — наприклад, визначення відносин відстаней між одними і тими ж сусідніми точками при різних \\(m\\). Розмірність атрактора також може бути визначена за допомогою крос-кореляційних сум.\n\n\n\n\n \n\n\nРисунок 3.1: Відрізок траєкторії у фазовому просторі системи Рьослера \\(i\\) (a); відповідний рекурентний графік (b). Вектор фазового простору в точці \\(j\\), який потрапляє в околицю (сіре коло в (a)) заданого вектора фазового простору вектора в точці \\(i\\) вважається точкою рекурентності (чорна точка на траєкторії в (a)). Вона позначається чорною точкою на рекурентній діаграмі у позиції \\((i, j)\\). Вектор фазового простору за межами околу (порожнє коло в (a)) призводить до білої точки в рекурентній діаграмі\n\n\n\n\n3.1.2 Рекурентний аналіз\nПроцесам в природі властива яскраво виражена рекурентна поведінка, така, як періодичність або іррегулярна циклічність. Більш того, рекурентність (повторюваність) станів в значенні проходження подальшої траєкторії достатньо близько до попередньої є фундаментальною властивістю дисипативних динамічних систем. Ця властивість була відмічена ще в 80-х роках XIX століття французьким математиком Пуанкаре (Poincare) і згодом сформульовано у вигляді “теореми рекурентності”, опублікованої в 1890 р.:\n\n\n\n\n\n\nПримітка\n\n\n\nЯкщо система зводить свою динаміку до обмеженої підмножини фазового простору, то система майже напевно, тобто з вірогідністю, практично рівною 1, скільки завгодно близько повертається до якого-небудь спочатку заданого режиму.\n\n\nСуть цієї фундаментальної властивості у тому, що, не дивлячись на те, що навіть саме мале збурення в складній динамічній системі може привести систему до експоненціального відхилення від її стану, через деякий час система прагне повернутися до стану, деяким чином близького до попереднього, і проходить при цьому подібні етапи еволюції.\nПереконатися в цьому можна за допомогою графічного зображення траєкторії системи у фазовому просторі. Проте можливості такого аналізу сильно обмежені. Як правило, розмірність фазового простору складної динамічної системи більша трьох, що робить практично незручним його розгляд напряму; єдина можливість — проекції в дво- і тривимірні простори, що часто не дає вірного уявлення про фазовий портрет.\nУ 1987 р. Екман (Eckmann) і співавтори запропонували спосіб відображення \\(m\\)-вимірної фазової траєкторії станів системи \\(\\vec x(t)\\) завдовжки \\(N\\) на двовимірну квадратну двійкову матрицю розміром \\(N \\times N\\) , в якій 1 (чорна точка) відповідає повторенню стану при деякому часі \\(i\\) в деякий інший час \\(j\\), а обидві координатні осі є осями часу. Таке представлення було назване рекурентною картою або діаграмою (recurrence plot, RP), оскільки воно фіксує інформацію про рекурентну поведінку системи.\nМатематично вищесказане описується як\n\\[\nR_{i,j}^{m,\\varepsilon_i}=\\Theta(\\varepsilon_i-\\| \\vec x_i - \\vec x_j \\|), \\cdot \\vec x \\in \\Re^m, \\cdot i, j=1...N\n\\]\nде \\(N\\) — кількість даних станів, \\(x_i, \\varepsilon_i\\) — розмір околиці точки \\(\\vec x\\) у момент \\(i\\), \\(\\| \\cdot \\|\\) — норма і \\(\\Theta(\\cdot)\\) — функція Хевісайда.\nНепрактично і, як правило, неможливо знайти повну рекурентність у значенні \\(\\vec x_i \\equiv \\vec x_j\\) (стан динамічної, а особливо — хаотичної системи не повторюється повністю еквівалентно початковому стану, а підходить до нього скільки завгодно близько). Таким чином, рекурентність визначається як достатня близькість стану \\(\\vec x_j\\) до стану \\(\\vec x_i\\). Іншими словами, рекурентними є стани \\(\\vec x_j\\), які потрапляють в \\(m\\)-вимірну околицю з радіусом \\(\\varepsilon_i\\) і центром в \\(\\vec x_i\\). Ці точки \\(\\vec x_j\\) називаються рекурентними точками (recurrence points).\nОскільки \\(R_{i,i}=1\\), \\(i=1,...,N\\) за визначенням, то рекурентна діаграма завжди міститьчорну діагональну лінію — лінію ідентичності (line of identity, LOI) під кутом \\(\\pi/4\\) до осей координат. Довільно узята рекурентна точка не несе якої-небудь корисної інформації про стани в часи \\(i\\) і \\(j\\). Тільки вся сукупність рекурентних точок дозволяє відновити властивості системи.\nЗовнішній вигляд рекурентної діаграми дозволяє судити про характер процесів, які протікають в системі, наявності і впливі шуму, станів повторення і завмирання (ламінарності), здійсненні в ході еволюції системи різких змін стану (екстремальних подій).\n\n\n    \nРисунок 3.2: Типові динамічні ряди і їх рекурентні карти\n\n\n\n\n3.1.3 Аналіз діаграм\nОчевидно, що процеси різної поведінки даватимуть рекурентні діаграми з різним рисунком. Таким чином, візуальна оцінка діаграм може дати уявлення про еволюцію досліджуваної траєкторії. Виділяють два основних класи структури зображення: топологія (typology), що представляється крупномасштабними структурами, і текстура (texture), що формується дрібномасштабними структурами.\nТопологія дає загальне уявлення про характер процесу. Виділяють чотири основні класи:\n\nоднорідні рекурентні діаграми типові для стаціонарних і автономних систем, в яких час релаксації малий у порівнянні з довжиною ряду;\nперіодичні структури, що повторюються (діагональні лінії, узори у шаховому порядку) відповідають різним осцилюючим системам з періодичністю в динаміці;\nдрейф відповідає системам з параметрами, що поволі змінюються, що робить білими лівий верхній і правий нижній кути рекурентної діаграми;\nрізкі зміни в динаміці системи, рівно як і екстремальні ситуації, обумовлюють появу білих областей або смуг.\n\nРекурентні діаграми спрощують виявлення екстремальних і рідкісних подій.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 3.3: Характернi топологiї рекурентних дiаграм: (а) — однорiдна (нормально розподiлений шум); (b) — перiодична (генератор Ван дер Поля); (c) — дрейф (вiдображення Iкеди з накладеною послiдовнiстю, що лiнiйно росте); (d) — контрастнi областi або смуги (узагальнений броунiвський рух)\n\n\nДокладний розгляд рекурентних діаграм дозволяє виявити дрібномасштабні структури — текстуру, яка складається з простих точок, діагональних, горизонтальних і вертикальних ліній. Комбінації вертикальних і горизонтальних ліній формують прямокутні кластери точок.\n\nсамотні, окремо розташовані рекурентні точки з’являються в тому разі, коли відповідні стани рідкісні, або нестійкі в часі, або викликані сильною флуктуацією. При цьому вони не є ознаками випадковості або шуму;\nдіагональні лінії \\(R_{i+k, j+k}=1\\) (при \\(k = 1...l\\) де \\(l\\) — довжина діагональної лінії) з’являються у разі, коли сегмент траєкторії у фазовому просторі пролягає паралельно іншому сегменту, тобто траєкторія повторює саму себе, повертаючись в одну і ту ж область фазового простору у різний час. Довжина таких ліній визначається часом, протягом якого сегменти траєкторії залишаються паралельними; напрям (кут нахилу) ліній характеризує внутрішній час підпроцесів, відповідних даним сегментам траєкторії. Проходження ліній паралельно лінії ідентичності (під кутом \\(\\pi/4\\) до осей координат) свідчить про однаковий напрям сегментів траєкторії, перпендикулярно — про протилежний («відображені» сегменти), що може також бути ознакою реконструкції фазового простору з невідповідною розмірністю вкладення. Нерегулярна поява діагональних ліній є ознакою хаотичного процесу;\nвертикальні (горизонтальні) лінії \\(R_{i, j+k}=1\\) (при \\(k = 1...\\upsilon\\), де \\(\\upsilon\\) — довжина вертикальної або горизонтальної лінії) виділяють проміжки часу, в котрі стан системи не змінюється або змінюється трохи (система як би «заморожена» на цей час), що є ознакою «ламінарних» станів.\n\n\n\n\nРисунок 3.4: Основнi концепцiї рекурентного аналiзу. Вiдображена дiаграма рекурентностi базується на часовому ряду, що було реконструйовано до 11 реконструйованих векторiв, вiд \\(\\vec{X}(0)\\) до \\(\\vec{X}(10)\\). Видiлено дiагональну лiнiю довжиною \\(d = 3\\), вертикальна лiнiя довжиною \\(v = 3\\) i бiлу вертикальну лiнiю довжиною \\(w = 5\\)"
  },
  {
    "objectID": "lab_2.html#хід-роботи",
    "href": "lab_2.html#хід-роботи",
    "title": "3  Лабораторна робота № 2",
    "section": "3.2 Хід роботи",
    "text": "3.2 Хід роботи\nСпочатку побудуємо дво- та тривимірні фазові портрети як для модельних значень, так і для реальних. Використовуватимемо бібліотеки neurokit2 для побудови атракторів та рекурентного аналізу.\n\n3.2.1 Процедура реконструкції фазового простору\nДля побудови фазового портрету скористаємось методами complexity_attractor() та complexity_embedding() бібліотеки neuralkit2. Синтаксис complexity_attractor() виглядає наступним чином:\ncomplexity_attractor(embedded='lorenz', alpha='time', color='last_dim', shadows=True, linewidth=1, **kwargs)\nПараметри\n\nembedded (Union[str, np.ndarray]) — результат функції complexity_embedding(). Також може бути рядком, наприклад, \"lorenz\" (атрактор Лоренца) або \"rossler\" (атрактор Рьосслера).\nalpha (Union[str, float]) — прозорість ліній. Якщо \"time\", то лінії будуть прозорими як функція часу (повільно).\ncolor (str) — Колір графіку. Якщо \"last_dim\", буде використано останній вимір (максимум 4-й) вбудованих даних, коли розмірність більша за 2. Корисно для візуалізації глибини (для 3-вимірного вбудовування), або четвертого виміру, але працюватиме це повільно.\nshadows (bool) — якщо значення True, 2D-проекції буде додано до бокових сторін 3D-атрактора.\nlinewidth (float) — задає товщину лінії.\n****kwargs** — До палітри кольорів (наприклад, name=\"plasma\") або до симулятора системи Лоренца передаються додаткові аргументи ключових слів, такі як duration (за замовчуванням = 100), sampling_rate (за замовчуванням = 10), sigma (за замовчуванням = 10), beta (за замовчуванням = 8/3), rho (за замовчуванням = 28).\n\nЯк вже зазначалося, побудова фазового простору, на основі якого і проводитиметься рекурентний аналіз, вимагає реконструкції. Виконати реконструкції фазового простору із одновимірного часового ряду можна із використанням методу часових затримок.\nМетод часових затримок є однією з ключових концепцій науки про складність, що ми використовуватимемо і в подальших лабораторних. Він базується на ідеї, що динамічна система може бути описана вектором чисел, який називається її “станом”, що має на меті забезпечити повний опис системи в певний момент часу. Множина всіх можливих станів називається “простором станів”.\nТеорема Такенса (1981) припускає, що послідовність вимірювань динамічної системи містить у собі всю інформацію, необхідну для повної реконструкції простору станів. Метод часових затримок намагається визначити стан \\(s\\) системи в певний момент часу \\(t\\), шукаючи в минулій історії спостережень схожі стани, і, вивчаючи еволюцію схожих станів, виводити інформацію про майбутнє системи.\nЯк візуалізувати динаміку системи? Послідовність значень стану в часі називається траєкторією. Залежно від системи, різні траєкторії можуть еволюціонувати до спільної підмножини простору станів, яка називається атрактором. Наявність та поведінка атракторів дає інтуїтивне уявлення про досліджувану динамічну систему.\nОдже, згідно Такенсу, ідея полягає в тому, щоб на основі одиничних вимірювань системи, отримати \\(m\\)-розмірні реконструйовані часові вкладення\n\\[\n    \\vec{y}_i = \\left( y_i, y_{i+\\tau}, ... , y_{i+(m-1)\\tau} \\right), \\tag{1}\n\\]\nде \\(i\\) проходить в діапазоні \\(1,..., N-(m-1)\\tau\\); значення \\(\\tau\\) представляє часову затримку, а \\(m\\) — це розмірність вкладень (кількість змінних, що включає кожна траєкторія).\nКод для реконструкції фазового простору може виглядати наступним чином:\nY = np.zeros((dimension, N - (dimension - 1) * delay)) # ініціалізуємо масив нулів,\n                                                       # що будуть представляти траєкторії\nfor i in range(dimension):\n    Y[i] = signal[i * delay : i * delay + Y.shape[1]]  # заповнюємо кожну траєкторію \n\nembedded = Y.T                                          \nreturn embedded                                        # повертаємо результат \nДля реконструкції фазового простору використовуватимемо метод complexity_embedding(). Його синтаксис виглядає наступним чином:\ncomplexity_embedding(signal, delay=1, dimension=3, show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень. Також може бути рядком, наприклад, \"lorenz\" (атрактор Лоренца), \"rossler\" (атрактор Росслера) або \"clifford\" (атрактор Кліффорда) для отримання попередньо визначеного атрактора.\ndelay (int) — часова затримка (часто позначається \\(\\tau\\) іноді називають запізненням). Ще розглянемо метод complexity_delay() для оцінки оптимального значення цього параметра.\ndimension (int) — розмірність вкладень (\\(m\\), іноді позначається як \\(d\\) або порядок). Далі звернемось до методу complexity_dimension(), щоб оцінити оптимальне значення для цього параметра.\nshow (bool) — Побудувати графік реконструйованого атрактора.\n****kwargs** — інші аргументи, що передаються до complexity_attractor().\n\nПовертає\n\narray — реконструйований атрактор розміру length - (dimension - 1) * delay\n\nДалі імпортуємо необхідні для подальшої роботи модулі\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\n\nІ виконаємо налаштування рисунків для виводу\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nТепер розглянемо можливість використання методу часових затримок і отриманих в подальшому атракторів у якості індикатора складності. Як і в попередній роботі, для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи yfinance:\n\nsymbol = 'BTC-USD'       # Символ індексу\nstart = \"2015-09-01\"     # Дата початку зчитування даних\nend = \"2020-03-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nСпочатку оберемо вид ряду: 1. вихідний ряд 2. детермінований (різниця між теперішнім та попереднім значенням) 3. прибутковості звичайні 4. стандартизовані прибутковості 5. абсолютні значення (волатильності) 6. стандартизований ряд\nДля подальших розрахунків накращим варіантом буде вибір стандартизованого вихідного ряду або прибутковостей, оскільки значення вихідного часового ряду відрізняються на декілька порядків, і можуть сильно перевищувати встановлений параметр \\(\\varepsilon\\). Тобто, для вихідних значень, що сильно різняться між собою, увесь часовий діапазон буде розглядатися як нерекурентний.\nСпочатку визначимо функції для виконання перетворення ряду:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nі тепер виконаємо перетворення, використовуючи дану функцію:\n\nsignal = time_ser.copy()\nret_type = 6    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rec = transformation(signal, ret_type) \n\nОскільки ми не матимемо змоги візуалізувати багатовимірний фазовий простір (\\(m&gt;3\\)), ми послуговуватимемось значеннями \\(m=2\\) та \\(m=3\\). Значення \\(\\tau\\) будемо варіювати як із власних переконань, так і з опорою на функціонал бібліотеки neuralkit2.\nСкористаємось методом complexity_simulate() для генерації різних тестових сигналів.\n\nsignal_random_walk = nk.complexity_simulate(duration=30, \n                                            sampling_rate=100, \n                                            method=\"randomwalk\") # симуляція випадкового блукання\n\n\nnk.complexity_attractor(embedded=nk.complexity_embedding(signal_random_walk, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"orange\"); \n\n\n\n\nРисунок 3.5: Двовимірний фазовий портрет випадкового блукання\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_random_walk, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"orange\");\n\n\n\n\nРисунок 3.6: Тривимірний фазовий портрет випадкового блукання\n\n\n\n\n\nsignal_ornstein = nk.complexity_simulate(duration=30, \n                                        sampling_rate=100, \n                                        method=\"ornstein\") # симуляція системи Орнштайна\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_ornstein, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРисунок 3.7: Двовимірний фазовий портрет системи Орнштайна\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_ornstein, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРисунок 3.8: Двовимірний фазовий портрет системи Орнштайна\n\n\n\n\n\nnk.complexity_attractor(color = \"last_dim\", alpha=\"time\", duration=1);\n\n\n\n\nРисунок 3.9: Тривимірний фазовий портрет атрактора Лоренца\n\n\n\n\n\nnk.complexity_attractor(\"rossler\", color = \"blue\", alpha=1, sampling_rate=5000);\n\n\n\n\nРисунок 3.10: Тривимірний фазовий портрет атрактора Рьосслера\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"lime\"); \n\n\n\n\nРисунок 3.11: Двовимірний фазовий портрет вихідних значень досліджуваного ряду Біткоїна\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"lime\"); \n\n\n\n\nРисунок 3.12: Тривимірний фазовий портрет вихідних значень досліджуваного ряду Біткоїна\n\n\n\n\nУ зазначених вище прикладах прикладах ми обирали параметри \\(m\\) і \\(\\tau\\) згідно нашим власним міркуванням. Але, як правило, при виконанні серйозного дослідження, що матиме прикладне застосування, лише власних переконань буває недостатньо. У нашому випадку бажано було б, щоб зазначені параметри обирались автоматично, опираючись на конкретну статистичну процедуру. Бібліотека neurokit2 представляє функціонал для автоматичного підбору параметрів розмірності та часової затримки. Коротко їх опишемо.\n\n\n3.2.2 Автоматизований підбір параметра часової затримки, \\(\\tau\\)\nЧасова затримка (Tau \\(\\tau\\) також відома як Lag) є одним з двох критичних параметрів, що беруть участь у процедурі реконструкції фазового простору. Він відповідає затримці у відліках між вихідним сигналом і його затриманою версією (версіями). Іншими словами, скільки відліків ми розглядаємо між певним станом сигналу та його найближчим минулим станом.\nЯкщо \\(\\tau\\) менше оптимального теоретичного значення, послідовні координати стану системи корельовані і атрактор недостатньо розгорнутий. І навпаки, коли \\(\\tau\\) більше, ніж повинно бути, послідовні координати майже незалежні, що призводить до некорельованої та неструктурованої хмари точок.\nВибір параметрів затримки та розмірності представляє нетривіальну задачу. Один з підходів полягає у їх (напів)незалежному виборі (оскільки вибір розмірності часто вимагає затримки) за допомогою функцій complexity_delay() та complexity_dimension(). Однак, існують методи спільного оцінювання, які намагаються знайти оптимальну затримку та розмірність одночасно.\nЗауважте також, що деякі автори (наприклад, Розенштейн, 1994) пропонують спочатку визначити оптимальну розмірність вбудовування, а потім розглядати оптимальне значення затримки як оптимальну затримку між першою та останньою координатами затримки (іншими словами, фактична затримка має дорівнювати оптимальній затримці, поділеній на оптимальну розмірність вбудовування мінус 1).\nДекілька авторів запропонували різні методи для вибору затримки:\n\nФрейзер і Свінні (1986) пропонують використовувати перший локальний мінімум взаємної інформації між затриманим і незатриманим часовими рядами, ефективно визначаючи значення Tau, для якого вони діляться найменшою інформацією (і де атрактор є найменш надлишковим). На відміну від автокореляції, взаємна інформація враховує також нелінійні кореляції.\nТейлер (1990) запропонував вибирати таке значення Tau, при якому автокореляція між сигналом та його зміщенною версією при Tau вперше перетинає значення \\(1/\\exp\\). Методи, що базуються на автокореляції, мають перевагу в короткому часі обчислень, коли вони обчислюються за допомогою алгоритму швидкого перетворення Фур’є (fast Fourier transform, FFT).\nКасдаглі (1991) пропонує замість цього брати перший нульовий перетин автокореляції.\nРозенштейн (1993) пропонує апроксимувати точку, де функція автокореляцій падає до \\(\\left( 1-1/\\exp \\right)\\) від свого максимального значення.\nРозенштейн (1994) пропонує наближатися до точки, близької до 40% нахилу середнього зміщення від діагоналі.\nКім (1999) пропонує оцінювати Tau за допомогою кореляційного інтегралу, який називається C-C методом, і який, як виявилося, узгоджується з результатами, отриманими за допомогою методу взаємної інформації. Цей метод використовує статистику в реконструйованому фазовому просторі, а не аналізує часову еволюцію ряду. Однак час обчислень для цього методу значно довший через необхідність порівнювати кожну унікальну пару парних векторів у реконструйованому сигналі на кожну затримку.\nЛайл (2021) описує “Реконструкцію симетричного проекційного атрактора” (Symmetric Projection Attractor Reconstruction, SPAR), де \\(1/3\\) від домінуючої частоти (тобто довжини середнього “циклу”) може бути підходящим значенням для приблизно періодичних даних, і робить атрактор чутливим до морфологічних змін. Див. також доповідь Астона. Цей метод також є найшвидшим, але може не підходити для аперіодичних сигналів. Аргумент алгоритму (за замовчуванням \"fft\").\n\nМожна також зазначити наступний метод для об’єднаного підбору параметрів затримки та розмірності:\n\nГаутама (2003) зазначає, що на практиці часто використовують фіксовану часову затримку і відповідно регулюють розмірність вбудовування. Оскільки це може призвести до великих значень \\(m\\) (а отже, до вкладених даних великого розміру) і, відповідно, до повільної обробки, вони описують метод оптимізації для спільного визначення \\(m\\) і \\(\\tau\\) на основі показника entropy ratio.\n\nРозглянемо оптимальні значення розмірності та затримки для часового сигналу Біткоїна:\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"fraser1986\")\n\n\n\n\nРисунок 3.13: Оптимальне значення розмірності на основі методу Фрейзера і Свінні для часового ряду Біткоїна\n\n\n\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"theiler1990\")\n\n\n\n\nРисунок 3.14: Оптимальне значення розмірності на основі методу Тейлера для часового ряду Біткоїна\n\n\n\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=500, show=True,\n                                        method=\"casdagli1991\")\n\ndelay\n\nnan\n\n\nЯк можна бачити по прикладу вище, не всі методи надають адекватну оцінку розмірності нашого сигналу. Спробуємо привести вихідні значення Біткоїна до прибутковостей та повторити процедуру Касдаглі ще раз.\n\nret_type = 4 \nret = transformation(signal, ret_type)\n\n\ndelay, parameters = nk.complexity_delay(ret, \n                                        delay_max=300, show=True,\n                                        method=\"casdagli1991\")\n\n\n\n\nРисунок 3.15: Оптимальне значення розмірності на основі методу Касдаглі для прибутковостей Біткоїна\n\n\n\n\nЦього разу нам вдалося досягти оптимального результату, але приклад вище демонструє, що кожна процедура має свої виключення.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"rosenstein1993\")\n\n\n\n\nРисунок 3.16: Оптимальне значення розмірності на основі методу Розенштайна (1993) для часового ряду Біткоїна\n\n\n\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"rosenstein1994\")\n\n\n\n\nРисунок 3.17: Оптимальне значення розмірності на основі методу Розенштайна (1994) для часового ряду Біткоїна\n\n\n\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"lyle2021\")\n\n\n\n\nРисунок 3.18: Оптимальне значення розмірності на основі методу Лайла для часового ряду Біткоїна\n\n\n\n\nТепер подивимось як це виглядатиме для об’єднаного підбору параметрів\n\ndelay, parameters = nk.complexity_delay(for_rec,\n    delay_max=np.arange(1, 30, 1), # діапазон значень затримки\n    dimension_max=20,              # максимальна розмірність вкладень\n    method=\"gautama2003\",\n    surrogate_n=5,                 # Кількість сурогатних сигналів \n                                   # для генерації\n    surrogate_method=\"random\",     # Спосіб генерації сигналів\n    show=True)\n \n\n\n\n\nРисунок 3.19: Оптимальне значення розмірності та затримки на основі методу Гаутами для часового ряду Біткоїна\n\n\n\n\n\ndimension = parameters[\"Dimension\"]\ndimension\n\n20\n\n\n\n\n3.2.3 Автоматизований підбір параметра розмірності вкладень, \\(m\\)\nЗа дану процедуру відповідає метод complexity dimension(). Її синтаксис виглядає наступним чином:\ncomplexity_dimension(signal, delay=1, dimension_max=20, method='afnn', show=False, **kwargs)\nХоча зазвичай використовують \\(m=2\\) або \\(m=3\\), але різні автори пропонують наступні процедури підбору:\n\nКореляційна розмірність (Correlation Dimension, CD): Одним з перших методів оцінки оптимального \\(m\\) був розрахунок кореляційної розмірності для вкладень різного розміру і пошук насичення (тобто плато) в її значенні при збільшенні розміру векторів. Одне з обмежень полягає в тому, що насичення буде також мати місце, коли даних недостатньо для адекватного заповнення простору високої розмірності (зауважте, що в загальному випадку не рекомендується мати настільки великі вбудовування, оскільки це значно скорочує довжину сигналу).\nНайближчі хибні сусіди (False Nearest Neighbour, FNN): Метод, запропонований Кеннелом та ін., базується на припущенні, що дві точки, які є близькими одна до одної в достатній розмірності вбудовування, повинні залишатися близькими при збільшенні розмірності. Алгоритм перевіряє сусідів при збільшенні розмірності вкладень, поки не знайде лише незначну кількість хибних сусідів при переході від розмірності \\(m\\) до \\(m+1\\). Це відповідає найнижчій розмірності вбудовування, яка, як передбачається, дає розгорнуту реконструкцію просторово-часового стану. Цей метод може не спрацювати в зашумлених сигналах через марну спробу розгорнути шум (а в чисто випадкових сигналах кількість хибних сусідів суттєво не зменшується зі збільшенням \\(m\\)). На рисунку нижче показано, як проекції на простори більшої розмірності можна використовувати для виявлення хибних найближчих сусідів. Наприклад, червона та жовта точки є сусідами в одновимірному просторі, але не в двовимірному.\n\n\n\n\n\n\n\nСередні хибні сусіди (Average False Neighbors, AFN): Ця модифікація методу FNN, розроблена Сао (1997), усуває один з його основних недоліків — необхідність евристичного вибору порогових значень \\(r\\). Метод використовує максимальну евклідову відстань для представлення найближчих сусідів і усереднює всі відношення відстані в \\(m+1\\) розмірності до розмірності \\(m\\) і визначає E1 та E2 як параметри. Оптимальна розмірність відповідає досягається тоді, коли E1 перестає змінюватися (досягає плато). E1 досягає плато при розмірності d0, якщо сигнал надходить від атрактора. Тоді d0+1* є оптимальною мінімальною розмірністю вкладення. E2 є корисною величиною для того, щоб відрізнити детерміновані сигнали від стохастичних. Константа E2, що близька до 1 для будь-якої розмірності вкладень \\(d\\), вказує на випадковість даних, оскільки майбутні значення не залежать від минулих значень.\n\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка у відліках. Для вибору оптимального значення цього параметра ми ще скористаємось методом complexity_delay().\ndimension_max (int) — максимальний розмір вкладення для тестування.\nmethod (str) — Може бути \"afn\" (середні хибні сусіди), \"fnn\" (найближчий хибний сусід) або \"cd\" (кореляційна розмірність).\nshow (bool) — Візуалізувати результат.\n****kwargs** — інші аргументи, такі як \\(R=10.0\\) або \\(A=2.0\\) (відносне та абсолютне граничне значення, тільки для методу \"fnn\").\n\nПовертає\n\ndimension (int) — оптимальна розмірність вкладень.\nparameters (dict) — словник python, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимальної розмірності.\n\nСпробуємо отримати оптимальне значення розмірності згідно зазначених процедур. В якості часової затримки можна взять \\(\\tau=100\\). Приблизно таке значення спостерігалося для кожної процедури.\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=100,\n                                                  dimension_max=10,\n                                                  method='cd',\n                                                  show=True)\n\n\n\n\nРисунок 3.20: Оптимальне значення розмірності на основі кореляційної розмірності для часового ряду Біткоїна\n\n\n\n\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=100,\n                                                  dimension_max=10,\n                                                  method='fnn',\n                                                  show=True)\n\n\n\n\nРисунок 3.21: Оптимальне значення розмірності на основі найближчих хибних сусідів для часового ряду Біткоїна\n\n\n\n\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=20,\n                                                  dimension_max=20,\n                                                  method='afnn',\n                                                  show=True)\n\n\n\n\nРисунок 3.22: Оптимальне значення розмірності на основі середніх найближчих хибних сусідів для часового ряду Біткоїна\n\n\n\n\nУ даному випадку розмірність вкладень можна обирати в діапазоні значень від 3 до 7. Тепер на основі отриманих результатів приступимо до побудови рекурентної діаграми.\n\n\n3.2.4 Побудова рекурентної матриці\nЯк вже зазначалося, рекурентний аналіз кількісно визначає кількість і тривалість рекурентних станів динамічної системи, що визначаються на основі реконструйованих траєкторій фазового простору.\nМи маємо змогу побудувати рекурентну матрицю, використовуючи метод recurrence_matrix().\nЙого синтаксис виглядає наступним чином:\nrecurrence_matrix(signal, delay=1, dimension=3, tolerance='default', show=False)\nПараметри\n\nsignal (Union[list, np.ndarray, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі.\ndimension (int) — розмірність вкладень, \\(m\\).\ntolerance (float) — радіус \\(\\varepsilon\\) багатовимірного околу в межах якого шукаються рекурентні траєкторії (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються схожими. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\). Емпіричним правилом є встановлення \\(r\\) таким чином, щоб відсоток точок, класифікованих як рекурентні, становив приблизно 2-5%.\nshow (bool) — візуалізувати рекурентну матрицю.\n\nПовертає\n\nnp.ndarray — рекурентну матрицю.\nnp.ndarray — матрицю відстаней.\n\nПобудуємо рекурентну матрицю для вихідних значень Біткоїна, його прибутковостей та стандартизованого вихідного ряду. Розмірність \\(m=4\\), часова затримка \\(\\tau=1\\), радіус \\(\\varepsilon=0.3\\).\n\nrc, _ = nk.recurrence_matrix(signal, \n                            delay=1, \n                            dimension=4, \n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРисунок 3.23: Рекурентна матриця для вихідних значень Біткоїна\n\n\n\n\nЯк можна бачити з представленого рисунку всі траєкторії залишаються доволі віддаленими один від одного, ніякої рекурентності тут не передбачається.\nТепер спробуємо подивитися на стандартизовані прибутковості.\n\nrc, _ = nk.recurrence_matrix(ret, \n                            delay=1, \n                            dimension=4,\n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРисунок 3.24: Рекурентна матриця для стандартизованих прибутковостей Біткоїна\n\n\n\n\nТепер можемо бачити, що Біткоїн став характризуватися чорними смугами, що відображають динаміку певних детермінованих процесів. У той же час білі смуги характеризують періоди абсолютно аномальної (непередбачуваної поведінки на даному ринку). Видно, що прибутковості залишаються доволі некорельованими, про що і свідчить переважне домінування саме білих областей.\nСпробуємо тепер подивитись на стандартизований вихідний ряд.\n\nrc, _ = nk.recurrence_matrix(for_rec, \n                            delay=1, \n                            dimension=4,\n                            tolerance=0.3,\n                            show=True)\n\n\n\n\nРисунок 3.25: Рекурентна матриця для стандартизованого вихідного ряду Біткоїна\n\n\n\n\nНа початку свого існування біткоїн характеризувався доволі високим ступенем передбачуваності, меншої волатильності власних коливань. Надалі почали предомінувати білі області, але видно, що тепер Біткоїну властива динаміка подібна до броунівсього руху."
  },
  {
    "objectID": "lab_3.html#теоретичні-відомості",
    "href": "lab_3.html#теоретичні-відомості",
    "title": "4  Лабораторна робота № 3",
    "section": "4.1 Теоретичні відомості",
    "text": "4.1 Теоретичні відомості\nДля якісного опису системи графічне представлення системи підходить якнайкраще. Однак головним недоліком графічного представлення є те, що воно змушує користувачів суб’єктивно інтуїтивно інтерпретувати закономірності та структури, представлені на рекурентній діаграмі.\nКрім того, зі збільшенням розміру даних, проблематичним представляється аналіз усіх \\(N^2\\) значень. Як наслідок, доводиться працювати з окремими ділянками вихідних даних. Аналіз у такий спосіб може створювати нові дефекти, які спотворюють об’єктивність спостережуваних закономірностей і призводять до неправильних інтерпретацій. Щоб подолати це обмеження і поширити об’єктивну оцінку серед дослідників, на початку 1990-х років Веббером та Збілутом були введені визначення та процедури для кількісної оцінки складності рекурентних діаграм, а згодом вони були розширені Марваном та ін.\nДрібномасштабні кластери можуть являти собою комбінацію ізольованих точок (випадкових рекурентностей). Подібна еволюція в різні періоди часу або в зворотному часовому порядку представлятиме діагональні лінії (детерміновані структури), а також вертикальні/горизонтальні лінії для позначення ламінарних станів (переривчастість) або станів, що предсталяють сингулярності. Для кількісного опису системи системи такі дрібномасштабні кластери слугують основою кількісного рекурентного аналізу (recurrence quantification analysis, RQA)."
  },
  {
    "objectID": "lab_3.html#хід-роботи",
    "href": "lab_3.html#хід-роботи",
    "title": "4  Лабораторна робота № 3",
    "section": "4.2 Хід роботи",
    "text": "4.2 Хід роботи\nПерш ніж переходити до опису кожної з мір та розрахунків, визначемось з інструментарієм для виконання RQA. Як і до цього, ми використовуватимемо бібліотеку neuralkit2.\nТепер імпортуємо бібліотеки для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nfrom tqdm import tqdm\n\nІ виконаємо налаштування рисунків для виводу:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд фондового індексу Доу-Джонса за період з 1 грудня 1993 по 1 грудня 2022, використовуючи yfinance:\n\nsymbol = '^DJI'          # Символ індексу\nstart = \"1993-01-01\"     # Дата початку зчитування даних\nend = \"2022-01-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиведемо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 4.1: Динаміка щоденних змін індексу Доу-Джонса\n\n\n\n\nКористуючись тими методами, що ми розглянули в попередній лабораторній роботі, побудуємо атрактор даного ряда та його рекурентну діаграму. Але перш за все, треба стандартизувати наш ряд. Для цього оголосимо функцію transformation(), що прийматиме на вхід часовий сигнал, тип ряду, і повертатиме його перетворення:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nДалі приводимо ряд до стандартизованого вигляду.\n\nsignal = time_ser.copy()\nret_type = 6    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rec = transformation(signal, ret_type) \n\nДля всього ряду і для віконної процедру визначимо наступні параметри:\n\nрозмірність вкладень \\(m=3\\);\nчасова затримка \\(\\tau=1\\);\nрадіус багатовимірного околу \\(\\varepsilon = 0.3\\).\n\nЗадамо необхідні параметри для обчислення та виводу:\n\nm = 3                         # розмірність вкладень\ntau = 1                       # часові затримка\neps = 0.3                     # радіус\n\nІ тепер подивимось на фазові траєкторії досліджуваної системи у дво- та тривимірному просторах:\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=2, delay=tau), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРисунок 4.2: Двовимірний фазовий портрет стандартизованих вихідних значень досліджуваного ряду Доу-Джонса\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=3, delay=tau), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\nРисунок 4.3: Тривимірний фазовий портрет стандартизованих вихідних значень досліджуваного ряду Доу-Джонса\n\n\n\n\nЯк можна бачити по візуальному огляду траєкторій у фазовому просторі важко робити висновки стосовно передбачуванності або хаотичності системи. Спробуємо ще раз, але тепер послуговуючись рекурентною діаграмою:\n\nrc, _ = nk.recurrence_matrix(for_rec, \n                            delay=1, \n                            dimension=m,\n                            tolerance=eps,\n                            show=True)\n\n\n\n\nРисунок 4.4: Рекурентна матриця для стандартизованого вихідного ряду Доу-Джонса\n\n\n\n\nЯк можна бачити, на основі рекурентної діаграми в перспективі ми можемо отримати куди більше інформації стосовно еволюції системи. Видно, що 2000-2008 рік характеризувалися найвищим ступенем самоорганізації (рекурентності) про що свідчать доволі велика щільність чорних областей. У той же час можна бачити, що останні роки характеризуються найменшим ступенем рекурентності. Можливо, прогнозованість подій у межах 2022 року варто було б охарактеризувати за допомогою інших індикаторів, але але рекурентна матриця говорить, що події минулих років мало корелюють з теперішнім.\nМи вже зазначали, що якісна репрезентація рекурентності станів не є достатньо об’єктивною. Найращим варіантом у даному випадку буде використання рекурентного аналізу наряду с алгоритмом рухомого вікна, що використовувався нами у першій лабораторній роботі, і буде використовуватись і надалі.\n\n4.2.1 Віконна процедура\nДля подальшої роботи створюємо віконну процедуру, в якій знов визначаємо вид ряду та ще декілька параметрів. Потім ми ініціалізуємо масиви для кожної рекурентної міри.\n\nret_type = 6            # вид ряду\nwindow = 250            # ширина вікна\ntstep = 1               # часовий крок вікна \nlength = len(time_ser)  # довжина самого ряду\n\nm = 1                   # розмірність вкладень\ntau = 1                 # часові затримка\neps = 0.3               # радіус\n\n                        # Ініціалізуємо масиви для збереження віконних значень \n                        # рекурентних мір\n\nRR = []                 # Частота повторення\nDET = []                # Детермінізм\nDIV = []                # Розбіжність\nAVG_DIAG_LINE = []      # Усереднена довжина діагональних ліній\nENT_DIAG = []           # Ентропія діагональних ліній\nLAM = []                # Ламінарність\nTT = []                 # Час затримки\nENT_VERT = []           # Ентропія вертикальних ліній\nENT_WHITE_VERT = []     # Ентропія білих вертикальних ліній\nAVG_WVERT_LINE = []     # Усереднена довжина білих вертикальних ліній\nVERT_DIV = []           # Розбіжність вертикальних ліній\nRATIO_DET_REC = []      # Відношення детермінізму до частоти повторень\nRATIO_LAM_DET = []      # Відношення ламінарності до детермінізму\nWHITE_VERT_DIV = []     # Розбіжність білих вертикальних ліній\nDIAG_RR = []            # Діагональна частота рекурентних значень\n\nДля подальших розрахунків ми використовуватимемо метод complexity_rqa() бібліотеки neuralkit2. Синтаксис даного методу виглядає наступним чином:\ncomplexity_rqa(signal, dimension=3, delay=1, tolerance='sd', min_linelength=2, method='python', show=False)\nПараметри\n\nsignal (Union[list, np.ndarray, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі.\ndimension (int) — розмірність вкладень, \\(m\\).\ntolerance (float) — радіус \\(\\varepsilon\\) багатовимірного околу в межах якого шукаються рекурентні траєкторії (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються схожими. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\).\nmin_linelength (int) — мінімальна довжина діагональних та вертикальних ліній За замовчування дорівнює 2.\nmethod (str) — Може бути \"pyrqa\" для виконання рекурентного аналізу, але із використанням бібліотеки PyRQA (потребує додаткового встановлення).\nshow (bool) — візуалізувати рекурентну матрицю.\n\nПовертає\n\nrqa (DataFrame) — результати процедури RQA.\ninfo (dict) — словник, що містить інформацію відносно параметрів, що використовувались для виконання RQA.\n\nТепер можемо приступити до віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n    \n    resultRQA, _ = nk.complexity_rqa(fragm,\n                                     delay=tau,\n                                     dimension=m,\n                                     tolerance=eps)\n    \n    # Обчислення відношення ламінарності до детермінізму\n    resultRQA['LamiDet'] = resultRQA['Laminarity']/resultRQA['Determinism']\n\n    # Обчислення дивергенції чорних вертикальних ліній\n    resultRQA['VDiv'] = 1./resultRQA['VMax']\n\n    # Обчислення дивергенції білих вертикальних ліній\n    resultRQA['WVDiv'] = 1./resultRQA['WMax']\n\n    RR.append(resultRQA['RecurrenceRate'])\n    DET.append(resultRQA['Determinism'])\n    DIV.append(resultRQA['Divergence']) \n    AVG_DIAG_LINE.append(resultRQA['L'])\n    ENT_DIAG.append(resultRQA['LEn'])\n    LAM.append(resultRQA['Laminarity']) \n    TT.append(resultRQA['TrappingTime']) \n    ENT_VERT.append(resultRQA['VEn'])\n    ENT_WHITE_VERT.append(resultRQA['WEn'])\n    AVG_WVERT_LINE.append(resultRQA['W']) \n    VERT_DIV.append(resultRQA['VDiv'])\n    WHITE_VERT_DIV.append(resultRQA['WVDiv'])\n    RATIO_DET_REC.append(resultRQA['DeteRec']) \n    RATIO_LAM_DET.append(resultRQA['LamiDet'])\n    DIAG_RR.append(resultRQA['DiagRec'])\n\n100%|██████████| 7054/7054 [02:34&lt;00:00, 45.76it/s]\n\n\nЗберігаємо отримані результати в текстових файлах:\n\nname = f\"RQA_classic_name={symbol}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_m={m}_ \\\n    tau={tau}_eps={eps}.txt\"\n\nnp.savetxt(\"RR\" + name, RR)\nnp.savetxt(\"DIAG_RR\" + name, DIAG_RR)\nnp.savetxt(\"DET\" + name, DET)\nnp.savetxt(\"DIV\" + name, DIV)\nnp.savetxt(\"VERT_DIV\" + name, VERT_DIV)\nnp.savetxt(\"WHITE_VERT_DIV\" + name, WHITE_VERT_DIV)\nnp.savetxt(\"LAM\" + name, LAM)\nnp.savetxt(\"TT\" + name, TT)\nnp.savetxt(\"AVG_DIAG_LINE\" + name, AVG_DIAG_LINE)\nnp.savetxt(\"AVG_WRITE_VERT_LINE\" + name, AVG_WVERT_LINE)\nnp.savetxt(\"ENT_DIAG\" + name, ENT_DIAG)\nnp.savetxt(\"ENT_VERT\" + name, ENT_VERT)\nnp.savetxt(\"ENT_WHITE_VERT\" + name, ENT_WHITE_VERT)\nnp.savetxt(\"RATIO_DET_REC\" + name, RATIO_DET_REC)\nnp.savetxt(\"RATIO_LAM_DET\" + name, RATIO_LAM_DET)\n\n\n\n4.2.2 Рекурентні міри\nТепер займемося побудовою та інтерпретацією отриманих результатів. Для візуалізації графіків визначимо наступну функцію:\n\ndef plot_recurrence_measure(measure, label, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(time_ser.index[window:length:tstep], \n                  time_ser.values[window:length:tstep], \n                  \"b-\", label=fr\"{ylabel}\")\n    p2, = ax2.plot(time_ser.index[window:length:tstep],\n                   measure, \n                   color=clr, \n                   label=fr'${label}$')\n\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(f\"{ylabel}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(label +\n        f\" RQA_classic_name={symbol}_window={window}_step={tstep}_ \\\n        rettype={ret_type}_m={m}_tau={tau}_eps={eps}.jpg\")\n        \n    plt.show();\n\n\n4.2.2.1 Частота рекурентності (Recurrence rate)\nНайпростішим показником є частота рекурентності, яка визначає щільністю рекурентних точок на рекурентній діаграмі, ігноруючи лінію ідентичності:\n\\[\nRR = \\frac{1}{N^2}\\displaystyle\\sum_{i,j=1}^{N}R(i,j)\n\\]\nде \\(N\\) — кількість точок на траєкторії фазового простору.\nЧастота рекурентності відповідає ймовірності того, що певний стан повториться.\n\nplot_recurrence_measure(measure=RR, label='RR')\n\n\n\n\nРисунок 4.5: Динаміка індексу Доу-Джонса та частоти рекурентності\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, міра рекурентності зростає при крахових подіях, що вказує на зростання ступеня самоорганізації та злагодженості торгівельної активності трейдерів на цьому ринку.\n\n\n4.2.2.2 Діагональна частота рекурентності (Diagonal recurrence rate)\nДаний підхід базується на діагональних рекурентних профілях часового ряду. Діагональний рекурентний профіль кількісно оцінює кількість рекурентних точок на різних лагах, подібно до функції автокореляцій. Для отримання діагонального профілю рекурентностей просто підраховується частка рекурентних точок на діагоналях, розташованих в нижньому правому або нижньому лівому куті рекурентної діаграми, і будується графік як функція відстані від головної діагоналі, тобто лагу.\nПо іншому можна сказати, що діагональна частота рекурентності фіксує величину автокореляції на різних лагах.\n\nplot_recurrence_measure(measure=DIAG_RR, label='DRR')\n\n\n\n\nРисунок 4.6: Динаміка індексу Доу-Джонса та діагональної частоти рекурентності\n\n\n\n\nЗ представленого рисунку видно, що діагональна частота рекурентності зростає у передкризові та кризові стани, що вказує на зростання величини автокореляції, що в свою чергу демонструє зростання ступеню самоорганізації у кризові та передкризові стани.\n\n\n4.2.2.3 Детермінізм (Determinism)\nНаступним показником можна визначити частку рекурентних траєкторій, які формують діагональні лінії мінімальної довжини \\({\\displaystyle \\ell _{\\min }}\\). Ця міра називається детермінізмом і пов’язана з передбачуваністю динамічної системи:\n\\[\nDET={\\frac {\\sum _{\\ell =\\ell _{\\min }}^{N}\\ell \\,P(\\ell )}{\\sum _{\\ell =1}^{N}\\ell P(\\ell )}},\n\\]\nде \\(P(\\ell )\\) — частотний розподіл довжин \\(\\ell\\) діагональних ліній (тобто підраховує кількість діагональних профілів довжини \\(\\ell\\) ).\n\n\n\n\n\n\nДодаткова інформація по детермінізму\n\n\n\nДетерміновані системи характеризуються значною варіацією діагональних ліній різної довжинию. Періодичні сигнали будуть характеризуватися довгими діагональними лініями, в той час як для хаотичних сигналів діагональні лінії будуть короткими. Для стохастичним систем діагональні лінії взагалі будуть відсутніми, за винятком випадкових закономріностей, що утворюватимуть дуже короткі діагональні лінії.\nБілий шум, наприклад, мав би рекурентну діаграму з майже ізольованими рекурентними точками та дуже малих відсотком діагональних ліній, тоді як детермінований процес демонстрував би дуже малу кількість одиночних рекурентностей, але велику щільність довгих діагональних ліній.\n\n\n\nplot_recurrence_measure(measure=DET, label='DET')\n\n\n\n\nРисунок 4.7: Динаміка індексу Доу-Джонса та детермінізму\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, як правило, у передкризові та кризові стани показник детермінізму починає зростати, що свідчить і про зростання ступеня передбачуваності (впорядкованості) флуктуацій системи.\n\n\n4.2.2.4 Ламінарність (Laminarity)\nКількість рекурентних станів, які утворюють вертикальні лінії, можна кількісно визначити таким же чином. Ця міра називається ламінарністю і пов’язана з кількістю ламінарних фаз (незмінностей) у системі:\n\\[\nLAM={\\frac {\\sum _{v=v_{\\min }}^{N}vP(v)}{\\sum _{v=1}^{N}vP(v)}},\n\\]\nде \\(P(v)\\) — частотний розподіл довжин \\(v\\) вертикальних ліній, які мають довжину принаймні \\(v_{\\min}\\).\n\n\n\n\n\n\nДодаткова інформація по ламінарності\n\n\n\nЛамінарність характеризує ймовірність системи перебувати в ламінарному (незмінному) стані. Зі збільшенням ізольованих рекурентних точок у системі, міра ламінарності спадатиме.\n\n\n\nplot_recurrence_measure(measure=LAM, label='LAM')\n\n\n\n\nРисунок 4.8: Динаміка індексу Доу-Джонса та ламінарності\n\n\n\n\nМожна бачити, що при кризових станах ступінь ламінарності зростає. Як ми могли бачити, зростає і щільність діагональних точок, і загалом зростає кількість рекурентних траєкторій у фазовому просторі. Це вказує на те, що фінасовий індекс Доу-Джонса “застрягає” у стані кризи. Кризи характеризуються трендостійкістю, персистентністю та детермінованістю своєї поведінки.\n\n\n4.2.2.5 Середня довжина діагональних ліній (The average diagonal lines length)\nТакож можна виміряти середню довжину діагональних ліній. Cередня довжина діагональних лінії визначається як\n\\[\nL={\\frac  {\\sum _{{\\ell =\\ell _{\\min }}}^{N}\\ell \\,P(\\ell )}{\\sum _{{\\ell =\\ell _{\\min }}}^{N}P(\\ell )}}.\n\\]\nЗагалом цей показник характеризує середній період часу при якому дві траєкторії фазового простору знаходяться в достатній близькості один до одного.\n\n\n\n\n\n\nДодаткова інформація по середній довжині діагональних ліній\n\n\n\nСередня довжина діагональних ліній визначає середній час при якому система залишається передбачуваною.\n\n\n\nplot_recurrence_measure(measure=AVG_DIAG_LINE, label='AVG L')\n\n\n\n\nРисунок 4.9: Динаміка індексу Доу-Джонса та середньої довжини діагональних ліній\n\n\n\n\nЯк і до цього, ми можемо бачити, що середній час перебування Доу-Джонса у детермінованому стані зростає під час кризових явищ, що говорить зростання ступеня колективізації трейдерів на ринку.\n\n\n4.2.2.6 Час захоплення/затримки (Trapping time)\nУсереднена довжина діагональної лінії пов’язана із часом передбачуваності динамічної системи та часом затримки. У даному випадку ми середню довжину вертикальних ліній:\n\\[\nTT={\\frac {\\sum _{{v=v_{\\min }}}^{{N}}vP(v)}{\\sum _{{v=v_{\\min }}} ^{{N}}P(v)}}.\n\\]\n\n\n\n\n\n\nДодаткова інформація по середній довжині вертикальних ліній\n\n\n\nСередня довжина вертикальних ліній визначає середній час перебування системи в ламінарному стані. Тобто, вона відповідає середньому періоду часу при якому система “завмирає” у певному стані. Очевидно, що зростання цiєї величини характеризує дедалi бiльший час затримки дослiджуваної системи в певному станi.\n\n\n\nplot_recurrence_measure(measure=TT, label='TT')\n\n\n\n\nРисунок 4.10: Динаміка індексу Доу-Джонса та час затримки\n\n\n\n\nНа представленому рисунку видно, що \\(TT\\) зростає в (перед-)кризові стани, що вказує на потребу системи перебувати ще більший час у стані кризи.\n\n\n4.2.2.7 Середня довжина білих вертикальних лінії (Average white vertical lines length)\nСередня довжина білих вертикальних ліній може бути визначена як\n\\[\nWVL_{mean} = \\sum_{w=w_{min}}^{N} w \\cdot P(w) \\Big / \\sum_{w=w_{min}}^{N} P(w),\n\\]\nде \\(P(w)\\) — це частотний розподіл білих вертикальних ліній довжиною \\(w\\), а \\(w_{min}\\) відповідає найменшій довжині білих вертикальних ліній (найменшому періоду повернення до стану рекурентності).\n\n\n\n\n\n\nДодаткова інформація по середній довжині білих вертикальних ліній\n\n\n\nПредставлену міру можна охарактеризувати як середній горизонт непередбачуваності системи.\n\n\n\nplot_recurrence_measure(measure=AVG_WVERT_LINE, label='WVL_{mean}')\n\n\n\n\nРисунок 4.11: Динаміка індексу Доу-Джонса та середньої довжини білих вертикальних ліній\n\n\n\n\nЗростання середньої довжини бiлих вертикальних лiнiй демонструє, що кризовi подiї характеризуються не лише детермiнiзмом динамiки фондового ринку, але i несхожiстю даних подiй у порiвняннi з попереднiми станами.\n\n\n4.2.2.8 Ентропія діагональних ліній (Diagonal lines entropy)\nДля відповідних діагональних сегментів можна розрахувати необхідну кількість інформації для опису всього розподілу цього типу ліній. Імовірність \\(p(\\ell )\\) того, що діагональна лінія має точну довжину \\(\\ell\\), можна оцінити за частотним розподілом \\(P(\\ell )\\) із \\(p( \\ell )={\\frac {P(\\ell )}{\\sum _{{\\ell = \\ell_{\\min }}}^{N}P(\\ell )}}\\). Ентропія Шеннона цієї ймовірності виглядає наступним чином:\n\\[\nDLEn = -\\sum_{{\\ell =\\ell _{\\min }}}^{N}p(\\ell )\\ln p(\\ell ).\n\\]\nДаний показник відображає складність досліджуваної структури.\n\n\n\n\n\n\nДодаткова інформація по ентропії діагональних ліній\n\n\n\nДля некорельованого шуму чи осциляцiй ми тримали б мале значення цієї ентропiї. Мале значення даної ентропії вказувало би на те, що розподіл діагональних ліній представляється асиметричним: існувала б невеличка частка діагональних ліній конкретної довжини, що характеризувала би всю рекурентність досліджуваної системи. Зростання даної ентропії характеризувало би зростання симетричності розподілу довжин діагональних ліній.\n\n\n\nplot_recurrence_measure(measure=ENT_DIAG, label='DLEn')\n\n\n\n\nРисунок 4.12: Динаміка індексу Доу-Джонса та ентропії діагональних ліній\n\n\n\n\nВидно, що ентропія діагональних ліній зростає під час кризових явищ, що вказує на зростання впливу детермінованих процесів із різним ступенем передбачуваності.\n\n\n4.2.2.9 Ентропія вертикальних ліній (Vertical lines entropy)\nМи можемо визначити Шеннонівську ентропію для розподілу вертикальних структур рекурентної діаграми. Імовірність \\(p( v )\\) того, що вертикальна лінія має точну довжину $ v $, можна оцінити за частотним розподілом \\(P( v )\\) із \\(p( v )= P( v ) \\Big / \\sum _{{ v = v_{\\min }}}^{N}P( v )\\). Ентропія Шеннона цієї ймовірності визначається як\n\\[\nVLEn =-\\sum_{{ v = v_{\\min }}}^{N}p( v )\\ln p( v ).\n\\]\nЦя міра, по аналогії до попередньої ентропії, також є мірою складності системи.\n\n\n\n\n\n\nДодаткова інформація по ентропії вертикальних ліній\n\n\n\nДля синусоїдального процесу ми би очікували мале значення даної ентропії, оскільки це простий періодичний процес. Для складного процесу з пам’ятю ми би очiкуємо високе значення цього типу рекурентної ентропiї. Це означати- ме, що ламiнарнiсть процесу характеризуються рiзноманiтними перiодами довгостроковості пам’яті системи.\n\n\n\nplot_recurrence_measure(measure=ENT_VERT, label='VLEn')\n\n\n\n\nРисунок 4.13: Динаміка індексу Доу-Джонса та ентропії вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що ентропія вертикальних ліній починає зростати під час крахових явищ, що вказує на зростання ступеню ламінарності, тобто зростання рівномірності розподілу вертикальних ліній різноманітних довжин.\n\n\n4.2.2.10 Дивергенція (Divergence)\nПоказник \\(L_{\\max }\\) може надати нам інформацію про максимальний ступінь передбачуваності досліджуваного періоду. Зворотнє значення максимальної довжини діагональних ліній \\(L_{\\max }\\) або дивергенція (розбіжність) може вказати нам на швидкість та тривалість розбіжності досліджуваних траєкторій. Даний показник можна визначити як\n\\[\n\\text{DIV} = {\\frac {1}{L_{\\max }}}.\n\\]\nДана міра схожа на старший показник Ляпунова. Однак взаємозв’язок між цією мірою та позитивним максимальним показником Ляпунова набагато складніший (щоб обчислити показник Ляпунова з RP, необхідно враховувати весь розподіл частот діагональних ліній). Дивергенція може мати тенденцію позитивного максимального показника Ляпунова, але не більше.\n\n\n\n\n\n\nДодаткова інформація по дивергенції\n\n\n\nЧим вище значення дивергенції, тим швидше розбігаються траєкторії фазового простору. І навпаки, чим нижче значення дивергенції, тим ближче досліджувані траєкторії прилягають один до одного.\n\n\n\nplot_recurrence_measure(measure=DIV, label='DIV')\n\n\n\n\nРисунок 4.14: Динаміка індексу Доу-Джонса та дивергенції\n\n\n\n\nДаний рисунок показує, що дивергенція діагональних ліній починає спадати в кризові та передкризові періоди, що також вказує на зростання ступеня впорядкованості динаміки системи в дані періоди часу.\n\n\n4.2.2.11 Дивергенція вертикальних ліній (Vertical line divergence)\nЗворотнє значення максимальної довжини вертикальних ліній \\(V_{max}\\) або розбіжність вертикальних ліній можна визначити як:\n\\[\nVDIV = {\\frac {1}{V_{\\max }}}.\n\\]\n\n\n\n\n\n\nДодаткова інформація по дивергенції вертикальних ліній\n\n\n\nМаксимальна довижна вертикальних ліній надавала нам інформацію про максимальний ступінь незмінюваності системи. Вертикальна дивергенція дозволяє нам охарактеризувати швидкість настання або спаду ламінарності у системі. Чи вище значення \\(VDIV\\), тим швидше система виходить із ламінарного стану. І навпаки, чим нижчий даний показник, тим ближче траєкторії фазового простору один до одного, і тим вищий ступінь ламінарності системи в конкретний момент часу.\n\n\n\nplot_recurrence_measure(measure=VERT_DIV, label='VDIV')\n\n\n\n\nРисунок 4.15: Динаміка індексу Доу-Джонса та дивергенції вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що періоди криз характеризуються спадом вертикальної дивергенції, тобто зростанням кількості вертикальних структур, що характеризують ще більший ступінь ламінарності станів.\n\n\n4.2.2.12 Дивергенція білих вертикальних ліній\nЗворотнє значення максимальної довжини білих вертикальних ліній (\\(WVL_{max}\\)) можна охарактеризувати як дивергенцію білих вертикальних ліній. Її можна визначити наступним чином:\n\\[\nWVDIV = \\frac{1}{WVL_{max}}.\n\\]\nЗростання даного показника має вказувати на зростання ступеня рекурентності системи, а його спад має демонструвати зростання непередбачуваності.\n\nplot_recurrence_measure(measure=WHITE_VERT_DIV, label='WVDIV')\n\n\n\n\nРисунок 4.16: Динаміка індексу Доу-Джонса та дивергенції білих вертикальних ліній\n\n\n\n\nНа даному рисунку видно, що дивергенція білих вертикальних ліній представляє доволі зашумлену динаміку, а тому не може бути використана в якості ефективного індикатора кризових явищ.\n\n\n4.2.2.13 Ентропія білих вертикальних ліній (White vertical lines entropy)\nІмовірність \\(p( \\omega )\\) того, що біла вертикальна лінія має точну довжину \\(\\omega\\), можна оцінити за частотним розподілом \\(P(\\omega )\\) із \\(p( \\omega )={\\frac {P( \\omega )}{\\sum _{{\\omega = \\omega_{\\min }}}^{N}P(\\omega )}}\\). Ентропія Шеннона цієї ймовірності,\n\\[\n{\\text{WVertEn}}=-\\sum_{{\\omega =\\omega _{\\min }}}^{N}p(\\omega )\\ln p(\\omega ),\n\\]\nде \\(\\omega_{min}\\) — мінімальна довжина білої вертикальної лінії.\n\nplot_recurrence_measure(measure=ENT_WHITE_VERT, label='WVLEN')\n\n\n\n\nРисунок 4.17: Динаміка індексу Доу-Джонса та ентропії білих вертикальних ліній\n\n\n\n\nВидно, що ентропія білих вертикальних ліній спадає у кризові та передкризові періоди фондового ринку, що вказує на зростання загальної передбачуваності системи та зміщення розподілу білих вертикальних ліній до конкретних довжин. Тобто, їх розподіл у періоди криз стає менш симетричним, що вказує на поступове заміщення білих вертикальних ліній чорними.\n\n\n4.2.2.14 Співвідношення частоти рекурентності до детермінізму \\(DET/RR\\)\nСпіввідношення між \\(DET\\) і \\(RR\\) (\\(RATIO\\)) можна використовувати для виявлення прихованих фазових переходів у системи:\n\\[\nRATIO_1=\\frac{DET}{RR}=N^2\\frac{\\displaystyle\\sum_{l=l_{min}}^{N}lP(l)}{\\left(\\displaystyle\\sum_{l=1}^{N}lP(l)\\right)^2}\n\\]\n\nplot_recurrence_measure(measure=RATIO_DET_REC, label='RATIO_1')\n\n\n\n\nРисунок 4.18: Динаміка індексу Доу-Джонса та співвідношення між мірою передбачуваності та рекурентності\n\n\n\n\nДаний показник спадає під час кризових явищ фондового ринку. Це говорить про те, що має зростати загальна щільність рекурентних точок, як ізольованих, так і просто розподілу вертикальних структур. Тобто, у кризові періоди \\(RR\\) представляється вищою за \\(DET\\).\n\n\n4.2.2.15 Співвідношення ламінарності до детермінізму (LAM/DET)\nТак само як і попередня міра, відношення ламінарності до детермінізму може дозволити нам виокремити приховані переходи в досліджуваному сигналі:\n\\[\nRATIO_2=\\frac{LAM}{DET}.\n\\]\n\nplot_recurrence_measure(measure=RATIO_LAM_DET, label='RATIO_2')\n\n\n\n\nРисунок 4.19: Динаміка індексу Доу-Джонса та співвідношення між мірою ламінарності та детермінізмом\n\n\n\n\nЯкщо виходити з динаміки показника \\(RATIO_2\\), можна сказати, що загальний ступінь детермінізму починає переважати над ламінарністю під час кризових явищ.\nАле по результатам представлених показників ми можемо сказати, що досліджувані крахові та передкрахові події характеризуються зростанням рекурентності, і подібного роду поведінка може бути використана в якості передвісника подальших криз."
  },
  {
    "objectID": "lab_4.html#теоретичні-відомості",
    "href": "lab_4.html#теоретичні-відомості",
    "title": "5  Лабораторна робота № 4",
    "section": "5.1 Теоретичні відомості",
    "text": "5.1 Теоретичні відомості\n\n5.1.1 Складність. Кількісні міри складності. Інформаційні методи оцінки складності.\nДане століття називають століттям складності. Сьогодні питання “що таке складність?” вивчають фізики, біологи, математики і інформатики, хоча при теперішніх досягненнях у розумінні оточуючого світу, однозначної відповіді на це питання немає.\nЗ цієї причини, відповідно до ідеї І. Пригожина, будемо досліджувати прояви складності системи, застосовуючи при цьому сучасні методи кількісного аналізу складності.\nСеред таких методів на увагу заслуговують: - інформаційно-ентропійні; - засновані на теорії хаосу; - скейлінгово-мультифрактальні.\nЗрозуміло, виходячи з різної природи методів, покладених в основу формування міри складності, вони приділяють певні вимоги до часових рядів, що слугують вхідними даними. Наприклад, перші дві групи методів вимагають стаціонарності вхідних даних. При цьому мають різну чутливість до таких характеристик, як детермінованність, стохастичність, причинність та кореляції. Тому у подальшому, порівнюючи комплексно ефективність різних показників складності, на вказані обставини ми будемо звертати увагу, підкреслюючи спеціально застосовність того чи іншого показника для характеристики різних сторін складності досліджуваних систем.\nРозгляд першої групи методів почнемо з добре відомої міри складності, запропонованої А. Колмогоровим.\nКолмогорівська складність. Поняття колмогорівської складності (або, як ще говорять, алгоритмічної ентропії) з’явилося в 1960-і роки на стику теорії алгоритмів, теорії інформації і теорії ймовірностей.\nІдея А. Колмогорова полягала в тому, щоб вимірювати кількість інформації, що міститься в індивідуальних скінчених об’єктах (а не у випадкових величинах, як у шеннонівській теорії інформації). Виявилось, що це можливо (хоча лише з точністю до обмеженого доданку). А. Колмогоров запропонував вимірювати кількість інформації в скінчених об’єктах за допомогою теорії алгоритмів, визначивши складність об’єкту як мінімальну довжину програми, що породжує цей об’єкт. Дане визначення стало базисом алгоритмічної теорії інформації, а також алгоритмічної теорії ймовірностей: об’єкт вважається випадковим, якщо його складність наближена до максимальної.\nЩо ж собою являє колмогорівська складність і як її виміряти? На практиці ми часто стикаємося з програмами, які стискують файли (для економії місця в архіві). Найбільш поширені називаються zip, gzip, compress, rar, arj та інші. Застосувавши таку програму до деякого файлу (з текстом, даними, програмою), ми отримуємо його стислу версію (яка, як правило, коротше початкового файлу). За нею можна відновити початковий файл з допомогою парної програми-“декомпресора”. Отже, у першому наближенні колмогорівську складність файлу можна описати як довжину його стислої версії. Тим самим файл, що має регулярну структуру і добре стискуваний, має малу колмогорівську складність (порівняно з його довжиною). Навпаки, погано стискуваний файл має складність, близьку до довжини.\nПрипустимо, що ми маємо фіксований спосіб опису (декомпресор) \\(D\\). Для даного слова \\(x\\) розглянемо всі його описи, тобто всі слова \\(y\\), для яких \\(D(y)\\) визначене \\(і\\) рівне \\(x\\). Довжину найкоротшого з них \\(l(y)\\) і називають колмогорівською складністю слова \\(x\\) при даному способі опису \\(D\\):\n\\[\nKS_{D}(x) = \\min\\{l(y)\\,|\\,D(y)=x\\},\n\\]\nде \\(l(y)\\) позначає довжину слова \\(y\\). Індекс \\(D\\) підкреслює, що визначення залежить від вибору способу \\(D\\).\nМожна показати, що існують оптимальні способи опису. Спосіб опису тим краще, чим він коротше. Тому природно дати таке визначення: спосіб \\(D_1\\) не гірше за спосіб \\(D_2\\), якщо \\(KS_{D_1}(x) \\leq KS_{D_2}(x)+c\\) при деякому \\(c\\) і при всіх \\(x\\).\nОтже, за Колмогоровим, складність об’єкту (наприклад, тексту — послідовності символів) — це довжина мінімальної програми яка виводить даний текст, а ентропія — це складність, що ділиться на довжину тексту. На жаль, це визначення чисто умоглядне. Надійного способу однозначно визначити цю програму не існує. Але є алгоритми, які фактично якраз і намагаються обчислити колмогорівські складність тексту і ентропію.\n\n\n5.1.2 Оцінка складності Колмогорова за схемою Лемпела-Зіва\nУніверсальна (в сенсі застосовності до різних мовних систем) міра складності кінцевої символьної послідовності була запропонована Лемпелем і Зівом. У рамках їх підходу складність послідовності оцінюється числом кроків процесу, що її породжує. Припустимими (редакційними) операціями при цьому є:\n\nгенерація символу (необхідна, як мінімум, для синтезу елементів алфавіту) і\nкопіювання “готового” фрагмента з передісторії (тобто з уже синтезованої частини тексту).\n\nНехай \\(\\Sigma\\) — скінчений алфавіт, \\(S\\) — текст (послідовність символів), складений з елементів \\(\\Sigma\\); \\(S[i]\\) — \\(i\\)-й символ тексту; \\(S[i:j]\\) — фрагмент тексту з \\(i\\)-го по \\(j\\)-й символ включно \\((i&lt;j)\\); \\(N=|S|\\) — довжина тексту \\(S\\). Тоді схему синтезу послідовності можна представити у вигляді конкатенації\n\\[\nH(S)=S[1:i_1]S[i_1+1:i_2]...S[i_{k-1}+1:i_k]...S[i_{m−1}+1:N], \\tag{1}\n\\]\nде \\(S[i_{k−1}+1:i_k]\\) — фрагмент \\(S\\), породжуваний на \\(k\\)-му кроці, а $m=m_{H}(S) — число кроків процесу. З усіляких схем породження \\(S\\) обирається мінімальна за числом кроків. Таким чином, складність послідовності \\(S\\) за Лемпелем-Зівом\n\\[\nc_{LZ}(S) = \\min_{H}\\{ m_{H}(S) \\}.\n\\]\nМінімальність числа кроків забезпечується вибором для копіювання на кожному кроці максимально довгого прототипу з передісторії. Якщо позначити через \\(j(k)\\) номер позиції, з якої починається копіювання на \\(k\\)-му кроці, то довжина фрагмента копіювання\n\\[\nl_{j(k)} = i_k - i_{k-1} - 1 = \\max_{j \\leq i_{k-1}}\\{ l_{j} : S[i_{k-1}+1:i_{k-1}+l_j]=S[j:j+l_{j}-1] \\}, \\tag{2}\n\\]\nа сам \\(k\\)-й компонент складнісного розкладання (1) можна записати у вигляді\n\\[\nS[i_{k-1}+1:i_{k}] =\n\\begin{cases}\n    S[j(k):j(k)+l_{j(k)}-1] & \\textrm{if} \\; j(k) \\neq 0, \\\\\n    S[i_{k-1}+1] & \\textrm{if} \\; j(k) = 0.\n\\end{cases} \\tag{3}\n\\]\nВипадок \\(j(k) = 0\\) відповідає ситуації, коли в позиції \\(i_{k−1}+1\\) стоїть символ, який раніше не зустрічався. При цьому ми застосовуємо операцію генерації символу.\nБудемо знаходити складність за Лемпелем-Зівом (LZ) для часового ряду, який являє собою, наприклад, щоденні значення індексу фондового ринку. Для дослідження динаміки LZ та порівняння з іншими фондовими ринками будемо знаходити дану міру складності для підряду фіксованої довжини (вікна). Для цього обчислимо логарифмічні прибутковості та перетворимо їх у послідовність бітів. При цьому можна задавати кількість станів, які диференційовані (система числення). Так, для двох різних станів маємо 0, 1, для трьох — 0, 1, 2 і т.д. Для двійкової системи кодування буде задаватися поріг по середньому значенню і стани, наприклад, прибутковостей (ret) кодуватимуться наступним чином:\n\\[\nret =\n\\begin{cases}\n0, & ret_t &lt; \\langle ret \\rangle \\\\\n1, & ret_t &gt; \\langle ret \\rangle.\n\\end{cases} \\tag{4}\n\\]\nТакож можна визначити так звану пермутаційну складність Лемпеля-Зіва (PLZС). У даному випадку би будемо опиратись на процедуру реконструкції фазового простору, що згадувалась в лабораторних 2 і 3. Згідно пермутаційній процедурі ми будемо брати фрагмент ряду довжини \\(m\\), що слугує розмірностю реконструйованого атрактора, та замінюємо кожне значення ряду його порядковим індексом. На подальшому ресунку представлено часовий ряд та його можливі порядкові шаблони:\n\n\n\nРисунок 5.1: Фрагмент часового ряду (а) та 6 можливих порядкових шаблонів, що можуть бути в цьому сигналі (b)\n\n\nАлгоритм Лемпеля-Зіва виконує дві операції: (1) додає новий біт в уже існуючу послідовність; (2) копіює вже сформовану послідовність. Алгоритмічна складність представляє собою кількість таких операцій, необхідних для формування заданої послідовності.\nДля випадкової послідовності довжини \\(n\\) алгоритмічна складність обчислюється за виразом \\(LZC_r = n / \\log(n)\\). Тоді відносна алгоритмічна складність знаходиться як відношення отриманої складності до складності випадкової послідовності: \\(LZC = LZC / LZC_{r}\\).\nОднак навіть цього підходу може бути недостатньо. Справа в тому, що складні сигнали проявляють притаманну їм складність на різних просторових і часових масштабах, тобто мають масштабно інваріантні властивості. Вони, зокрема проявляються через степеневі закони розподілу. Тому розрахунки алгоритмічної складності на “поверховому” масштабі сигналу можуть бути неприйнятними і призводити до помилкових висновків.\nДля подолання таких труднощів використовуються мультимасштабні методи, до розгляду яких ми і переходимо.\n\n\n5.1.3 Процедура грануляції для мультискейлінгового дослідження часових рядів. Мультимасштабні міри складності\nІдея цієї групи методів включає дві послідовно виконувані процедури:\n\nпроцес “грубого дроблення” (coarse graining — “грануляції”) початкового часового ряду — усереднення даних на сегментах, що не перетинаються, розмір яких (вікно усереднення) збільшуватиметься на одиницю при переході на наступний за величиною масштаб;\nобчислення на кожному з масштабів певного (до сих пір мономасштабного) показника складності.\n\nПроцес “грубого дроблення” (“грануляція”) полягає в усереднені послідовних відліків ряду в межах вікон, що не перетинаються, а розмір яких \\(\\tau\\) — збільшується при переході від масштабу до масштабу. Кожен елемент “гранульованого” часового ряду \\(y_{j}^{\\tau}\\) знаходиться у відповідності до виразу:\n\\[\ny_{j}^{\\tau} = \\frac{1}{\\tau}\\sum_{i=(j-1)\\tau+1}^{j\\tau}x_i, \\; 1 \\leq j \\leq N/\\tau,\n\\]\nде \\(\\tau\\) характеризує фактор масштабування. Довжина кожного “гранульованого” ряду залежить від розміру вікна \\(і\\) рівна \\(N/\\tau\\). Для масштабу рівного 1 “гранульований” ряд просто тотожний оригінальному.\n\n\n\nРисунок 5.2: Схематична ілюстрація процесу грубого дроблення (“грануляції”) початкового часового ряду для масштабів 2 і 3\n\n\nБібліотека neurokit2 представляє метод для обчислення як мономасштабного показника складності Лемпеля-Зіва, так і його мультимасштабного аналогу.\nСинтаксис мономасштабної процедури виглядає наступним чином:\ncomplexity_lempelziv(signal, delay=1, dimension=2, permutation=False, symbolize='mean', **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — часова затримка, \\(\\tau\\). Використовується лише тоді, коли permutation=True.\ndimension (int) — розмірність вкладень, \\(m\\). Використовується лише коли permutation=True.\npermutation (bool) — якщо значення True, поверне складність Лемпеля-Зіва на основі порядкових патернів.\nsymbolize (str) — використовується тільки коли permutation=False. Метод перетворення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення None, щоб пропустити процес (якщо вхідний сигнал вже є дискретним). Можна скористатися методом complexity_symbolize() для використання іншої процедури символізації ряду.\n****kwargs** — інші аргументи, які передаються до complexity_ordinalpatterns() (якщо permutation=True) або complexity_symbolize().\n\nПовертає\n\nlzc (float) — складність Лемпеля-Зіва (LZC).\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення LZC.\n\nСинтаксис мультимасштабної процедури вже інший:\nentropy_multiscale(signal, scale='default', dimension=3, tolerance='sd', method='MSEn', show=False, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму.\nscale (str або int або list) — список масштабних коефіцієнтів, що використовуються для процедури крос-грануляції часового ряду. Якщо значення \"default\", буде використано range(len(signal) / (dimension + 10)). Якщо \"max\", використовуватиме всі масштаби до половини довжини сигналу. Якщо ціле число, створить діапазон до вказаного цілого числа.\ndimension (int) — розмірність вкладення, \\(m\\).\ntolerance (float) — поріг пропускання \\(\\varepsilon\\) (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються подібними. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\).\nmethod (str) — яку версію мультимасштабного показника обчислювати. Переважна кількість показників за цим методом відповідають саме ентропійним підходам. Нас цікавитиме саме \"LZC\".\nshow (bool) — візуалізувати залежність показника від масштабу.\n****kwargs** — необов’язкові аргументи.\n\nПовертає\n\nfloat — точкова оцінка мультимасштабного показника окремого часового ряду, що відповідає площі під кривою значень цього показника, яка, по суті, є сумою вибіркових значень, наприклад, \"LZC\" в діапазоні масштабних коефіцієнтів.\ndict — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення мультимасштабного показника. Значення показника, що відповідають кожному фактору \"Scale\", зберігаються під ключем \"Value\".\n\n\n\n5.1.4 Шеннонівська складність\nЕнтропійний аналіз часових рядів за допомогою ентропійних показників різного роду буде проведено у наступних роботах. Зараз же ми розглянемо найпростішу з ентропій — ентропію Шеннона та порівняємо її можливості кількісно оцінювати складність часових послідовностей у порівнянні з мірою Лемпеля-Зіва.\nЕнтропія Шеннона — це статистичний квантифікатор, який широко використовується для характеристики складних процесів. Він здатний виявляти аспекти нелінійності в досліджуваних сигналах, сприяючи більш надійному поясненню нелінійної динаміки різних точок аналізу, що, в свою чергу, покращує розуміння природи складних систем, які характеризуються складністю та нерівноважністю. Окрім складності та нерівноважності, більшість, але не всі, складні системи також характеризуються неоднорідним розподілом зв’язків. Поняття ентропії було використано Шенноном в теорії інформації для передачі даних.\nЕнтропія - це міра невизначеності та випадковості в системі. Якщо припустити, що всі наявні дані належать до одного класу, то неважко передбачити клас нових даних. У цьому випадку ентропія дорівнює 0. Будучи величиною між 0 і 1, коли всі ймовірності рівні, ентропія набуває найбільшого значення. Невизначеність, що виникає, коли подія \\(E\\) відбувається з ймовірністю \\(p\\), можна позначити як \\(S(p)\\). Якщо ймовірність появи класу дорівнює 1, тоді ентропія мінімальна, \\(S(1) = 0\\). Відповідно до концепції Шеннона, якщо у нас наявні ймовірності реалізації певної події \\(p_1, p_2, p_3, ..., p_n\\), на виході отримується кількість інформації, що необхідна для опису цієї події. Тоді, Шеннонівська ентропія може бути визначена як\n\\[\nS = -\\sum_{i=1}^{n}p_i \\ln p_{i}.  \n\\]\nСинтаксис методу для розрахунку Шеннонівської ентропії виглядає наступним чином:\nentropy_shannon(signal=None, base=2, symbolize=None, show=False, freq=None, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\nbase (float) — основа логарифму, що за замовчуванням дорівнює 2, що дає одиницю в бітах. Зауважте, що scipy.stats.entropy() за замовчуванням використовує число Ейлера (np.e) (натуральний логарифм), що дає міру інформації, виражену в натах.\nsymbolize (str) — метод приведення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням дорівнює нулю, що пропускає процес (і припускає, що вхідні дані вже є дискретними).\nshow (bool) — якщо значення True, виводить дискретність сигналу.\nfreq (np.array) — замість сигналу можна надати вектор ймовірностей.\n****kwargs** — необов’язкові аргументи. Наразі не використовуються.\n\nПовертає\n\nshanen (float) — Шеннонівську ентропію.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення Шеннонівської ентопії.\n\n\n\n5.1.5 Інформація Фішера\nІнформацію Фішера було введено Р. А. Фішером у 1925 році як міру “внутрішньої точності” в теорії статистичних оцінок. Вона є центральною для багатьох статистичних галузей, що виходять далеко за межі теорії складності. Даний показник вимірює кількість інформації, яку спостережувана випадкова величина несе про невідомий параметр. В аналізі складності вимірюється кількість інформації, яку система несе “про себе”. Він базується на розкладанні за сингулярним значенням реконструйованого фазового простору. Значення показника Фішера зазвичай антикорельоване з іншими показниками складності (чим більше інформації система приховує про себе, тим більш передбачуваною і, відповідно, менш складною вона є).\nІнформацію Фішера можна визначити, використовуючи метод fisher_information() бібліотеки neurokit2. Її синтаксис виглядає наступним чином:\nfisher_information(signal, delay=1, dimension=2)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\ndelay (int) — затримка в часі, \\(\\tau\\).\ndimension (int) — розмірність векторів фазового простору, \\(m\\).\n\nПовертає\n\nfi (float) — обчислена міра інформації Фішера.\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення інформації Фішера.\n\n\n\n5.1.6 Складність та параметри Хьорта\nПараметри Хьорта — це показники статистичних властивостей, які спочатку були введені Хьортом (Hjorth, 1970) для опису загальних характеристик сигналів електроенцифалограми у кількох кількісних термінах, але які можуть бути застосовані до будь-якого часового ряду. Параметрами є активність, рухливість і складність:\n\nПараметр активності (\\(Activity\\)) — це просто дисперсія сигналу, яка відповідає середній потужності сигналу (якщо його середнє значення дорівнює 0).\n\n\\[\nActivity = \\sigma^{2}_{signal}.\n\\]\n\nПараметр рухливості (\\(Mobility\\)) являє собою середню частоту або частку середньоквадратичного відхилення спектра потужності. Він визначається як квадратний корінь з дисперсії першої похідної сигналу, поділений на дисперсію сигналу.\n\n\\[\nMobility = \\frac{\\sigma_{dd}/\\sigma_{d}}{Complexity}.\n\\]\n\nПараметр складності (\\(Complexity\\)) дає оцінку смуги пропускання сигналу, яка вказує на схожість форми сигналу з чистою синусоїдою (для якої значення сходиться до 1). Іншими словами, це міра “надмірної деталізації” по відношенню до “найм’якшої” можливої форми кривої. Параметр “Складність” визначається як відношення рухливості першої похідної сигналу до рухливості самого сигналу.\n\n\\[\nComplexity = \\frac{\\sigma_d}{\\sigma_{signal}},\n\\]\nде \\(d\\) та \\(dd\\) представляють перші та другі похідні сигналу, відповідно.\n\n\n\nРисунок 5.3: Характеристичні зміни форми кривої, що ілюструє залежність кожного параметра\n\n\nБібліотека neurokit2 представляє метод для отримання відповідних показників. Її синтаксис виглядає наступним чином:\ncomplexity_hjorth(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nhjorth (float) — показник складності Хьорта.\ninfo (dict) — словник, що містить додаткові показники Хьорта, такі як \"Mobility\" та \"Activity\".\n\n\n\n5.1.7 Час декореляції\nЧас декореляції (decorrelation time, DT) визначається як час (у відліках) першого перетину нуля функції автокореляції. Коротший час декореляції відповідає менш корельованому сигналу. Наприклад, зменшення часу декореляції в сигналах електроенцифалограми спостерігається перед нападами, що пов’язано зі зменшенням потужності низьких частот.\nБібліотека neurokit2 представляє функціонал для визначення часу декореляції, а саме метод complexity_decorrelation(). Її синтаксис є наступним:\ncomplexity_decorrelation(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nfloat — час декореляції.\ndict — словник, що містить додаткову інформацію про додаткові показники.\n\n\n\n5.1.8 Відносна грубість (нерівність, шорсткість)\nВідносна шорсткість — це відношення локальної дисперсії (автоковаріації з лагом 1) до глобальної дисперсії (автоковаріації з лагом 0), яке можна використовувати для класифікації різних “шумів”. Його також можна використовувати як індекс для перевірки застосовності фрактального аналізу (показники фрактальності будуть використовуватись у наступних роботах).\nСинтаксис даного методу в бібліотеці neurokit2 виглядає наступним чином:\ncomplexity_relativeroughness(signal, **kwargs)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (часовий ряд) у вигляді вектора значень.\n****kwargs** (optional) — інші аргументи, що потребуються методу nk.signal_autocor().\n\nПовертає\n\nrr (float) — значення відносної грубості.\ninfo (dict) — словник, що містить інформацію відносно параметрів, що використовувались для обчислення показника грубості.\n\n\n\n5.1.9 Взаємна інформація\nКоли йдеться про виявлення зв’язків між змінними, ми часто використовуємо кореляцію Пірсона. Проблема полягає в тому, що цей показник знаходить лише лінійні зв’язки, що іноді може призвести до неправильної інтерпретації зв’язку між двома змінними. Тим не менш, інші статистичні методи вимірюють нелінійні зв’язки, такі як взаємна інформація (mutual information, MI).\nВзаємна інформація між двома випадковими величинами вимірює нелінійний зв’язок між ними. Крім того, вона показує, скільки інформації можна отримати з випадкової величини, спостерігаючи за іншою випадковою величиною.\nВона тісно пов’язана з поняттям ентропії. Тобто, зменшення невизначеності випадкової величини пов’язане з отриманням інформації з іншої випадкової величини. Отже, високе значення взаємної інформації вказує на велике зменшення невизначеності, тоді як низьке значення вказує на мале зменшення. Якщо взаємна інформація дорівнює нулю, це означає, що дві випадкові величини є незалежними.\nВзаємну інформацію можна розрахувати наступним чином:\n\\[\nI(X; Y) = \\sum_{y \\in Y}\\sum_{x \\in X}p(x, y) \\cdot \\log{\\left( \\frac{p(x,y)}{p(x)p(y)} \\right)},\n\\]\nде \\(p(x)\\) та \\(p(y)\\) ймовірності спостереження окремо \\(x\\) або \\(y\\), а \\(p(x,y)\\) ймовірність спостереження одночасно \\(x\\) та \\(y\\).\nОсновна відмінність між кореляцією та взаємною інформацією полягає в тому, що кореляція є мірою лінійної залежності, тоді як взаємна інформація вимірює загальну залежність (включаючи нелінійні зв’язки). Тому взаємна інформація виявляє залежності, які не залежать тільки від коваріації. Таким чином, взаємна інформація дорівнює нулю, коли дві випадкові величини є строго незалежними.\nБібліотека neurokit2 представляє інструментарій для знаходження взаємної інформації між двома сигналами \\(x\\) та \\(y\\). У даній роботі ми спробуємо віднайти взаємну інформацію як між двома часовими рядами, так і авто-взаємну інформацію, подібно до автокореляції.\nСинтаксис потрібної нам процедури виглядає наступним чином:\nmutual_information(x, y, method='varoquaux', bins='default', **kwargs)\nПараметри\n\nx (Union[list, np.array, pd.Series]) — масив значень.\ny (Union[list, np.array, pd.Series]) — масив значень.\nmethod (str) — метод для обчислення взаємної інформації: \"nolitsa\", \"varoquaux\", \"knn\", \"max\".\nbins (int) — кількість бінів гістограми. Використовується лише для \"nolitsa\" та \"varoquaux\". Якщо \"default\", кількість бінів оцінюється згідно методики Hacine-Gharbi (2018).\n****kwargs** — додаткові ключові аргументи для обраного методу.\n\nПовертає\n\nfloat — розрахована взаємна інформація.\n\nІснують різноманітні підходи до розрахунку взаємної інформації:\n\nnolitsa: Класична взаємна інформація (трохи швидше, ніж метод \"sklearn\").\nvaroquaux: Застосовує фільтр Гауса до об’єднаної гістограми. Величину згладжування можна налаштовувати за допомогою аргументу sigma (за замовчуванням sigma=1).\nknn: Непараметрична (тобто не заснована на біннінгу) оцінка за найближчими сусідами. Додаткові параметри включають k (за замовчуванням, k=3), кількість найближчих сусідів для використання.\nmax: Максимальний коефіцієнт взаємної інформації, тобто \\(MI\\) є максимальним при певній комбінації кількості бінів.\n\nІснує безліч різноманітних показників складності, що базуються на теорії інформації та інших парадигах, які ми ще представлятимемо в подальшому. Розглянемо ефективність використання зазначених показників у якості індикаторів або індикаторів-передвісників крахових подій."
  },
  {
    "objectID": "lab_4.html#хід-роботи",
    "href": "lab_4.html#хід-роботи",
    "title": "5  Лабораторна робота № 4",
    "section": "5.2 Хід роботи",
    "text": "5.2 Хід роботи\nСпочатку імпортуємо необхідні модулі для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nfrom tqdm import tqdm\n\nІ виконаємо налаштування рисунків для виведення:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nЦього разу розглянему можливість побудови індикаторів-передвісників на прикладі фондового індексу S&P 500, але, окрім цього, додамо ще Біткоїн для розрахунку взаємної інформації між фондовим ринком та криптовалютним. Очевидно, що фондовий індекс S&P 500 мав би проіснувати довше за Біткоїн. До того ж, криптовалютний ринок працює безперервно на відміну від фондового, а тому треба буде об’єднати значення двох активів за тими датами що співпадають.\nВиконуємо зчитування фондового індексу:\n\nsymbol_1 = '^GSPC'         # Символ першого індексу\nstart_1 = \"2014-01-01\"     # Дата початку зчитування даних\nend_1 = \"2023-08-24\"       # Дата закінчення зчитування даних\n\ndata_1 = yf.download(symbol_1, start_1, end_1)  # вивантажуємо дані\ntime_ser_1 = data_1['Adj Close'].copy()         # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'      # підпис по вісі Ох \nylabel_1 = symbol_1        # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиконуємо зчитування криптовалютного індексу:\n\nsymbol_2 = 'BTC-USD'       # Символ другого індексу\nstart_2 = \"2014-01-01\"     # Дата початку зчитування даних\nend_2 = \"2023-08-24\"       # Дата закінчення зчитування даних\n\ndata_2 = yf.download(symbol_2, start_2, end_2)  # вивантажуємо дані\ntime_ser_2 = data_2['Adj Close'].copy()         # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'      # підпис по вісі Ох \nylabel_2 = symbol_2        # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nТепер створимо новий масив даних, що об’єднуватиме в собі значення S&P 500 та BTC по їх спільним датам:\n\n# приводимо значення індексів до типу DataFrame, щоб мати змогу їх об'єднати \n# за допомогою бібліотеки pandas\ndf_time_ser_1 = pd.DataFrame(time_ser_1) \ndf_time_ser_2 = pd.DataFrame(time_ser_2)\n\n\njoined = df_time_ser_1.merge(df_time_ser_2, # об'єднуємо по датам тієї бази, що містить \n                             on='Date',     # більше дат\n                             how='left')  \n\njoined = joined.rename(columns={joined.columns[0]: symbol_1,  # переіменовуємо колонки по \n                                joined.columns[1]: symbol_2}) # змінним symbol_1 та symbol_2\n\njoined = joined.dropna()  # видаляємо рядки, що містять нульові значення\n\nВиводимо отриману базу:\n\njoined\n\n\n\n\n\n\n\n\n^GSPC\nBTC-USD\n\n\nDate\n\n\n\n\n\n\n2014-09-17\n2001.569946\n457.334015\n\n\n2014-09-18\n2011.359985\n424.440002\n\n\n2014-09-19\n2010.400024\n394.795990\n\n\n2014-09-22\n1994.290039\n402.152008\n\n\n2014-09-23\n1982.770020\n435.790985\n\n\n...\n...\n...\n\n\n2023-08-17\n4370.359863\n26664.550781\n\n\n2023-08-18\n4369.709961\n26049.556641\n\n\n2023-08-21\n4399.770020\n26124.140625\n\n\n2023-08-22\n4387.549805\n26031.656250\n\n\n2023-08-23\n4436.009766\n26431.640625\n\n\n\n\n2249 rows × 2 columns\n\n\n\nІ візуалізуємо сам графік. Спочатку оголосимо функцію для попарної візуалізації рядів зі збереженням їх абсолютних значень:\n\ndef plot_pair(x_values, y_values, x_label, y_label, file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y_values[0], \n                  \"b-\", label=fr\"{y_label[0]}\")\n    p2, = ax2.plot(x_values,\n                   y_values[1], \n                   color=clr, \n                   label=fr'${y_label[1]}$')\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y_label[0]}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nі тепер візуалізуємо отримані ряди:\n\nvalues_plot = joined.iloc[:,0].values, joined.iloc[:,1].values\nylabels = ylabel_1, ylabel_2\nfile_name = f'joined {symbol_1}_{symbol_2}'\n\n\nplot_pair(joined.index, values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.4: Динаміка індексу S&P 500 та Біткоїна за досліджуваний період\n\n\n\n\n\n5.2.1 Розрахунок взаємної інформації\nРозглянемо взаємну інформацію як індикатор нелінійної кореляції між двома фінансовими активами, і спробуємо сказати, чи є між ними “істинний” взаємозв’язок. Виконуватимемо розрахунки із використанням алгоритму руховому вікна. Також визначимо функцію transform() для нормалізації ряду.\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\nret_type = 6                           # вид ряду\nwindow = 100                           # ширина вікна\ntstep = 1                              # часовий крок вікна \nlength = len(joined.iloc[:,0].values)  # довжина самого ряду\n\nMI = []                                # масив для віконної взаємної інформації\n\nТепер приступимо до розрахунків:\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагменти\n    fragm_1 = joined[symbol_1][i:i+window]  \n    fragm_2 = joined[symbol_2][i:i+window]\n\n    # виконуємо процедуру трансформації ряду \n    fragm_1 = transformation(fragm_1, ret_type)    \n    fragm_2 = transformation(fragm_2, ret_type)\n\n    # розраховуємо взаємну інформацію \n    mut_inf = nk.mutual_information(fragm_1, fragm_2)\n    \n    # та додаємо результат до масиву значень\n    MI.append(mut_inf)\n\n100%|██████████| 2149/2149 [00:03&lt;00:00, 706.05it/s]\n\n\nЗберігаємо отриманий результат у текстовому файлі:\n\nnp.savetxt(f\"mutual_inf_name1={symbol_1}_name2={symbol_2}_ \\\n    window={window}_step={tstep}_rettype={ret_type}.txt\" , MI)\n\nВізуалізуємо результат між відповідними показниками:\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(joined.index[window:length:tstep], \n                joined[symbol_1][window:length:tstep].values, \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(joined.index[window:length:tstep],\n                joined[symbol_2][window:length:tstep].values,\n                'red', \n                label=fr\"{symbol_2}\")\np3, = ax3.plot(joined.index[window:length:tstep],\n                MI,\n                'magenta', \n                label=r\"$MI$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"mutual_inf_name1={symbol_1}_name2={symbol_2}_ \\\n    window={window}_step={tstep}_rettype={ret_type}.jpg\")\n\nplt.show();\n\n\n\n\nРисунок 5.5: Динаміка індексу S&P 500, Біткоїна та взаємної інформації\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, на фондовому та криптовалютному ринках дійсно спостерігалися фази зростання взаємної інформації між ними. Найкраще це видно напередодні кризи 2018-го року, під час 2019, після коронавірусної пандемії та напередодні 2023 року. Для даного індикатора залишається простір для експериментів, що можуть вивести його на рівень достатньо потужного передвісника криз на фондовому ринку чи криптовалютному.\nЯк вже зазначалося, окрім обчислення взаємної інформації для двох пар часових сигналів, ми можемо обчислити автовзаємну інформація, тобто взаємну інформацію ряду самого із собою по різним часовим лагам, як це було пророблено для автокореляції. Недолік автокореляції полягає в тому, що вони визначає саме лінійний зв’язок теперішніх значень з попередніми. Автовзаємна інформація в свою чергу є показником нелінійного зв’язку теперішніх значень із попередніми.\nДля обчислення автовзаємної інформації визначимо наступну функцію:\n\ndef automut(x, maxlag):\n    n = len(x)                               # визначаємо довжину сигналу\n    lags = np.arange(0, maxlag, dtype=\"int\") # оголошуємо масив лагів від 0 до maxlag\n    mi = np.zeros(len(lags))                 # оголошуємо масив під значення взаємної інформації\n    for i, lag in enumerate(lags):           # проходимось по кожному лагу\n        \n        # виконуємо зміщення на lag значень \n        y1 = x[:n-lag].copy()\n        y2 = x[lag:].copy()\n\n        # і розраховуємо взаємну інформацію між часовим рядом y1\n        # та його зміщенною на lag кроків копією \n        mi[i] = nk.mutual_information(y1, y2, bins=100)\n\n    return mi\n\nВиведемо залежність автовзаємної інформації від лагу для всього ряду S&P 500 та Біткоїна. Спочатку розрахуємо вихідні значення ряду, далі прибутковості і потім волатильності. Для кожного з відповідних сигналів виведемо взаємну інформацію.\nВиконуємо перетворення S&P 500 та Біткоїна\n\nsp_init = transformation(time_ser_1, ret_type=1)\nsp_ret = transformation(time_ser_1, ret_type=4)\nsp_vol = np.abs(sp_ret.copy())\n\nbtc_init = transformation(time_ser_2, ret_type=1)\nbtc_ret = transformation(time_ser_2, ret_type=4)\nbtc_vol = np.abs(btc_ret.copy())\n\nРозраховуємо автовзаємну інформацію S&P 500 та Біткоїна\n\nmax_lag = 100\n\nmu_sp_init = automut(sp_init, max_lag)\nmu_sp_ret = automut(sp_ret, max_lag)\nmu_sp_vol = automut(sp_vol, max_lag)\n\nmu_btc_init = automut(btc_init, max_lag)\nmu_btc_ret = automut(btc_ret, max_lag)\nmu_btc_vol = automut(btc_vol, max_lag)\n\nlags = np.arange(0, max_lag, dtype=\"int\") # оголошуємо масив лагів від 0 до maxlag\n\n\nfig, ax = plt.subplots()                     # Створюємо порожній графік\n\nax.plot(lags, mu_sp_init, label=r'$MI $ ' + f'{symbol_1}')  # Додаємо дані до графіку\nax.plot(lags, mu_sp_ret, label=r'$MI$ ' + r'$g(t)$')                          \nax.plot(lags, mu_sp_vol, label=r'$MI$ ' +  r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Automutual information\")     # Додаємо підпис для вісі Оу\n\nplt.savefig(f'Automutual information {symbol_1}.jpg')  # Зберігаємо графік \nplt.show();                                            # Виводимо графік\n\n\n\n\nРисунок 5.6: Зміна з часом автовзаємної інформації для вихідного ряду x, нормалізованих прибутковостей g та модулів mod(g) фондового індексу S&P 500\n\n\n\n\n\nfig, ax = plt.subplots()                     # Створюємо порожній графік\n\nax.plot(lags, mu_btc_init, label=r'$MI $ ' + f'{symbol_2}')  # Додаємо дані до графіку\nax.plot(lags, mu_btc_ret, label=r'$MI$ ' + r'$g(t)$')                          \nax.plot(lags, mu_btc_vol, label=r'$MI$ ' +  r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Automutual information\")     # Додаємо підпис для вісі Оу\n\nplt.savefig(f'Automutual information {symbol_2}.jpg')  # Зберігаємо графік \nplt.show();                                            # Виводимо графік\n\n\n\n\nРисунок 5.7: Зміна з часом автовзаємної інформації для вихідного ряду x, нормалізованих прибутковостей g та модулів mod(g) криптовалютного індексу BTC\n\n\n\n\nЯк ми можемо бачити з представлених графіків, ступінь взаємної інформації це показник, що найкращим чином працює саме для вихідних значень часових сигналів. Для вихідного ряду ступінь взаємної інформації залишається доволі високим. Для прибутковостей і волатильностей взаємна інформація спадає одразу на першому лагу, що свідчить про незалежність значень на подальших часових затримках.\n\n\n5.2.2 Розрахунок мономасштабної складності Лемпеля-Зіва\nПродовжимо розраховувати й інші показники складності. Розглянемо можливість використання показника складності Лемпеля-Зіва в якості індикатора катастрофічних подій.\n\nret_type = 4                           # вид ряду\nwindow = 250                           # ширина вікна\ntstep = 1                              # часовий крок вікна \nlength = len(time_ser_1.values)        # довжина самого ряду\nm = 4                                  # розмірність вкладень \ntau = 1                                # часова затримка         \n\nLZC = []                               # класична складність Лемпеля-Зіва\nPLZC = []                              # пермутаційна складність Лемпеля-Зіва\n\n\nfor i in tqdm(range(0,length-window,tstep)):    # фрагменти довжиною window  \n                                                # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо класичну складність Лемпеля-Зіва \n    lzc, _ = nk.complexity_lempelziv(fragm)\n\n    # та пермутаційну складність Лемпеля-Зіва\n    plzc, _ = nk.complexity_lempelziv(fragm, \n                                      delay=tau, \n                                      dimension=m, \n                                      permutation=True)\n\n\n    # та додаємо результати до масиву значень\n    LZC.append(lzc)\n    PLZC.append(plzc)\n\n100%|██████████| 2177/2177 [00:18&lt;00:00, 116.70it/s]\n\n\nЗберігаємо результати в текстових файлах:\n\nnp.savetxt(f\"lzc_name={symbol_1}_window={window}_step={tstep}_rettype={ret_type}.txt\" , LZC)\nnp.savetxt(f\"plzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.txt\" , PLZC)\n\nТа візуалізуємо їх:\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n                time_ser_1.values[window:length:tstep], \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep],\n                LZC,\n                'gold', \n                label=fr\"$LZC$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep],\n                PLZC,\n                'red', \n                label=fr\"$PLZC$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"plzc_lzc_name={symbol_1}_ \\\n    window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.jpg\")\n\nplt.show();\n\n\n\n\nРисунок 5.8: Динаміка індексу S&P 500, класичної мономасштабної складності Лемпеля-Зіва та її пермутаційного аналогу\n\n\n\n\nНа даному рисунку видно, що 2 міри поводять себе асиметрично по відношенню один до одного: \\(LCZ\\) вказує на зростання складності, наприклад, події 2019 року. У той же час \\(PLCZ\\) вказує на спад складності системи в цей період. Варто дослідити мультимасштабну динаміку міри Лемпеля-Зіва для більш змістовних висновків.\n\n\n5.2.3 Обчислення мультимасштабної складності Лемпеля-Зіва\n\nret_type = 4\nret_sp = transformation(time_ser_1, ret_type)\n\n\nmslzc, info = nk.entropy_multiscale(ret_sp, method=\"LZC\", \n                                    scale=200, show=True)\n\n\n\n\nРисунок 5.9: Залежність від масштабу класичної складності Лемпеля-Зіва для S&P 500\n\n\n\n\nМультимасштабна динаміка пермутаційного показника складності Лемпеля-Зіва\n\nmsplzc, info = nk.entropy_multiscale(ret_sp, \n                                        method=\"LZC\",  \n                                        permutation=True,\n                                        dimension=m,\n                                        delay=tau, \n                                        scale=200, \n                                        show=True)\n\n\n\n\nРисунок 5.10: Залежність від масштабу пермутаційної складності Лемпеля-Зіва для S&P 500\n\n\n\n\nТепер розрахуємо віконну динаміку мультимасштабних показників Лемпеля-Зіва. Ми повертатимемо сумарну складність Лемпеля-Зіва за всіма масштабам.\n\nret_type = 4                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду\nm = 4                             # розмірність вкладень \ntau = 1                           # часова затримка         \n\nMSLZC = []                        # мультимасштабна складність Лемпеля-Зіва\nMSPLZC = []                       # мультимасштабна пермутаційна складність Лемпеля-Зіва\n\n\nfor i in tqdm(range(0,length-window,tstep)):    # фрагменти довжиною window  \n                                                # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо мультимасштабну складність Лемпеля-Зіва \n    mslzc, _ = nk.entropy_multiscale(fragm)\n\n    # та мультимасштабну пермутаційну складність Лемпеля-Зіва\n    msplzc, _ = nk.entropy_multiscale(fragm, \n                                      delay=tau, \n                                      dimension=m, \n                                      permutation=True)\n\n\n    # та додаємо результати до масиву значень\n    MSLZC.append(mslzc)\n    MSPLZC.append(msplzc)\n\n100%|██████████| 2177/2177 [00:49&lt;00:00, 43.90it/s]\n\n\n\nnp.savetxt(f\"mslzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}.txt\" , MSLZC)\nnp.savetxt(f\"msplzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.txt\" , MSPLZC)\n\n\nfig, ax = plt.subplots(figsize=(13,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n                time_ser_1.values[window:length:tstep], \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep],\n                MSLZC,\n                'gold', \n                label=fr\"$MSLZC$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep],\n                MSPLZC,\n                'red', \n                label=fr\"$MSPLZC$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"msplzc_mslzc_name={symbol_1}_ \\\n    window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.jpg\")\n\nplt.show();\n\n\n\n\nРисунок 5.11: Динаміка індексу S&P 500, класичної мультимасштабної складності Лемпеля-Зіва та її пермутаційного аналогу\n\n\n\n\nТепер бачимо однозначну картину: обидві міри поводять себе синхронно, та спадають у кризові та передкризові періоди, що вказує на зростання ступеня детермінованості та самоорганізації ринку.\n\n\n5.2.4 Обчислення Шеннонівської ентропії\nЯк уже зазначалося, Шеннонівська ентропія — це міра непередбачуваності стану, або, еквівалентно, його середнього інформаційного вмісту. Ентропія Шеннона є однією з перших і найбільш базових мір ентропії та фундаментальним поняттям теорії інформації.\nРозраховуватимемо її в ковзному вікні.\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду  \nlog_base = np.exp(1)      \n\nshannon = []                      # ентропія Шеннона\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо ентропію Шеннона\n    p, be = np.histogram(fragm,         # розраховуємо щільність ймовірностей\n                        bins='auto', \n                        density=True)  \n    r = be[1:] - be[:-1]                # знаходимо dx\n    P = p * r                           # представляємо ймовірність як f(x)*dx\n    P = P[P!=0]                         # фільтруємо по всім ненульовим ймовірностям\n\n    sh_ent, _ = nk.entropy_shannon(freq=P, base=log_base) # розраховуємо ентропію \n    sh_ent /= np.log(len(P))                              # та нормалізуємо\n\n    # та додаємо результат до масиву значень\n    shannon.append(sh_ent)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 2135.92it/s]\n\n\n\nnp.savetxt(f\"shannon_ent_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\" , shannon)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], shannon\nylabels = ylabel_1, \"ShEn\"\nfile_name = f\"shannon_ent_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], \n            values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.12: Динаміка індексу S&P 500 та ентропії Шеннона\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, ентропія Шеннона реагує спадом на кризові періоди індексу S&P 500, що вказує на приріст ступеня періодизації системи, її детермінованості.\n\n\n5.2.5 Розрахунок інформаційного показника Фішера\nПерш за все задаємо параметри для розрахунків:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду  \nm = 3                             # розмірність вкладень\ntau = 1                           # часова затримка\n\nfisher = []                       # інформація Фішера\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    fish_inf, _ = nk.fisher_information(signal=fragm,\n                                        dimension=m, \n                                        delay=tau) \n\n    # та додаємо результат до масиву значень\n    fisher.append(fish_inf)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 2937.25it/s]\n\n\n\nnp.savetxt(f\"fisher_inf_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_dimension={m}_delay={tau}.txt\", fisher)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], fisher\nylabels = ylabel_1, \"FI\"\nfile_name = f\"fisher_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_dimension={m}_delay={tau}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.13: Динаміка індексу S&P 500 та інформаційного показника Фішера\n\n\n\n\nНа даному рисунку видно, що показник Фішера спадає у кризові та передкризові періоди, що говорить про спад кількості інформації, що необхідна для опису самоорганізованої динаміки фінансових криз, тобто зростання корельованості між діями трейдерів на ринку.\n\n\n5.2.6 Обчислення часу декореляції\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\ndecorrelation_time = []           # час декореляції\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    dec_time, _ = nk.complexity_decorrelation(fragm) \n\n    # та додаємо результат до масиву значень\n    decorrelation_time.append(dec_time)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 1857.09it/s]\n\n\n\nnp.savetxt(f\"dec_time_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", decorrelation_time)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], decorrelation_time\nylabels = ylabel_1, \"DT\"\nfile_name = f\"dec_time_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, \n            xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.14: Динаміка індексу S&P 500 та часу декореляції\n\n\n\n\nНа представленому рисунку видно, що час декореляції зростає у період краху, що вказує на зростання кореляції системи в цей період.\n\n\n5.2.7 Обчислення відносної шорсткості\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\nrelative_roughness = []           # відносна шорсткість\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    rr, _ = nk.complexity_relativeroughness(fragm) \n\n    # та додаємо результат до масиву значень\n    relative_roughness.append(rr)\n\n100%|██████████| 2177/2177 [00:01&lt;00:00, 2094.81it/s]\n\n\n\nnp.savetxt(f\"rel_rough_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", relative_roughness)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], relative_roughness\nylabels = ylabel_1, \"RR\"\nfile_name = f\"rel_rough={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, \n            xlabel, ylabels, file_name)\n\n\n\n\nРисунок 5.15: Динаміка індексу S&P 500 та показника відносної шорсткості\n\n\n\n\nПоказник відносної шорсткості демонструє, що крахові події як, наприклад, у 2015, 2016, 2019, 2020 та 2023 роках характеризуються зростанням шорсткості своєї динаміка. Подібного роду поведінка є індикатором зростання шумової активності ринку: кореляційних характеристик та загальної варіації ринку в цілому. Зростання цього показника в періоди криз є індикатором зростання фрактальності ринку в дані періоди часу.\n\n\n5.2.8 Розрахунок показників складності Хьорта\nЗавершуємо хід роботи показниками складності Хьорта:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\nactivity = []                     # параметр активності\nmobility = []                     # параметр рухливості\ncomplexity = []                   # параметр складності\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо показники складності Хьорта\n    cmpl, info = nk.complexity_hjorth(fragm) \n\n    # та додаємо результат до масиву значень\n    activity.append(info['Activity'])\n    mobility.append(info['Mobility'])\n    complexity.append(cmpl)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 3338.20it/s]\n\n\n\nnp.savetxt(f\"activity_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", activity)\nnp.savetxt(f\"mobility_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", mobility)\nnp.savetxt(f\"complexity_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", complexity)    \n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\nax4.spines.right.set_position((\"axes\", 1.19))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n              time_ser_1.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep], \n               activity, \"r--\", label=r\"$Act$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep], \n               mobility, \"g-\", label=r\"$Mob$\")\np4, = ax4.plot(time_ser_1.index[window:length:tstep],\n               complexity, \"m-\", label=r\"$Comp$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel_1}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax4.legend(handles=[p1, p2, p3, p4])\n\nplt.savefig(f\"hjorth_name={symbol_1}_ret={ret_type}_wind={window}_step={tstep}.jpg\")\nplt.show();\n\n\n\n\nРисунок 5.16: Динаміка індексу S&P500 наряду з показниками активності, мобільності та складності Хьорта\n\n\n\n\nНа даному рисунку видно, що параметр активності (\\(Act\\)) представляється найменш інформативним, оскільки він вказує тільки на зростання сукупної дисперсії сигналу. Видно тільки те, що активність значно почала зростати напередодні 2022 року, але для попередніх кризових станів ми не бачимо передвісницької поведінки цього індикатора, тому він ще вимагатиме додактових досліджень та експериментів, що виходять за рамки даного посібника.\nПитання передчасної ідентифікації наростання кризового явища найкраще вирішує показник мобільності (\\(Mob\\)). Ми бачимо, що даний показник зростає під час 2015-2016 років, напередодні 2019, при настанні коронавірусної пандемії, перед 2023 роком та 2024.\nПоказник складності Хьорта (\\(Comp\\)) реагує асиметричним чином: у той час коли мобільність зростає, показник складності спадає, вказуючи на те, що динаміки системи прагне до вищого ступеня періодичності або корельованості."
  },
  {
    "objectID": "lab_5.html#теоретичні-відомості",
    "href": "lab_5.html#теоретичні-відомості",
    "title": "6  Лабораторна робота № 5",
    "section": "6.1 Теоретичні відомості",
    "text": "6.1 Теоретичні відомості\nПитання динаміки розвитку і функціонування складних систем може розглядатись у двох варіантах:\n\nяк дослідження шумової активності;\nяк детерміністичного випадку з певним ступенем порядку.\n\nОстанніми роками було використано кілька підходів для ідентифікації механізмів, що лежать в основі розвитку та функціонування складних систем. Особливо корисні результати було отримано при їх дослідженні методами теорії випадкових матриць, моно- та мультифрактального аналізу, теорії хаосу з реконструкцією траєкторії системи у фазовому просторі та визначення її параметрів, рекурентного аналузу. Ми розглянули ці методи у попередніх роботах. Однак, застосування деяких із методів висуває вимоги до стаціонарності досліджуваних даних, потребує довгих часових рядів та комплексного обчислення кількох параметрів.\nІншим підходом до розгляду питання вивчення особливостей складних систем є обчислення характеристик ентропії. Для практичного застосування у якості міри невизначеності, а значить і складності сигналу, використовують десятки різновидів ентропії.\nКонцепція термодинамічної ентропії як міри хаосу системи добре відома у фізиці, однак, останніми роками поняття ентропії було застосоване до складних систем інших об’єктів (біологічних, економічних, соціальних тощо). Так, один із найбільш часто використовуваних методів визначення ентропії базується на обчисленні спектру потужності Фур’є та застосовується для вивчення сигналів (часових рядів) різної природи. Проте, використання дискретного перетворення Фур’є для аналізу часових рядів має свої недоліки, зокрема, на результати впливає нестаціонарність рядів, варіювання їх довжини від сотень до сотень тисяч, та обмеження самого методу (незмінність частотно-часових характеристик протягом всього часу функціонування системи). Тому виникає питання про розрахунок значень ентропії за допомогою інших методів.\nВведемо поняття ентропії, скориставшись інформацією, яку можна знайти у Вікіпедії.\nТермодинамічна ентропія \\(S\\), часто просто іменована ентропія, в хімії і термодинаміці є мірою кількості енергії у фізичній системі, яка не може бути використана для виконання роботи. Вона також є мірою безладдя, присутнього в системі.\nПоняття ентропії була вперше введено у 1865 році Рудольфом Клаузіусом. Він визначив зміну ентропії термодинамічної системи при оборотному процесі як відношення зміни загальної кількості тепла \\(\\Delta Q\\) до величини абсолютної температури \\(T\\):\n\\[\n\\Delta S = \\Delta Q / T.\n\\]\nРудольф Клаузіус дав величині \\(S\\) ім’я “ентропія”, що походить від грецького слова τρoπή, “зміна” (зміна, перетворення). Зверніть увагу на те, що рівність відноситься до зміни ентропії.\nУ 1877 році, Людвіг Больцман зрозумів, що ентропія системи може відноситися до кількості можливих “мікростанів” (мікроскопічних станів) що узгоджуються з їх термодинамічними властивостями. Розглянемо, наприклад, ідеальний газ у посудині. Мікростан визначений як позиції і імпульси кожного атома, що становить систему. Зв’язність пред’являє до нас вимоги розглядати тільки ті мікростани, для яких: (i) місцерозташування всіх частин розташовані в рамках судини, (ii) для отримання загальної енергії газу кінетичні енергії атомів підсумовуються. Больцман постулював що\n\\[\nS = k_{B}\\ln{\\Omega},\n\\]\nде константу \\(k_{B} = 1,38 \\cdot 10^{-23} Дж/К\\) ми знаємо тепер як сталу Больцмана, a \\(\\Omega\\) є числом мікростанів, які можливі в наявному макроскопічному стані. Цей постулат, відомий як принцип Больцмана, може бути оцінений як початок статистичної механіки, яка описує термодинамічні системи використовуючи статистичну поведінку компонентів, із яких вони складаються. Принцип Больцмана зв’язує мікроскопічні властивості системи (\\(\\Omega\\)) з однією з її термодинамічних властивостей (\\(S\\)).\nЗгідно визначенню Больцмана, ентропія є просто функцією стану. Більш того, оскільки (\\(\\Omega\\)) може бути тільки натуральним числом (1, 2, 3), ентропія повинна бути додатною — виходячи з властивостей логарифма.\nУ випадку дискретних станів квантової механіки кількість станів підраховується звичайним чином. В рамках класичної механіки мікроскопічний стан системи описується координатами \\(q_{i}\\) й імпульсами \\(p_{i}\\) окремих частинок, які пробігають неперервні значення. Для підрахунку станів у класичних системах фазовий простір розбивають на невеликі комірки із об’ємом, який відповідає сталій Планка. У такому випадку\n\\[\nS = k_{B}\\ln\\frac{1}{( 2\\pi\\hbar )^{s}} \\int \\prod_{i=1}^{s} dq_{i}dp_{i},\n\\]\nде \\(s\\) — число незалежних координат, \\(\\hbar\\) — приведена стала Планка, а інтегрування проводиться по області фазового простору, який відповідає певному макроскопічному стану.\nКлод Шеннон (Shannon, 1948) запропонував формулу для оцінки невизначеності кодової інформації в каналах зв’язку, звану ентропією Шеннона:\n\\[\nS = -k\\sum_{i=1}^{n}p_{i}\\ln{p_{i}},\n\\]\nде \\(p_{i}\\) — вірогідність того, що символ \\(i\\) зустрічається в коді, який містить \\(N\\) символів, \\(k\\) — розмірний множник.\nЗв’язок між ентропією і інформацією можна прослідкувати на наступному прикладі. Розглянемо тіло при абсолютному нулі температури, і хай ми маємо повну інформацію про координати і імпульси кожної частинки. Для простоти покладемо, що імпульси всіх частинок рівні нулю. В цьому випадку термодинамічна ймовірність рівна одиниці, а ентропія — нулю. При кінцевих температурах ентропія в рівновазі досягає максимуму. Можна зміряти всі макропараметри, що характеризують даний макростан. Проте ми практично нічого не знаємо про мікростан системи. Точніше кажучи, ми знаємо, що даний макростан можна реалізувати за допомогою дуже великого числа мікростанів. Таким чином, нульовій ентропії відповідає повна інформація (ступінь незнання рівний нулю), а максимальної ентропії — повне незнання мікростанів (ступінь незнання максимальний).\nУ теорії інформації ентропія (інформаційна ентропія) визначається як кількість інформації. Нехай \\(P\\) — апріорна вірогідність деякої події (ймовірність до проведення досвіду), а \\(P_{1}\\) – ймовірність цієї події після проведення досвіду. Для простоти вважатимемо, що \\(P_{1} = 1\\). За Шенноном, кількість інформації \\(I\\), яка дає точну відповідь (після проведення експерименту)\n\\[\nI = K \\log{P}.\n\\]\nЦя кількість інформації, за визначенням, дорівнює одному біту.\nФізичний сенс \\(I\\) — це міра нашого незнання. Іншими словами, \\(I\\) — це та інформація, яку ми можемо одержати, вирішивши завдання. У прикладі (тіло при абсолютному нулі температури), що розглядається вище, міра нашого незнання рівна нулю, оскільки \\(P = 1\\). Після проведення досвіду ми одержуємо нульову інформацію \\(I = 0\\), оскільки все було відомо до досвіду. Якщо розглядати тіло при кінцевих температурах, то до проведення досвіду число мікростанів, а отже, і \\(P\\) дуже велике. Після проведення досвіду ми одержуємо велику інформацію, оскільки нам стають відомими координати і імпульси всіх частинок.\nАналогія між кількістю інформації і ентропією \\(S\\), визначуваною з принципу Больцмана, очевидна. Досить покласти множник \\(K\\) рівним постійній Больцмана \\(k_{B}\\) і використовувати натуральний логарифм. Саме з цієї причини величину $ I $ називають інформаційною ентропією. Інформаційна ентропія (кількість інформації) була визначена по аналогії із звичайною ентропією, і вона має властивості, характерні для звичайній ентропії: адитивність, екстремальні властивості і т.д. Проте ототожнювати звичайну ентропію з інформаційною не можна, оскільки неясно, яке відношення має друге начало до інформації. Нагадаємо, що екстенсивна величина — ця така характеристика системи, яка росте із збільшенням розмірів системи, тобто, якщо наша система складається з двох незалежних підсистем \\(А\\) і \\(В\\), то ентропію всієї системи можна одержати складанням ентропій підсистем:\n\\[\nS(\\,A+B)\\, = S(\\,A)\\, + S(\\,B)\\,.\n\\]\nСаме ця властивість і означає екстенсивність, або адитивність, ентропії."
  },
  {
    "objectID": "lab_5.html#хід-роботи",
    "href": "lab_5.html#хід-роботи",
    "title": "6  Лабораторна робота № 5",
    "section": "6.2 Хід роботи",
    "text": "6.2 Хід роботи\n\nimport EntropyHub as eh\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport yfinance as yf\n#import antropy as ant\nimport neurokit2\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nplt.style.use('classic')\n\nparams = {\n    'axes.labelsize': 26,\n    'axes.titlesize':26, \n    \"axes.grid\" : False,\n    'font.size': 26, \n    'legend.fontsize': 26, \n    'xtick.labelsize': 26, \n    'ytick.labelsize': 26, \n    'lines.linewidth': 2,\n    'axes.facecolor': 'white',\n    'figure.facecolor': 'white',\n    'axes.titlesize': 'small',\n    'font.family': 'Times New Roman',\n    'savefig.dpi': 300\n}\n\nplt.rcParams.update(params)\n\nxlabel = 'time, days'\n\n\nsymbol = \"BTC-USD\"\nsymbol_for_graph = \"BTC-USD\"\n\nstart = \"2019-01-01\"\nend = \"2022-09-28\"\n\nsymbol_plot = symbol.split('.')[0]\nsymbol_for_graph_plot = symbol_for_graph.split('.')[0]\n\ndata = yf.download(symbol, start, end)\nclose = data['Adj Close'].copy()\nfor_graph = data['Adj Close'].copy()\n\nnp.savetxt(f'{symbol}_initial_time_series.txt', close.values)\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n6.2.1 Виведення графіку досліджуваного ряду\n\nfig, ax = plt.subplots()\nclose.plot(figsize=(15,8), xlabel=xlabel, ylabel=symbol_plot)\nax.legend([symbol_plot]);\n\nplt.savefig(f'{symbol}.jpg')\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\nfor_graph.plot(figsize=(15,8), xlabel=xlabel, ylabel=symbol_for_graph_plot)\nax.legend([symbol_for_graph]);\n\nplt.savefig(f'{symbol_for_graph_plot}.jpg')\nplt.show()\n\n\n\n\n\n\n6.2.2 Задання ширини вікна та кроку\n\nwindow = 100 # розмір вікна\ntstep = 1 # крок вікна \n\n\n\n6.2.3 Approximate entropy (Апроксимаційна ентропія)\n\nSteven M. Pincus, Approximate entropy as a measure of system complexity, Proceedings of the National Academy of Sciences, 88.6 (1991): 2297-2301.\n\nТеоретичний опис методики розрахунку\nЕнтропія подібності (Approximate Entropy, ApEn) є “статистикою регулярності”, що визначає можливість передбачувати флуктуації в часових рядах. Інтуїтивно вона означає, що наявність повторюваних шаблонів (послідовностей певної довжини, побудованих із чисел ряду, що слідують одне за іншим) флуктуацій у часовому ряді призводить до більшої передбачуваності часового ряду порівняно із рядами, де повторюваності шаблонів немає. Порівняно велике значення ApEn показує ймовірність того, що подібні між собою шаблони спостережень не будуть слідувати один за одним. Іншими словами, часовий ряд, що містить велику кількість повторюваних шаблонів, має порівняно мале значення ApEn, а значення ApEn для менш передбачуваного (більш складного) процесу є більшим.\nПри розрахунку ApEn для даного часового ряду \\(S_{N}\\), що складається із \\(N\\) значень \\(t(\\,1)\\,, t(\\,2)\\,, t(\\,3)\\,, ... , t(\\,N)\\,\\) вибираються два параметри, \\(m\\) та \\(r\\). Перший з цих параметрів, \\(m\\), вказує довжину шаблона, а другий — \\(r\\) — визначає критерій подібності. Досліджуються підпослідовності елементів часового ряду \\(S_{N}\\), що складаються з \\(m\\) чисел, взятих, починаючи з номера \\(i\\), і називаються векторами \\(p_{m} (\\,i)\\,\\). Два вектори (шаблони), \\(p_{m}(\\,i)\\,\\) та $ p_{m}(,j),$, будуть подібними, якщо всі різниці пар їх відповідних координат є меншими за значення \\(r\\), тобто якщо\n\\[\n| t(\\,i+k)\\, - t(\\,j+k)\\, | &lt; r \\quad \\textrm{для} \\quad 0 \\leq k &lt; m.\n\\]\nДля розглядуваної множини \\(P_{m}\\) всіх векторів довжини \\(m\\) часового ряду \\(S_{N}\\) можна обраховуються значення\n\\[\nC_{im}(\\,r)\\, = \\frac{n_{im}(\\,r)\\,}{N-m+1},\n\\]\nде \\(n_{im}(\\,r)\\,\\) — кількість векторів у \\(P_{m}\\), що подібні вектору \\(p_{m}(\\,i)\\,\\) (враховуючи вибраний критерій подібності \\(r\\)). Значення \\(C_{im}(\\,r)\\,\\) є часткою векторів довжини \\(m\\), що мають схожість із вектором такої ж довжини, елементи якого починаються з номера \\(i\\). Для даного часового ряду обраховуються значення \\(C_{im}(\\,r)\\,\\) для кожного вектора у \\(P_{m}\\), після чого знаходиться середнє значення \\(C_{m}(\\,r)\\,\\), яке виражає розповсюдженість подібних векторів довжини \\(m\\) у ряду \\(S_{N}\\). Безпосередньо ентропія подібності для часового ряду \\(S_{N}\\) з використанням векторів довжини \\(m\\) та критерію подібності \\(r\\) визначається за формулою:\n\\[\nApEn(\\,S_{N}, m, r)\\, = \\ln(\\,\\frac{C_{m}(\\,r)\\,}{C_{m+1}(\\,r)\\,})\\,,\n\\]\nтобто, як натуральний логарифм відношення повторюваності векторів довжиною \\(m\\) до повторюваності векторів довжиною \\(m+1\\).\nТаким чином, якщо знайдуться подібні вектори у часовому ряді, ApEn оцінить логарифмічну ймовірність того, що наступні інтервали після кожного із векторів будуть відрізнятись. Менші значення ApEn відповідають більшій ймовірності того, що за векторами слідують подібні їм. Якщо часовий ряд дуже нерегулярний — наявність подібних векторів не може бути передбачуваною і значення ApEn є порівняно великим.\nЗауважимо, що ApEn є нестійкою до вхідних даних характеристикою, оскільки досить сильно залежить від параметрів \\(m\\) та \\(r\\).\n\nm = 3 #розмірність вкладень\ntau = 1 #часова затримка\nr = 0.45 #параметр подібності\nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб. 3-відносні приб. 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nApEn = [] #масив для зберігання значень ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду:\n    \n    # 1 - вихідний ряд, 2 - детрендований, 3 - прибутковості, 4 - стандартизовані прибутковості, 5 - логарифмічні\n    \n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:\n        fragm = np.log(fragm) - np.log(fragm.shift(1))\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Ap, _ = eh.ApEn(Sig = fragm, m = m, tau = tau, r = r)\n    ApEn.append(Ap[-1])\n\n\nname_for_save = f\"ApEn_{symbol}_{window}_{tstep}_{m}_{tau}_{r}_{ret_type}.txt\" #ім'я файлу для зберігання\nwith open(name_for_save, 'w') as f: #відкриваємо на запис \n    ApEn_file = [str(line) + '\\n' for line in ApEn]\n    f.writelines(ApEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8)) # візуалізуємо результати розрахунку\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f'{symbol_for_graph_plot}', color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, ApEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], ApEn, label='ApEn', color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"ApEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, embedding dimension={m}, delay={tau}, r={r}.jpg\")\nplt.show()\n\n\n\n\n\n\n6.2.4 Fuzzy entropy (Нечітка ентропія)\n\nWeiting Chen, et al. Characterization of surface EMG signal based on fuzzy entropy, IEEE Transactions on neural systems and rehabilitation engineering, 15.2 (2007): 266-272.\nHong-Bo Xie, Wei-Xing He, and Hui Liu, Measuring time series regularity using nonlinear similarity-based sample entropy, Physics Letters A, 372.48 (2008): 7140-7146.\n\n\nm = 3 #розмірність вкладень\ntau = 1 #часова затримка\ncharacteristic_func = \"default\" #вид функції приналежності: default, sigmoid, gudermannian, linear\nr = (0.4, 2.0) #параметри, що подаються до функції приналежності. для default та sigmoid 2 значення r, \n                  #для gudermannian та linear 1 значення r  \nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб. 3-відносні приб. 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nFuzzEn = [] #масив для зберігання значень ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1: \n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:\n        fragm = np.log(fragm) - np.log(fragm.shift(1))\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Fuzz, _, _ = eh.FuzzEn(Sig = fragm, m = m, tau = tau, Fx = characteristic_func, r = r) #Рахуємо нечітку ентропію \n    FuzzEn.append(Fuzz[-1]) #дожаємо розрахованє значення до масиву значень \n\n\nname_for_save = f\"FuzzEn_{symbol}_{window}_{tstep}_{m}_{tau}_{characteristic_func}_{r}_{ret_type}.txt\" #ім'я файлу для зберігання\nwith open(name_for_save, 'w') as f: #відкриваємо на запис \n    FuzzEn_file = [str(line) + '\\n' for line in FuzzEn]\n    f.writelines(FuzzEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], close.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, FuzzEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], FuzzEn, label=\"FuzzEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"FuzzEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, membership function={characteristic_func}, embedding dimension={m}, delay={tau}, r={r}.jpg\")\n\nplt.show()\n\n\n\n\n\n\n6.2.5 Sample entropy (Ентропія шаблонів)\n\nJoshua S Richman and J. Randall Moorman, Physiological time-series analysis using approximate entropy and sample entropy, American Journal of Physiology-Heart and Circulatory Physiology (2000).\n\n\nm = 3 #розмірність вкладень\ntau = 1 #часова затримка\nr = 0.4 #параметр подібності\nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб. 3-відносні приб. 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nSampEn = [] #масив для зберігання значень ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:\n        fragm = np.log(fragm) - np.log(fragm.shift(1))\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Samp, _, _ = eh.SampEn(Sig = fragm, m = m, tau = tau, r = r, Logx=np.exp(1))\n    SampEn.append(Samp[-1])\n\n\nname_for_save = f\"SampEn_{symbol}_{window}_{tstep}_{m}_{tau}_{r}_{ret_type}.txt\" #ім'я файлу для зберігання\nwith open(name_for_save, 'w') as f: #відкриваємо на запис \n    SampEn_file = [str(line) + '\\n' for line in SampEn]\n    f.writelines(SampEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], close.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, SampEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], SampEn, label=\"SampEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"SampEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, embedding dimension={m}, delay={tau}, r={r}.jpg\")\n\nplt.show()\n\n\n\n\n\n\n6.2.6 Permutation entropy (Ентропія перестановок)\n\nChristoph Bandt and Bernd Pompe, Permutation entropy: A natural complexity measure for time series, Physical Review Letters, 88.17 (2002): 174102.\nXiao-Feng Liu, and Wang Yue, Fine-grained permutation entropy as a measure of natural complexity for time series, Chinese Physics B, 18.7 (2009): 2690.\nChunhua Bian, et al., Modiﬁed permutation-entropy analysis of heartbeat dynamics, Physical Review E, 85.2 (2012) : 021906\nBilal Fadlallah, et al., Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information, Physical Review E, 87.2 (2013): 022911.\nHamed Azami and Javier Escudero, Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation, Computer methods and programs in biomedicine, 128 (2016): 40-51.\nZhiqiang Huo, et al., Edge Permutation Entropy: An Improved Entropy Measure for Time-Series Analysis, 45th Annual Conference of the IEEE Industrial Electronics Soc, (2019), 5998-6003.\nZhe Chen, et al., Improved permutation entropy for measuring complexity of time series under noisy condition, Complexity, 1403829 (2019).\nMaik Riedl, Andreas MuЁller, and Niels Wessel, Practical considerations of permutation entropy, The European Physical Journal Special Topics, 222.2 (2013): 249-262.\n\n\nm = 4             # розмірність вкладень\ntau = 3           # часова затримка\nType = 'weighted' # none - класична; \n                  # finegrain - Fine-grained permutation entropy; \n                  # modified - Modiﬁed permutation entropy; \n                  # weighted - Weighted permutation entropy; \n                  # ampaware - Amplitude-aware permutation entropy; \n                  # edge - Edge permutation entropy; \n                  # uniquant - Uniform quantization-based permutation entropy; \n            \ntpx = -1          # finegrain tpx is the α parameter, a positive scalar (default: 1)\n                  # ampaware tpx is the A parameter, a value in range [0 1] (default: 0.5)\n                  # edge tpx is the r sensitivity parameter, a scalar &gt; 0 (default: 1)\n                  # uniquant tpx is the L parameter, an integer &gt; 1 (default: 4).\n\nlog = np.exp(1)    # основа логарифма\nnorm = True\nret_type = 4       # вид ряду: 1-вихідний, 2-абсолютні приб. 3-відносні приб. 4-нормалізовані приб. \nn = close.shape[0] # задаємо кількість значень\n\nPEn = []           # масив для зберігання значень нормалізованої перм. ентропії\nCPEn = []          # масив для зберігання значень умовної перм. ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window].copy() # відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    _, Pnorm, cPE = eh.PermEn(fragm, m = m, tau = tau, Typex = Type, tpx = tpx, Logx = log, Norm = norm)\n    PEn.append(Pnorm[-1])\n    CPEn.append(cPE[-1])\n\n\nPEn_for_save = f\"PEn_symbol={symbol}_window={window}_step={tstep}_d={m}_tau={tau}_ret={ret_type}_type={Type}_param={tpx}.txt\" #ім'я файлу для зберігання пермутаційної ентропії\nCPEn_for_save = f\"CPEn_symbol={symbol}_window={window}_step={tstep}_d={m}_tau={tau}_ret={ret_type}_type={Type}_param={tpx}.txt\" #ім'я файлу для зберігання умовної пермутаційної ентропії\nwith open(PEn_for_save, 'w') as f: #відкриваємо на запис \n    PEn_file = [str(line) + '\\n' for line in PEn]\n    f.writelines(PEn_file)\n    \nwith open(CPEn_for_save, 'w') as f: #відкриваємо на запис \n    CPEn_file = [str(line) + '\\n' for line in CPEn]\n    f.writelines(CPEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, PEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], PEn, label=\"PEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"PEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, type={Type}, tpx={tpx}, embedding dimension={m}, delay={tau}.jpg\")\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, CPEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], CPEn, label=\"CPEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"CPEn, symbol={symbol}, window size={window}, time step={tstep}, returns_type={ret_type}, type={Type}, tpx={tpx}, embedding dimension={m}, delay={tau}.jpg\")\n\nplt.show()\n\n\n\n\n\n\n6.2.7 Distribution entropy (Розподільна ентропія)\n\nLi, Peng, et al., Assessing the complexity of short-term heartbeat interval series by distribution entropy, Medical & biological engineering & computing 53.1 (2015): 77-87.\n\n\nm = 3 #розмірність вкладень\ntau = 1 #часова затримка\nbins = 'sturges' # Метод визначення бінів гістограми. Окрім цього можна обрати sqrt, rice, doanes\nnorm = True\nlog = np.exp(1)\nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб., 3-відносні приб., 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nDistEn = [] #масив значень для зберігання розподіленої ентропії \n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Dist = eh.DistEn(fragm, m = m, tau = tau, Bins = bins, Logx = log, Norm = norm)\n    DistEn.append(Dist[0])\n\nNote: 3/14 bins were empty\nNote: 3/14 bins were empty\nNote: 3/14 bins were empty\nNote: 12/14 bins were empty\nNote: 12/14 bins were empty\nNote: 12/14 bins were empty\nNote: 9/14 bins were empty\nNote: 7/14 bins were empty\nNote: 4/14 bins were empty\nNote: 2/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\nNote: 1/14 bins were empty\n\n\n\nDistEn_for_save = f\"DistEn_symbol={symbol}_window={window}_step={tstep}_d={m}_tau={tau}_series_type={ret_type}_bins={bins}.txt\" #ім'я файлу для зберігання розподіленої ентропії\nwith open(DistEn_for_save, 'w') as f: #відкриваємо на запис \n    DistEn_file = [str(line) + '\\n' for line in DistEn]\n    f.writelines(DistEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, DistEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], DistEn, label=\"DistEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"DistEn_symbol={symbol}_window={window}_step={tstep}_d={m}_tau={tau}_series_type={ret_type}_bins={bins}.jpg\")\n\nplt.show()\n\n\n\n\n\n\n6.2.8 Dispersion entropy (Дисперсійна ентропія)\n\nMostafa Rostaghi and Hamed Azami, Dispersion entropy: A measure for time-series analysis IEEE Signal Processing Letters 23.5 (2016): 610-614.\nHamed Azami and Javier Escudero, Amplitude-and ﬂuctuation-based dispersion entropy, Entropy 20.3 (2018): 210.\nLi Yuxing, Xiang Gao and Long Wang, Reverse dispersion entropy: A new complexity measure for sensor signal, Sensors 19.23 (2019): 5203.\nWenlong Fu, et al., Fault diagnosis for rolling bearings based on ﬁne-sorted dispersion entropy and SVM optimized with mutation SCA-PSO, Entropy 21.4 (2019): 404.\n\nТеоретичний опис методики розрахунку\n\nnorm = True\nfluct = False # Якщо True повертаємо флуктуаційно-дисперсійну ентропію\nm = 3 \ntau = 1\nrho = 1 # *If Typex = \"finesort\", rho is the tuning parameter, a positive scalar (default:1)\n\nclasses = 3 # кількість символів, що задіяні при перетворені\nType = 'ncdf' # тип символьного перетворення. \"ncdf\" Normalised cumulative distribution function [19]\n                                            # \"kmeans\" K-means clustering algorithm. Note: The ”kmeans” algorithm uses random initialization conditions. This causes results to vary slightly each time it is called.\n                                            # \"linear\" Linear segmentation of signal range\n                                            # \"finesort\" Fine-sorted dispersion entropy\n                                            # \"equal\" Approx. equal number of symbols.\n\nlog = np.exp(1)\nret_type = 4 #вид ряду: 1-вихідний, 2-абсолютні приб., 3-відносні приб., 4-нормалізовані приб. \nn = close.shape[0] #задаємо кількість значень\n\nDispEn = [] # масив значень для зберігання дисперсійної ентропії \nRevDispEn = [] # для зберігання оборотної дисперсійної ентропії\n\n\nfor i in range(0,n-window,tstep):\n    fragm = close.iloc[i:i+window] #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values\n    \n    Disp, RevDisp = eh.DispEn(fragm, m = m, tau = tau, c = classes, Typex = Type, \n                              Logx = log, Fluct = fluct, Norm = norm, rho = rho)\n    DispEn.append(Disp)\n    RevDispEn.append(RevDisp)\n\n\nDispEn_for_save = f\"DispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_series_type={ret_type}_fluct={fluct}_rho={rho}_classes={classes}_type={Type}.txt\" #ім'я файлу для зберігання DispEn\nRevDispEn_for_save = f\"RevDispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_series_type={ret_type}_fluct={fluct}_rho={rho}_classes={classes}_type={Type}.txt\" #ім'я файлу для зберігання RevDispEn\nwith open(DispEn_for_save, 'w') as f: #відкриваємо на запис \n    DispEn_file = [str(line) + '\\n' for line in DispEn]\n    f.writelines(DispEn_file)\n    \nwith open(RevDispEn_for_save, 'w') as f: #відкриваємо на запис \n    RevDispEn_file = [str(line) + '\\n' for line in RevDispEn]\n    f.writelines(RevDispEn_file)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, DispEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], DispEn, label=\"DispEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"DispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_series_type={ret_type}_fluct={fluct}_rho={rho}_classes={classes}_type={Type}.jpg\")\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15,8))\n\nax.plot(close.index[window:n:tstep], for_graph.values[window:n:tstep], label=f\"{symbol_for_graph_plot}\", color='b')\nax.set_xlabel(\"time, days\")\nax.set_ylabel(f\"{symbol_for_graph_plot}, RevDispEn\")\n\nax2 = ax.twinx()\nax2.plot(close.index[window:n:tstep], RevDispEn, label=\"RevDispEn\", color='r')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nplt.savefig(f\"RevDispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_series_type={ret_type}_fluct={fluct}_rho={rho}_classes={classes}_type={Type}.jpg\")\n\nplt.show()"
  },
  {
    "objectID": "lab_6.html#теоретичні-відомості",
    "href": "lab_6.html#теоретичні-відомості",
    "title": "7  Лабораторна робота № 6",
    "section": "7.1 Теоретичні відомості",
    "text": "7.1 Теоретичні відомості\n\n7.1.1 Означення фрактала\nФракталами називають геометричні об’єкти: лінії, поверхні, просторові тіла, що мають сильно шорстку поверхню або форму і характеризуються властивістю самоподібності. Слово фрактал походить від латинського слова fractus і перекладається як дробовий, ламаний. Самоподібність як основна характеристика фрактала означає, що він більш-менш однорідно змінюється при широкому діапазоні масштабів. Так, при збільшенні маленькі фрагменти фрактала виходять дуже схожими на великі. В ідеальному випадку така самопподібність призводить до того, що фрактальний об’єкт виявляється інваріантним щодо розтягувань, тобто йому, як кажуть, притаманна дилатаційна симетрія. Вона передбачає незмінність основних геометричних особливостей фрактала при зміні масштабу.\nОчевидно, що фрактальні об’єкти реального світу не є нескінченно самоподібними й існує мінімальний масштаб $l_{min}, такий, що на масштабі \\(l \\approx l_{min}\\) властивість самоподібності зникає. Окрім цього, на достатньо великих масштабах довжин \\(l &gt; l_{max}\\), де \\(l_{max}\\) — характерний геометричний розмір об’єктів, ця властивість самоподібності також порушується. Тому властивості природніх фракталів розглядаються лише на масштабах \\(l\\), що задовільняють відношення \\(l_{min} \\ll l \\ll l_{max}\\). Такі обмеження природні, оскільки, коли ми приводимо в якості прикладу фракталу — ламану, негладку траєкторію броунівської частинки, то ми розуміємо, що цей образ представляє очевидну ідеалізацію. Справа в тому, що на малих масштабах приховується граничність маси і розмірів броунівської частнки, а також кінцевість часу зіткнення. При врахуванні цих обставин траєкторія броунівської частинки починає представляти гладку криву.\nВарто зазначити, що властивість точної самоподібності характерна лише для регулярних фракталів. Якщо замість детермінованого способу побудови включити в алгоритм їхнього створення деякий елемент випадковості (як це буває, наприклад, у багатьох процесах диференційованого зростання кластерів, електричному пробої тощо), то виникають так звані випадкові фрактали. Основна їхня відмінність від регулярних полягає в тому, що властивості самоподібності є справедливими тільки після відповідного усереднення за всіма статистично незалежними реалізаціями об’єкта. При цьому збільшена частина фрактала не точно ідентична вихідному фрагменту, проте їхні статистичні характеристики збігаються.\n\n\n7.1.2 Довжина берегової лінії\n\n\n\nРисунок 7.1: Визначення довжини берегової лінії між точками А та В\n\n\nПершочергово поняття фрактала у фізиці виникло у зв’язку із завданням про визначення довжини берегової лінії. Під час її вимірювання за наявною картою місцевості з’ясувалася цікава деталь — чим більш великомасштабна карта береться, тим довшою виявляється ця берегова лінія. Нехай, наприклад, відстань по прямій між розсташованими на береговій лінії точками А та В дорівнює R (див. Рисунок 7.1). Тоді, щоб виміряти довжину берегової лінії між цима точками, ми розташуємо по берегу жорстко пов’язані один з одним вузол так, що відстань між сусідніми вузлами дорівнювала б, наприклад, \\(l=10\\) км. Довжину берегової лінії в кілометрах між точками А і В ми приймемо тоді рівною числу вузлів мінус 1, помноженому на 10. Наступне вимірювання цієї довжини ми зробимо так само, але відстань між сусідніми вузлами зробимо вже рівною \\(l=1\\) км.\nВиявляється, що результат цих вимірювань буде різним. При зменшенні масштабу \\(l\\) ми отримуватимемо все більші й більші значення довжини. На відміну від гладкої кривої, лінія морського узбережжя виявляється найчастіше настільки порізаною (аж до найменших масштабів), що зі зменшенням довжини ланки \\(l\\) величина \\(L\\) — довжина берегової лінії — не прагне до кінцевого межі, а збільшується за степеневим законом\n\\[\nL \\approx l\\left( \\frac{R}{l} \\right)^{D}, \\tag{1}\n\\]\nде \\(D &gt; 1\\) — деякий степеневий показник, котрий іменується фрактальною розмірністю берегової лінії. Чим більше значення \\(D\\), тим більш ломаною або деталізованою представляється ця берегова лінія. Походження залежності (1) має бути інтуїтивно зрозумілим: чим менший масштаб ми використовуємо, тим меньші деталі узбережжя будуть враховані і тим менший вклад вони внесуть у вимірювану довжину. Навпаки, збільшуючи масштаб, ми “розгортаємо” узбережжя, зменшуючи довжину \\(L\\).\nТаким чином, ми бачимо, що для визначення довжини берегової лінії \\(L\\) за допомогою жорсткого масштабу \\(l\\), необхідно зробити \\(N=L/l\\) кроків, причому величина \\(L\\) змінюється з \\(l\\) так, що \\(N\\) залежить від \\(l\\) за законом \\(N \\approx (R/I)^{D}\\). У результаті зі зменшенням масштабу довжина берегової лінії необмежено зростає. Ця обставина різко відрізняє фрактальну криву від звичайної гладкої кривої (типу кола, еліпса), для якої межа довжини апроксимованої ламаної \\(L\\), яка апроксимує, за наближення до нуля довжини її ланки \\(l\\) є скінченною. У результаті для гладкої кривої її фрактальна розмірність \\(D = 1\\), тобто збігається з топологічною.\n\n\n7.1.3 Фрактальна розмірність множин\nВище було введено поняття про фрактальну розмірність берегової лінії. Дамо тепер загальне визначення цієї величини. Нехай \\(d\\) — звичайна Евклідова розмірність простору, в якому розташований наш фрактальний об’єкт (\\(d=1\\) — лінія, \\(d=2\\) — площина, \\(d=3\\) — звичайний тривимірний простір). Покриємо тепер цей об’єкт цілком \\(d\\)-мірними “кулями” радіуса 1. Припустимо, що нам потребувалося для цього не менше, ніж \\(N(l)\\) куль. Тоді, якщо за досить малих \\(l\\) величина \\(N(l)\\) змінюється з \\(l\\) за степеневим законом\n\\[\nN(l) \\sim \\frac{1}{l^D}, \\tag{2}\n\\]\nтоді \\(D\\) — називається хаусдорфовою або фрактальною розмірністю цього об’єкта. Очевидно, що ця формула еквівалентна відношеню \\(N \\approx \\left( R/l \\right)^{D}\\), що використовувалось вище для визначення довжини берегової лінії.\nФормулу (2) можна переписати у вигляді\n\\[\nD = -\\lim_{l \\to 0} \\frac{\\ln{N(l)}}{\\ln{l}}. \\tag{3}\n\\]\nЦе відношення й слугує загальним визначенням фрактальної розмірності \\(D\\). У відповідності до нього величина \\(D\\) представляє локальну характеристику досліджуваного об’єкта.\n\n\n7.1.4 Процедури обчислення монофрактальних розмірностей\nНаразі існує багато визначень та методів вимірювання фрактальної розмірності. Найпоширенішими одновимірними фрактальними розмірностями є розмірність Хаусдорфа, розмірність Хігучі, розмірність Петросяна та Коробчаста розмірність. Розмірність Хаусдорфа є найпростішою фрактальною розмірністю. Але її обчислювальна складність є високою, що ускладнює її практичне застосування. Коробкова розмірність є відносно простою, і фрактальну розмірність сигналу можна отримати, регулюючи розмір довжини сторони коробки. Тому вона є широко впізнаваємою та застосовуваною. Який показник фрактальної розмірності найточніше описує складність сигналу та здатний ідентифікувати кризові явища і представляє ключовий момент цієї лабораторної роботи.\n\n7.1.4.1 R/S-аналіз\nМетод R/S-аналізу, розроблений Мандельбротом та Уоллесом, базується на попередньо створеному методі гідрологічного аналізу Херста, і дозволяє обчислювати параметр самоподібності \\(H\\), який вимірює інтенсивність довготривалих залежностей у часовому ряді. Коефіцієнт \\(H\\), який називають коефіцієнтом Херста, містить мінімальні прогнози стосовно природи системи, що вивчається, і може класифікувати часові ряди. За допомогою цього показника розрізняють випадкові (гаусові) та невипадкові ряди; окрім того, він пов’язаний із фрактальною розмірністю, що, у свою чергу, характеризує ступінь згладженості графіка, побудованого на основі часового ряду. Методом R/S-аналізу можливо також виявити максимальну довжину інтервалу (цикл), на якому значення зберігають інформацію про початкові дані системи (довготривала пам’ять).\nАналіз починається з побудови ряду логарифмічних прибутковостей, \\(G(t) \\equiv \\ln{x(t + \\Delta t)} - \\ln{x(t)}\\), де \\(x(t)\\) — значення вихідного часового ряду в момент \\(t\\), \\(\\Delta t\\) — часовий крок. Отримана послідовність \\(G(t)\\) розбивається на \\(d\\) підпослідовностей довжини \\(n\\).\nДля кожної підпослідовності \\(m=1,...,d\\):\n\nшукається середнє значення \\(\\mu_m\\) та стандартне відхилення \\(S_m\\);\nдані нормалізуються шляхом віднімання середнього значення послідовності \\(X_{i,m}=G_{i,m}-\\mu_m\\), \\(i=1,...,n\\);\nзнаходиться кумулятивна сума послідовності \\(X\\)ів: \\(Y_{i,m}=\\sum_{j=1}^{i}X_{j,m}\\), \\(i=1,...,n\\);\nу межах кожної підпослідовності знаходиться розмах між максимальним та мінімальним значеннями: \\(R_m = \\max\\{Y_{1,m},...,Y_{n,m}\\}-\\min\\{Y_{1,m},...,Y_{n,m}\\}\\), який стандартизується середнім квадратичним відхиленням \\(R_{m}/S_{m}\\);\nобчислюється середнє \\((R/S)_n\\) нормованих значень розмаху для всіх підпослідовностей довжини \\(n\\).\n\nR/S-статистика, обрахована таким чином, відповідає співвідношенню \\((R/S)_{n} \\cong cn^{H}\\), де значення \\(H\\) може бути отримане шляхом обчислення \\((R/S)_n\\) для послідовностей інтервалів зі збільшенням часового горизонту:\n\\[\n\\log{(R/S)}_{n} = \\log{c} + H\\log{n}. \\tag{4}\n\\]\nЗнайти коефіцієнт Херста можна, побудувавши залежність \\((R/S)_n\\) vs. \\(n\\) у подвійному логарифмічному масштабі і взявши коефіцієнт нахилу прямої, яка інтерполює точки отриманого графіка. Якщо значення \\(H=0.5\\), говорять про послідовність, що представляє собою білий шум; \\(0.5 &lt; H \\leq 1\\) свідчить про персистентний ряд, коли існує тенденція слідування великих значень ряду за великими і навпаки; \\(H&lt;0.5\\) вказує на антиперсистентний ряд.\nПри збільшенні часового горизонту коефіцієнт нахилу інтерполюючої прямої повинен прямувати до значення \\(H=0.5\\); сам процес переходу свідчить про втрату впливу початкових умов на поточні значення, і, таким чином, можна говорити про горизонт довгої пам’яті — це точка, до якої коефіцієнт нахилу інтерполюючої прямої відмінний від 0.5, а після — близько 0.5.\n\n\n\n\n\n\nПримітка до R/S-аналізу\n\n\n\nМіж фрактальною розмірністю та показником Херста також існує зв’язок\n\\[\nD_f = 2-H.\n\\]\nЯкщо для берегової лінії ми визначали масштабування її довжини \\(L\\) в залежності від зміни \\(l\\), то у випадку з R/S-аналізом ми визначаємо зміну нормованого розмаху значень ряду в межах масштабу \\(n\\).\n\n\n\n\n7.1.4.2 Аналіз детрендованих флуктуацій\nАналіз детрендований флуктуацій (Detrended fluctuation analysis, DFA) базується на гіпотезі про те, що корельований часовий ряд може бути відображений на самоподібний процес шляхом інтегрування. Таким чином, вимірювання властивостей самоподібності може непрямо свідчити про кореляційні властивості ряду. Переваги АДФ порівняно з іншими методами (спектральний аналіз, R/S-аналіз) полягають в тому, що він виявляє довгочасові кореляції нестаціонарних часових рядів, а також дозволяє ігнорувати очевидні випадкові кореляції, що є наслідком нестаціонарності.\nІснують DFA різних порядків, що відрізняються трендами, які вилучаються з даних.\nРозглянемо DFA найнижчого порядку.\n\nДля часового ряду довжини \\(N\\) знаходиться кумулятивна сума, \\(y(k)=\\sum_{i=1}^{k}\\left( x_i - \\bar{x} \\right)\\), де \\(x_i\\) — це \\(i\\)-те значення часового ряду, \\(\\bar{x}\\) — його середнє значення, \\(k=1,...,N\\).\nОтриманий ряд \\(y(k)\\) розбивається на \\(m\\) підпослідовностей (вікон) однакової ширини \\(n\\) і для кожної підпослідовності (у кожному вікні) виконується наступне:\n\nза допомогою методу найменших квадратів знаходиться локальний лінійний тренд \\(y_{t}(k)\\);\n\nпідпослідовність детрендується шляхом віднімання значення локального тренду \\(y_{t}(k)\\) від значень ряду \\(y(k)\\), що належать послідовності \\(t\\);\nзнаходиться середнє \\(\\bar{y_t}\\) детрендований значень.\n\n\nДля отриманих таким чином значень на всіх підпослідовностях знаходиться:\n\\[\nF_n = \\sqrt{\\frac{1}{m}\\bar{y_t}},\n\\]\nде \\(n\\) — кількість точок у підпослідовності (ширина вікна), \\(m\\) — кількість підпослідовностей, \\(\\bar{y_t}\\) — середнє детрендованих значень для підпослідовності \\(t\\).\nВказана процедура повторюється для різних значень \\(n\\), внаслідок чого ми отримує набір залежностей \\(F_n\\) від \\(n\\). Побудова залежності \\(\\log{F(n)}\\) від \\(\\log{n}\\) та інтерполяція отриманих значень лінією регресії дає змогу обчислити показник скейлінга \\(\\alpha\\), що є коефіцієнтом кута нахилу інтерполяційної прямої і характеризує зміну кореляцій флуктуацій часового ряду \\(F_n\\) при збільшенні часового інтервалу \\(n\\).\nПорівняно із R/S-аналізом, DFA дає більші можливості інтерпретації скейлінгового показника \\(\\alpha\\):\n\nдля випадкового ряду (перемішаного чи “сурогатного”) \\(\\alpha = 0.5\\);\nпри наявності лише короткочасових кореляцій \\(\\alpha\\) може відрізнятись від 0.5, проте має тенденцію прямувати до 0.5 при збільшенні розміру вікна;\nЗначення \\(0.5 &lt; \\alpha \\leq 1.0\\) показує персистентні довгочасові кореляції, що відповідають степеневому закону;\n\\(0 &lt; \\alpha &lt; 0.5\\) означає антиперсистентний ряд;\ncпеціальний випадок, коли \\(\\alpha = 1\\), означає наявність \\(1/f\\) шуму.\nдля випадків, коли \\(\\alpha \\geq 1\\), кореляції існують, проте перестають відображувати степеневу залежність;\nвипадок \\(\\alpha = 1.5\\) свідчить про Броунівський шум, інтегрований білий шум.\n\nУ випадку степеневої залежності функції автокореляцій спостерігається спад автокореляції з показником \\(\\gamma\\):\n\\[\nC(L) \\sim L^{-\\gamma}.\n\\]\nНа додачу до цього, спектральна густина також спадає за степеневим законом:\n\\[\nP(f) \\sim f^{-\\beta}.\n\\]\nВідповідні показники виражаються через наступні відношення:\n\n\\(\\gamma=2-2\\alpha\\);\n\\(\\beta=2\\alpha-1\\).\n\nУ DFA другого порядку (DFA2) обчислюються відхилення \\(F^2(v,s)\\) профілю від інтерполяційного многочлена другого порядку. Таким чином, вилучаються впливи можливих лінійних та параболічних трендів для масштабів, більших за розглядувані. Взагалі, у DFA порядку \\(n\\) обчислюються відхилення профілю від інтерполяційного многочлена \\(n\\)-го порядку, що вилучає вплив всіх можливих трендів порядків до (\\(n-1\\)) для масштабів, більших від розміру вікна.\nПотім обчислюється найближчий поліном \\(y_{ν}(s)\\) для профілю на кожному із \\(2N_s\\) сегментів \\(v\\) і визначається відхилення\n\\[\nF^2(v,s) \\equiv \\frac{1}{s}\\sum_{i=1}^{s}\\left( x_{(v-1)s+i} - y_{i}(i) \\right)^{2}. \\tag{5}\n\\]\nДалі знаходиться середнє значення флуктуацій всіх детрендованих профілів:\n\\[\nF_2(s) \\equiv \\sqrt{\\left( \\frac{1}{2N_s} \\sum_{v=1}^{2N_s}F^{2}(v,s) \\right)}. \\tag{6}\n\\]\nЗначення формули (6) можна трактувати як середньоквадратичний зсув (переміщення) точки випадкових блукань у ланцюжку після \\(s\\) кроків.\n\n\n7.1.4.3 Фрактальна розмірність Хігучі\nФрактальна розмірність Хігучі — це один з різновидів монофрактальної розмірності, яка визначається наступним чином:\nПрипустимо, що у нас є часовий ряд\n\\[\nx(1), x(2),...,x(N)\n\\]\nі реконструйований часовий ряд \\(x_{m}^{k}\\):\n\\[\\begin{align*}\nx_{m}^{k} = \\{ x(m), x(m+k), x(m+2k), ..., \\\\\nx\\left( m+\\left[ \\frac{N-m}{k} \\right] \\cdot k \\right) \\},\n\\end{align*}\\]\nдля \\(m=1,2,...,k\\); де \\(m\\) представляє початковий час; \\(k=2,...,k_{max}\\) представляють ступінь часового зміщення. Позначення \\([\\cdot]\\) представляє цілу частину \\(x\\). Для кожного реконструйованого часового ряду \\(x_{m}^{k}\\) розраховується середня довжина часової послідовності \\(L_{m}(k)\\):\n\\[\nL_{m}(k) = \\frac{\\sum_{i=1}^{[\\frac{(N-m)}{k}]} | x(m+ik) - x(m+(i-1)\\cdot k) | \\cdot (N-1)}{[\\frac{N-m}{k}] \\cdot k}\n\\]\nДалі, для всіх середніх довжин \\(L_{m}(k)\\), знаходиться загальне середнє:\n\\[\nL(k) = \\frac{1}{k}\\sum_{m=1}^{k}L_{m}(k).\n\\]\nЗгідно методу Хігучі узагальне середнє значення \\(L(k)\\) пропорційне масштабу \\(k\\), тобто\n\\[\nL(k) \\propto k^{-D}.\n\\]\nДалі логарифмуємо обидві сторони й отримуємо наступну рівність:\n\\[\n\\ln{L(k)} \\propto D \\cdot \\ln{\\left( \\frac{1}{k} \\right)}.\n\\]\nІнтерполювавши лінію регресії через залежність \\(\\ln{L(k)}\\) від \\(\\ln{\\left( \\frac{1}{k} \\right)}\\), ми можемо отримати показник фрактальності \\(D\\), отримавши кут нахилу цієї лінії. Показник \\(D\\) і представлятиме фрактальну розмірність Хігучі.\n\n\n7.1.4.4 Фрактальна розмірність Петросяна\nСпочатку, для часового ряду \\(\\{ x_1, x_2,...,y_{N} \\}\\), створюємо його дискретизовану (бінарну) версію, \\(z_i\\):\n\\[\nz_i =\n\\begin{cases}\n    1, & x_i &gt; \\langle x \\rangle, \\\\\n    -1, & x_i \\leq \\langle x \\rangle.\n\\end{cases}\n\\]\nФрактальну розмірність Петросяна може бути визначена як\n\\[\nD = \\frac{\\log_{10}{N}}{\\log_{10} + \\log_{10}{\\left( \\frac{N}{N+0.4N_{\\Delta}} \\right)}},\n\\]\nде \\(N_{\\Delta}\\) — кількість загальних змін знаку змінної \\(z_i\\):\n\\[\nN_{\\Delta} = \\sum_{i=1}^{N-2} \\left|\\frac{z_{i+1}-z_i}{2}\\right|.\n\\]\n\n\n7.1.4.5 Фрактальна розмірність Каца\nПредставимо, що сигнал складається з пари точок \\(\\left( x_i, y_i \\right)\\). Тоді, фрактальна розмірність Каца визначається як\n\\[\nD = \\frac{\\log{N}}{\\log{N} + \\log{\\frac{d}{L}}},\n\\]\nде \\(L\\) визначається наступним чином:\n\\[\nL = \\sum_{i=0}^{N-2}\\sqrt{\\left( y_{i+1}-y_{i} \\right)^{2} + \\left( x_{i+1}-x_{i} \\right)^{2}}.\n\\]\nЗначення \\(d\\) визначається як максимальна відстань від початкової точки \\(\\left( x_1, y_1 \\right)\\) до всіх інших точок, а саме \\(d\\) може бути розраховане наступним чином:\n\\[\nd = \\max{\\left( \\sqrt{\\left( x_i - x_1 \\right)^{2} - \\left( y_i - y_1 \\right)^{2}} \\right)}.\n\\]\n\n\n7.1.4.6 Фрактальна розмірність Севчика\nСпочатку, для множини значень \\(\\left( x_i, y_i \\right)\\) виконується нормалізація:\n\\[\nx_{i}^{*} = \\frac{x_i-x_{min}}{x_{max}-x_{min}}, \\; y_{i}^{*} = \\frac{y_i-y_{min}}{y_{max}-y_{min}}.\n\\]\nФрактальна розмірність Севчика може бути визначена як\n\\[\nD = 1 + \\frac{\\ln{L}}{\\ln{[2 \\cdot \\left( N-1 \\right)]}},\n\\]\nде \\(L\\) — це довжина сигналу, що може бути визначена як\n\\[\nL = \\sum_{i=0}^{N-2}\\sqrt{\\left( y_{i+1}^{*}-y_{i}^{*} \\right)^{2} + \\left( x_{i+1}^{*}-x_{i}^{*} \\right)^{2}}.\n\\]\n\n\n7.1.4.7 Фрактальна розмірність через нормалізовану щільність довжини\nДаний показник розраховується в наступний спосіб:\n\nДля часового ряду \\(\\{ x_1, x_2,...,x_n \\}\\) виконується стандартизація: \\(y_i = \\frac{x_i - \\mu}{\\sigma}\\), де \\(\\mu\\) — це середнє значення ряду, \\(\\sigma\\) — це стандартне відхилення.\nРозраховується нормалізована щільність довжини:\n\n\\[\nNLD = \\frac{1}{N}\\sum_{i=2}^{N}\\left| y_i - y_{i-1} \\right|\n\\]\nФактичний розрахунок фрактальної розмірності сигналу базується на побудові монотонної калібрувальної кривої, \\(D = f(NLD)\\), за набором функцій Вейєрштрасса, для яких значення \\(D\\) задаються теоретично.\n\nДля обчислювальних цілей необхідно створити математичну модель цієї залежності. Автори даного підходу тестували дві моделі:\n\nлогарифмічну модель: \\(D = a \\cdot \\log{\\left(NLD - NLD_{0} \\right)} + C\\)\nстепеневу модель: \\(D = a \\cdot \\left(NLD - NLD_{0} \\right)^{k}\\). Бібліотека neurokit2 використовує саме степеневу модель. Параметр \\(a=1.9079\\), \\(k=0.18383\\) і \\(NLD_{0}=0.097178\\), згідно статті Kalauzi et al. 2009.\n\n\n\n\n7.1.4.8 Фрактальна розмірність через нахил спектральної щільності потужності\nФрактальну розмірність можна обчислити на основі аналізу нахилу спектральної щільності потужності (power spectral density slope, PSD) в сигналах, що характеризуються частотною степеневою залежністю.\nСпочатку виконується перетворення часового ряду до частотної області і далі сигнал розбивається на синусоїдальні та косинусоїдальні хвилі певної амплітуди, які разом “складаються”, щоб представити вихідний сигнал. Якщо існує систематичний зв’язок між частотами в сигналі і потужністю цих частот, то в логарифмічних координатах це проявляється в лінійній залежності. Кут нахилу лінії регресії приймається як оцінка фрактальної розмірності.\nНахил 0 відповідає білому шуму, а нахил менше 0, але більше -1, відповідає рожевому шуму, тобто шуму \\(1/f\\). Спектральні нахили крутіші за -2 вказують на дробовий броунівський рух, що є втіленням процесів випадкового блукання.\n\n\n7.1.4.9 Кореляційна розмірність\nКореляційна розмірність (\\(D_2\\)) — це похідна величина від кореляційного інтеграла Кореляційний інтеграл (кореляційна сума) може бути поданий в такому вигляді:\n\\[\nC(\\varepsilon) = \\frac{1}{N^{2}}\\sum_{\\substack{i,j=1 \\\\ i\\neq j}}^{N}\\Theta \\left( \\varepsilon - \\| \\vec{x}(i) - \\vec{x}(j) \\| \\right), \\; \\vec{x}(i) \\in \\Re^{m}.\n\\]\nСама кореляційна розмірність може бути виведена з наступної степеневої залежності:\n\\[\nC(\\varepsilon) \\sim \\varepsilon^{\\nu},\n\\]\nабо слідуючим чином:\n\\[\nD_2 = \\lim_{M\\to\\infty}\\lim_{\\varepsilon\\to 0}\\frac{\\log{\\left( g_{\\varepsilon}/N^2 \\right)}}{\\log{\\varepsilon}},\n\\]\nде \\(g_{\\varepsilon}\\) — це сумарна кількість пар точок, відстань між якими менша за радіус \\(\\varepsilon\\).\nЗа формулою \\(C(\\varepsilon)\\) ми відбираємо \\(i\\)-ту траєкторію та всі інші \\(j\\)-ті траєкторії, і дивимося, чи потрапляють \\(j\\)-ті траєкторії в округ \\(i\\)-ої траєкторії з радіусом \\(\\varepsilon\\). Якщо відстань між ними не перевищує округ зі згаданим радіусом, ми ставимо 1. Але якщо відстань між траєкторіями більша за \\(\\varepsilon\\), тоді ставимо 0. Далі все це підсумовується, ділиться на загальну кількість траєкторій. По суті кореляційний інтеграл це середня ймовірність того, що дві розглянуті траєкторії фазового простору, що розглядаються, будуть знаходитися досить близько одна до одної. Чим тісніше розташовані точки фазового простору одна до одної, тим більше значення кореляційного інтеграла. Чим більш рівновіддаленими видаються траєкторії одна від одної, тим ближче значення кореляційного інтеграла до нуля.\nЗначення кореляційної розмірності ми можемо відшукати аналогічно попередним фрактальним показникам: ми шукаємо залежність кореляційного інтеграла від значення \\(\\varepsilon\\). Ця залежність будується в логарифмічному масштабі.\n\n\n\n\n\n\nДодаткова інформація по кореляційні розмірності\n\n\n\nКореляційна розмірність за аналогією з попередніми показниками — це теж тангенс кута нахилу лінії регресії, побудованої в логарифмічному масштабі, але для залежності кореляційного інтеграла від \\(\\varepsilon\\). За аналогією з іншими показниками, кореляційна розмірність визначає швидкість зміни значення кореляційного інтеграла (крутість нахилу лінії регресії).\n\n\n\n\n\nРисунок 7.2: Зміна значення кореляційного інтеграла в залежності від ступеня розкиданості точок по фазовому простору системи\n\n\nЗа фазовим простором цієї хмари точок ( Рисунок 7.2 ) можна бачити, що більша згуртованість точок одна до одної має асоціюватися з меншою кореляційною розмірністю. Рисунок (а) характеризується найвищою близькістю точок одна до одної, але при цьому найменшою кореляційною розмірністю. Це можна пояснити так: якщо ми будуємо сітку з кіл радіусом \\(\\varepsilon\\) і поступово її збільшуємо, ми вже перестаємо бачити нові прилеглі траєкторії до тих \\(i\\)-их траєкторій, які ми розглядали із самого спочатку, на початкових значеннях \\(\\varepsilon\\). На рисунках (b) і (c) видно, що хмара траєкторій фазового простору є більш рівномірно розподіленою. За поступового збільшення радіуса кіл із центрами в кожній \\(i\\)-ій траєкторії ми повинні спостерігати пропорційне збільшення значення кореляційного інтеграла. У даному випадку при поступовому збільшенні \\(\\varepsilon\\) ми спостерігаємо появу все більшої і більшої кількості точок.\nУ періодичних системах кореляційна розмірність залишається постійною і дорівнює розмірності вкладення. Наприклад, для простої періодичної системи, такої як синусоїда, кореляційна розмірність дорівнюватиме 1 (оскільки вона лежить на одновимірній кривій), а якщо систему реконструюють у двовимірному фазовому просторі (за двома координатами), то кореляційна розмірність дорівнюватиме 2. У таких системах кореляційна розмірність не змінюється.\nДля хаотичних систем кореляційна розмірність має характерну поведінку, яка залежить від кількості змінних (розмірностей), необхідних для точного опису динаміки системи. На відміну від періодичних систем, кореляційна розмірність зростає в міру збільшення розмірності фазового простору, поки не досягне плато.\nЕлектрокардіограма (ЕКГ): ЕКГ-сигнали відображають електричну активність серця. Складність ЕКГ-сигналу може бути оцінена за допомогою кореляційної розмірності. Очікується, що кореляційна розмірність ЕКГ здорового серця буде вищою через наявність складних патернів і варіабельності. З іншого боку, аномальні ЕКГ-сигнали, наприклад, від пацієнтів з аритміями або серцевими захворюваннями, можуть мати нижчу кореляційну розмірність через втрату складності сигналу.\nЕлектроенцефалограма (ЕЕГ): Сигнали ЕЕГ реєструють електричну активність мозку. Кореляційна розмірність може використовуватися для аналізу складності мозкової активності, яка може змінюватися залежно від різних когнітивних станів, стадій сну або неврологічних розладів. У здорових людей сигнали ЕЕГ у стані бадьорості та уваги можуть мати вищу кореляційну розмірність порівняно з сигналами у стадії сну, коли активність мозку є більш регулярною і синхронізованою.\nДихальні сигнали: Дихальні сигнали, такі як частота дихання або повітряний потік, також можуть бути проаналізовані за допомогою кореляційної розмірності. Складність цих сигналів може змінюватися залежно від таких факторів, як стрес, фізичне навантаження або наявність респіраторних захворювань. За нормального дихання може спостерігатися вища кореляційна розмірність, тоді як порушення в дихальних сигналах, наприклад, за обструктивного апное уві сні або дихальних розладів, можуть призвести до зниження кореляційної розмірності.\nАналіз ходи: Кореляційна розмірність може бути використана для аналізу моделей ходи. Вона може допомогти в розумінні складності рухів людини під час ходьби або бігу. Зміни в кореляційній розмірності сигналів ходи можуть свідчити про зміну стабільності ходи або про наявність відхилень у ході, викликаних неврологічними або опорно-руховими захворюваннями.\nВаріативність динаміки серцевого ритму (ВСР): ВСР являє собою зміну часових інтервалів між послідовними ударами серця. Вона перебуває під впливом вегетативної нервової системи і відображає адаптивність і складність серцево-судинної системи. Вищий рівень ВСР, що відповідає вищій кореляційній розмірності, зазвичай асоціюється з кращим станом серцево-судинної системи та її адаптивністю до фізіологічних змін і змін навколишнього середовища. Її падіння може асоціюватися з аномальною динамікою серця.\nПослідовності ДНК: Кореляційна розмірність може бути використана і при аналізі послідовностей ДНК. Вона допомагає виявити самоподібні або фрактальні патерни всередині послідовностей, що може мати значення для розуміння генетичної складності, еволюційних зв’язків і регуляції генів. Висока кореляційна розмірність — висока складність ланцюжка ДНК. Мала кореляційна розмірність — спрощений ланцюжок ДНК.\nФінансові ринки Вища кореляційна розмірність у даних часових рядів фінансового ринку свідчить про більшу складність та існування в їхній основі самоподібних моделей або фрактальних структур. Хаотична поведінка цін на акції може бути пов’язана з періодами високої волатильності та непередбачуваності. З іншого боку, нижча величина кореляційної розмірності може свідчити про більш передбачувані та менш складні рухи цін, що відповідає періодам стабільності або менш волатильним ринковим умовам.\n\n\n\n\n\n\nПримітка по кореляційні розмірності\n\n\n\nКореляційна розмірність розглядає кількість інформації (кубиків, кіл), необхідну для опису тільки пари точок у фазовому просторі."
  },
  {
    "objectID": "lab_6.html#хід-роботи",
    "href": "lab_6.html#хід-роботи",
    "title": "7  Лабораторна робота № 6",
    "section": "7.2 Хід роботи",
    "text": "7.2 Хід роботи\nРозглянемо як можна застосовувати зазначені показники в якості індикаторів кризових станів.\nСпочатку імпортуємо необхідні бібліотеки для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nfrom tqdm import tqdm\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nУ даній роботі скористаємось монофрактальними показниками для ідентифікації кризових явищ на ринку золота. Розглянемо значення золота за весь період, що представляє Yahoo! Finance. Для цього нам не треба буде вказувати початкову та кінцеву дати:\n\nsymbol = 'GC=F'                       # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\nРисунок 7.3: Динаміка щоденних змін індексу золота\n\n\n\n\nВизначимо функцію transformation() для стандартизації ряду:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n7.2.1 Обчислення показника Херста із використанням R/S-аналізу\nДля подальших розрахунків використовуватимемо бібліотеку neurokit2 та fathon. Другу можна встановити в наступний спосіб:\n\n!pip install fathon\n\nДалі імпортуємо саму бібліотеку та дотичні до неї модулі:\n\nimport fathon\nfrom fathon import fathonUtils as fu\n\nБібліотека neurokit містить необхідний метод для R/S-аналізу — fractal_hurst. Його синтаксис виглядає наступним чином:\nfractal_hurst(signal, scale='default', corrected=True, show=False)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму бібліотеки pandas.\nscale (list) — список, що містить довжини вікон (кількість точок даних у кожній підмножині ряду), на які розбито сигнал.\ncorrected (bool) — якщо значення True, до вихідних даних буде застосовано поправочний коефіцієнт Аніса-Ллойда-Пітерса відповідно до очікуваного значення для окремих значень (R/S).\nshow (bool) — якщо значення True, виводить залежність \\((R/S)_n\\) від \\(n\\) (scale) у подвійному логарифмічному масштабі.\n\nПовертає\n\nh (float) — показник Херста.\n**kwargs — словник, що містить інформацію відносно використовуваних у процедурі параметрів.\n\nРозглянемо ступінь трендостійкості в динаміці фондового індексу золота, використовуючи весь часовий ряд. Далі знайдемо значення показника Херста в рамках віконної процедури.\n\n7.2.1.1 Увесь часовий ряд\nПершочергово знайдемо значення прибутковостей для нашого ряду та стандартизуємо їх. Після цього виконаємо обчислення.\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rs = transformation(signal, ret_type) \n\nВиконуємо R/S-аналіз:\n\nh, info = nk.fractal_hurst(for_rs, corrected=False, show=True)\n\n\n\n\nРисунок 7.4: Залежність значень R/S від скейлінгу побудованих в логарифмічному масштабі\n\n\n\n\nЯк ми можемо бачити з Рисунок 7.4, значення \\(h=0.53\\), що свідчить про подібність динаміки золота до випадкового блукання. Але оскільки закони, що регулюють ринок, змінюються з часом, мають змінюватись і кореляції всередині системи, а одже коефіцієнт Херста також має залежати від періоду в якому він розглядається.\n\n\n7.2.1.2 Віконна процедура\nВизначимо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nПриступимо до віконної процедури:\n\n# встановлюємо параметри\nret_type = 4                   # вид ряду\nwindow = 250                   # ширина вікна\ntstep = 1                      # часовий крок вікна \nlength = len(time_ser.values)  # довжина самого ряду\ncorr = False                   # поправочний коефіцієнт Аніса-Ллойда-Пітерса\n\nH = []                         # масив для віконного Херсту\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо взаємну інформацію \n    h, _ = nk.fractal_hurst(fragm, corrected=corr, show=False)\n    \n    # та додаємо результат до масиву значень\n    H.append(h)\n\n100%|██████████| 5518/5518 [00:13&lt;00:00, 402.42it/s]\n\n\n\nnp.savetxt(f\"rs_hurst_name={symbol}_window={window}_step={tstep}_ \\\n           rettype={ret_type}_corrected={corr}.txt\" , H)\n\nВізуалізуємо результат:\n\nmeasure_label = r'$H$'\nfile_name = f\"rs_hurst_name={symbol}_window={window}_step={tstep}_ \\\n           rettype={ret_type}_corrected={corr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          H, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 7.5: Динаміка індексу золота та показника Херста\n\n\n\n\nНа представленому рисунку ( Рисунок 7.5 ) можемо бачити, що показник Херста зростає в передкризовий період та спадає під час кризи. Перед кризою динаміка ринку характеризується зростанням трендостійкості (персистентності), що відзеркалює зростання скорельованості дій між трейдерами ринку.\n\n\n\n7.2.2 Обчислення на основі DFA\nБібліотека fathon представляє інструментарій як для виконання класичного аналізу детрендованих флуктуацій, так і для його мультифрактального аналогу, мова про який піде в наступній лабораторній.\n\n7.2.2.1 Для всього ряду\nСпочатку представимо значення \\(\\alpha\\) для всього ряду. Процедура розрахунків на основі бібліотеки fathon виглядатиме наступним чином:\n\nзнаходимо стандартизовані прибутковості ряду\n\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_dfa = transformation(signal, ret_type) \n\n\ncumulat = fu.toAggregated(for_dfa) # знаходимо кумулятивні накопичення\n\nrev = True # чи повторювати розрахунок ф-ції флуктуацій з кінця\norder = 2  # порядок локального лінійного тренду \n\npydfa = fathon.DFA(cumulat) # ініціалізація об'єкту DFA\n                            # для виконання подальших обчислень\n\nwin_beg = 100               # початкова ширина сегментів\nwin_end = 2000              # кінцева ширина сегментів\n\nwins = fu.linRangeByStep(win_beg, win_end) # генеруємо масив \n                                           # лінійно розділених \n                                           # елементів.\n\nn, F = pydfa.computeFlucVec(wins, \n                            polOrd=order, \n                            revSeg=rev)    # знаходимо функцію флуктуацій\n\nH, H_intercept = pydfa.fitFlucVec()        # знаходимо показник альфа\n\nВиводимо залежність функції флуктуацій від характеристичного масштабу:\n\npolyfit = np.polyfit(np.log(n), np.log(F), 1)\nfluctfit = np.exp(1) ** np.polyval(polyfit, np.log(n))\n\nБудуємо залежність функції флуктуацій від масштабу в подвійному логарифмічному масштабі:\n\nfig, ax = plt.subplots()\nfig.suptitle(\"Показник Херста на основі DFA\")\n\nax.scatter(\n        np.log(n),\n        np.log(F),\n        marker=\"o\",\n        zorder=1,\n        label=\"_no_legend_\",\n    )\n\nlabel = fr\"$\\alpha$ = {H:.2f}\"\nax.plot(np.log(n), np.log(fluctfit), \n        color=\"#E91E63\", zorder=2, \n        linewidth=3, label=label)\n\nax.set_ylabel(r'$\\ln{F_{2}(n)}$')\nax.set_xlabel(r'$\\ln{n}$')\n\nax.legend(loc=\"lower right\")\n\nplt.show()\n\n\n\n\nРисунок 7.6: Логарифмічна залежність значень функції флуктуацій від скейлінгу\n\n\n\n\nПроцедура DFA показує, що значення індексу золота представляються скоріше антиперсистентними, але представлений результат доволі близький до того, що був отриманий за допомогою R/S-аналізу. Розглянемо значення \\(\\alpha\\) в рамках алгоритму ковзного вікна.\n\n\n7.2.2.2 Віконна процедура\nВизначимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nrev = True      # чи повторювати розрахунок ф-ції флуктуацій з кінця\norder = 2       # порядок поліноміального тренду\n\nperiods = 1\n\nwin_beg = 10             # початковий масштаб сегментів\nwin_end = window-1       # кінцевий масштаб сегментів\n\n\n\nlength = len(time_ser.values) # довжина ряду\n\nalpha = []               # масив показників альфа (Херста)\nD_f = []                 # фрактальна розмірність\nbeta = []                # показник спектральної щільності\ngamma = []               # показник автокореляції\n\nЗнайдемо показник Херста (\\(\\alpha\\)), фрактальну розмірність (\\(D_f\\)), показник спектральної щільності (\\(\\beta\\)) та показник автокореляції (\\(\\gamma\\)):\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # знаходимо кумулятивні накопичення\n    cumulat_wind = fu.toAggregated(fragm) \n\n    # ініціалізація об'єкту DFA\n    pydfa = fathon.DFA(cumulat_wind) \n\n    # генеруємо масив лінійно розділених елементів\n    wins = fu.linRangeByStep(win_beg, win_end) \n\n    # знаходимо функцію флуктуацій\n    n, F_wind = pydfa.computeFlucVec(wins, polOrd=order, revSeg=rev)    \n\n    # знаходимо показник альфа\n    H_wind, _ = pydfa.fitFlucVec()\n\n    # знаходимо фрактальну розмірність        \n    D = 2. - H_wind\n\n    # показник спектральної щільності\n    bi = 2. * H_wind - 1 \n\n    # показник автокореляції\n    gi = 2. - 2. * H_wind\n\n    alpha.append(H_wind)\n    D_f.append(D)\n    beta.append(bi)\n    gamma.append(gi)\n\n100%|██████████| 5518/5518 [01:07&lt;00:00, 81.48it/s]\n\n\nЗберігаємо абсолютні значення показників до текстових файлів:\n\nnp.savetxt(f\"alpha_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", alpha)\nnp.savetxt(f\"D_f_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", D_f)\nnp.savetxt(f\"beta_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", beta)\nnp.savetxt(f\"gamma_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", gamma)\n\nОголошуємо мітки для рисунків та назви збережений рисунків:\n\nlabel_alpha = fr'$\\alpha$'\nlabel_d = fr'$D_f$'\nlabel_beta = fr'$\\beta$'\nlabel_gamma = fr'$\\gamma$'\n\nfile_name_alpha = f\"alpha_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_d = f\"D_f_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_beta = f\"beta_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_gamma = f\"gamma_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\n\nВиводимо результати:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alpha, \n          ylabel, \n          label_alpha,\n          xlabel,\n          file_name_alpha)\n\n\n\n\nРисунок 7.7: Динаміка індексу золота та показника альфа\n\n\n\n\nЯкщо порівнювати з R/S-аналізом, Рисунок 7.7 демонструє, що динаміка узагальненого показника Херста отриманого за допомогою DFA є набагато стабільнішою. Тепер ми здатні диференціювати значну частку крахових подій, що мали місце на ринку золота. Узагальнений Херст показує, що передкризові явища характеризуються зростанням трендостійкості ринку або, іншими словами, підвищенням ступеня самоорганізації системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_f, \n          ylabel, \n          label_d,\n          xlabel,\n          file_name_d)\n\n\n\n\nРисунок 7.8: Динаміка індексу золота та фрактальної розмірності\n\n\n\n\nРисунок 7.8 показує, що \\(D_f\\) характеризується спадом при кризових станах. Це є індикатором того, що вищий ступень організованості ринку відзеркалюється в більш згладженій або менш шорстких флуктуаціях досліджуваного сигналу.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          beta, \n          ylabel, \n          label_beta,\n          xlabel,\n          file_name_beta)\n\n\n\n\nРисунок 7.9: Динаміка індексу золота та показника спектральної щільності\n\n\n\n\nНа даному рисунку ми бачимо динаміку показника \\(\\beta\\), що відноситься до спектральної густини потужності (\\(P(f)=1/f^{\\beta}\\)), зростає в кризові періоди, що говорить про спад потужності сигналу, що припадає на одиничний інтервал частоти. Це також є свідченням зростання кореляційних властивостей системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          gamma, \n          ylabel, \n          label_gamma,\n          xlabel,\n          file_name_gamma)\n\n\n\n\nРисунок 7.10: Динаміка індексу золота та показника автокореляції\n\n\n\n\nНа Рисунок 7.10 видно, що показник \\(\\gamma\\) спадає в кризові та передкризові періоди. Це є показником сповільнення спаду функції автокореляції, що в свою чергу також вказує на зростання корельованності динаміки системи."
  },
  {
    "objectID": "lab_11.html",
    "href": "lab_11.html",
    "title": "8  Лабораторна робота № 11",
    "section": "",
    "text": "9 Висновок\nУ даній роботі було розглянуто показники незворотності (асиметрії) системи на основі діаграм Пуанкаре, графу видимості та пермутаційних шаблонів. Було продемонстровано побудову діграми Пуанкаре та зв’язків видимості як для всього ряду так і для деяких із його фрагментів. Видно, що значення на діаграмі Пуанкаре характеризуються розподілом точок, що виходять за межі нормального Гаусового розподілу. Для графу видимості видно, що кризові стани характеризуються значною концентрацією зв’язків, що є довгостроковими. Граф прибутковостей виглядає рівномірнорозподіленим. Таким чином, поставало актуальним проводити розрахунок показників незворотності графового типу для вихідного ряду.\nПоказники на основі діаграми Пуанкаре демонструють зріст або спад у кризові періоди, що вказує на зростання асиметрії у даний період часу. Дані показники можуть слугувати в якості індикаторів кризових явищ.\nВиходячи з 3 показників незворотності, що представлені вище, видно, що дані показники починають зростати в передкризовий період, вказуючи на стартовий період хаосу. Найгірше серед них себе поводить $ Dist_{deg} $. Найращим чином $ Dist_{clust} $ та $ Dist_{perm} $. Їх і варто використовувати в якості індикаторів-передвісників крахів."
  },
  {
    "objectID": "lab_11.html#незворотність-на-основі-діаграм-пуанкаре",
    "href": "lab_11.html#незворотність-на-основі-діаграм-пуанкаре",
    "title": "8  Лабораторна робота № 11",
    "section": "8.1 Незворотність на основі діаграм Пуанкаре",
    "text": "8.1 Незворотність на основі діаграм Пуанкаре\nДіаграма Пуанкаре для часового ряду являє собою графік, на осі \\(x\\) якого розташовані значення для поточного часу \\(t\\), а на осі \\(y\\) — його наступні значення в часі \\(t+\\tau\\). Усі наступні значення, які рівні один одному (\\(x(t) = x(t+\\tau)\\)), розташовані на лінії ідентичності (line of identity, LI). Інтервали, що представляють зростаючу тендецію, відмічені вище LI (\\(x(t)&lt;x(t+\\tau)\\)), тоді як спадна тенденція характеризуватиметься скупченням точок нижче LI (\\(x(t)&gt;x(t+\\tau)\\)). Оцінюючи асиметрію точок на діаграмі, ми можемо вивести різні кількісні показники незворотності (асиметрії) досліджуваних систем.\nІндекс Гузіка (GIx)\nGIx можна визначити як відношення відстаней точок вище LI до відстаней усіх точок на діаграмі:\n\\[\nGIx = \\frac{\\sum_{i=1}^{a} \\left( D_{i}^{+} \\right)^{2}}{\\sum_{i=1}^{m} \\left( D_{i} \\right)^{2} },\n\\]\nде \\(a = C(P_{i}^{+})\\) позначає кількість точок над LI; \\(m = C(P_{i}^{+}) + C(P_{i}^{-})\\) позначає кількість точок на графіку Пуанкаре; \\(D_{i}^{+}\\) це відстань від точки над LI до самої LI. Відстань точки до LI можна визначити як\n\\[\nD_{i} = \\frac{|x(i+\\tau) - x(i)|}{\\sqrt{2}}.\n\\]\nІндекс Порти\nІндекс Порти (PIx) визначається як кількість точок нижче LI, поділена на загальну кількість точок на графіку Пуанкаре, за винятком тих, що знаходяться на LI:\n\\[\nPIx = \\frac{b}{m},\n\\]\nде \\(b = C(P_{i}^{-})\\) кількість точок нижче LI.\nІндекс Кошти\nІндекс Кошти бере до уваги кількість інкриментів (\\(x(i+1)-x(i) &gt; 0\\)) та декриментів (\\(x(i+1)-x(i) &lt; 0\\)). Вони представляються симетричними, якщо рівні один одному. Даний індекс розраховується для двовимірної мультимасштабної площини (\\(x(i), x(i+L)\\)), де новий крос-гранульований ряд \\(y_{\\tau}(i) = x(i+L)-x(i)\\) для \\(1 \\leq i \\leq N-\\tau\\) відображає асиметрію інкриментів та декриментів ряду, і індекс незворотності для діапазону масштабів \\(\\tau\\) визначається наступним виразом:\n\\[\nCIx_{\\tau} = \\frac{\\sum_{y_{\\tau}&lt;0} H[y_{\\tau}] - \\sum_{y_{\\tau}&gt;0} H[y_{\\tau}]}{N-\\tau}.\n\\]\nУзагальнений індекс Кошти для діапазону мастабів \\(\\tau\\) може бути визначений як\n\\[\nCIx = \\frac{1}{L} \\sum_{\\tau=1}^{L} |CIx_{\\tau}|,\n\\]\nде \\(L\\) — це максимальний масштаб.\nІндекс Ейлера\nОпираючись на асиметрію розподілу точок нижче та вище LI, Ейлер запропонував індекс асиметрії:\n\\[\nEIx = \\frac{\\sum_{i=1}^{N-1} \\left[ x(i)-x(i+\\tau) \\right]^{3}}{\\left[ \\sum_{i=1}^{N-1} \\left[ x(i)-x(i+\\tau) \\right]^{2} \\right]^{\\frac{3}{2}}}.\n\\]\nЗначне відхилення \\(EIx\\) від 0 вказує на асиметрію системи. Якщо \\(EIx&gt;0\\), розподіл точок на діаграмі Пуанкаре значно зміщений у сторону вище LI. Зворотня ситуація спостерігається для \\(EIx&lt;0\\). Для \\(EIx \\approx 0\\) досліджувані сегменти представляються зворотніми в часі.\nІндекс площі\nІндекс площі (AIx) визначається як сукупна площа секторів, що сформовані точками над LI поділена на сукупну площу секторів, що відповідають усім точкам на графіку Пуанкаре (крім тих, що розташовані точно на LI). Площа сектора, що відповідає певній точці \\(P_{i}\\) на графіку Пуанкаре, обчислюється як\n\\[\nS_{i} = \\frac{1}{2} \\times R\\theta_{i} \\times r^{2},\n\\]\nде \\(r\\) — це радіус сектора; $R{i} = {LI} - {i} $; $ {LI}$ — це фазовий кут, і \\(\\theta_{i} = \\arctan{\\left[ \\frac{x(i+\\tau)}{x(i)} \\right]}\\), що визначає фазовий кут \\(i\\)-ої точки. Далі, \\(AIx\\) визначається за наступною формулою:\n\\[\nAIx = \\frac{\\sum_{i=1}^{a}|S_{i}|}{\\sum_{i=1}^{m}|S_{i}|}.\n\\]\nІндекс кута нахилу\nНа додачу до представлених вище мір, було запропоновано розраховувати незворотність сигналу з відношення кутів нахилу точок над LI до нахилу всіх точок на діаграмі:\n\\[\nSIx = \\frac{\\sum_{i=1}^{a}|R\\theta_{i}|}{\\sum_{i=1}^{m}|R\\theta_{i}|}.\n\\]"
  },
  {
    "objectID": "lab_11.html#методи-складних-мереж",
    "href": "lab_11.html#методи-складних-мереж",
    "title": "8  Лабораторна робота № 11",
    "section": "8.2 Методи складних мереж",
    "text": "8.2 Методи складних мереж\nГрафи видимості (VG) базуються на простому відображенні часових рядів у мережеву область, використовуючи локальну опуклість скалярно-позначених часових рядів, де кожне спостереження є вершиною в складній мережі. Дві вершини і пов’язані ребром, якщо для всіх вершин застосовується наступна умова:\n\\[\nx_{k} &lt; x_{j} + \\left( x_{i} - x_{j} \\right) \\frac{t_{j}-t_{k}}{t_{j}-t_{i}}.\n\\]\nМатрицю суміжності (\\(A_{ij}\\)) представленого ненаправленого та незваженого VG можна представити як:\n\\[\nA_{ij}^{VG} = A_{ji}^{VG} = \\prod_{k=i+1}^{j-1} H \\left( x_{k} &lt; x_{j} + \\left( x_{i} - x_{j} \\right) \\frac{t_{j}-t_{k}}{t_{j}-t_{i}} \\right),\n\\]\nде \\(H( \\cdot )\\) — це функція Гевісайда.\nГраф горизонтальної видимості (HVG) є спрощеною версією цього алгоритму. Для досліджуваного часового ряду набори вершин VG і HVG однакові, тоді як набір ребер HVG відображає взаємну горизонтальну видимість двох спостережень $x_{i} $ та $ x_{j}$. Тобто можна побудувати ребро \\((i,j)\\), якщо \\(x_{k} &lt; \\min(x_{i}, x_{j})\\) для всіх \\(k\\) при \\(t_{i} &lt; t_{k} &lt; t_{j}\\) так що\n\\[\nA_{ij}^{VG} = A_{ji}^{VG} = \\prod_{k=i+1}^{j-1} H \\left( x_{i} - x_{k} \\right) H \\left( x_{j} - x_{k} \\right).\n\\]\nVG і HVG фіксують по суті одні й ті ж властивості досліджуваної системи, оскільки HVG є підграфом VG з тим же набором вершин, але володіє тільки підмножиною ребер VG. Зверніть увагу, що VG інваріантний щодо суперпозиції лінійних трендів, тоді як HVG — ні.\nОскільки визначення VGs та HVGs чітко враховує часовий порядок спостережень, напрямок часу нерозривно пов’язаний з отриманою структурою мережі. Щоб врахувати цей факт, ми визначаємо набір нових статистичних мережевих показників на основі двох простих характеристик вершин:\n\nОскільки кількість ребер інцидентних вершині \\(i\\) можна визначити як \\(k_{i}^{r} = \\sum_{j} A_{ij}\\), для (H)VG ми можемо переписати дану кількісну характеристику для вершини в час \\(t_{i}\\) відносно її минулих та майбутніх вершин:\n\n\\[\nk_{i}^{r} = \\sum_{j&lt;i} A_{ij} \\quad \\mathrm{and} \\quad k_{i}^{a} \\sum_{j&gt;i} A_{ij},\n\\]\nде $k_{i} = k_{i}^{r} + k_{i}^{a} $, і $ k_{i}^{r} $ та $ k_{i}^{a}$ сприймаються як вхідні (минулі) та вихідні (майбутні) вершини.\n\nЛокальний коефіцієнт кластеризації \\(C_{i} = \\left( \\begin{matrix} k_{i}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j,k} A_{ij}A_{jk}A_{ki}\\) інша властивість старшного порядку структурного сусідства вершини \\(i\\). Для дослідження незворотності, ми можемо переписати дані характеристики наступним чином:\n\n\\[\nC_{i}^{r} = \\left( \\begin{matrix} k_{i}^{r}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j&lt;i,k&lt;i} A_{ij}A_{jk}A_{ki} \\quad \\textrm{and} \\quad C_{i}^{a} = \\left( \\begin{matrix} k_{i}^{a}\\\\ 2 \\end{matrix} \\right)^{-1} \\sum_{j&gt;i,k&gt;i} A_{ij}A_{jk}A_{ki}.\n\\]\nЯкщо уявити нашу систему зворотною в часі, ми припускаємо, що розподілу ймовірностей прямих і зворотних за часом характеристик повинні бути однаковими. Для незворотних процесів ми очікуємо виявити статистичну нееквівалентність. Ця нееквівалентність буде визначатися через дивергенцію Кульбака-Лейблера:\n\\[\nD_{KL}(p||q) = \\sum_{i=1}^{N} p(x_{i}) \\cdot \\log{\\left[ \\frac{p(x_{i})}{q(x_{i})} \\right]},\n\\]\nде \\(p(\\cdot)\\) відповідатиме розподілу вхідних характеристикам, а \\(q(\\cdot)\\) відповідатиме зворотнім."
  },
  {
    "objectID": "lab_11.html#незворотність-на-основі-пермутаційних-шаблонів",
    "href": "lab_11.html#незворотність-на-основі-пермутаційних-шаблонів",
    "title": "8  Лабораторна робота № 11",
    "section": "8.3 Незворотність на основі пермутаційних шаблонів",
    "text": "8.3 Незворотність на основі пермутаційних шаблонів\nІдея аналізу пермутаційних шаблонів (PP — permutation patterns) спочатку була запропонована Бандтом і Помпе, щоб надати дослідникам простий та ефективний інструмент для характеристики складності динаміки реальних систем. Він уникає порогу амплітуди і замість цього має справу з порядковими шаблонами перестановок. Їх частоти дозволяють відрізнити детерміновані процеси від абсолютно випадкових. Розрахунки PP припускають, що часовий ряд розбивається на пересічні підвектори довжини \\(d_{E}\\):\n\\[\n\\vec{X}(i) = \\left\\{ x(i), x(i+\\tau), ... , x(i+[d_{E}-1]\\tau) \\right\\},\n\\]\nде часова затримка \\(\\tau\\) відповідає часу розділення між елементами.\nПісля цього кожен вектор представляється у вигляді порядкового шаблону \\(\\pi = \\{ r_0, r_1, ... , r_{d_{E}-1} \\}\\), що має задовільняти наступній умові:\n\\[\nx(i+r_0) \\leq x(i+r_1) \\leq ... \\leq x(i+r_{d_{E}-1}).\n\\]\nЦікава для нас міра незворотності часу на основі PP може бути отримана шляхом врахування їх відносної частоти як для початкового, так і для оберненого часового ряду. Відповідно, якщо обидва типи мають приблизно однакові розподіли ймовірностей своїх патернів, часові ряди представляються зворотними, а для іншого випадку робиться протилежний висновок.\nРізницю між розподілами прямих часових рядів (\\(P^{d}\\)) та зворотних (\\(P^{r}\\)) можна оцінити за допомогою дивергенції Кульбака-Лейблера."
  },
  {
    "objectID": "lab_11.html#підключення-необхідних-бібліотек",
    "href": "lab_11.html#підключення-необхідних-бібліотек",
    "title": "8  Лабораторна робота № 11",
    "section": "8.4 Підключення необхідних бібліотек",
    "text": "8.4 Підключення необхідних бібліотек\n\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport pandas as pd\nimport yfinance as yf\nfrom ts2vg import NaturalVG, HorizontalVG\nimport networkx as nx\nfrom scipy.stats import entropy\nfrom numba import jit\nfrom ordpy import ordinal_distribution\nfrom tqdm import tqdm\nfrom scipy.integrate import quad\nfrom scipy.stats import gaussian_kde\nfrom scipy.spatial import distance\nfrom KDEpy import FFTKDE\nimport neuralkit2 as nk"
  },
  {
    "objectID": "lab_11.html#встановлення-параметрів-для-побудови-графіків",
    "href": "lab_11.html#встановлення-параметрів-для-побудови-графіків",
    "title": "8  Лабораторна робота № 11",
    "section": "8.5 Встановлення параметрів для побудови графіків",
    "text": "8.5 Встановлення параметрів для побудови графіків\n\nplt.style.use('classic')\nplt.rcParams.update({'legend.fontsize': 26})\nplt.rcParams['lines.linewidth'] = 2\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams[\"figure.facecolor\"] = 'white'\nplt.rcParams['font.size'] = '26'\nplt.rcParams['font.family'] = 'Times New Roman'\nplt.rcParams['savefig.dpi'] = 300\nxlabel = 'time, days'"
  },
  {
    "objectID": "lab_11.html#визначення-функції-для-побудови-рекурентного-графа",
    "href": "lab_11.html#визначення-функції-для-побудови-рекурентного-графа",
    "title": "8  Лабораторна робота № 11",
    "section": "8.6 Визначення функції для побудови рекурентного графа",
    "text": "8.6 Визначення функції для побудови рекурентного графа\n\ndef recurrence_net(time_ser, rec_thr, dim, tau, dist_type='euclidien'):\n    time_series = nk.complexity_embedding(time_ser, dimension=dim, delay=tau)\n    rp = (distance.cdist(time_series, time_series, dist_type) &lt;= rec_thr).astype(int)\n    adj_matrix_RN = rp\n    np.fill_diagonal(adj_matrix_RN, 0)\n\n    rec_nw = nx.from_numpy_matrix(adj_matrix_RN)\n    \n    return rec_nw\n\ndef node_positions_recurrence_net(ts, xs):\n    return {i: (xs[i], ts[i]) for i in range(len(ts))}"
  },
  {
    "objectID": "lab_11.html#визначення-функції-для-розрахунку-індексу-кошти",
    "href": "lab_11.html#визначення-функції-для-розрахунку-індексу-кошти",
    "title": "8  Лабораторна робота № 11",
    "section": "8.7 Визначення функції для розрахунку індексу Кошти",
    "text": "8.7 Визначення функції для розрахунку індексу Кошти\n\ndef Costa_1(time_ser, taus):\n    Cst = []\n    for tau in taus:\n        fragm_Costa = np.array([time_ser[tau:], time_ser[:-tau]])\n        DiffCosta = np.diff(fragm_Costa,axis=0)\n        IncCosta = np.sum(DiffCosta&gt;0)\n        DecCosta = np.sum(DiffCosta&lt;0)\n        C = (IncCosta-DecCosta)/(len(time_ser)-tau)\n        Cst.append(C)\n    Costa = np.mean(np.abs(Cst))\n    return Costa"
  },
  {
    "objectID": "lab_11.html#оголошення-функцій-для-підрахунку-показників-незворотності",
    "href": "lab_11.html#оголошення-функцій-для-підрахунку-показників-незворотності",
    "title": "8  Лабораторна робота № 11",
    "section": "8.8 Оголошення функцій для підрахунку показників незворотності",
    "text": "8.8 Оголошення функцій для підрахунку показників незворотності\n\n8.8.1 Пермутаційна незворотність\n\ndef PermIrrever(time_ser, d_e, tau, delta=1e-10, distance_irr=\"kullback\"):\n    pattern, dist = ordinal_distribution(time_ser, dx=d_e, taux=tau, return_missing=True)\n    \n    m, n = pattern.shape\n    pf = []\n    pb = []\n\n    is_used = np.zeros((m))\n\n    for i in range(m):\n        if is_used[i] == 1:\n            continue\n\n        is_used[i] = 1\n        pf.append(i)\n        permb = pattern[i,::-1]\n        for j in range(m-1,-1,-1):\n            if np.sum(pattern[j,:] == permb) == n: \n                is_used[j] = 1\n                pb.append(j)\n                break\n    \n    if distance_irr == \"kullback\":\n        KLD_perm = dist[pf] * np.log((dist[pf] + delta) / (dist[pb] + delta))\n        return np.sum(KLD_perm)   \n    else:\n        return distance.jensenshannon(dist[pf] + delta, dist[pb] + delta)\n\n\n\n8.8.2 Графо-динамічна незворотність\n\ndef GraphIrrever(fragm_1, graph_type='classic', delta=1e-10, d_e_rec=3, tau_rec=1, eps_rec=0.1, dist_rec='chebyshev', distance_irr='kullback'):\n    \n    # будуємо граф \n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm_1)\n    elif graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm_1)\n    else:\n        g = recurrence_net(fragm_1, rec_thr=eps_rec*np.abs(np.std(fragm_1)), dim=d_e_rec, tau=tau_rec, dist_type=dist_rec)\n    \n    # розраховуємо вхідні та вихідні характеристики \n    \n    adjacency_mat = g.adjacency_matrix()\n    ret_deg, adv_deg = GetDegree(adjacency_mat)\n    ret_clust, adv_clust = GetLocalClusteringCoefficient(adjacency_mat, ret_deg, adv_deg)\n    \n    # використовуємо kde для знаходження функції щільності ймовірностей \n    \n    pdf_ret_deg = gaussian_kde(sorted(ret_deg), bw_method='scott')       \n    pdf_adv_deg = gaussian_kde(sorted(adv_deg), bw_method='scott')    \n    pdf_ret_clust = gaussian_kde(sorted(ret_clust), bw_method='scott')    \n    pdf_adv_clust = gaussian_kde(sorted(adv_clust), bw_method='scott')\n    \n    a_deg = min(min(ret_deg), min(adv_deg))  \n    b_deg = max(max(ret_deg), max(adv_deg))  \n    a_clust = min(min(ret_clust), min(adv_clust))\n    b_clust = max(max(ret_clust), max(adv_clust))\n                           \n    if distance_irr == 'kullback':\n        dkl_deg = lambda x: pdf_ret_deg.pdf(x) * np.log((pdf_ret_deg.pdf(x) + delta)/(pdf_adv_deg.pdf(x) + delta))        \n        dkl_clust = lambda x: pdf_ret_clust.pdf(x) * np.log((pdf_ret_clust.pdf(x) + delta)/(pdf_adv_clust.pdf(x) + delta))\n\n        distance_deg = quad(dkl_deg, a_deg, b_deg)[0]       \n        distance_clust = quad(dkl_clust, a_clust, b_clust)[0]\n    \n    if distance_irr == 'shannon':                                 \n        width_deg = (b_deg-a_deg)/len(ret_deg)\n        width_clust = (b_clust-a_clust)/len(ret_clust)\n\n        lin_deg = np.arange(a_deg, b_deg, width_deg) \n        lin_clust = np.arange(a_clust, b_clust, width_clust)\n\n        p_ret_deg = pdf_ret_deg.pdf(lin_deg) \n        p_adv_deg = pdf_adv_deg.pdf(lin_deg) \n        p_ret_clust = pdf_ret_clust.pdf(lin_clust) \n        p_adv_clust = pdf_adv_clust.pdf(lin_clust)\n    \n        distance_deg = distance.jensenshannon(p_ret_deg + delta, p_adv_deg + delta)\n        distance_clust = distance.jensenshannon(p_ret_clust + delta, p_adv_clust + delta)\n      \n    return distance_deg, distance_clust\n\n\n\n8.8.3 Функція для отримання ступеня вершини\n\n@jit(nopython=True, nogil=True) \ndef GetDegree(AM):\n    numNodes = AM.shape[0]\n    retarded_degree = np.zeros((numNodes))\n    advanced_degree = np.zeros((numNodes))\n     \n    for i in range(numNodes):\n        retarded_degree[i] = AM[i, :i].sum()\n\n    for i in range(numNodes):\n        advanced_degree[i] = AM[i, i:].sum()\n        \n    return retarded_degree, advanced_degree\n    \n\n\n\n8.8.4 Функція для отримання локальної кластеризації\n\n@jit(nopython=True, nogil=True) \ndef GetLocalClusteringCoefficient(AM, ret_deg, adv_deg):\n    \n    numNodes = AM.shape[0]\n    retardedCC = np.zeros( (numNodes) )\n    advancedCC = np.zeros( (numNodes) )\n    ret_norm = ret_deg * (ret_deg - 1) / 2\n    adv_norm = adv_deg * (adv_deg - 1) / 2\n    \n    for i in range(numNodes):\n        if ret_norm[i] != 0: \n            counter = 0\n            \n            for j in range(i):\n                for k in range(j): \n                    if AM[i, j] == 1 and AM[j, k] == 1 and AM[k, i] == 1: \n                        counter += 1\n                        \n            retardedCC[i] = counter / ret_norm[i]\n    \n    for i in range(numNodes-2):\n        if adv_norm[i] != 0: \n            counter = 0\n            \n            for j in range(i+1, numNodes):\n                for k in range(i+1, j): \n                    if AM[i, j] == 1 and AM[j, k] == 1 and AM[k, i] == 1: \n                        counter += 1\n                        \n            advancedCC[i] = counter / adv_norm[i]\n                 \n                \n    return retardedCC, advancedCC"
  },
  {
    "objectID": "lab_11.html#завантажуємо-дані-з-сайту-yahoo-finance",
    "href": "lab_11.html#завантажуємо-дані-з-сайту-yahoo-finance",
    "title": "8  Лабораторна робота № 11",
    "section": "8.9 Завантажуємо дані з сайту Yahoo! Finance",
    "text": "8.9 Завантажуємо дані з сайту Yahoo! Finance\n\nname_1 = \"^N225\"\nname_2 = \"^N225\"\n\nstart = \"1980-01-01\"\nend = \"2022-12-13\"\n\ndata = yf.download(name_1, start, end)\ntime_ser_1 = data['Adj Close'].copy()\nfor_graph = data['Adj Close'].copy()\n\ndate_in_num = mdates.date2num(time_ser_1.index)\n\nnp.savetxt(f'{name_1}_initial_time_series.txt', time_ser_1.values)\n\n[*********************100%***********************]  1 of 1 completed"
  },
  {
    "objectID": "lab_11.html#виводимо-досліджувані-ряди",
    "href": "lab_11.html#виводимо-досліджувані-ряди",
    "title": "8  Лабораторна робота № 11",
    "section": "8.10 Виводимо досліджувані ряди",
    "text": "8.10 Виводимо досліджувані ряди\n\ntime_ser_1.plot(figsize=(8,6), xlabel=xlabel, ylabel=fr\"{name_1.split('.')[0]}\")\nplt.savefig('ts1.jpg', bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\nfor_graph.plot(figsize=(8,6), xlabel=xlabel, ylabel=fr\"{name_2.split('.')[0]}\")\nplt.savefig('ts2.jpg', bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "lab_11.html#встановлення-параметрів-для-розрахунків",
    "href": "lab_11.html#встановлення-параметрів-для-розрахунків",
    "title": "8  Лабораторна робота № 11",
    "section": "8.11 Встановлення параметрів для розрахунків",
    "text": "8.11 Встановлення параметрів для розрахунків\n\nwindow = 500 # розмір ковзного вікна\ntstep = 1 # часовий крок\n\nret_type = 1 # тип ряду: 1 - вихідний, 2 - детрендований\n                        # 3 - стандартні прибутковості, \n                        # 4 - стандартизовані прибутковості, \n                        # 5 - абсолютні значення (волатильності)\n                        # 6 - стандартизований вихідний часовий ряд\n\n\n# параметри для рекурентного графу\nd_e_rec = 3 # розмірність вкладень\ntau_rec = 1 # часова затримка\neps_rec = 1.3 # радіус\ndist_rec = 'chebyshev' # відстань між траєкторіями: canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, \n                                                        # ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, \n                                                        # ‘jensenshannon’, ‘kulsinski’, ‘kulczynski1’, ‘mahalanobis’, \n                                                        # ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, \n                                                        # ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n\n\n# параметри для мір незворотності\nd_e_perm = 3 # розмірність вкладень для пермутаційних патернів \ntau_perm  = 1 # часова затримка для пермутаційних патернів\ndistance_irr = 'kullback' # відстань між розподілами: kullback, shannon\ngraph_type = 'classic' # тип графу: classic, horizontal, recurrent\n\n# параметри для мір асиметрії\ntau_assym = 1 # часова затримка для діаграми Пуанкаре\ntau_Costa_begin = 1 # початковий часовий масштаб для індексу Кошти\ntau_Costa_end = 20 # кінцевий часовий масштаб для індексу Кошти\ntaus_Costa = np.arange(tau_Costa_begin, tau_Costa_end+1) # формуємо масив масштабів\n                \nlength = len(time_ser_1)"
  },
  {
    "objectID": "lab_11.html#виводимо-діаграму-пуанкаре-та-розраховуємо-міри-на-її-основі",
    "href": "lab_11.html#виводимо-діаграму-пуанкаре-та-розраховуємо-міри-на-її-основі",
    "title": "8  Лабораторна робота № 11",
    "section": "8.12 Виводимо діаграму Пуанкаре та розраховуємо міри на її основі",
    "text": "8.12 Виводимо діаграму Пуанкаре та розраховуємо міри на її основі\n\nfor_puank = time_ser_1.copy()\n\nif ret_type == 1:\n    pass\nelif ret_type == 2:\n    for_puank = for_puank.diff()\nelif ret_type == 3:\n    for_puank = for_puank.pct_change()\nelif ret_type == 4:\n    for_puank = for_puank.pct_change()\n    for_puank -= for_puank.mean()\n    for_puank /= for_puank.std()\nelif ret_type == 5: \n    for_puank = for_puank.pct_change()\n    for_puank -= for_puank.mean()\n    for_puank /= for_puank.std()\n    for_puank = for_puank.abs()\nelif ret_type == 6:\n    for_puank -= for_puank.mean()\n    for_puank /= for_puank.std()\n\nfor_puank = for_puank.dropna().values\n\n\nfig, ax1 = plt.subplots(1, 1, figsize=(15,10))\n\nax1.scatter(for_puank[:-tau_assym],for_puank[tau_assym:], marker=\"X\", s=180, c=\"g\")\n\nlow_x, high_x = ax1.get_xlim()\nlow_y, high_y = ax1.get_ylim()\nax1.axline([low_x, low_y], [high_x, high_y])\n\nax1.set_aspect('equal', 'box')\nax1.set_xlabel(r'$g(t)$', fontsize=26)\nax1.set_ylabel(r'$g(t+\\tau)$', fontsize=26) \nax1.set_xlim(left=low_x, right=high_x)\nax1.set_ylim(bottom=low_y, top=high_y)\nplt.yticks(fontsize=24)\nplt.xticks(fontsize=24)\nplt.locator_params(axis='y', nbins=7)\n\nplt.savefig(f\"Poincare_plot_{name_2.split('.')[0]}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\nВиходячи з даної діаграми для прибутковостей N225 видно, що значення даного індексу розподілені відносно рівномірно. Тут $ = 1 $, тому можемо сказати, що у короткостроковій перспективі значення прибутковостей мають рівну ймовірність як для спадання, так і для зростання."
  },
  {
    "objectID": "lab_11.html#розрахунки-з-використання-ковзного-вікна",
    "href": "lab_11.html#розрахунки-з-використання-ковзного-вікна",
    "title": "8  Лабораторна робота № 11",
    "section": "8.13 Розрахунки з використання ковзного вікна",
    "text": "8.13 Розрахунки з використання ковзного вікна\n\n8.13.1 Масив для збереження результатів\n\nPIx = []\nGIx = []\nSIx = []\nAIx = []\nEIx = []\nCIx = []\n\n\n\n8.13.2 Розрахунок відповідних мір\n\nfor i in range(0,length-window,tstep):\n    fragm_1 = time_ser_1.iloc[i:i+window].copy() #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm_1 = fragm_1.diff()\n    elif ret_type == 3:\n        fragm_1 = fragm_1.pct_change()\n    elif ret_type == 4:\n        fragm_1 = fragm_1.pct_change()\n        fragm_1 -= fragm_1.mean()\n        fragm_1 /= fragm_1.std()\n    elif ret_type == 5: \n        fragm_1 = fragm_1.pct_change()\n        fragm_1 -= fragm_1.mean()\n        fragm_1 /= fragm_1.std()\n        fragm_1 = fragm_1.abs()\n    elif ret_type == 6:\n        fragm_1 -= fragm_1.mean()\n        fragm_1 /= fragm_1.std()\n        \n    fragm_1 = fragm_1.dropna().values \n    \n    Temp_fragm = np.array([fragm_1[:-tau_assym], fragm_1[tau_assym:]])\n    \n    \n    T2   = np.transpose(np.arctan(Temp_fragm[1,:]/Temp_fragm[0,:])*180/np.pi)\n    Dup  = abs(np.diff(Temp_fragm[:,T2&gt;45],axis=0))\n    Dtot = abs(np.diff(Temp_fragm[:,T2!=45],axis=0))\n    Sup  = np.sum(abs(T2[T2&gt;45]-45))\n    Stot = np.sum(abs(T2[T2!=45]-45))\n    Aup  = np.sum(abs(np.transpose(((T2[T2&gt;45]-45))*np.sqrt(np.sum(Temp_fragm[:,T2&gt;45]**2,axis=0)))))\n    Atot = np.sum(abs(np.transpose(((T2[T2!=45]-45))*np.sqrt(np.sum(Temp_fragm[:,T2!=45]**2,axis=0)))))\n    Ethird = np.sum(np.transpose(Temp_fragm[0,:]-Temp_fragm[1,:])**3)\n    Etot = (np.sum(np.transpose(Temp_fragm[0,:]-Temp_fragm[1,:])**2))**(3/2)\n\n    \n    Porta = sum(T2&lt;45)/sum(T2!=45)\n    Gudzik = np.sum(Dup**2)/np.sum(Dtot**2)\n    Slope = Sup/Stot\n    Area = Aup/Atot\n    Eiler = Ethird/Etot\n    Costa = Costa_1(fragm_1, taus_Costa)\n    \n    PIx.append(Porta)\n    GIx.append(Gudzik)\n    SIx.append(Slope)\n    AIx.append(Area)\n    EIx.append(Eiler)\n    CIx.append(Costa)\n\n\n\n8.13.3 Збереження значень до .txt файлу\n\nnp.savetxt(f\"Porta_idx_{name_1}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", PIx)\nnp.savetxt(f\"Gudzik_idx_{name_1}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", GIx)\nnp.savetxt(f\"Slope_idx_{name_1}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", SIx)\nnp.savetxt(f\"Area_idx_{name_1}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", AIx)\nnp.savetxt(f\"Eiler_idx_{name_1}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", EIx)\nnp.savetxt(f\"Costa_idx_{name_1}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", CIx)\n\n\n\n8.13.4 Виведення індексу Порти\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], PIx, label=r\"$ PIx $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + \"$PIx$\")\n\nplt.savefig(f\"PIx_{name_2.split('.')[0]}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n8.13.5 Виведення індексу Кошти\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], GIx, label=r\"$ GIx $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + \"$GIx$\")\n\nplt.savefig(f\"GIx_{name_2.split('.')[0]}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n8.13.6 Виведення індексу кута нахилу\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], SIx, label=r\"$ SIx $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + \"$SIx$\")\n\nplt.savefig(f\"SIx_{name_2.split('.')[0]}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n8.13.7 Виведення індексу площі секторів\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], AIx, label=r\"$ AIx $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + \"$AIx$\")\n\nplt.savefig(f\"AIx_{name_2.split('.')[0]}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n8.13.8 Виведення індексу Ейлера\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], EIx, label=r\"$ EIx $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + \"$EIx$\")\n\nplt.savefig(f\"EIx_{name_2.split('.')[0]}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n8.13.9 Виведення індексу Кошти\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], CIx, label=r\"$ CIx $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + \"$CIx$\")\n\nplt.savefig(f\"CIx_{name_2.split('.')[0]}_{tau_assym}_{window}_{tstep}.jpg\", bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "lab_11.html#побудова-графу-досліджуваного-ряду",
    "href": "lab_11.html#побудова-графу-досліджуваного-ряду",
    "title": "8  Лабораторна робота № 11",
    "section": "8.14 Побудова графу досліджуваного ряду",
    "text": "8.14 Побудова графу досліджуваного ряду\n\nindex_begin = 500 # початковий індекс для графу\nindex_end = 2000 # кінцевий індекс для графу\n\nfor_graph_plot = time_ser_1.copy()\n\nif ret_type == 1:\n    pass\nelif ret_type == 2:\n    for_graph_plot = for_graph_plot.diff()\nelif ret_type == 3:\n    for_graph_plot = for_graph_plot.pct_change()\nelif ret_type == 4:\n    for_graph_plot = for_graph_plot.pct_change()\n    for_graph_plot -= for_graph_plot.mean()\n    for_graph_plot /= for_graph_plot.std()\nelif ret_type == 5: \n    for_graph_plot = for_graph_plot.pct_change()\n    for_graph_plot -= for_graph_plot.mean()\n    for_graph_plot /= for_graph_plot.std()\n    for_graph_plot = for_graph_plot.abs()\nelif ret_type == 6:\n    for_graph_plot -= for_graph_plot.mean()\n    for_graph_plot /= for_graph_plot.std()\n\nfor_graph_plot = for_graph_plot.copy().dropna().values\n\ndate = date_in_num[index_begin:index_end] # вилучаємо необхідні по індексам дати\n\n# будуємо граф у залежності від типу графа\n\nif graph_type == 'classic':\n    g = NaturalVG(directed=None).build(for_graph_plot[index_begin:index_end], xs=date)\n    pos = g.node_positions()\n    nxg = g.as_networkx()\nelif graph_type == 'horizontal':\n    g = HorizontalVG(directed=None).build(for_graph_plot[index_begin:index_end], xs=date)\n    pos = g.node_positions()\n    nxg = g.as_networkx()\nelse:\n    g = recurrence_net(for_graph_plot[index_begin:index_end], \n                       rec_thr=eps_rec * np.abs(np.std(for_graph_plot[index_begin:index_end])), \n                       dim=d_e_rec, \n                       tau=tau_rec, \n                       dist_type=dist_rec)\n    \n    pos = node_positions_recurrence_net(for_graph_plot[index_begin:index_end], date)\n    nxg = g\n    \n    \n# встановлення параметрів для побудови графів\n\ngraph_plot_options = {\n    'with_labels': False,\n    'node_size': 2,\n    'node_color': [(0, 0, 0, 1)],\n    'edge_color': [(0, 0, 0, 0.15)],\n}"
  },
  {
    "objectID": "lab_11.html#виводимо-звязки-видимості",
    "href": "lab_11.html#виводимо-звязки-видимості",
    "title": "8  Лабораторна робота № 11",
    "section": "8.15 Виводимо зв’язки видимості",
    "text": "8.15 Виводимо зв’язки видимості\n\nfig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\nnx.draw_networkx(nxg, ax=ax1, pos=pos, **graph_plot_options)\nax1.tick_params(bottom=True, labelbottom=True)\nax1.plot(time_ser_1.index[index_begin:index_end], for_graph_plot[index_begin:index_end], label=fr\"{name_2}\")\nax1.set_title('Visibility Connections', fontsize=22)\nax1.set_xlabel(xlabel)\nax1.set_ylabel(fr\"{name_2}\")\nax1.legend(loc='upper right')\n\nplt.savefig(f\"Time_ser_connections_symbol={name_2}_idx_beg={index_begin}_idx_end={index_end}_sertype={ret_type}_network_type={graph_type}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\nВиходячи з графу взятого нами фрагменту видно, що крах поблизу 1988 року характеризується високою концентрацією вузлів, які охоплюють проміжок до 1982 року. Це вказує на високий ступінь довготривалої пам’яті для кризових явищ фондового ринку, що в свою чергу впливає і на їх незворотність."
  },
  {
    "objectID": "lab_11.html#виводимо-граф",
    "href": "lab_11.html#виводимо-граф",
    "title": "8  Лабораторна робота № 11",
    "section": "8.16 Виводимо граф",
    "text": "8.16 Виводимо граф\n\npos = nx.spring_layout(nxg,k=0.15,iterations=100)\n# знаходимо вузол близький до центру графа (0.5,0.5)\ndmin = 1\nncenter = 0\nfor n in pos: \n    x, y = pos[n]\n    d = (x - 0.5)**2 + (y - 0.5)**2\n    if d &lt; dmin:\n        ncenter = n\n        dmin = d\n\n# розфарбовуємо в залежності від ступеня вершини\n\np = dict(nx.degree(nxg))\nfig, ax2 = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\nax2.set_title('Graph representation', fontsize=22)\nnx.draw_networkx_edges(nxg, ax=ax2, pos=pos, nodelist=[ncenter], alpha=0.4,width=0.1)\nnx.draw_networkx_nodes(nxg, ax=ax2, pos=pos, nodelist=list(p.keys()),\n                       node_size=10, edgecolors='r', linewidths=0.01,\n                       node_color=list(p.values()),\n                       cmap=plt.cm.Blues_r)\n        \nvmin = np.asarray(list(p.values())).min()\nvmax = np.asarray(list(p.values())).max()\n\nsm = plt.cm.ScalarMappable(cmap=plt.cm.Blues_r, norm=plt.Normalize(vmin=vmin, vmax=vmax))\ncb = plt.colorbar(sm, ax=ax2)\ncb.set_label('degree')\n\nplt.savefig(f\"Graph_representation_symbol={name_2}_idx_beg={index_begin}_idx_end={index_end}_sertype={ret_type}_network_type={graph_type}.jpg\", bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "lab_11.html#побудова-мір-незворотності-на-основі-пермутаційних-шаблонів-та-графів",
    "href": "lab_11.html#побудова-мір-незворотності-на-основі-пермутаційних-шаблонів-та-графів",
    "title": "8  Лабораторна робота № 11",
    "section": "8.17 Побудова мір незворотності на основі пермутаційних шаблонів та графів",
    "text": "8.17 Побудова мір незворотності на основі пермутаційних шаблонів та графів"
  },
  {
    "objectID": "lab_11.html#віконна-процедура",
    "href": "lab_11.html#віконна-процедура",
    "title": "8  Лабораторна робота № 11",
    "section": "8.18 Віконна процедура",
    "text": "8.18 Віконна процедура\n\n8.18.1 Масив для збереження результатів розрахунків\n\nDegree = []\nClust = []\nPerm = []\n\n\n\n8.18.2 Розрахунок відповідних мір\n\nfor i in range(0,length-window,tstep):\n    fragm_1 = time_ser_1.iloc[i:i+window].copy() #відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm_1 = fragm_1.diff()\n    elif ret_type == 3:\n        fragm_1 = fragm_1.pct_change()\n    elif ret_type == 4:\n        fragm_1 = fragm_1.pct_change()\n        fragm_1 -= fragm_1.mean()\n        fragm_1 /= fragm_1.std()\n    elif ret_type == 5: \n        fragm_1 = fragm_1.pct_change()\n        fragm_1 -= fragm_1.mean()\n        fragm_1 /= fragm_1.std()\n        fragm_1 = fragm_1.abs()\n    elif ret_type == 6:\n        fragm_1 -= fragm_1.mean()\n        fragm_1 /= fragm_1.std()\n        \n    fragm_1 = fragm_1.dropna().values\n        \n    deg, clust = GraphIrrever(fragm_1, \n                                    graph_type=graph_type, \n                                    delta=1e-10, \n                                    d_e_rec=d_e_rec, \n                                    tau_rec=tau_rec, \n                                    eps_rec=eps_rec, \n                                    dist_rec=dist_rec, \n                                    distance_irr=distance_irr)  \n    \n    perm = PermIrrever(fragm_1, \n                          d_e=d_e_perm, \n                          tau=tau_perm, \n                          delta=1e-10, \n                          distance_irr=distance_irr)\n      \n    Degree.append(deg)\n    Clust.append(clust)\n    Perm.append(perm)\n\n\n\n8.18.3 Збереження результатів до .txt файлів\n\nnp.savetxt(f\"{distance_irr}_deg_symbol={name_1}_wind={window}_step={tstep}_ret_type={ret_type}_graph_type={graph_type}.txt\",Degree)\nnp.savetxt(f\"{distance_irr}_clust_symbol={name_1}_wind={window}_step={tstep}_ret_type={ret_type}_graph_type={graph_type}.txt\",Clust)\nnp.savetxt(f\"{distance_irr}_perm_symbol={name_1}_wind={window}_step={tstep}_ret_type={ret_type}_d_e={d_e_perm}_tau={tau_perm}.txt\",Perm)\n\n\n\n8.18.4 Виведення ступеня незворотності на основі степені вершини\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], Degree, label=r\"$ Dist_{deg} $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + \"$Dist_{deg}$\")\n\nplt.savefig(f\"{distance_irr}_Degree_symbol={name_1}_wind={window}_step={tstep}_ret_type={ret_type}_graph_type={graph_type}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n8.18.5 Виведення ступеня незворотності на основі показника локальної кластеризації\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], Clust, label=r\"$ Dist_{clust} $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + r\"$Dist_{Clust}$\")\n\nplt.savefig(f\"{distance_irr}_Clust_symbol={name_1}_wind={window}_step={tstep}_ret_type={ret_type}_graph_type={graph_type}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n8.18.6 Виведення ступеня незворотності на основі пермутаційних шаблонів\n\nfig, ax = plt.subplots(1, 1, figsize=(15,8))\nax.plot(time_ser_1.index[window:length:tstep], for_graph[window:length:tstep], label=fr\"{name_2.split('.')[0]}\")\n\nax2 = ax.twinx()\nax2.plot(time_ser_1.index[window:length:tstep], Perm, label=r\"$ Dist_{\\pi} $\", color='g')\n\nfig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{name_2.split('.')[0]}$,$\" + r\"$Dist_{\\pi}$\")\n\nplt.savefig(f\"{distance_irr}_Perm_symbol={name_1}_wind={window}_step={tstep}_ret_type={ret_type}_d_e={d_e_perm}_tau={tau_perm}.jpg\", bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "lab_12.html",
    "href": "lab_12.html",
    "title": "9  Лабораторна робота № 12",
    "section": "",
    "text": "10 Висновок\nСтабільні розподіли — захоплюючий і плідний об’єкт досліджень в теорії ймовірностей; більше того, в даний час вони мають велику цінність при моделюванні складних процесів у фізиці, астрономії, економіці, теорії комунікацій, тощо.\nУ даній роботі було представлено теоретичні та чисельні обгрунтування в сторону альфа-стабільного розподілу Леві в якості практичної моделі для кращого розуміння та передбачення кризових явищ у складних системах.\nТут ми представляємо розрахунки як для усього ряду, так і для його підфрагментів, використовуючи алгоритм сковзного вікна. Виходячи з усього ряду прибутковостей, видно, що хвости їх розподілу далеко виходять за межі Гауссового. Найкраще емпіричний розподіл прибутковостей збігається саме з теоретичним альфа-стабільним розподілом Леві.\nВикористовуючи алгоритм сковзного вікна, ми побачили, що параметри альфа-стабільного розподілу змінюються з часом.Виходячи з цього, можна сказати наступне:\nПараметр \\(\\alpha\\) (індекс стабільності хвостів розподілу) для усіх вікон починає спадати у (перед)кризовий період, що робить його індикатором(-передвісником) кризових явищ. Під час криз у розподілі прибутковостей зростає ексцес, а самі хвости стають важчими, на що даний показник реагує передчасно. Видно, що даний показник поводить себе доволі стабільно для усіх вікон. Тим не менш, для вікна 500 днів він виглядає найбільш згладженим.\nНаступним розглянемо параметр асиметрії \\(\\beta\\):\nДинаміка даної міри виглядає набагато хаотичніше у порівнянні з індексом стабільності. Особливо це помітно для вікна довжиною 250 днів. Для представлених результатів можна зробити наступний висновок: у передкризовий період даний показник має зростати, вказуючи на значну правосторонню асиметрію розподілу прибутковостей (переважання позитивних прибутковостей). Для кризового періоду цей показник має спадати, вказуючи на домінацію негативних прибутковостей (лівостороння асиметрія розподілу). Даний показник важко розглядати у якості надійного індикатора, оскільки його коливання представляються значними навіть при незначних падіннях представленого фондового індексу.\nНаступним розглянемо індекс розташування (зміщення) альфа-стабільного розподілу:\nПоказник розташування $ $ потроху спадає у кризовий період, демонструючи зміщення розподілу в сторону негативних прибутковостей. Тим не менш, цей розподіл представляєть настільки ж хаотичним як і показник асиметрії $ $.\nОстанні рисунки демонструють динаміку показника масштабування розподілу $ $:\nІз представлених результатів видно, що даний показник спадає у (перед)кризовий період, вказуючи на зменшення масштабу (форми) альфа-стабільного розподілу Леві. Накраще $ $ себе показує для вікна 250 днів. Для 500 днів показник стає занадто нечутливим до багатьох криз на фондовому ринку.\nОтже, з усіх 4-ох показників показник стабільності $ $ є найкращим для ідентифікації кризових явищ та побудови надійних стратегій ризик-менеджменту."
  },
  {
    "objectID": "lab_12.html#теоретичні-відомості",
    "href": "lab_12.html#теоретичні-відомості",
    "title": "9  Лабораторна робота № 12",
    "section": "9.1 Теоретичні відомості",
    "text": "9.1 Теоретичні відомості\nАльфа-стабільний розподіл є узагальненням розподілу Гауса, яке цінується за те, що воно тягне за собою жирні хвости. З цієї причини він широко використовується при обробці сигналів, наприклад, у медицині чи фінансах.\nЗагальний клас стабільних розподілів був введений і отримав цю назву французьким математиком Полем Леві на початку 1920-х років.\nРаніше ця тема привертала лише помірну увагу провідних експертів, хоча були і ентузіасти, з яких можна згадати російського математика Олександра Яковича Хінчіна. Натхненням для Леві стало бажання узагальнити відому центральну граничну теорему, згідно з якою будь-який розподіл ймовірностей з кінцевою дисперсією збігається до гауссового розподілу.\nСтабільні розподіли мають три виняткові властивості, які можна коротко підсумувати, заявляючи, що вони 1) інваріантні при додаванні, 2) мають власну область збіжності і 3) дозволяють канонічну форму характеристичної функції.\nІнваріантність при додавані\nВипадкова величина \\(X\\) підпорядковується стабільному розподілу \\(P(x) = \\text{Prob}\\{X \\leq x\\}\\) якщо для будь-якого \\(n \\geq 2\\) існують додатнє значення \\(c_{n}\\) та дійсне значення \\(d_{n}\\) такі, що\n\\[\nX_1 + X_2 + ... + X_n \\stackrel{d}{=} c_{n}X + d_{n},\n\\]\nде \\(X_1, X_2, ..., X_n\\) характеризуються як незалежні, ідентично розподілені випадкові величини. Також \\(\\stackrel{d}{=}\\) позначає рівність розподілів, тобто, випадкові величини з обох сторін мають однаковий розподіл ймовірностей.\nЗагалом, сума незалежних, ідентично розподілених випадкових величин результує у випадкову величину з іншим розподілом. Однак, для випадкових величин, що характеризуються стабільним розподілом, сума ідентично розподілених випадкових величин до величини такого самого розподілу. У цьому випадку результуюча випадкова величина (розподіл) може відрізнятися від попередніх величин характерним масштабом (\\(c_{n}\\)) та зміщенням (\\(d_{n}\\)). Якщо \\(d_{n} = 0\\), розподіл називається строго стабільним.\nВідомо, що нормована константа \\(c_{n}\\) має вид\n\\[\nc_{n} = n^{1/\\alpha} \\, \\text{with} \\, 0 &lt; \\alpha \\leq 2.\n\\]\nПараметр \\(\\alpha\\) має назву характеристична експонента або індекс стабільності стабільного розподілу.\nПопередня теорема має альтернативну версію, що включає в суму лише дві випадкові величини. Випадкова величина \\(X\\) підпорядковується стабільному розподілу, якщо для будь-яких позитивних значень \\(A\\) та \\(B\\) існує позитивне число \\(C\\) та дійсне число \\(D\\) такі, що\n\\[\nA X_1 + B X_2 \\stackrel{d}{=} C X + D,\n\\]\nде \\(X_1\\) та \\(X_2\\) незалежні копії \\(X\\). Тоді існує значення \\(\\alpha \\in (0, 2]\\) при яких значення \\(C\\) задовільняє рівність \\(C^{\\alpha} = A^{\\alpha} + B^{\\alpha}\\).\nДля строго стабільних розподілів \\(D = 0\\). Це означає, що всі лінійні комбінації випадкових незалежних, ідентично розподілених випадкових величин, що підкоряються строго стабільному розподілу, результують у випадкову величину з одним і тим же типом розподілу.\nСтабільний розподіл вважається симетричним, якщо випадкова величина \\(-X\\) має такий самий тип розподілу. Симетричний стабільний розподіл обов’язково строго стабільний.\nОскільки аналітичний вираз функції щільності ймовірностей для стабільного розподілу невідома, за винятком кількох членів стабільного сімейства, більшість традиційних методів математичної статистики не можуть бути використані. Відбовідними вийнятками є\n\nГаусовий розподіл \\(S_2(0,\\mu, \\sigma) = \\mathcal{N}(\\mu, 2\\sigma^2)\\). Гаусовий розподіл є спеціальним випадком стабільного розподілу при \\(\\alpha = 2\\) так що \\(\\mathcal{N}(\\mu, \\sigma) = S(2,0,\\mu, \\frac{\\sigma}{\\sqrt{2}})\\), де \\(\\mu\\) позначає середнє значення нормального розподілу, а \\(\\sigma\\) — це стандартне відхилення. Функція щільності ймовірностей має вид\n\n\\[\n\\frac{1}{\\sigma\\sqrt{2\\pi}}\\text{exp}^{-(x-\\mu)^{2}/2\\sigma^{2}}.\n\\]\n\nРозподіл Коші. Розподіл Коші — це ще одне представлення стабільного розподілу при \\(\\alpha = 1\\) та \\(\\beta = 0\\) такими, що \\(Cauchy(\\delta, \\gamma) = S_1(1,0,\\gamma,\\delta)\\), де \\(\\gamma\\) — це параметр масштабування, а \\(\\delta\\) — це параметр зсуву розподілу Коші. Функція щільності ймовірностей представлена як:\n\n\\[\n\\frac{\\gamma}{\\pi((x-\\delta)^2 + \\gamma^2)}, \\, -\\infty &lt; x &lt; \\infty.\n\\]\n\nРозподіл Леві також є вийнятком із класу стабільних розподілів, де \\(\\alpha = 0.5\\) і \\(\\beta = 1\\). Іншими словами, \\(Levy(\\delta, \\gamma) = S_{1/2}(0.5, 1, \\gamma, \\delta)\\). Функція щільності ймовірностей має вид\n\n\\[\n\\sqrt{\\frac{\\gamma}{2\\pi}}\\frac{1}{(x-\\delta)^{3/2}}\\exp{\\left[ \\frac{-\\gamma}{2(x-\\delta} \\right]}, \\, \\delta &lt; x &lt; \\infty.\n\\]\nЯкщо \\(X \\sim S_{1/2}(0.5, 1, \\gamma, \\delta\\), тоді для \\(x &gt; 0\\)\n\\[\nP(X \\leq x) = 2 \\left( 1 - \\phi \\left( \\sqrt{\\frac{\\gamma}{x}} \\right) \\right),\n\\]\nде \\(\\phi\\) позначає кумулятивну функцію нормального розподілу.\nОбласть збіжності\nІнше (еквівалентне) визначення стверджує, що стабільні розподіли — це єдині розподіли, які можна отримати при границі нормалізованих сум незалежних, ідентично розподілених випадкових величин. Кажуть, що випадкова величина \\(X\\) має область збіжності, тобто якщо існує послідовність незалежних, ідентично розподілених випадкових величин \\(Y_1, Y_2, ...\\) і послідовності позитивних чисел \\({\\gamma_n}\\) і дійсних чисел \\({\\delta_n}\\) таких, що\n\\[  \n\\frac{Y_1 + Y_2 + ... + Y_n}{\\gamma_n} + \\delta_n \\stackrel{d}{\\Rightarrow} X.  \n\\]\nКоли \\(X\\) це гаусова випадкова величина, а \\(Y_i's\\) є незалежними, ідентично розподіленими випадковими величинами з визначенною дисперсією, тоді рівняння вище є твердженням звичайної центральної граничної теореми. Область збіжності \\(X\\) вважається нормальною коли \\(\\gamma_n = n^{1/\\alpha}\\).\nКанонічні представлення характеристичної функції\nЧотири параметри використовуються для опису випадкової величини, що слідує за стабільним розподілом: \\(X \\sim S(\\alpha, \\beta, \\mu, \\gamma)\\). Параметр \\(\\alpha \\in (0, 2]\\) — це той, який нас найбільше зацікавить. Цей параметр визначає товщину хвостів. Параметр \\(\\beta \\in [-1, 1]\\) є параметром асиметрії. Останні два параметри позначають розташування \\((\\mu \\in \\Re)\\) і масштаб \\((\\gamma &gt; 0)\\) розподілу. Альфа-стабільний розподіл немає жодного аналітичного виразу для щільності ймовірності \\(X\\), але ми можемо охарактеризувати його характеристичною функцією:\n\\[\\begin{equation}\n    \\begin{split}\n    \\phi(t) &= E\\left[\\exp(itX)\\right] \\\\\n            &=\n        \\begin{cases}\n            \\exp\\left(i \\mu t - \\gamma^{\\alpha}|t|^{\\alpha} \\left[1-i\\beta\\text{sign}(t)\\tan{\\frac{\\pi\\alpha}{2}}\\right]\\right) & \\text{if} \\, \\alpha \\neq 1 \\\\ \\exp\\left(i \\mu t - \\gamma|t| \\left[1+i\\beta\\text{sign}(t)\\frac{2}{\\pi}\\log{|t|}\\right]\\right) & \\text{if} \\, \\alpha = 1.\n        \\end{cases}\n    \\end{split}\n\\end{equation}\\]\nМи могли б використовувати перетворення Фур’є, щоб отримати функцію щільності розподілу ймовірностей з характеристичної функції:\n\\[\nf(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^{+\\infty} \\phi(t) \\cdot \\exp(-itX) dt.\n\\]\nАле наведена вище параметризація не є повністю задовільною, оскільки функція щільності розподілу ймовірностей не є безперервною, зокрема, при \\(\\alpha = 1\\). Дійсно, коли \\(\\beta &gt; 0\\), щільність розподілу зміщується вправо, коли \\(\\alpha &lt; 1\\) і вліво, коли \\(\\alpha &gt; 1\\), зі зсувом в сторону \\(+\\infty\\) (відповідно \\(-\\infty\\)), коли \\(\\alpha\\) прагне до 1. Таким чином, для прикладного аналізу даних та інтерпретації коефіцієнтів слід уникати такої параметризації.\nІснує багато параметризацій для стабільних законів, і ці різні параметризації викликали велику плутанину. Різноманітність параметризацій обумовлена поєднанням історичної еволюції плюс численними проблемами, які були проаналізовані за допомогою спеціалізованих форм стабільного розподілу. Є вагомі причини використовувати різні параметризації в різних ситуаціях. Якщо в пріорітеті чисельні розрахунки, робота із даними, то краще використовувати одну параметризацію. Якщо бажані прості алгебраїчні властивості розподілу або аналітичні властивості строго стійких законів, то краще розглянути декілька параметризацій. Нолан запропонував використовувати параметризацію Золотарьова (M), яку також часто позначають як \\(S^{0}\\). Характеристична функція, що відповідає \\(X \\sim S^{0}(\\alpha, \\beta, \\mu_{0}, \\gamma)\\), дорівнює:\n\\[\\begin{equation}\n    \\begin{split}\n    \\phi(t) &= E\\left[\\exp(itX)\\right] \\\\\n            &=\n        \\begin{cases}\n            \\exp\\left(i \\mu_{0} t - \\gamma^{\\alpha}|t|^{\\alpha} \\left[1+i\\beta\\text{sign}(t)\\tan{\\frac{\\pi\\alpha}{2}}\\left( \\gamma^{1-\\alpha}|t|^{1-\\alpha}-1 \\right)\\right]\\right) & \\text{if} \\, \\alpha \\neq 1 \\\\ \\exp\\left(i \\mu_{0} t - \\gamma|t| \\left[1+i\\beta\\text{sign}(t)\\frac{2}{\\pi}\\left(\\log{|t|} + \\log{\\gamma}\\right) \\right]\\right) & \\text{if} \\, \\alpha = 1.\n        \\end{cases}\n    \\end{split}\n\\end{equation}\\]\nЦя альтернативна параметризація недалека від зазначеної напочатку. Єдина відмінність стосується параметра \\(\\mu\\), який у даній параметризації коригує зсув для значень \\(\\alpha\\) близьких до 1:\n\\[\n\\mu_{0} = \\begin{cases} \\mu + \\beta\\gamma\\tan{\\frac{\\pi\\alpha}{2}} & \\text{if} \\, \\alpha \\neq 1 \\\\ \\mu + \\beta\\frac{2}{\\pi}\\gamma\\log{\\gamma} & \\text{if} \\, \\alpha = 1. \\end{cases}\n\\]"
  },
  {
    "objectID": "lab_12.html#метод-розрахунку-параметрів-alpha-стабільного-розподілу",
    "href": "lab_12.html#метод-розрахунку-параметрів-alpha-стабільного-розподілу",
    "title": "9  Лабораторна робота № 12",
    "section": "9.2 Метод розрахунку параметрів \\(\\alpha\\)-стабільного розподілу",
    "text": "9.2 Метод розрахунку параметрів \\(\\alpha\\)-стабільного розподілу\nЧисельні методи, такі як метод Маккаллоха, заснований на квантилях, і метод оцінки максимальної правдоподібності були розроблені в результаті відсутності аналітичних рішень. Припустимо, що \\(\\text{X} = (X_1, ... , X_T)\\) вектор, що складається з \\(T\\) незалежних ідентично розподілених випадкових величин із розподілу Парето, і також \\(x \\sim S_{\\alpha}(\\alpha, \\beta, \\delta, \\gamma)\\). Визначивши \\(\\theta = (\\alpha, \\beta, \\delta, \\gamma)\\), Митник, Доганоглу та Ченайо розробили алогритм максимальної правдоподібності і показали, що \\(\\theta\\) можна розрахувати, максимізуючи функцію логарифмічної правдоподібності\n\\[\nl(\\theta, x) = \\sum_{i=1}^{T}\\log{f(x_{i}, \\theta)}.\n\\]\nДюмушель застосував метод максимальної правдоподібності до стабільного розподілу і визначив функцію правдоподібності наступним чином:\n\\[\nL(\\theta) = \\prod_{k=1}^{n}S_{\\alpha,\\beta} \\left( \\frac{X_{k} - \\delta}{\\gamma} \\right) \\Big/ \\gamma,\n\\]\nде \\(\\theta = (\\alpha, \\beta, \\delta, \\gamma)\\) опираючись на \\(x = (x_1, ... , x_n)\\) для розміру вибірки \\(n\\)."
  },
  {
    "objectID": "lab_12.html#хід-роботи",
    "href": "lab_12.html#хід-роботи",
    "title": "9  Лабораторна робота № 12",
    "section": "9.3 Хід роботи",
    "text": "9.3 Хід роботи\n\n9.3.1 Імпортування необхідних бібліотек\n\nimport numpy as np                 # бібліотека для роботи з масивами чисел\nimport matplotlib.pyplot as plt    # бібліотека для побудови графіків\nimport yfinance as yf              # бібліотека для зчитування фінансових даних з Yahoo Finance\nimport levy                        # бібліотека для роботи з альфа-стабільним розподілом Леві\nimport pandas as pd                # бібліотека для фільтрації даних та їх обробки\nfrom scipy.stats import norm       # бібліотека для побудови теоретичного Гауссового розподілу\nfrom tqdm import tqdm              # бібліотека для виводу шкали завантаження\n\n\n\n9.3.2 Визначення стилю рисунків\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\n\n\n9.3.3 Зчитування з Yahoo Finance\n\n# встановлення назви індексу\nsymbol = \"^BSESN\" \n\n# встановлення діапазону з яким будемо працювати\nstart = \"1980-01-01\"\nend = \"2022-11-07\"\n\n# завантаження даних з Yahoo\ndata = yf.download(symbol, start, end)\ntime_ser = data['Adj Close'].copy()\n\n# підпис по вісі Ох \nxlabel = 'time, days'\n\n# підпис по вісі Оу\nylabel = symbol                       \n\n# збереження результату в текстовий документ \nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser_1.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n9.3.4 Виведення графіку досліджуваного ряду\n\ntime_ser.plot(figsize=(8,6), xlabel=xlabel, ylabel=fr\"{symbol}\")\nplt.savefig('ts1.jpg', bbox_inches=\"tight\")\nplt.show();\n\n\n\n\n\n\n9.3.5 Побудова розподілу Леві та розрахунок параметрів для всього ряду\n\nfor_levy = time_ser.copy()\n\nif ret_type == 1:\n    pass\nelif ret_type == 2:\n    for_levy = for_levy.diff()\nelif ret_type == 3:\n    for_levy = for_levy.pct_change()\nelif ret_type == 4:\n    for_levy = for_levy.pct_change()\n    for_levy -= for_levy.mean()\n    for_levy /= for_levy.std()\nelif ret_type == 5: \n    for_levy = for_levy.pct_change()\n    for_levy -= for_levy.mean()\n    for_levy /= for_levy.std()\n    for_levy = for_levy.abs()\nelif ret_type == 6:\n    for_levy -= for_levy.mean()\n    for_levy /= for_levy.std()\n\nfor_levy = for_levy.dropna().values\n\n\n\n9.3.6 Підганяємо розподіл Леві та Гаусовий для порівняння\n\nparams = levy.fit_levy(for_levy)\nmean, std = norm.fit(for_levy)\n\n\n\n9.3.7 Отримуємо параметри розподілу Леві у відповідності до однієї із параметризацій, що пропонує пакет \\(levy\\)\n\nalpha, beta, mu, sigma = params[0].get('1')\n\n\n\n9.3.8 Будуємо теоретичні та емпіричні розподіли\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10))\n\nax[0].hist(for_levy, bins=50, density=True, alpha=0.6, color='b')\n\nxmin = for_levy.min()\nxmax = for_levy.max()\n\nx = np.linspace(xmin, xmax, len(for_levy))\npdf = levy.levy(x, alpha, beta, mu, sigma)\npdf_norm = norm.pdf(x, mean, std)\n\nfig.suptitle(fr'Теоретичні та емпіричні $\\alpha$-стабільні розподіли для {symbol}', fontsize=20)\n\nax[0].plot(x, pdf, 'k', linewidth=2)\nax[0].plot(x, pdf_norm, 'r', linewidth=2)\nax[0].set_yscale('log')\nax[0].set_xlabel(r'$ x $')\nax[0].set_ylabel(r'$ f_{\\alpha}(x), \\, \\mathrm{ePDF}$')\n\nax[1].hist(for_levy, bins=50, density=True, alpha=0.6, color='g')\nax[1].plot(x, pdf, 'k', linewidth=2)\nax[1].plot(x, pdf_norm, 'r', linewidth=2)\nax[1].set_xlabel(r'$ x $')\nax[1].set_ylabel(r'$ f_{\\alpha}(x), \\, \\mathrm{ePDF}$')\n\n\nplt.show();\n\n\n\n\nРисунок 9.1: Теоретичні та емпіричні альфа-стабільні функції щільності ймовірностей\n\n\n\n\n\n\n9.3.9 Виводимо параметри розподілу Леві для заданого індексу\n\nprint(fr\"Параматери alpha = {alpha:.2f}, beta = {beta:.2f}, mu = {mu:.2f}, sigma = {sigma:.2f}\")\n\nПараматери alpha = 1.63, beta = -0.11, mu = -0.01, sigma = 0.53\n\n\nДля досліджуваного індексу бачимо, що параметр $ &lt; 2.0 $ та $ &lt; 0 $, що вказує на відхилення розподілу даного індексу від нормального. Тобто, для даного ряду переважаючими є кризові явища, на що вказують важкі хвости розподілу. Із порівняльного аналізу Гауссового та Леві розподілів бачимо, що хвости нормального розподілу значно недооцінюють ймовірність появи кризових явищ чого, наприклад, не скажешь про альфа-стабільний розподіл. Взявши логарифм значень ймовірності по осі $ Oy $ ми можемо спостерігати, що, наприклад, недооцінка негативних прибутковостей Гауссовим розподілом, у порівнянні з альфа-стабільним, складає $ ^{15} $ порядків. Для позитивних прибутковостей, що перевищують значення $ +10$ недооцінка Гауссовим розподілом складає $ ^{27} $ порядків. Теоретичне значення альфа-стабільного розподілу достатньо точно враховує важкі хвости емпіричного розподілу, що також виражається високим ексцесом розподілу. Також варто зазначити, що коефіцієнт асиметрії $ $ вказує на невеличке зміщення розподілу в ліву сторону, що також демонструє переважання кризових явищ.\n\n\n9.3.10 Дослідження поведінки альфа-стабільного розподілу Леві\n\nx = np.arange(-5, 5, .01)\nbeta_1 = 0\nmu = 0 \nsigm = 1 \n\nbeta_2 = 1.0\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 10))\n\nax[0][0].plot(x, levy.levy(x, 0.5, beta_1, mu, sigm), label = r\"$ \\alpha=0.5 $\")\nax[0][0].plot(x, levy.levy(x, 0.75, beta_1, mu, sigm), label = r\"$ \\alpha=0.75 $\")\nax[0][0].plot(x, levy.levy(x, 1.0, beta_1, mu, sigm), label = r\"$ \\alpha=1.0 $\")\nax[0][0].plot(x, levy.levy(x, 1.25, beta_1, mu, sigm), label = r\"$ \\alpha=1.25 $\")\nax[0][0].plot(x, levy.levy(x, 1.5, beta_1, mu, sigm), label = r\"$ \\alpha=1.5 $\")\n\nax[0][0].set_title(r\"Симетричні $\\alpha$-стабільні розподіли, $ \\beta = 0 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[0][0].legend(fontsize=20)\n\nax[0][0].set_xlabel(r\"$ x $\")\nax[0][0].set_ylabel(r\"$ f_{\\alpha}(x) $\")\n\n\nax[0][1].plot(x, levy.levy(x, 0.5, beta_2, mu, sigm), label = r\"$ \\alpha=0.5 $\")\nax[0][1].plot(x, levy.levy(x, 0.75, beta_2, mu, sigm), label = r\"$ \\alpha=0.75 $\")\nax[0][1].plot(x, levy.levy(x, 1.0, beta_2, mu, sigm), label = r\"$ \\alpha=1.0 $\")\nax[0][1].plot(x, levy.levy(x, 1.25, beta_2, mu, sigm), label = r\"$ \\alpha=1.25 $\")\nax[0][1].plot(x, levy.levy(x, 1.5, beta_2, mu, sigm), label = r\"$ \\alpha=1.5 $\")\n\nax[0][1].set_title(r\"Зміщенні $\\alpha$-стабільні розподіли, $ \\beta = 1 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[0][1].legend(fontsize=20)\n\nax[0][1].set_xlabel(r\"$ x $\")\nax[0][1].set_ylabel(r\"$ f_{\\alpha}(x) $\")\n\n\nax[1][0].plot(x, levy.levy(x, 0.5, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=0.5 $\")\nax[1][0].plot(x, levy.levy(x, 0.75, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=0.75 $\")\nax[1][0].plot(x, levy.levy(x, 1.0, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.0 $\")\nax[1][0].plot(x, levy.levy(x, 1.25, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.25 $\")\nax[1][0].plot(x, levy.levy(x, 1.5, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.5 $\")\n\nax[1][0].set_title(r\"Симетричні $\\alpha$-стабільні розподіли, $ \\beta = 0 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\n\nax[1][0].legend(fontsize=20, loc=\"lower right\")\n\nax[1][0].set_xlabel(r\"$ x $\")\nax[1][0].set_ylabel(r\"$ F_{\\alpha}(x) $\")\n\nax[1][1].plot(x, levy.levy(x, 0.5, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=0.5 $\")\nax[1][1].plot(x, levy.levy(x, 0.75, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=0.75 $\")\nax[1][1].plot(x, levy.levy(x, 1.0, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.0 $\")\nax[1][1].plot(x, levy.levy(x, 1.25, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.25 $\")\nax[1][1].plot(x, levy.levy(x, 1.5, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.5 $\")\n\nax[1][1].set_title(r\"Зміщенні $\\alpha$-стабільні розподіли, $\\beta = 1$, $\\mu = 0$, $\\sigma = 1$\", y=1.03, fontsize=20)\n\nax[1][1].legend(fontsize=20, loc=\"lower right\")\n\nax[1][1].set_xlabel(r\"$ x $\")\nax[1][1].set_ylabel(r\"$ F_{\\alpha}(x) $\")\n\nfig.tight_layout()\nplt.show();\n\n\n\n\nРисунок 9.2: Залежність функції щільності ймовірностей альфа-стабільного розподілу Леві та кумулятивної функції щільності від різних значень параметрів розподілу\n\n\n\n\n\n\n9.3.11 Задання ширини вікна та кроку\nУ даному блоці оберемо тип ряду для якого і виконуватимуться розрахунки. Перед нами представлено 6 варіантів представлення часового ряду. Виконуватимемо подальші обчислення для стандартизованих прибутковостей, оскільки згідно багатьом роботам було показано, що розподіл прибутковостей тих же самих фінансових активів (фондових індексів, валютних, криптовалют, тощо) виходить за межі нормального Гауссового розподілу. Покажемо це у даному ноутбуці та застосуємо альфа-стабільний розподіл Леві для кращого моделювання складних систем та передчасної ідентифікації кризових явищ.\n\nwindow = 250 # ширина сковзного вікна\ntstep = 1 # крок\n\nret_type = 4 # тип ряду: 1 - вихідний, 2 - детрендований (різниця між теперішнім значенням та попереднім)\n                        # 3 - стандартизовані прибутковості, 4 - стандартизований ряд, 5 - абсолютні значення (волатильність)\n                        # 6 - стандартизований вихідний ряд\n        \nlength = len(time_ser)\n\n\n\n9.3.12 Розраховуємо параметри Леві, використовуючи алгоритм сковзного вікна\n\nalpha = []\nbeta = []\nmu = []\nsigma = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент та в подальшому відбираємо потрібний тип ряду\n    if ret_type == 1:\n        pass\n    elif ret_type == 2:\n        fragm = fragm.diff()\n    elif ret_type == 3:\n        fragm = fragm.pct_change()\n    elif ret_type == 4:\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5: \n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        fragm = fragm.abs()\n    elif ret_type == 6:\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values \n    \n    params = levy.fit_levy(fragm)\n    \n    a, b, m, s = params[0].get('0')\n    \n    alpha.append(a)\n    beta.append(b)\n    mu.append(m)\n    sigma.append(s)\n\n100%|██████████| 5995/5995 [24:13&lt;00:00,  4.12it/s]\n\n\n\n\n9.3.13 Зберігаємо значення у текстовому документі\n\nnp.savetxt(f\"alpha_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", alpha)\nnp.savetxt(f\"beta_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", beta)\nnp.savetxt(f\"mu_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", mu)\nnp.savetxt(f\"sigma_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", sigma)\n\n\n\n9.3.14 Визначимо функцію для побудови парних графіків\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n\n9.3.15 Виводимо динаміку показника стабільності \\(\\alpha\\)\n\nmeasure_label = r'$\\alpha$'\nfile_name = f\"alpha_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alpha, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 9.3: Динаміка фондового індексу BSESN та показника стабільності\n\n\n\n\n\n\n9.3.16 Виводимо динаміку індексу асиметрії \\(\\beta\\)\n\nmeasure_label = r'$\\beta$'\nfile_name = f\"beta_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          beta, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 9.4: Динаміка фондового індексу BSESN та показника асиметрії\n\n\n\n\n\n\n9.3.17 Виводимо динаміку параметру зміщення \\(\\mu\\)\n\nmeasure_label = r'$\\mu$'\nfile_name = f\"mu_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          mu, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 9.5: Динаміка фондового індексу BSESN та показника зміщення\n\n\n\n\n\n\n9.3.18 Виводимо динаміку параметра масштабу \\(\\sigma\\)\n\nmeasure_label = r'$\\sigma$'\nfile_name = f\"sigma_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          sigma, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\nРисунок 9.6: Динаміка фондового індексу BSESN та показника масштабу"
  }
]