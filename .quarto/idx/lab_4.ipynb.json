{"title":"Лабораторна робота № 4","markdown":{"yaml":{"title":"Лабораторна робота № 4"},"headingText":"Теоретичні відомості","containsRefs":false,"markdown":"\n\n\n\n\n**Тема.** Інформаційні методи оцінки складності.\n\n**Мета.** Навчитися використовувати основні показники складності з теорії інформації для аналізу часових рядів.\n\n\n\n\n### Складність. Кількісні міри складності. Інформаційні методи оцінки складності.\n\nДане століття називають століттям складності. Сьогодні питання \"що таке складність?\" вивчають фізики, біологи, математики і інформатики, хоча при теперішніх досягненнях у розумінні оточуючого світу, однозначної відповіді на це питання немає. \n\nЗ цієї причини, відповідно до ідеї І. Пригожина, будемо досліджувати прояви складності системи, застосовуючи при цьому сучасні методи кількісного аналізу складності [@e19070310]. \n\nСеред таких методів на увагу заслуговують: \n\n- інформаційно-ентропійні; \n- засновані на теорії хаосу; \n- скейлінгово-мультифрактальні.\n\nЗрозуміло, виходячи з різної природи методів, покладених в основу формування міри складності, вони приділяють певні вимоги до часових рядів, що слугують вхідними даними. Наприклад, перші дві групи методів вимагають стаціонарності вхідних даних. При цьому мають різну чутливість до таких характеристик, як детермінованність, стохастичність, причинність та кореляції. Тому у подальшому, порівнюючи комплексно ефективність різних показників складності, на вказані обставини ми будемо звертати увагу, підкреслюючи спеціально застосовність того чи іншого показника для характеристики різних сторін складності досліджуваних систем.\n\nРозгляд першої групи методів почнемо з добре відомої міри складності, запропонованої А. Колмогоровим [@doi:10.1080/00207166808803030].\n\n**Колмогорівська складність**. Поняття колмогорівської складності (або, як ще говорять, алгоритмічної ентропії) з'явилося в 1960-і роки на стику теорії алгоритмів, теорії інформації і теорії ймовірностей. \n\nІдея А. Колмогорова полягала в тому, щоб вимірювати кількість інформації, що міститься в індивідуальних скінчених об'єктах (а не у випадкових величинах, як у шеннонівській теорії інформації). Виявилось, що це можливо (хоча лише з точністю до обмеженого доданку). А. Колмогоров запропонував вимірювати кількість інформації в скінчених об’єктах за допомогою теорії алгоритмів, визначивши складність об'єкту як мінімальну довжину програми, що породжує цей об'єкт. Дане визначення стало базисом алгоритмічної теорії інформації, а також алгоритмічної теорії ймовірностей: об'єкт вважається випадковим, якщо його складність наближена до максимальної.\n\nЩо ж собою являє колмогорівська складність і як її виміряти? На практиці ми часто стикаємося з програмами, які стискують файли (для економії місця в архіві). Найбільш поширені називаються zip, gzip, compress, rar, arj та інші. Застосувавши таку програму до деякого файлу (з текстом, даними, програмою), ми отримуємо його стислу версію (яка, як правило, коротше початкового файлу). За нею можна відновити початковий файл з допомогою парної програми-\"декомпресора\". Отже, у першому наближенні колмогорівську складність файлу можна описати як довжину його стислої версії. Тим самим файл, що має регулярну структуру і добре стискуваний, має малу колмогорівську складність (порівняно з його довжиною). Навпаки, погано стискуваний файл має складність, близьку до довжини.\n\nПрипустимо, що ми маємо фіксований спосіб опису (декомпресор) $D$. Для даного слова $x$ розглянемо всі його описи, тобто всі слова $y$, для яких $D(y)$ визначене $і$ рівне $x$. Довжину найкоротшого з них $l(y)$ і називають колмогорівською складністю слова $x$ при даному способі опису $D$:\n\n$$\nKS_{D}(x) = \\min\\{l(y)\\,|\\,D(y)=x\\},\n$$\n\nде $l(y)$ позначає довжину слова $y$. Індекс $D$ підкреслює, що визначення залежить від вибору способу $D$. \n\nМожна показати, що існують оптимальні способи опису. Спосіб опису тим краще, чим він коротше. Тому природно дати таке визначення: спосіб $D_1$ не гірше за спосіб $D_2$, якщо $KS_{D_1}(x) \\leq KS_{D_2}(x)+c$ при деякому $c$ і при всіх $x$.\n\nОтже, за Колмогоровим, складність об'єкту (наприклад, тексту --- послідовності символів) --- це довжина мінімальної програми яка виводить даний текст, а ентропія --- це складність, що ділиться на довжину тексту. Також можна розглядати алгоритмічну складність як мінімальний час (або інші обчислювальні ресурси), необхідний для виконання цієї задачі на комп'ютері. А ще ми можемо говорити про комунікаційну складність завдань, в яких задіяно більше одного процесора: це кількість бітів, які потрібно передати при розв'язанні цього завдання [@Bonchev2009;@10.1162/ARTL_a_00157]. На жаль, це визначення чисто умоглядне. Надійного способу однозначно визначити цю програму не існує. Але є алгоритми, які фактично якраз і намагаються обчислити колмогорівську складність тексту [@Li2008] і ентропію [@shannon]. \n\n### Оцінка складності Колмогорова за схемою Лемпела-Зіва\n\nУніверсальна (в сенсі застосовності до різних мовних систем) міра складності кінцевої символьної послідовності була запропонована Лемпелем і Зівом (LZ) [@1055501]. Складність Лемпеля-Зіва (LZC) є класичною мірою, яка для ергодичних джерел пов'язує поняття складності (у розумінні Колмогорова-Чайтіна) та швидкості ентропії [@PhysRevE.84.036214;@ZOZOR2005285]. Для ергодичного динамічного процесу кількість нової інформації, отриманої за одиницю часу (швидкість ентропії), може бути оцінена шляхом вимірювання здатності цього джерела генерувати нові патерни. Завдяки простоті методу LZC, швидкість ентропії може бути оцінена з однієї дискретної послідовності вимірювань з низькими обчислювальними витратами [@10.1063/1.4808251]. У рамках їх підходу складність послідовності оцінюється числом кроків процесу, що її породжує. Припустимими (редакційними) операціями при цьому є: \n\n1. генерація символу (необхідна, як мінімум, для синтезу елементів алфавіту) і\n2. копіювання \"готового\" фрагмента з передісторії (тобто з уже синтезованої частини тексту). \n\nНехай $\\Sigma$ --- скінчений алфавіт, $S$ --- текст (послідовність символів), складений з \nелементів $\\Sigma$; $S[i]$ --- $i$-й символ тексту; $S[i:j]$ --- фрагмент тексту з $i$-го по $j$-й символ включно $(i<j)$; $N=|S|$ --- довжина тексту $S$. Тоді схему синтезу послідовності можна представити у вигляді конкатенації\n\n$$\nH(S)=S[1:i_1]S[i_1+1:i_2]...S[i_{k-1}+1:i_k]...S[i_{m−1}+1:N], \n$$ {#eq-4-1}\n\nде $S[i_{k−1}+1:i_k]$ --- фрагмент $S$, породжуваний на $k$-му кроці, а $m=m_{H}(S)$ --- число кроків \nпроцесу. З усіляких схем породження $S$ обирається мінімальна за числом кроків. Таким чином, складність послідовності $S$ за LZ\n\n$$\nc_{LZ}(S) = \\min_{H}\\{ m_{H}(S) \\}. \n$$\n\nМінімальність числа кроків забезпечується вибором для копіювання на кожному кроці максимально довгого прототипу з передісторії. Якщо позначити через $j(k)$ номер позиції, з якої починається копіювання на $k$-му кроці, то довжина фрагмента копіювання \n\n$$\nl_{j(k)} = i_k - i_{k-1} - 1 = \\max_{j \\leq i_{k-1}}\\{ l_{j} : S[i_{k-1}+1:i_{k-1}+l_j]=S[j:j+l_{j}-1] \\},\n$$ {#eq-4-2}\n\nа сам $k$-й компонент складнісного розкладання (@eq-4-1) можна записати у вигляді \n\n$$\nS[i_{k-1}+1:i_{k}] = \n\\begin{cases}\n    S[j(k):j(k)+l_{j(k)}-1] & \\textrm{if} \\; j(k) \\neq 0, \\\\\n    S[i_{k-1}+1] & \\textrm{if} \\; j(k) = 0.\n\\end{cases} \n$$ {#eq-4-3}\n\nВипадок $j(k) = 0$ відповідає ситуації, коли в позиції $i_{k−1}+1$ стоїть символ, який раніше не зустрічався. При цьому ми застосовуємо операцію генерації символу. \n\nБудемо знаходити складність за LZ для часового ряду, який являє собою, наприклад, щоденні значення індексу фондового ринку або криптовалютного. Для дослідження динаміки LZ та порівняння з іншими складними системами будемо знаходити дану міру складності для підряду фіксованої довжини (вікна). Для цього обчислимо логарифмічні прибутковості та перетворимо їх у послідовність бітів. При цьому можна задавати кількість станів, які диференційовані (система числення). Так, для двох різних станів маємо 0, 1, для трьох --- 0, 1, 2 і т.д. Для двійкової системи кодування буде задаватися поріг по середньому значенню і стани, наприклад, прибутковостей (*ret*) кодуватимуться наступним чином [@Giglio_2008;@RePEc:ebl:ecbull:eb-11-00319;@RePEc:pra:mprapa:22720]:\n\n$$\nret = \\begin{cases}\n    0, & ret_t < \\langle ret \\rangle \\\\\n    1, & ret_t > \\langle ret \\rangle.\n    \\end{cases} \n$$ {#eq-4-4}\n\n\nТакож можна визначити так звану пермутаційну складність Лемпеля-Зіва (PLZС) [@BAI2015102;@e23070832]. У даному випадку би будемо опиратись на процедуру реконструкції фазового простору, що згадувалась в лабораторних 2 і 3. Згідно пермутаційній процедурі ми будемо брати фрагмент ряду довжини $m$, що слугує розмірностю реконструйованого атрактора, та замінювати кожне значення ряду його порядковим індексом. На подальшому ресунку представлено часовий ряд та його можливі порядкові шаблони:\n\n::: {#fig-permutation}\n\n![](Images\\lab_4\\Permutation-entropy-method-Permutation-entropy-PE-was-calculated-for-both-unaveraged.png)\n\nФрагмент часового ряду (а) та 6 можливих порядкових шаблонів, що можуть бути в цьому сигналі (b) [@hillen2013joint]\n\n:::\n\nАлгоритм Лемпеля-Зіва виконує дві операції: (1) додає новий біт в уже існуючу послідовність; (2) копіює вже сформовану послідовність. Алгоритмічна складність представляє собою кількість таких операцій, необхідних для формування заданої послідовності.\n\nДля випадкової послідовності довжини $n$ алгоритмічна складність обчислюється за виразом $LZC_r = n / \\log(n)$. Тоді відносна алгоритмічна складність знаходиться як відношення отриманої складності до складності випадкової послідовності: $LZC = LZC / LZC_{r}$.\n\nОднак навіть цього підходу може бути недостатньо. Справа в тому, що складні сигнали проявляють притаманну їм складність на різних просторових і часових масштабах, тобто мають масштабно інваріантні властивості. Вони, зокрема проявляються через степеневі закони розподілу. Тому розрахунки алгоритмічної складності на \"поверховому\" масштабі сигналу можуть бути неприйнятними і призводити до помилкових висновків. \n\nДля подолання таких труднощів використовуються мультимасштабні методи, до розгляду яких ми і переходимо.\n\n### Процедура грануляції для мультискейлінгового дослідження часових рядів. Мультимасштабні міри складності \n\nІдея цієї групи методів включає дві послідовно виконувані процедури: \n\n1. процес \"грубого дроблення\" (coarse graining --- \"грануляції\") початкового часового ряду --- усереднення даних на сегментах, що не перетинаються, розмір яких (вікно усереднення) збільшуватиметься на одиницю при переході на наступний за величиною масштаб;\n2. обчислення на кожному з масштабів певного (до сих пір мономасштабного) показника складності. \n\nПроцес \"грубого дроблення\" (\"грануляція\") полягає в усереднені послідовних відліків ряду в межах вікон, що не перетинаються, а розмір яких $\\tau$ --- збільшується при переході від масштабу до масштабу. Кожен елемент \"гранульованого\" часового ряду $y_{j}^{\\tau}$ знаходиться у відповідності до виразу [@costa2008multiscale]:\n\n$$\ny_{j}^{\\tau} = \\frac{1}{\\tau}\\sum_{i=(j-1)\\tau+1}^{j\\tau}x_i, \\; 1 \\leq j \\leq N/\\tau,\n$$\n\nде $\\tau$ характеризує фактор масштабування. Довжина кожного \"гранульованого\" ряду залежить від розміру вікна $і$ рівна $N/\\tau$. Для масштабу рівного 1 \"гранульований\" ряд просто тотожний оригінальному.\n\n::: {#fig-granulation}\n\n![](Images\\lab_4\\3-Figure1-1.png){width=80%}\n\nСхематична ілюстрація процесу грубого дроблення (\"грануляції\") початкового часового ряду для масштабів 2 і 3\n\n:::\n\nБібліотека `neurokit2` представляє метод для обчислення як мономасштабного показника складності Лемпеля-Зіва, так і його мультимасштабного аналогу. \n\nСинтаксис **мономасштабної** процедури виглядає наступним чином:\n\n**`complexity_lempelziv(signal, delay=1, dimension=2, permutation=False, symbolize='mean', **kwargs)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень.\n- **delay** (*int*) --- часова затримка, $\\tau$. Використовується лише тоді, коли `permutation=True`.\n- **dimension** (*int*) --- розмірність вкладень, $m$. Використовується лише коли `permutation=True`.\n- **permutation** (*bool*) --- якщо значення `True`, поверне складність Лемпеля-Зіва на основі порядкових патернів.\n- **symbolize** (*str*) --- використовується тільки коли `permutation=False`. Метод перетворення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення `None`, щоб пропустити процес (якщо вхідний сигнал вже є дискретним). Можна скористатися методом `complexity_symbolize()` для використання іншої процедури символізації ряду.\n- **kwargs** --- інші аргументи, які передаються до `complexity_ordinalpatterns()` (якщо `permutation=True`) або `complexity_symbolize()`.\n\n**Повертає**\n\n- **lzc** (*float*) --- складність Лемпеля-Зіва (LZC).\n- **info** (*dict*) --- словник, що містить додаткову інформацію про параметри, які використовуються для обчислення LZC.\n\nСинтаксис мультимасштабної процедури вже інший:\n\n**`entropy_multiscale(signal, scale='default', dimension=3, tolerance='sd', method='MSEn', show=False, **kwargs)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму.\n- **scale** (*str* або *int* або *list*) --- список масштабних коефіцієнтів, що використовуються для процедури крос-грануляції часового ряду. Якщо значення `\"default\"`, буде використано `range(len(signal) / (dimension + 10))`. Якщо `\"max\"`, використовуватиме всі масштаби до половини довжини сигналу. Якщо ціле число, створить діапазон до вказаного цілого числа.\n- **dimension** (*int*) --- розмірність вкладення, $m$.\n- **tolerance** (*float*) --- поріг пропускання $\\varepsilon$ (часто позначається як $r$), відстань, на якій дві точки даних вважаються подібними. Якщо `\"sd\"` (за замовчуванням), буде встановлено значення $0.2 \\cdot SD_{signal}$.\n- **method** (*str*) --- яку версію мультимасштабного показника обчислювати. Переважна кількість показників за цим методом відповідають саме ентропійним підходам. Нас цікавитиме саме `\"LZC\"`.\n- **show** (*bool*) --- візуалізувати залежність показника від масштабу.\n- **kwargs** --- необов'язкові аргументи.\n\n**Повертає**\n\n- *float* --- точкова оцінка мультимасштабного показника окремого часового ряду, що відповідає площі під кривою значень цього показника, яка, по суті, є сумою вибіркових значень, наприклад, `\"LZC\"` в діапазоні масштабних коефіцієнтів.\n- *dict* --- словник, що містить додаткову інформацію про параметри, які використовуються для обчислення мультимасштабного показника. Значення показника, що відповідають кожному фактору `\"Scale\"`, зберігаються під ключем `\"Value\"`.\n\n### Шеннонівська складність \n\nЕнтропійний аналіз часових рядів за допомогою ентропійних показників різного роду буде проведено у наступних роботах. Зараз же ми розглянемо найпростішу з ентропій --- ентропію Шеннона та порівняємо її можливості кількісно оцінювати складність часових послідовностей у порівнянні з мірою Лемпеля-Зіва. \n\nЕнтропія Шеннона --- це статистичний квантифікатор, який широко використовується для характеристики складних процесів. Він здатний виявляти аспекти нелінійності в досліджуваних сигналах, сприяючи більш надійному поясненню нелінійної динаміки різних точок аналізу, що, в свою чергу, покращує розуміння природи складних систем, які характеризуються складністю та нерівноважністю. Окрім складності та нерівноважності, більшість, але не всі, складні системи також характеризуються неоднорідним розподілом зв'язків. Поняття ентропії було використано Шенноном в теорії інформації для передачі даних [@shannon].\n\nЕнтропія - це міра невизначеності та випадковості в системі. Якщо припустити, що всі наявні дані належать до одного класу, то неважко передбачити клас нових даних. У цьому випадку ентропія дорівнює 0. Будучи величиною між 0 і 1, коли всі ймовірності рівні, ентропія набуває найбільшого значення. Невизначеність, що виникає, коли подія $E$ відбувається з ймовірністю $p$, можна позначити як $S(p)$. Якщо ймовірність появи класу дорівнює 1, тоді ентропія мінімальна, $S(1) = 0$. Відповідно до концепції Шеннона, якщо у нас наявні ймовірності реалізації певної події $p_1, p_2, p_3, ..., p_n$, на виході отримується кількість інформації, що необхідна для опису цієї події. Тоді, Шеннонівська ентропія може бути визначена як\n\n$$\nS = -\\sum_{i=1}^{n}p_i \\ln p_{i}.  \n$$\n\nСинтаксис методу для розрахунку Шеннонівської ентропії виглядає наступним чином:\n\n**`entropy_shannon(signal=None, base=2, symbolize=None, show=False, freq=None, **kwargs)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень.\n- **base** (*float*) --- основа логарифму, що за замовчуванням дорівнює 2, що дає одиницю в бітах. Зауважте, що `scipy.stats.entropy()` за замовчуванням використовує число Ейлера (`np.e`) (натуральний логарифм), що дає міру інформації, виражену в натах.\n- **symbolize** (*str*) --- метод приведення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням дорівнює нулю, що пропускає процес (і припускає, що вхідні дані вже є дискретними).\n- **show** (*bool*) --- якщо значення `True`, виводить дискретність сигналу.\n- **freq** (*np.array*) --- замість сигналу можна надати вектор ймовірностей.\n- **kwargs** --- необов'язкові аргументи. Наразі не використовуються.\n\n**Повертає**\n\n- **shanen** (*float*) --- Шеннонівську ентропію.\n- **info** (*dict*) --- словник, що містить додаткову інформацію про параметри, які використовуються для обчислення Шеннонівської ентопії.\n\n\n### Інформація Фішера\n\nІнформацію Фішера було введено Р. А. Фішером у 1922 році як міру \"внутрішньої точності\" в теорії статистичних оцінок [@doi:10.1098/rsta.1922.0009]. Вона є центральною для багатьох статистичних галузей, що виходять далеко за межі теорії складності. Даний показник вимірює кількість інформації, яку спостережувана випадкова величина несе про невідомий параметр. В аналізі складності вимірюється кількість інформації, яку система несе \"про себе\". Він базується на розкладанні за сингулярним значенням реконструйованого фазового простору. Значення показника Фішера зазвичай антикорельоване з іншими показниками складності (чим більше інформації система приховує про себе, тим більш передбачуваною і, відповідно, менш складною вона є).\n\nІнформацію Фішера можна визначити, використовуючи метод `fisher_information()` бібліотеки `neurokit2`. Її синтаксис виглядає наступним чином:\n\n**`fisher_information(signal, delay=1, dimension=2)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень.\n- **delay** (*int*) --- затримка в часі, $\\tau$.\n- **dimension** (*int*) --- розмірність векторів фазового простору, $m$.\n\n**Повертає**\n\n- **fi** (*float*) --- обчислена міра інформації Фішера.\n- **info** (*dict*) --- словник, що містить додаткову інформацію про параметри, які використовуються для обчислення інформації Фішера.\n\n### Складність та параметри Хьорта\n\nПараметри Хьорта --- це показники статистичних властивостей, які спочатку були введені Хьортом [@HJORTH1970306] для опису загальних характеристик сигналів електроенцифалограми у кількох кількісних термінах, але які можуть бути застосовані до будь-якого часового ряду. Параметрами є активність, рухливість і складність: \n\n- Параметр **активності** ($Activity$) --- це просто дисперсія сигналу, яка відповідає середній потужності сигналу (якщо його середнє значення дорівнює 0).\n\n$$\nActivity = \\sigma^{2}_{signal}.\n$$\n\n- Параметр **рухливості** ($Mobility$) являє собою середню частоту або частку середньоквадратичного відхилення спектра потужності. Він визначається як квадратний корінь з дисперсії першої похідної сигналу, поділений на дисперсію сигналу.\n\n$$\nMobility = \\frac{\\sigma_{dd}/\\sigma_{d}}{Complexity}. \n$$\n\n- Параметр **складності** ($Complexity$) дає оцінку смуги пропускання сигналу, яка вказує на схожість форми сигналу з чистою синусоїдою (для якої значення сходиться до 1). Іншими словами, це міра \"надмірної деталізації\" по відношенню до \"найм'якшої\" можливої форми кривої. Параметр \"Складність\" визначається як відношення рухливості першої похідної сигналу до рухливості самого сигналу.\n\n$$\nComplexity = \\frac{\\sigma_d}{\\sigma_{signal}}, \n$$\n\nде $d$ та $dd$ представляють перші та другі похідні сигналу, відповідно. \n\nБібліотека `neurokit2` представляє метод для отримання відповідних показників. Її синтаксис виглядає наступним чином:\n\n**`complexity_hjorth(signal)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень.\n\n**Повертає**\n\n- **hjorth** (*float*) --- показник складності Хьорта. \n- **info** (*dict*) --- словник, що містить додаткові показники Хьорта, такі як `\"Mobility\"` та `\"Activity\"`. \n\n### Час декореляції\n\nЧас декореляції (decorrelation time, DT) визначається як час (у відліках) першого перетину нуля функції автокореляції. Коротший час декореляції відповідає менш корельованому сигналу. Наприклад, зменшення часу декореляції в сигналах електроенцифалограми спостерігається перед нападами, що пов'язано зі зменшенням потужності низьких частот [@MORMANN2005569].\n\nБібліотека `neurokit2` представляє функціонал для визначення часу декореляції, а саме метод `complexity_decorrelation()`. Її синтаксис є наступним:\n\n**`complexity_decorrelation(signal)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (часовий ряд) у вигляді вектора значень.\n\n**Повертає**\n\n- *float* --- час декореляції. \n- *dict* --- словник, що містить додаткову інформацію про додаткові показники.  \n\n\n### Відносна грубість (нерівність, шорсткість)\n\nВідносна шорсткість --- це відношення локальної дисперсії (автоковаріації з лагом 1) до глобальної дисперсії (автоковаріації з лагом 0), яке можна використовувати для класифікації різних \"шумів\". Його також можна використовувати як індекс для перевірки застосовності фрактального аналізу (показники фрактальності будуть використовуватись у наступних роботах) [@10.3389/fphys.2012.00208]. \n\nСинтаксис даного методу в бібліотеці `neurokit2` виглядає наступним чином:\n\n**`complexity_relativeroughness(signal, **kwargs)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (часовий ряд) у вигляді вектора значень.\n- **kwargs** (*optional*) --- інші аргументи, що потребуються методу `nk.signal_autocor()`. \n\n**Повертає**\n\n- **rr** (*float*) --- значення відносної грубості. \n- **info** (*dict*) --- словник, що містить інформацію відносно параметрів, що використовувались для обчислення показника грубості. \n\n### Взаємна інформація\n\nКоли йдеться про виявлення зв'язків між змінними, ми часто використовуємо кореляцію Пірсона. Проблема полягає в тому, що цей показник знаходить лише *лінійні* зв'язки, що іноді може призвести до неправильної інтерпретації зв'язку між двома змінними. Тим не менш, інші статистичні методи вимірюють нелінійні зв'язки, такі як **взаємна інформація (mutual information, MI)** [@cover1999elements].\n\nВзаємна інформація між двома випадковими величинами вимірює нелінійний зв'язок між ними. Крім того, вона показує, **скільки інформації можна отримати з випадкової величини**, спостерігаючи за іншою випадковою величиною.\n\nВона тісно пов'язана з поняттям ентропії. Тобто, зменшення невизначеності випадкової величини пов'язане з отриманням інформації з іншої випадкової величини. Отже, високе значення взаємної інформації вказує на велике зменшення невизначеності, тоді як низьке значення вказує на мале зменшення. Якщо взаємна інформація дорівнює нулю, це означає, що дві випадкові величини є незалежними.\n\nВзаємну інформацію можна розрахувати наступним чином:\n\n$$\nI(X; Y) = \\sum_{y \\in Y}\\sum_{x \\in X}p(x, y) \\cdot \\log{\\left( \\frac{p(x,y)}{p(x)p(y)} \\right)}, \n$$\n\nде $p(x)$ та $p(y)$ ймовірності спостереження окремо $x$ або $y$, а $p(x,y)$ ймовірність спостереження одночасно $x$ та $y$.\n\nОсновна відмінність між кореляцією та взаємною інформацією полягає в тому, що **кореляція є мірою лінійної залежності**, тоді як взаємна інформація вимірює загальну залежність (включаючи **нелінійні** зв'язки). Тому взаємна інформація виявляє залежності, які не залежать тільки від **коваріації**. Таким чином, взаємна інформація дорівнює нулю, коли дві випадкові величини є строго незалежними.\n\nБібліотека `neurokit2` представляє інструментарій для знаходження взаємної інформації між двома сигналами $x$ та $y$. У даній роботі ми спробуємо віднайти взаємну інформацію як між двома часовими рядами, так і **авто-взаємну інформацію**, подібно до автокореляції. \n\nСинтаксис потрібної нам процедури виглядає наступним чином:\n\n**`mutual_information(x, y, method='varoquaux', bins='default', **kwargs)`**\n\n**Параметри**\n\n- **x** (*Union[list, np.array, pd.Series]*) --- масив значень.\n- **y** (*Union[list, np.array, pd.Series]*) --- масив значень.\n- **method** (*str*) --- метод для обчислення взаємної інформації: `\"nolitsa\"`, `\"varoquaux\"`, `\"knn\"`, `\"max\"`. \n- **bins** (*int*) --- кількість бінів гістограми. Використовується лише для `\"nolitsa\"` та `\"varoquaux\"`. Якщо `\"default\"`, кількість бінів оцінюється згідно методики Hacine-Gharbi (2018).\n- **kwargs** --- додаткові ключові аргументи для обраного методу. \n\n**Повертає**\n\n- *float* --- розрахована взаємна інформація. \n\nІснують різноманітні підходи до розрахунку взаємної інформації:\n\n- **nolitsa**: Класична взаємна інформація (трохи швидше, ніж метод `\"sklearn\"`).\n- **varoquaux**: Застосовує фільтр Гауса до об'єднаної гістограми. Величину згладжування можна налаштовувати за допомогою аргументу `sigma` (за замовчуванням `sigma=1`).\n- **knn**: Непараметрична (тобто не заснована на біннінгу) оцінка за найближчими сусідами. Додаткові параметри включають `k` (за замовчуванням, `k=3`), кількість найближчих сусідів для використання.\n- **max**: Максимальний коефіцієнт взаємної інформації, тобто $MI$ є максимальним при певній комбінації кількості бінів.\n\nІснує безліч різноманітних показників складності, що базуються на теорії інформації та інших парадигах, які ми ще представлятимемо в подальшому. Розглянемо ефективність використання зазначених показників у якості індикаторів або індикаторів-передвісників крахових подій.   \n\n## Хід роботи\n\nСпочатку імпортуємо необхідні модулі для подальшої роботи:\n\nІ виконаємо налаштування рисунків для виведення:\n\nЦього разу розглянему можливість побудови індикаторів-передвісників на прикладі фондового індексу S&P 500, але, окрім цього, додамо ще Біткоїн для розрахунку взаємної інформації між фондовим ринком та криптовалютним. Очевидно, що фондовий індекс S&P 500 мав би проіснувати довше за Біткоїн. До того ж, криптовалютний ринок працює безперервно на відміну від фондового, а тому треба буде об'єднати значення двох активів за тими датами що співпадають.  \n\n**Виконуємо зчитування фондового індексу**:\n\n**Виконуємо зчитування криптовалютного індексу**:\n\n::: {.callout-warning}\n## Увага\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо. \n\n:::\n\n---\n\n---\n\nТепер створимо новий масив даних, що об'єднуватиме в собі значення S&P 500 та BTC по їх спільним датам:\n\nВиводимо отриману базу:\n\nІ візуалізуємо сам графік. Спочатку оголосимо функцію для попарної візуалізації рядів зі збереженням їх абсолютних значень:\n\nі тепер візуалізуємо отримані ряди:\n\n::: {.callout-important}\n## Важливо\n\nНе виконуйте блоки коду, що відповідають секції \"Розрахунок взаємної інформації\", якщо ви працюєте з текстовим файлом. \n\n:::\n\n### Розрахунок взаємної інформації\n\nРозглянемо взаємну інформацію як індикатор нелінійної кореляції між двома фінансовими активами, і спробуємо сказати, чи є між ними \"істинний\" взаємозв'язок. Виконуватимемо розрахунки із використанням алгоритму руховому вікна. Також визначимо функцію `transform()` для нормалізації ряду.  \n\nТепер приступимо до розрахунків:\n\nЗберігаємо отриманий результат у текстовому файлі:\n\nВізуалізуємо результат між відповідними показниками:\n\nЯк ми можемо бачити з представленого рисунку, на фондовому та криптовалютному ринках дійсно спостерігалися фази зростання взаємної інформації між ними. Найкраще це видно напередодні кризи 2018-го року, під час 2019, після коронавірусної пандемії та напередодні 2023 року. Для даного індикатора залишається простір для експериментів, що можуть вивести його на рівень достатньо потужного передвісника криз на фондовому ринку чи криптовалютному. \n\nЯк вже зазначалося, окрім обчислення взаємної інформації для двох пар часових сигналів, ми можемо обчислити *авто*взаємну інформація, тобто взаємну інформацію ряду самого із собою по різним часовим лагам, як це було пророблено для автокореляції. Недолік автокореляції полягає в тому, що вони визначає саме *лінійний* зв'язок теперішніх значень з попередніми. Автовзаємна інформація в свою чергу є показником *нелінійного* зв'язку теперішніх значень із попередніми. \n\nДля обчислення автовзаємної інформації визначимо наступну функцію: \n\nВиведемо залежність автовзаємної інформації від лагу для всього ряду S&P 500 та Біткоїна. Спочатку розрахуємо вихідні значення ряду, далі прибутковості і потім волатильності. Для кожного з відповідних сигналів виведемо взаємну інформацію. \n\n**Виконуємо перетворення S&P 500 та Біткоїна**\n\n**Розраховуємо автовзаємну інформацію S&P 500 та Біткоїна**\n\nЯк ми можемо бачити з представлених графіків, ступінь взаємної інформації це показник, що найкращим чином працює саме для вихідних значень часових сигналів. Для вихідного ряду ступінь взаємної інформації залишається доволі високим. Для прибутковостей і волатильностей взаємна інформація спадає одразу на першому лагу, що свідчить про незалежність значень на подальших часових затримках.  \n\n### Розрахунок мономасштабної складності Лемпеля-Зіва\n\nПродовжимо розраховувати й інші показники складності. Розглянемо можливість використання показника складності Лемпеля-Зіва в якості індикатора катастрофічних подій.\n\nЗберігаємо результати в текстових файлах:\n\nТа візуалізуємо їх:\n\nНа даному рисунку видно, що 2 міри поводять себе асиметрично по відношенню один до одного: $LCZ$ вказує на зростання складності, наприклад, події 2019 року. У той же час $PLCZ$ вказує на спад складності системи в цей період. Варто дослідити мультимасштабну динаміку міри Лемпеля-Зіва для більш змістовних висновків. \n\n### Обчислення мультимасштабної складності Лемпеля-Зіва\n\n**Мультимасштабна динаміка пермутаційного показника складності Лемпеля-Зіва**\n\nТепер розрахуємо віконну динаміку мультимасштабних показників Лемпеля-Зіва. Ми повертатимемо сумарну складність Лемпеля-Зіва за всіма масштабам.\n\nТепер бачимо однозначну картину: обидві міри поводять себе синхронно, та спадають у кризові та передкризові періоди, що вказує на зростання ступеня детермінованості та самоорганізації ринку.\n\n### Обчислення Шеннонівської ентропії\n\nЯк уже зазначалося, Шеннонівська ентропія --- це міра непередбачуваності стану, або, еквівалентно, його середнього інформаційного вмісту. Ентропія Шеннона є однією з перших і найбільш базових мір ентропії та фундаментальним поняттям теорії інформації. \n\nРозраховуватимемо її в ковзному вікні. \n\nЯк ми можемо бачити з представленого рисунку, ентропія Шеннона реагує спадом на кризові періоди індексу S&P 500, що вказує на приріст ступеня періодизації системи, її детермінованості. \n\n### Розрахунок інформаційного показника Фішера\n\nПерш за все задаємо параметри для розрахунків:\n\nНа даному рисунку видно, що показник Фішера спадає у кризові та передкризові періоди, що говорить про спад кількості інформації, що необхідна для опису самоорганізованої динаміки фінансових криз, тобто зростання корельованості між діями трейдерів на ринку. \n\n### Обчислення часу декореляції\n\nНа представленому рисунку видно, що час декореляції зростає у період краху, що вказує на зростання кореляції системи в цей період. \n\n### Обчислення відносної шорсткості\n\nПоказник відносної шорсткості демонструє, що крахові події як, наприклад, у 2015, 2016, 2019, 2020 та 2023 роках характеризуються зростанням шорсткості своєї динаміка. Подібного роду поведінка є індикатором зростання шумової активності ринку: кореляційних характеристик та загальної варіації ринку в цілому. Зростання цього показника в періоди криз є індикатором зростання фрактальності ринку в дані періоди часу. \n\n### Розрахунок показників складності Хьорта\n\nЗавершуємо хід роботи показниками складності Хьорта:\n\nНа даному рисунку видно, що параметр активності ($Act$) представляється найменш інформативним, оскільки він вказує тільки на зростання сукупної дисперсії сигналу. Видно тільки те, що активність значно почала зростати напередодні 2022 року, але для попередніх кризових станів ми не бачимо передвісницької поведінки цього індикатора, тому він ще вимагатиме додактових досліджень та експериментів, що виходять за рамки даного посібника. \n\nПитання передчасної ідентифікації наростання кризового явища найкраще вирішує показник мобільності ($Mob$). Ми бачимо, що даний показник зростає під час 2015-2016 років, напередодні 2019, при настанні коронавірусної пандемії, перед 2023 роком та 2024. \n\nПоказник складності Хьорта ($Comp$) реагує асиметричним чином: у той час коли мобільність зростає, показник складності спадає, вказуючи на те, що динаміки системи прагне до вищого ступеня періодичності або корельованості. \n\n## Висновок\n\nТаким чином, розглянуті інформаційні міри складності дозволяють дослідити певні аспекти складності систем будь-якої природи. Особливо продуктивним являється мультимасштабна версія введених мір. Ретельний аналіз часових рядів для систем різної природи, різного рівня складності, порівняння їх із тестовими сигналами, вивчення поведінки систем у різних (не обов'язково рівноважних, стаціонарних) умовах дозволить зрозуміти природу складності і спрогнозувати можливу поведінку систем у критичних умовах. Так, порівняння вихідного часового ряду з відповідними мірами складності свідчить про очевидне їх реагування на кризові явища. Однак питання використання їх у якості передвісників вимагає додаткових досліджень.\n\n## Завдання для самостійної роботи\n\n1. Дослідіть і порівняйте результати для тестових рядів різного ступеня складності\n2. Проведіть аналіз фондових індексів для країн з розвиненою економікою і таких, що розвиваються. Порівняйте результати. Поясніть, в чому їх схожість та відмінності\n3. Яким чином поводять себе міри складності у період фінансових шоків і криз?\n4. Наскільки чутливими є результати розрахунків до вибору ширини вікна та кроку?\n","srcMarkdownNoYaml":"\n\n\n\n\n**Тема.** Інформаційні методи оцінки складності.\n\n**Мета.** Навчитися використовувати основні показники складності з теорії інформації для аналізу часових рядів.\n\n\n\n## Теоретичні відомості\n\n### Складність. Кількісні міри складності. Інформаційні методи оцінки складності.\n\nДане століття називають століттям складності. Сьогодні питання \"що таке складність?\" вивчають фізики, біологи, математики і інформатики, хоча при теперішніх досягненнях у розумінні оточуючого світу, однозначної відповіді на це питання немає. \n\nЗ цієї причини, відповідно до ідеї І. Пригожина, будемо досліджувати прояви складності системи, застосовуючи при цьому сучасні методи кількісного аналізу складності [@e19070310]. \n\nСеред таких методів на увагу заслуговують: \n\n- інформаційно-ентропійні; \n- засновані на теорії хаосу; \n- скейлінгово-мультифрактальні.\n\nЗрозуміло, виходячи з різної природи методів, покладених в основу формування міри складності, вони приділяють певні вимоги до часових рядів, що слугують вхідними даними. Наприклад, перші дві групи методів вимагають стаціонарності вхідних даних. При цьому мають різну чутливість до таких характеристик, як детермінованність, стохастичність, причинність та кореляції. Тому у подальшому, порівнюючи комплексно ефективність різних показників складності, на вказані обставини ми будемо звертати увагу, підкреслюючи спеціально застосовність того чи іншого показника для характеристики різних сторін складності досліджуваних систем.\n\nРозгляд першої групи методів почнемо з добре відомої міри складності, запропонованої А. Колмогоровим [@doi:10.1080/00207166808803030].\n\n**Колмогорівська складність**. Поняття колмогорівської складності (або, як ще говорять, алгоритмічної ентропії) з'явилося в 1960-і роки на стику теорії алгоритмів, теорії інформації і теорії ймовірностей. \n\nІдея А. Колмогорова полягала в тому, щоб вимірювати кількість інформації, що міститься в індивідуальних скінчених об'єктах (а не у випадкових величинах, як у шеннонівській теорії інформації). Виявилось, що це можливо (хоча лише з точністю до обмеженого доданку). А. Колмогоров запропонував вимірювати кількість інформації в скінчених об’єктах за допомогою теорії алгоритмів, визначивши складність об'єкту як мінімальну довжину програми, що породжує цей об'єкт. Дане визначення стало базисом алгоритмічної теорії інформації, а також алгоритмічної теорії ймовірностей: об'єкт вважається випадковим, якщо його складність наближена до максимальної.\n\nЩо ж собою являє колмогорівська складність і як її виміряти? На практиці ми часто стикаємося з програмами, які стискують файли (для економії місця в архіві). Найбільш поширені називаються zip, gzip, compress, rar, arj та інші. Застосувавши таку програму до деякого файлу (з текстом, даними, програмою), ми отримуємо його стислу версію (яка, як правило, коротше початкового файлу). За нею можна відновити початковий файл з допомогою парної програми-\"декомпресора\". Отже, у першому наближенні колмогорівську складність файлу можна описати як довжину його стислої версії. Тим самим файл, що має регулярну структуру і добре стискуваний, має малу колмогорівську складність (порівняно з його довжиною). Навпаки, погано стискуваний файл має складність, близьку до довжини.\n\nПрипустимо, що ми маємо фіксований спосіб опису (декомпресор) $D$. Для даного слова $x$ розглянемо всі його описи, тобто всі слова $y$, для яких $D(y)$ визначене $і$ рівне $x$. Довжину найкоротшого з них $l(y)$ і називають колмогорівською складністю слова $x$ при даному способі опису $D$:\n\n$$\nKS_{D}(x) = \\min\\{l(y)\\,|\\,D(y)=x\\},\n$$\n\nде $l(y)$ позначає довжину слова $y$. Індекс $D$ підкреслює, що визначення залежить від вибору способу $D$. \n\nМожна показати, що існують оптимальні способи опису. Спосіб опису тим краще, чим він коротше. Тому природно дати таке визначення: спосіб $D_1$ не гірше за спосіб $D_2$, якщо $KS_{D_1}(x) \\leq KS_{D_2}(x)+c$ при деякому $c$ і при всіх $x$.\n\nОтже, за Колмогоровим, складність об'єкту (наприклад, тексту --- послідовності символів) --- це довжина мінімальної програми яка виводить даний текст, а ентропія --- це складність, що ділиться на довжину тексту. Також можна розглядати алгоритмічну складність як мінімальний час (або інші обчислювальні ресурси), необхідний для виконання цієї задачі на комп'ютері. А ще ми можемо говорити про комунікаційну складність завдань, в яких задіяно більше одного процесора: це кількість бітів, які потрібно передати при розв'язанні цього завдання [@Bonchev2009;@10.1162/ARTL_a_00157]. На жаль, це визначення чисто умоглядне. Надійного способу однозначно визначити цю програму не існує. Але є алгоритми, які фактично якраз і намагаються обчислити колмогорівську складність тексту [@Li2008] і ентропію [@shannon]. \n\n### Оцінка складності Колмогорова за схемою Лемпела-Зіва\n\nУніверсальна (в сенсі застосовності до різних мовних систем) міра складності кінцевої символьної послідовності була запропонована Лемпелем і Зівом (LZ) [@1055501]. Складність Лемпеля-Зіва (LZC) є класичною мірою, яка для ергодичних джерел пов'язує поняття складності (у розумінні Колмогорова-Чайтіна) та швидкості ентропії [@PhysRevE.84.036214;@ZOZOR2005285]. Для ергодичного динамічного процесу кількість нової інформації, отриманої за одиницю часу (швидкість ентропії), може бути оцінена шляхом вимірювання здатності цього джерела генерувати нові патерни. Завдяки простоті методу LZC, швидкість ентропії може бути оцінена з однієї дискретної послідовності вимірювань з низькими обчислювальними витратами [@10.1063/1.4808251]. У рамках їх підходу складність послідовності оцінюється числом кроків процесу, що її породжує. Припустимими (редакційними) операціями при цьому є: \n\n1. генерація символу (необхідна, як мінімум, для синтезу елементів алфавіту) і\n2. копіювання \"готового\" фрагмента з передісторії (тобто з уже синтезованої частини тексту). \n\nНехай $\\Sigma$ --- скінчений алфавіт, $S$ --- текст (послідовність символів), складений з \nелементів $\\Sigma$; $S[i]$ --- $i$-й символ тексту; $S[i:j]$ --- фрагмент тексту з $i$-го по $j$-й символ включно $(i<j)$; $N=|S|$ --- довжина тексту $S$. Тоді схему синтезу послідовності можна представити у вигляді конкатенації\n\n$$\nH(S)=S[1:i_1]S[i_1+1:i_2]...S[i_{k-1}+1:i_k]...S[i_{m−1}+1:N], \n$$ {#eq-4-1}\n\nде $S[i_{k−1}+1:i_k]$ --- фрагмент $S$, породжуваний на $k$-му кроці, а $m=m_{H}(S)$ --- число кроків \nпроцесу. З усіляких схем породження $S$ обирається мінімальна за числом кроків. Таким чином, складність послідовності $S$ за LZ\n\n$$\nc_{LZ}(S) = \\min_{H}\\{ m_{H}(S) \\}. \n$$\n\nМінімальність числа кроків забезпечується вибором для копіювання на кожному кроці максимально довгого прототипу з передісторії. Якщо позначити через $j(k)$ номер позиції, з якої починається копіювання на $k$-му кроці, то довжина фрагмента копіювання \n\n$$\nl_{j(k)} = i_k - i_{k-1} - 1 = \\max_{j \\leq i_{k-1}}\\{ l_{j} : S[i_{k-1}+1:i_{k-1}+l_j]=S[j:j+l_{j}-1] \\},\n$$ {#eq-4-2}\n\nа сам $k$-й компонент складнісного розкладання (@eq-4-1) можна записати у вигляді \n\n$$\nS[i_{k-1}+1:i_{k}] = \n\\begin{cases}\n    S[j(k):j(k)+l_{j(k)}-1] & \\textrm{if} \\; j(k) \\neq 0, \\\\\n    S[i_{k-1}+1] & \\textrm{if} \\; j(k) = 0.\n\\end{cases} \n$$ {#eq-4-3}\n\nВипадок $j(k) = 0$ відповідає ситуації, коли в позиції $i_{k−1}+1$ стоїть символ, який раніше не зустрічався. При цьому ми застосовуємо операцію генерації символу. \n\nБудемо знаходити складність за LZ для часового ряду, який являє собою, наприклад, щоденні значення індексу фондового ринку або криптовалютного. Для дослідження динаміки LZ та порівняння з іншими складними системами будемо знаходити дану міру складності для підряду фіксованої довжини (вікна). Для цього обчислимо логарифмічні прибутковості та перетворимо їх у послідовність бітів. При цьому можна задавати кількість станів, які диференційовані (система числення). Так, для двох різних станів маємо 0, 1, для трьох --- 0, 1, 2 і т.д. Для двійкової системи кодування буде задаватися поріг по середньому значенню і стани, наприклад, прибутковостей (*ret*) кодуватимуться наступним чином [@Giglio_2008;@RePEc:ebl:ecbull:eb-11-00319;@RePEc:pra:mprapa:22720]:\n\n$$\nret = \\begin{cases}\n    0, & ret_t < \\langle ret \\rangle \\\\\n    1, & ret_t > \\langle ret \\rangle.\n    \\end{cases} \n$$ {#eq-4-4}\n\n\nТакож можна визначити так звану пермутаційну складність Лемпеля-Зіва (PLZС) [@BAI2015102;@e23070832]. У даному випадку би будемо опиратись на процедуру реконструкції фазового простору, що згадувалась в лабораторних 2 і 3. Згідно пермутаційній процедурі ми будемо брати фрагмент ряду довжини $m$, що слугує розмірностю реконструйованого атрактора, та замінювати кожне значення ряду його порядковим індексом. На подальшому ресунку представлено часовий ряд та його можливі порядкові шаблони:\n\n::: {#fig-permutation}\n\n![](Images\\lab_4\\Permutation-entropy-method-Permutation-entropy-PE-was-calculated-for-both-unaveraged.png)\n\nФрагмент часового ряду (а) та 6 можливих порядкових шаблонів, що можуть бути в цьому сигналі (b) [@hillen2013joint]\n\n:::\n\nАлгоритм Лемпеля-Зіва виконує дві операції: (1) додає новий біт в уже існуючу послідовність; (2) копіює вже сформовану послідовність. Алгоритмічна складність представляє собою кількість таких операцій, необхідних для формування заданої послідовності.\n\nДля випадкової послідовності довжини $n$ алгоритмічна складність обчислюється за виразом $LZC_r = n / \\log(n)$. Тоді відносна алгоритмічна складність знаходиться як відношення отриманої складності до складності випадкової послідовності: $LZC = LZC / LZC_{r}$.\n\nОднак навіть цього підходу може бути недостатньо. Справа в тому, що складні сигнали проявляють притаманну їм складність на різних просторових і часових масштабах, тобто мають масштабно інваріантні властивості. Вони, зокрема проявляються через степеневі закони розподілу. Тому розрахунки алгоритмічної складності на \"поверховому\" масштабі сигналу можуть бути неприйнятними і призводити до помилкових висновків. \n\nДля подолання таких труднощів використовуються мультимасштабні методи, до розгляду яких ми і переходимо.\n\n### Процедура грануляції для мультискейлінгового дослідження часових рядів. Мультимасштабні міри складності \n\nІдея цієї групи методів включає дві послідовно виконувані процедури: \n\n1. процес \"грубого дроблення\" (coarse graining --- \"грануляції\") початкового часового ряду --- усереднення даних на сегментах, що не перетинаються, розмір яких (вікно усереднення) збільшуватиметься на одиницю при переході на наступний за величиною масштаб;\n2. обчислення на кожному з масштабів певного (до сих пір мономасштабного) показника складності. \n\nПроцес \"грубого дроблення\" (\"грануляція\") полягає в усереднені послідовних відліків ряду в межах вікон, що не перетинаються, а розмір яких $\\tau$ --- збільшується при переході від масштабу до масштабу. Кожен елемент \"гранульованого\" часового ряду $y_{j}^{\\tau}$ знаходиться у відповідності до виразу [@costa2008multiscale]:\n\n$$\ny_{j}^{\\tau} = \\frac{1}{\\tau}\\sum_{i=(j-1)\\tau+1}^{j\\tau}x_i, \\; 1 \\leq j \\leq N/\\tau,\n$$\n\nде $\\tau$ характеризує фактор масштабування. Довжина кожного \"гранульованого\" ряду залежить від розміру вікна $і$ рівна $N/\\tau$. Для масштабу рівного 1 \"гранульований\" ряд просто тотожний оригінальному.\n\n::: {#fig-granulation}\n\n![](Images\\lab_4\\3-Figure1-1.png){width=80%}\n\nСхематична ілюстрація процесу грубого дроблення (\"грануляції\") початкового часового ряду для масштабів 2 і 3\n\n:::\n\nБібліотека `neurokit2` представляє метод для обчислення як мономасштабного показника складності Лемпеля-Зіва, так і його мультимасштабного аналогу. \n\nСинтаксис **мономасштабної** процедури виглядає наступним чином:\n\n**`complexity_lempelziv(signal, delay=1, dimension=2, permutation=False, symbolize='mean', **kwargs)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень.\n- **delay** (*int*) --- часова затримка, $\\tau$. Використовується лише тоді, коли `permutation=True`.\n- **dimension** (*int*) --- розмірність вкладень, $m$. Використовується лише коли `permutation=True`.\n- **permutation** (*bool*) --- якщо значення `True`, поверне складність Лемпеля-Зіва на основі порядкових патернів.\n- **symbolize** (*str*) --- використовується тільки коли `permutation=False`. Метод перетворення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення `None`, щоб пропустити процес (якщо вхідний сигнал вже є дискретним). Можна скористатися методом `complexity_symbolize()` для використання іншої процедури символізації ряду.\n- **kwargs** --- інші аргументи, які передаються до `complexity_ordinalpatterns()` (якщо `permutation=True`) або `complexity_symbolize()`.\n\n**Повертає**\n\n- **lzc** (*float*) --- складність Лемпеля-Зіва (LZC).\n- **info** (*dict*) --- словник, що містить додаткову інформацію про параметри, які використовуються для обчислення LZC.\n\nСинтаксис мультимасштабної процедури вже інший:\n\n**`entropy_multiscale(signal, scale='default', dimension=3, tolerance='sd', method='MSEn', show=False, **kwargs)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму.\n- **scale** (*str* або *int* або *list*) --- список масштабних коефіцієнтів, що використовуються для процедури крос-грануляції часового ряду. Якщо значення `\"default\"`, буде використано `range(len(signal) / (dimension + 10))`. Якщо `\"max\"`, використовуватиме всі масштаби до половини довжини сигналу. Якщо ціле число, створить діапазон до вказаного цілого числа.\n- **dimension** (*int*) --- розмірність вкладення, $m$.\n- **tolerance** (*float*) --- поріг пропускання $\\varepsilon$ (часто позначається як $r$), відстань, на якій дві точки даних вважаються подібними. Якщо `\"sd\"` (за замовчуванням), буде встановлено значення $0.2 \\cdot SD_{signal}$.\n- **method** (*str*) --- яку версію мультимасштабного показника обчислювати. Переважна кількість показників за цим методом відповідають саме ентропійним підходам. Нас цікавитиме саме `\"LZC\"`.\n- **show** (*bool*) --- візуалізувати залежність показника від масштабу.\n- **kwargs** --- необов'язкові аргументи.\n\n**Повертає**\n\n- *float* --- точкова оцінка мультимасштабного показника окремого часового ряду, що відповідає площі під кривою значень цього показника, яка, по суті, є сумою вибіркових значень, наприклад, `\"LZC\"` в діапазоні масштабних коефіцієнтів.\n- *dict* --- словник, що містить додаткову інформацію про параметри, які використовуються для обчислення мультимасштабного показника. Значення показника, що відповідають кожному фактору `\"Scale\"`, зберігаються під ключем `\"Value\"`.\n\n### Шеннонівська складність \n\nЕнтропійний аналіз часових рядів за допомогою ентропійних показників різного роду буде проведено у наступних роботах. Зараз же ми розглянемо найпростішу з ентропій --- ентропію Шеннона та порівняємо її можливості кількісно оцінювати складність часових послідовностей у порівнянні з мірою Лемпеля-Зіва. \n\nЕнтропія Шеннона --- це статистичний квантифікатор, який широко використовується для характеристики складних процесів. Він здатний виявляти аспекти нелінійності в досліджуваних сигналах, сприяючи більш надійному поясненню нелінійної динаміки різних точок аналізу, що, в свою чергу, покращує розуміння природи складних систем, які характеризуються складністю та нерівноважністю. Окрім складності та нерівноважності, більшість, але не всі, складні системи також характеризуються неоднорідним розподілом зв'язків. Поняття ентропії було використано Шенноном в теорії інформації для передачі даних [@shannon].\n\nЕнтропія - це міра невизначеності та випадковості в системі. Якщо припустити, що всі наявні дані належать до одного класу, то неважко передбачити клас нових даних. У цьому випадку ентропія дорівнює 0. Будучи величиною між 0 і 1, коли всі ймовірності рівні, ентропія набуває найбільшого значення. Невизначеність, що виникає, коли подія $E$ відбувається з ймовірністю $p$, можна позначити як $S(p)$. Якщо ймовірність появи класу дорівнює 1, тоді ентропія мінімальна, $S(1) = 0$. Відповідно до концепції Шеннона, якщо у нас наявні ймовірності реалізації певної події $p_1, p_2, p_3, ..., p_n$, на виході отримується кількість інформації, що необхідна для опису цієї події. Тоді, Шеннонівська ентропія може бути визначена як\n\n$$\nS = -\\sum_{i=1}^{n}p_i \\ln p_{i}.  \n$$\n\nСинтаксис методу для розрахунку Шеннонівської ентропії виглядає наступним чином:\n\n**`entropy_shannon(signal=None, base=2, symbolize=None, show=False, freq=None, **kwargs)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень.\n- **base** (*float*) --- основа логарифму, що за замовчуванням дорівнює 2, що дає одиницю в бітах. Зауважте, що `scipy.stats.entropy()` за замовчуванням використовує число Ейлера (`np.e`) (натуральний логарифм), що дає міру інформації, виражену в натах.\n- **symbolize** (*str*) --- метод приведення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням дорівнює нулю, що пропускає процес (і припускає, що вхідні дані вже є дискретними).\n- **show** (*bool*) --- якщо значення `True`, виводить дискретність сигналу.\n- **freq** (*np.array*) --- замість сигналу можна надати вектор ймовірностей.\n- **kwargs** --- необов'язкові аргументи. Наразі не використовуються.\n\n**Повертає**\n\n- **shanen** (*float*) --- Шеннонівську ентропію.\n- **info** (*dict*) --- словник, що містить додаткову інформацію про параметри, які використовуються для обчислення Шеннонівської ентопії.\n\n\n### Інформація Фішера\n\nІнформацію Фішера було введено Р. А. Фішером у 1922 році як міру \"внутрішньої точності\" в теорії статистичних оцінок [@doi:10.1098/rsta.1922.0009]. Вона є центральною для багатьох статистичних галузей, що виходять далеко за межі теорії складності. Даний показник вимірює кількість інформації, яку спостережувана випадкова величина несе про невідомий параметр. В аналізі складності вимірюється кількість інформації, яку система несе \"про себе\". Він базується на розкладанні за сингулярним значенням реконструйованого фазового простору. Значення показника Фішера зазвичай антикорельоване з іншими показниками складності (чим більше інформації система приховує про себе, тим більш передбачуваною і, відповідно, менш складною вона є).\n\nІнформацію Фішера можна визначити, використовуючи метод `fisher_information()` бібліотеки `neurokit2`. Її синтаксис виглядає наступним чином:\n\n**`fisher_information(signal, delay=1, dimension=2)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень.\n- **delay** (*int*) --- затримка в часі, $\\tau$.\n- **dimension** (*int*) --- розмірність векторів фазового простору, $m$.\n\n**Повертає**\n\n- **fi** (*float*) --- обчислена міра інформації Фішера.\n- **info** (*dict*) --- словник, що містить додаткову інформацію про параметри, які використовуються для обчислення інформації Фішера.\n\n### Складність та параметри Хьорта\n\nПараметри Хьорта --- це показники статистичних властивостей, які спочатку були введені Хьортом [@HJORTH1970306] для опису загальних характеристик сигналів електроенцифалограми у кількох кількісних термінах, але які можуть бути застосовані до будь-якого часового ряду. Параметрами є активність, рухливість і складність: \n\n- Параметр **активності** ($Activity$) --- це просто дисперсія сигналу, яка відповідає середній потужності сигналу (якщо його середнє значення дорівнює 0).\n\n$$\nActivity = \\sigma^{2}_{signal}.\n$$\n\n- Параметр **рухливості** ($Mobility$) являє собою середню частоту або частку середньоквадратичного відхилення спектра потужності. Він визначається як квадратний корінь з дисперсії першої похідної сигналу, поділений на дисперсію сигналу.\n\n$$\nMobility = \\frac{\\sigma_{dd}/\\sigma_{d}}{Complexity}. \n$$\n\n- Параметр **складності** ($Complexity$) дає оцінку смуги пропускання сигналу, яка вказує на схожість форми сигналу з чистою синусоїдою (для якої значення сходиться до 1). Іншими словами, це міра \"надмірної деталізації\" по відношенню до \"найм'якшої\" можливої форми кривої. Параметр \"Складність\" визначається як відношення рухливості першої похідної сигналу до рухливості самого сигналу.\n\n$$\nComplexity = \\frac{\\sigma_d}{\\sigma_{signal}}, \n$$\n\nде $d$ та $dd$ представляють перші та другі похідні сигналу, відповідно. \n\nБібліотека `neurokit2` представляє метод для отримання відповідних показників. Її синтаксис виглядає наступним чином:\n\n**`complexity_hjorth(signal)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень.\n\n**Повертає**\n\n- **hjorth** (*float*) --- показник складності Хьорта. \n- **info** (*dict*) --- словник, що містить додаткові показники Хьорта, такі як `\"Mobility\"` та `\"Activity\"`. \n\n### Час декореляції\n\nЧас декореляції (decorrelation time, DT) визначається як час (у відліках) першого перетину нуля функції автокореляції. Коротший час декореляції відповідає менш корельованому сигналу. Наприклад, зменшення часу декореляції в сигналах електроенцифалограми спостерігається перед нападами, що пов'язано зі зменшенням потужності низьких частот [@MORMANN2005569].\n\nБібліотека `neurokit2` представляє функціонал для визначення часу декореляції, а саме метод `complexity_decorrelation()`. Її синтаксис є наступним:\n\n**`complexity_decorrelation(signal)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (часовий ряд) у вигляді вектора значень.\n\n**Повертає**\n\n- *float* --- час декореляції. \n- *dict* --- словник, що містить додаткову інформацію про додаткові показники.  \n\n\n### Відносна грубість (нерівність, шорсткість)\n\nВідносна шорсткість --- це відношення локальної дисперсії (автоковаріації з лагом 1) до глобальної дисперсії (автоковаріації з лагом 0), яке можна використовувати для класифікації різних \"шумів\". Його також можна використовувати як індекс для перевірки застосовності фрактального аналізу (показники фрактальності будуть використовуватись у наступних роботах) [@10.3389/fphys.2012.00208]. \n\nСинтаксис даного методу в бібліотеці `neurokit2` виглядає наступним чином:\n\n**`complexity_relativeroughness(signal, **kwargs)`**\n\n**Параметри**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (часовий ряд) у вигляді вектора значень.\n- **kwargs** (*optional*) --- інші аргументи, що потребуються методу `nk.signal_autocor()`. \n\n**Повертає**\n\n- **rr** (*float*) --- значення відносної грубості. \n- **info** (*dict*) --- словник, що містить інформацію відносно параметрів, що використовувались для обчислення показника грубості. \n\n### Взаємна інформація\n\nКоли йдеться про виявлення зв'язків між змінними, ми часто використовуємо кореляцію Пірсона. Проблема полягає в тому, що цей показник знаходить лише *лінійні* зв'язки, що іноді може призвести до неправильної інтерпретації зв'язку між двома змінними. Тим не менш, інші статистичні методи вимірюють нелінійні зв'язки, такі як **взаємна інформація (mutual information, MI)** [@cover1999elements].\n\nВзаємна інформація між двома випадковими величинами вимірює нелінійний зв'язок між ними. Крім того, вона показує, **скільки інформації можна отримати з випадкової величини**, спостерігаючи за іншою випадковою величиною.\n\nВона тісно пов'язана з поняттям ентропії. Тобто, зменшення невизначеності випадкової величини пов'язане з отриманням інформації з іншої випадкової величини. Отже, високе значення взаємної інформації вказує на велике зменшення невизначеності, тоді як низьке значення вказує на мале зменшення. Якщо взаємна інформація дорівнює нулю, це означає, що дві випадкові величини є незалежними.\n\nВзаємну інформацію можна розрахувати наступним чином:\n\n$$\nI(X; Y) = \\sum_{y \\in Y}\\sum_{x \\in X}p(x, y) \\cdot \\log{\\left( \\frac{p(x,y)}{p(x)p(y)} \\right)}, \n$$\n\nде $p(x)$ та $p(y)$ ймовірності спостереження окремо $x$ або $y$, а $p(x,y)$ ймовірність спостереження одночасно $x$ та $y$.\n\nОсновна відмінність між кореляцією та взаємною інформацією полягає в тому, що **кореляція є мірою лінійної залежності**, тоді як взаємна інформація вимірює загальну залежність (включаючи **нелінійні** зв'язки). Тому взаємна інформація виявляє залежності, які не залежать тільки від **коваріації**. Таким чином, взаємна інформація дорівнює нулю, коли дві випадкові величини є строго незалежними.\n\nБібліотека `neurokit2` представляє інструментарій для знаходження взаємної інформації між двома сигналами $x$ та $y$. У даній роботі ми спробуємо віднайти взаємну інформацію як між двома часовими рядами, так і **авто-взаємну інформацію**, подібно до автокореляції. \n\nСинтаксис потрібної нам процедури виглядає наступним чином:\n\n**`mutual_information(x, y, method='varoquaux', bins='default', **kwargs)`**\n\n**Параметри**\n\n- **x** (*Union[list, np.array, pd.Series]*) --- масив значень.\n- **y** (*Union[list, np.array, pd.Series]*) --- масив значень.\n- **method** (*str*) --- метод для обчислення взаємної інформації: `\"nolitsa\"`, `\"varoquaux\"`, `\"knn\"`, `\"max\"`. \n- **bins** (*int*) --- кількість бінів гістограми. Використовується лише для `\"nolitsa\"` та `\"varoquaux\"`. Якщо `\"default\"`, кількість бінів оцінюється згідно методики Hacine-Gharbi (2018).\n- **kwargs** --- додаткові ключові аргументи для обраного методу. \n\n**Повертає**\n\n- *float* --- розрахована взаємна інформація. \n\nІснують різноманітні підходи до розрахунку взаємної інформації:\n\n- **nolitsa**: Класична взаємна інформація (трохи швидше, ніж метод `\"sklearn\"`).\n- **varoquaux**: Застосовує фільтр Гауса до об'єднаної гістограми. Величину згладжування можна налаштовувати за допомогою аргументу `sigma` (за замовчуванням `sigma=1`).\n- **knn**: Непараметрична (тобто не заснована на біннінгу) оцінка за найближчими сусідами. Додаткові параметри включають `k` (за замовчуванням, `k=3`), кількість найближчих сусідів для використання.\n- **max**: Максимальний коефіцієнт взаємної інформації, тобто $MI$ є максимальним при певній комбінації кількості бінів.\n\nІснує безліч різноманітних показників складності, що базуються на теорії інформації та інших парадигах, які ми ще представлятимемо в подальшому. Розглянемо ефективність використання зазначених показників у якості індикаторів або індикаторів-передвісників крахових подій.   \n\n## Хід роботи\n\nСпочатку імпортуємо необхідні модулі для подальшої роботи:\n\nІ виконаємо налаштування рисунків для виведення:\n\nЦього разу розглянему можливість побудови індикаторів-передвісників на прикладі фондового індексу S&P 500, але, окрім цього, додамо ще Біткоїн для розрахунку взаємної інформації між фондовим ринком та криптовалютним. Очевидно, що фондовий індекс S&P 500 мав би проіснувати довше за Біткоїн. До того ж, криптовалютний ринок працює безперервно на відміну від фондового, а тому треба буде об'єднати значення двох активів за тими датами що співпадають.  \n\n**Виконуємо зчитування фондового індексу**:\n\n**Виконуємо зчитування криптовалютного індексу**:\n\n::: {.callout-warning}\n## Увага\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо. \n\n:::\n\n---\n\n---\n\nТепер створимо новий масив даних, що об'єднуватиме в собі значення S&P 500 та BTC по їх спільним датам:\n\nВиводимо отриману базу:\n\nІ візуалізуємо сам графік. Спочатку оголосимо функцію для попарної візуалізації рядів зі збереженням їх абсолютних значень:\n\nі тепер візуалізуємо отримані ряди:\n\n::: {.callout-important}\n## Важливо\n\nНе виконуйте блоки коду, що відповідають секції \"Розрахунок взаємної інформації\", якщо ви працюєте з текстовим файлом. \n\n:::\n\n### Розрахунок взаємної інформації\n\nРозглянемо взаємну інформацію як індикатор нелінійної кореляції між двома фінансовими активами, і спробуємо сказати, чи є між ними \"істинний\" взаємозв'язок. Виконуватимемо розрахунки із використанням алгоритму руховому вікна. Також визначимо функцію `transform()` для нормалізації ряду.  \n\nТепер приступимо до розрахунків:\n\nЗберігаємо отриманий результат у текстовому файлі:\n\nВізуалізуємо результат між відповідними показниками:\n\nЯк ми можемо бачити з представленого рисунку, на фондовому та криптовалютному ринках дійсно спостерігалися фази зростання взаємної інформації між ними. Найкраще це видно напередодні кризи 2018-го року, під час 2019, після коронавірусної пандемії та напередодні 2023 року. Для даного індикатора залишається простір для експериментів, що можуть вивести його на рівень достатньо потужного передвісника криз на фондовому ринку чи криптовалютному. \n\nЯк вже зазначалося, окрім обчислення взаємної інформації для двох пар часових сигналів, ми можемо обчислити *авто*взаємну інформація, тобто взаємну інформацію ряду самого із собою по різним часовим лагам, як це було пророблено для автокореляції. Недолік автокореляції полягає в тому, що вони визначає саме *лінійний* зв'язок теперішніх значень з попередніми. Автовзаємна інформація в свою чергу є показником *нелінійного* зв'язку теперішніх значень із попередніми. \n\nДля обчислення автовзаємної інформації визначимо наступну функцію: \n\nВиведемо залежність автовзаємної інформації від лагу для всього ряду S&P 500 та Біткоїна. Спочатку розрахуємо вихідні значення ряду, далі прибутковості і потім волатильності. Для кожного з відповідних сигналів виведемо взаємну інформацію. \n\n**Виконуємо перетворення S&P 500 та Біткоїна**\n\n**Розраховуємо автовзаємну інформацію S&P 500 та Біткоїна**\n\nЯк ми можемо бачити з представлених графіків, ступінь взаємної інформації це показник, що найкращим чином працює саме для вихідних значень часових сигналів. Для вихідного ряду ступінь взаємної інформації залишається доволі високим. Для прибутковостей і волатильностей взаємна інформація спадає одразу на першому лагу, що свідчить про незалежність значень на подальших часових затримках.  \n\n### Розрахунок мономасштабної складності Лемпеля-Зіва\n\nПродовжимо розраховувати й інші показники складності. Розглянемо можливість використання показника складності Лемпеля-Зіва в якості індикатора катастрофічних подій.\n\nЗберігаємо результати в текстових файлах:\n\nТа візуалізуємо їх:\n\nНа даному рисунку видно, що 2 міри поводять себе асиметрично по відношенню один до одного: $LCZ$ вказує на зростання складності, наприклад, події 2019 року. У той же час $PLCZ$ вказує на спад складності системи в цей період. Варто дослідити мультимасштабну динаміку міри Лемпеля-Зіва для більш змістовних висновків. \n\n### Обчислення мультимасштабної складності Лемпеля-Зіва\n\n**Мультимасштабна динаміка пермутаційного показника складності Лемпеля-Зіва**\n\nТепер розрахуємо віконну динаміку мультимасштабних показників Лемпеля-Зіва. Ми повертатимемо сумарну складність Лемпеля-Зіва за всіма масштабам.\n\nТепер бачимо однозначну картину: обидві міри поводять себе синхронно, та спадають у кризові та передкризові періоди, що вказує на зростання ступеня детермінованості та самоорганізації ринку.\n\n### Обчислення Шеннонівської ентропії\n\nЯк уже зазначалося, Шеннонівська ентропія --- це міра непередбачуваності стану, або, еквівалентно, його середнього інформаційного вмісту. Ентропія Шеннона є однією з перших і найбільш базових мір ентропії та фундаментальним поняттям теорії інформації. \n\nРозраховуватимемо її в ковзному вікні. \n\nЯк ми можемо бачити з представленого рисунку, ентропія Шеннона реагує спадом на кризові періоди індексу S&P 500, що вказує на приріст ступеня періодизації системи, її детермінованості. \n\n### Розрахунок інформаційного показника Фішера\n\nПерш за все задаємо параметри для розрахунків:\n\nНа даному рисунку видно, що показник Фішера спадає у кризові та передкризові періоди, що говорить про спад кількості інформації, що необхідна для опису самоорганізованої динаміки фінансових криз, тобто зростання корельованості між діями трейдерів на ринку. \n\n### Обчислення часу декореляції\n\nНа представленому рисунку видно, що час декореляції зростає у період краху, що вказує на зростання кореляції системи в цей період. \n\n### Обчислення відносної шорсткості\n\nПоказник відносної шорсткості демонструє, що крахові події як, наприклад, у 2015, 2016, 2019, 2020 та 2023 роках характеризуються зростанням шорсткості своєї динаміка. Подібного роду поведінка є індикатором зростання шумової активності ринку: кореляційних характеристик та загальної варіації ринку в цілому. Зростання цього показника в періоди криз є індикатором зростання фрактальності ринку в дані періоди часу. \n\n### Розрахунок показників складності Хьорта\n\nЗавершуємо хід роботи показниками складності Хьорта:\n\nНа даному рисунку видно, що параметр активності ($Act$) представляється найменш інформативним, оскільки він вказує тільки на зростання сукупної дисперсії сигналу. Видно тільки те, що активність значно почала зростати напередодні 2022 року, але для попередніх кризових станів ми не бачимо передвісницької поведінки цього індикатора, тому він ще вимагатиме додактових досліджень та експериментів, що виходять за рамки даного посібника. \n\nПитання передчасної ідентифікації наростання кризового явища найкраще вирішує показник мобільності ($Mob$). Ми бачимо, що даний показник зростає під час 2015-2016 років, напередодні 2019, при настанні коронавірусної пандемії, перед 2023 роком та 2024. \n\nПоказник складності Хьорта ($Comp$) реагує асиметричним чином: у той час коли мобільність зростає, показник складності спадає, вказуючи на те, що динаміки системи прагне до вищого ступеня періодичності або корельованості. \n\n## Висновок\n\nТаким чином, розглянуті інформаційні міри складності дозволяють дослідити певні аспекти складності систем будь-якої природи. Особливо продуктивним являється мультимасштабна версія введених мір. Ретельний аналіз часових рядів для систем різної природи, різного рівня складності, порівняння їх із тестовими сигналами, вивчення поведінки систем у різних (не обов'язково рівноважних, стаціонарних) умовах дозволить зрозуміти природу складності і спрогнозувати можливу поведінку систем у критичних умовах. Так, порівняння вихідного часового ряду з відповідними мірами складності свідчить про очевидне їх реагування на кризові явища. Однак питання використання їх у якості передвісників вимагає додаткових досліджень.\n\n## Завдання для самостійної роботи\n\n1. Дослідіть і порівняйте результати для тестових рядів різного ступеня складності\n2. Проведіть аналіз фондових індексів для країн з розвиненою економікою і таких, що розвиваються. Порівняйте результати. Поясніть, в чому їх схожість та відмінності\n3. Яким чином поводять себе міри складності у період фінансових шоків і криз?\n4. Наскільки чутливими є результати розрахунків до вибору ширини вікна та кроку?\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"highlight-style":"arrow","css":["style.css"],"output-file":"lab_4.html"},"language":{"toc-title-document":"Зміст","toc-title-website":"На цій сторінці","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Анотація","section-title-appendices":"Додатки","section-title-footnotes":"Зноски","section-title-references":"Використана література","section-title-reuse":"Повторне використання","section-title-copyright":"Copyright","section-title-citation":"Цитата","appendix-attribution-cite-as":"Будь-ласка, цитуйте цю роботу як:","appendix-attribution-bibtex":"BibTeX:","title-block-author-single":"Автор","title-block-author-plural":"Автори","title-block-affiliation-single":"Приналежність","title-block-affiliation-plural":"Приналежності","title-block-published":"Дата публікації","title-block-modified":"Змінено","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Розгорнути код","code-tools-hide-all-code":"Приховати код","code-tools-view-source":"Переглянути код","code-tools-source-code":"Вихідний код","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Копіювати","copy-button-tooltip-success":"Скопійовано!","repo-action-links-edit":"Редагувати сторінку","repo-action-links-source":"Переглянути код","repo-action-links-issue":"Повідомити про проблему","back-to-top":"Back to top","search-no-results-text":"Пошук не дав результату","search-matching-documents-text":"Результати пошуку","search-copy-link-title":"Скопіюйте посилання для пошуку","search-hide-matches-text":"Приховати додаткові результати","search-more-match-text":"Додатковий результат у цьому документі","search-more-matches-text":"Додаткові результати у цьому документі","search-clear-button-title":"Очистити","search-detached-cancel-button-title":"Скасувати","search-submit-button-title":"Надіслати","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рис.","crossref-tbl-title":"Таблиця","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лема","crossref-cor-title":"Наслідок","crossref-prp-title":"Твердження","crossref-cnj-title":"Гіпотеза","crossref-def-title":"Визначення","crossref-exm-title":"Приклад","crossref-exr-title":"Завдання","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Додаток","crossref-sec-prefix":"Розділ","crossref-eq-prefix":"Рівняння","crossref-lof-title":"Список Рисунків","crossref-lot-title":"Список Таблиць","crossref-lol-title":"Список Каталогів","environment-proof-title":"Доведення","environment-remark-title":"Зауваження","environment-solution-title":"Рішення","listing-page-order-by":"Сортувати по","listing-page-order-by-default":"попередньо вибраний","listing-page-order-by-date-asc":"Найновіші","listing-page-order-by-date-desc":"Найстріші","listing-page-order-by-number-desc":"За спаданням","listing-page-order-by-number-asc":"За зростанням","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Опис","listing-page-field-author":"Автор","listing-page-field-filename":"Ім'я файлу","listing-page-field-filemodified":"Змінено","listing-page-field-subtitle":"Підзаголовок","listing-page-field-readingtime":"Час читання","listing-page-field-categories":"Категорії","listing-page-minutes-compact":"{0} хвилин","listing-page-category-all":"Все","listing-page-no-matches":"Немає відповідних елементів"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","bibliography":["references.bib"],"csl":"physical-review-b.csl","callout-appearance":"default","grid":{"body-width":"1050px"},"page-layout":"full","theme":{"light":"cosmo","dark":"superhero"},"title":"Лабораторна робота № 4"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":true,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"number-sections":true,"output-file":"lab_4.pdf"},"language":{"toc-title-document":"Зміст","toc-title-website":"На цій сторінці","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Анотація","section-title-appendices":"Додатки","section-title-footnotes":"Зноски","section-title-references":"Використана література","section-title-reuse":"Повторне використання","section-title-copyright":"Copyright","section-title-citation":"Цитата","appendix-attribution-cite-as":"Будь-ласка, цитуйте цю роботу як:","appendix-attribution-bibtex":"BibTeX:","title-block-author-single":"Автор","title-block-author-plural":"Автори","title-block-affiliation-single":"Приналежність","title-block-affiliation-plural":"Приналежності","title-block-published":"Дата публікації","title-block-modified":"Змінено","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Розгорнути код","code-tools-hide-all-code":"Приховати код","code-tools-view-source":"Переглянути код","code-tools-source-code":"Вихідний код","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Копіювати","copy-button-tooltip-success":"Скопійовано!","repo-action-links-edit":"Редагувати сторінку","repo-action-links-source":"Переглянути код","repo-action-links-issue":"Повідомити про проблему","back-to-top":"Back to top","search-no-results-text":"Пошук не дав результату","search-matching-documents-text":"Результати пошуку","search-copy-link-title":"Скопіюйте посилання для пошуку","search-hide-matches-text":"Приховати додаткові результати","search-more-match-text":"Додатковий результат у цьому документі","search-more-matches-text":"Додаткові результати у цьому документі","search-clear-button-title":"Очистити","search-detached-cancel-button-title":"Скасувати","search-submit-button-title":"Надіслати","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рис.","crossref-tbl-title":"Таблиця","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лема","crossref-cor-title":"Наслідок","crossref-prp-title":"Твердження","crossref-cnj-title":"Гіпотеза","crossref-def-title":"Визначення","crossref-exm-title":"Приклад","crossref-exr-title":"Завдання","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Додаток","crossref-sec-prefix":"Розділ","crossref-eq-prefix":"Рівняння","crossref-lof-title":"Список Рисунків","crossref-lot-title":"Список Таблиць","crossref-lol-title":"Список Каталогів","environment-proof-title":"Доведення","environment-remark-title":"Зауваження","environment-solution-title":"Рішення","listing-page-order-by":"Сортувати по","listing-page-order-by-default":"попередньо вибраний","listing-page-order-by-date-asc":"Найновіші","listing-page-order-by-date-desc":"Найстріші","listing-page-order-by-number-desc":"За спаданням","listing-page-order-by-number-asc":"За зростанням","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Опис","listing-page-field-author":"Автор","listing-page-field-filename":"Ім'я файлу","listing-page-field-filemodified":"Змінено","listing-page-field-subtitle":"Підзаголовок","listing-page-field-readingtime":"Час читання","listing-page-field-categories":"Категорії","listing-page-minutes-compact":"{0} хвилин","listing-page-category-all":"Все","listing-page-no-matches":"Немає відповідних елементів"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"csl":"physical-review-b.csl","callout-appearance":"default","title":"Лабораторна робота № 4","documentclass":"book","colorlinks":true},"extensions":{"book":{"selfContainedOutput":true}}},"docx":{"identifier":{"display-name":"MS Word","target-format":"docx","base-format":"docx"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"docx","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"page-width":6.5},"pandoc":{"default-image-extension":"png","to":"docx","toc":true,"number-sections":true,"output-file":"lab_4.docx"},"language":{"toc-title-document":"Зміст","toc-title-website":"На цій сторінці","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Анотація","section-title-appendices":"Додатки","section-title-footnotes":"Зноски","section-title-references":"Використана література","section-title-reuse":"Повторне використання","section-title-copyright":"Copyright","section-title-citation":"Цитата","appendix-attribution-cite-as":"Будь-ласка, цитуйте цю роботу як:","appendix-attribution-bibtex":"BibTeX:","title-block-author-single":"Автор","title-block-author-plural":"Автори","title-block-affiliation-single":"Приналежність","title-block-affiliation-plural":"Приналежності","title-block-published":"Дата публікації","title-block-modified":"Змінено","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Розгорнути код","code-tools-hide-all-code":"Приховати код","code-tools-view-source":"Переглянути код","code-tools-source-code":"Вихідний код","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Копіювати","copy-button-tooltip-success":"Скопійовано!","repo-action-links-edit":"Редагувати сторінку","repo-action-links-source":"Переглянути код","repo-action-links-issue":"Повідомити про проблему","back-to-top":"Back to top","search-no-results-text":"Пошук не дав результату","search-matching-documents-text":"Результати пошуку","search-copy-link-title":"Скопіюйте посилання для пошуку","search-hide-matches-text":"Приховати додаткові результати","search-more-match-text":"Додатковий результат у цьому документі","search-more-matches-text":"Додаткові результати у цьому документі","search-clear-button-title":"Очистити","search-detached-cancel-button-title":"Скасувати","search-submit-button-title":"Надіслати","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рис.","crossref-tbl-title":"Таблиця","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лема","crossref-cor-title":"Наслідок","crossref-prp-title":"Твердження","crossref-cnj-title":"Гіпотеза","crossref-def-title":"Визначення","crossref-exm-title":"Приклад","crossref-exr-title":"Завдання","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Додаток","crossref-sec-prefix":"Розділ","crossref-eq-prefix":"Рівняння","crossref-lof-title":"Список Рисунків","crossref-lot-title":"Список Таблиць","crossref-lol-title":"Список Каталогів","environment-proof-title":"Доведення","environment-remark-title":"Зауваження","environment-solution-title":"Рішення","listing-page-order-by":"Сортувати по","listing-page-order-by-default":"попередньо вибраний","listing-page-order-by-date-asc":"Найновіші","listing-page-order-by-date-desc":"Найстріші","listing-page-order-by-number-desc":"За спаданням","listing-page-order-by-number-asc":"За зростанням","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Опис","listing-page-field-author":"Автор","listing-page-field-filename":"Ім'я файлу","listing-page-field-filemodified":"Змінено","listing-page-field-subtitle":"Підзаголовок","listing-page-field-readingtime":"Час читання","listing-page-field-categories":"Категорії","listing-page-minutes-compact":"{0} хвилин","listing-page-category-all":"Все","listing-page-no-matches":"Немає відповідних елементів"},"metadata":{"bibliography":["references.bib"],"csl":"physical-review-b.csl","callout-appearance":"default","title":"Лабораторна робота № 4"},"extensions":{"book":{"selfContainedOutput":true}}},"epub":{"identifier":{"display-name":"ePub","target-format":"epub","base-format":"epub"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"epub","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":false,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"default-image-extension":"png","html-math-method":"mathml","to":"epub","toc":true,"number-sections":true,"output-file":"lab_4.epub"},"language":{"toc-title-document":"Зміст","toc-title-website":"На цій сторінці","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Анотація","section-title-appendices":"Додатки","section-title-footnotes":"Зноски","section-title-references":"Використана література","section-title-reuse":"Повторне використання","section-title-copyright":"Copyright","section-title-citation":"Цитата","appendix-attribution-cite-as":"Будь-ласка, цитуйте цю роботу як:","appendix-attribution-bibtex":"BibTeX:","title-block-author-single":"Автор","title-block-author-plural":"Автори","title-block-affiliation-single":"Приналежність","title-block-affiliation-plural":"Приналежності","title-block-published":"Дата публікації","title-block-modified":"Змінено","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Розгорнути код","code-tools-hide-all-code":"Приховати код","code-tools-view-source":"Переглянути код","code-tools-source-code":"Вихідний код","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Копіювати","copy-button-tooltip-success":"Скопійовано!","repo-action-links-edit":"Редагувати сторінку","repo-action-links-source":"Переглянути код","repo-action-links-issue":"Повідомити про проблему","back-to-top":"Back to top","search-no-results-text":"Пошук не дав результату","search-matching-documents-text":"Результати пошуку","search-copy-link-title":"Скопіюйте посилання для пошуку","search-hide-matches-text":"Приховати додаткові результати","search-more-match-text":"Додатковий результат у цьому документі","search-more-matches-text":"Додаткові результати у цьому документі","search-clear-button-title":"Очистити","search-detached-cancel-button-title":"Скасувати","search-submit-button-title":"Надіслати","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рис.","crossref-tbl-title":"Таблиця","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лема","crossref-cor-title":"Наслідок","crossref-prp-title":"Твердження","crossref-cnj-title":"Гіпотеза","crossref-def-title":"Визначення","crossref-exm-title":"Приклад","crossref-exr-title":"Завдання","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Додаток","crossref-sec-prefix":"Розділ","crossref-eq-prefix":"Рівняння","crossref-lof-title":"Список Рисунків","crossref-lot-title":"Список Таблиць","crossref-lol-title":"Список Каталогів","environment-proof-title":"Доведення","environment-remark-title":"Зауваження","environment-solution-title":"Рішення","listing-page-order-by":"Сортувати по","listing-page-order-by-default":"попередньо вибраний","listing-page-order-by-date-asc":"Найновіші","listing-page-order-by-date-desc":"Найстріші","listing-page-order-by-number-desc":"За спаданням","listing-page-order-by-number-asc":"За зростанням","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Опис","listing-page-field-author":"Автор","listing-page-field-filename":"Ім'я файлу","listing-page-field-filemodified":"Змінено","listing-page-field-subtitle":"Підзаголовок","listing-page-field-readingtime":"Час читання","listing-page-field-categories":"Категорії","listing-page-minutes-compact":"{0} хвилин","listing-page-category-all":"Все","listing-page-no-matches":"Немає відповідних елементів"},"metadata":{"bibliography":["references.bib"],"csl":"physical-review-b.csl","callout-appearance":"default","title":"Лабораторна робота № 4"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf","docx","epub"]}