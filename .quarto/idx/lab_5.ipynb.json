{"title":"Лабораторна робота № 5","markdown":{"yaml":{"title":"Лабораторна робота № 5"},"headingText":"Теоретичні відомості","containsRefs":false,"markdown":"\n\n\n\n\n**Тема.** Ентропійний аналіз складних систем\n\n**Мета.** Навчитись  розраховувати  значення  різних  типів  ентропії  часового  ряду  та досліджувати динаміку зміни її значень для оцінки якості прогнозів часових рядів\n\n\n\n\nПитання динаміки розвитку і функціонування складних систем може розглядатись у двох варіантах:\n\n- як дослідження шумової активності;\n- як детерміністичного випадку з певним ступенем порядку. \n\nОстанніми роками було використано кілька підходів для ідентифікації механізмів, що \nлежать в основі еволюції складних. Особливо корисні результати \nбуло  отримано  при  їх  дослідженні  методами  теорії  випадкових  матриць,  моно-  та \nмультифрактального аналізу, теорії хаосу з реконструкцією траєкторії системи  у фазовому \nпросторі,  рекурентного аналізу тощо.  Ми  розглянули  ці  методи  у \nпопередніх роботах. Однак, застосування деяких із методів висуває вимоги до стаціонарності \nдосліджуваних даних, потребує довгих часових рядів та комплексного обчислення кількох \nпараметрів. \n\nІншим широко відомим підходом моделювання особливостей складних систем є обчислення характеристик різних видів ентропії. \n\nКонцепція термодинамічної ентропії як міри хаосу системи добре відома у фізиці, однак, останніми роками поняття ентропії було застосоване до складних систем інших об’єктів (біологічних, економічних, соціальних тощо). Так, один із найбільш часто використовуваних методів визначення ентропії базується на обчисленні спектру потужності Фур'є та застосовується для вивчення часових рядів різної природи. Проте, використання дискретного перетворення Фур’є для аналізу часових рядів має свої недоліки, зокрема, на результати впливає нестаціонарність рядів, варіювання їх довжини від сотень до сотень тисяч, та обмеження самого методу (незмінність частотно-часових характеристик протягом всього часу функціонування системи). Тому виникає питання про розрахунок значень ентропії за допомогою інших методів.\n\nВведемо поняття ентропії, скориставшись інформацією, яку можна знайти у [Вікіпедії](https://uk.wikipedia.org/wiki/Ентропія).\n\n**Термодинамічна ентропія**  $S$,  часто  просто  іменована  **ентропія**,  в  хімії  і термодинаміці є мірою кількості енергії у  фізичній системі, яка не може бути використана для виконання роботи. Вона також є мірою безладдя, присутнього в системі. \n\nПоняття  ентропії  була  вперше  введено  у  1865  році  Рудольфом  Клаузіусом [@clausius1867mechanical].  Він визначив  зміну  ентропії  термодинамічної  системи  при  оборотному  процесі  як  відношення зміни загальної кількості тепла $\\Delta Q$ до величини абсолютної температури $T$: \n\n$$ \n\\Delta S = \\Delta Q / T. \n$$\n\nРудольф Клаузіус дав величині $S$ ім'я \"*ентропія*\", що походить від грецького слова τρoπή, \n\"<i>зміна</i>\" (зміна,  перетворення). \n\nУ 1877 році, Людвіг  Больцман [@Boltzmann1970] зрозумів, що  ентропія системи може відноситися до кількості  можливих \"мікростанів\"  (мікроскопічних  станів),  що  узгоджуються  з  їх термодинамічними  властивостями.  Розглянемо,  наприклад,  ідеальний  газ  у  посудині. Мікростан визначений як позиції і імпульси кожного атома, що становить систему. Зв'язність пред'являє до нас вимоги розглядати тільки ті мікростани, для яких: (i) місце розташування всіх частин обмежене границями судини, (ii) для отримання загальної енергії газу кінетичні енергії атомів підсумовуються. Больцман постулював що \n\n$$ \nS = k_{B}\\ln{\\Omega}, \n$$ \n\nде  константу  $k_{B} = 1,38 \\cdot 10^{-23} Дж/К$   ми  знаємо  тепер  як  сталу  Больцмана, a $\\Omega$  є  числом мікростанів,  які  можливі  в  наявному  макроскопічному  стані.  Цей  постулат,  відомий  як принцип  Больцмана,  може  бути  оцінений  як  початок  статистичної  механіки, що описує термодинамічні системи з використанням статистичної поведінки компонентів. Принцип Больцмана зв'язує мікроскопічні властивості системи ($\\Omega$) з однією з її  термодинамічних властивостей ($S$). \n\nЗгідно  визначенню  Больцмана,  ентропія  є  просто  функцією  стану.  Більш  того, оскільки  ($\\Omega$)  може  бути  тільки  натуральним числом,  ентропія  повинна  бути додатною --- виходячи з властивостей логарифма.\n\nУ  випадку  дискретних  станів  квантової  механіки  кількість  станів  підраховується звичайним  чином.  У  рамках  класичної  механіки  мікроскопічний  стан  системи  описується координатами $q_{i}$ й імпульсами $p_{i}$ окремих частинок, які пробігають неперервні значення. У такому випадку\n\n$$ \nS = k_{B}\\ln\\frac{1}{( 2\\pi\\hbar )^{s}} \\int \\prod_{i=1}^{s} dq_{i}dp_{i}, \n$$\n\nде $s$ --- число незалежних координат, $\\hbar$ --- приведена стала Планка, а інтегрування проводиться по області фазового простору, який відповідає певному макроскопічному стану. \n\nКлод  Шеннон  [@shannon]  запропонував  формулу  для  оцінки  невизначеності кодової інформації в каналах зв'язку, звану ентропією Шеннона:\n\n$$ \nS = -k\\sum_{i=1}^{n}p_{i}\\ln{p_{i}}, \n$$\n\nде $p_{i}$ --- вірогідність того, що символ $i$ зустрічається в коді, який містить $N$ символів, $k$ --- розмірний множник.  \n\nЗв'язок між ентропією і інформацією можна прослідкувати на наступному прикладі. Розглянемо тіло при абсолютному нулі температури, і хай ми маємо повну інформацію про координати і імпульси кожної частинки. Для простоти покладемо, що імпульси всіх частинок рівні нулю. В цьому випадку термодинамічна ймовірність рівна одиниці, а ентропія --- нулю. При  кінцевих  температурах  ентропія  в  рівновазі  досягає  максимуму.  Можна  зміряти  всі макропараметри, що характеризують даний макростан. Проте ми практично нічого не знаємо про мікростан системи. Точніше кажучи, ми знаємо, що даний макростан можна реалізувати за допомогою дуже великого числа мікростанів. Таким чином, нульовій ентропії відповідає повна інформація (ступінь незнання рівний нулю), а максимальної ентропії --- повне незнання мікростанів (ступінь незнання максимальний).\n\nУ  теорії  інформації  ентропія  (інформаційна  ентропія)  визначається  як  кількість інформації. Нехай $P$ --- апріорна вірогідність деякої події (ймовірність до проведення досвіду), а  $P_{1}$ --- ймовірність цієї події після проведення досвіду. Для простоти вважатимемо, що $P_{1} = 1$. За  Шенноном,  кількість  інформації  $I$,  яка  дає  точну  відповідь  (після  проведення експерименту) \n\n$$ \nI = K \\log{P}. \n$$  \n\nЦя кількість інформації, за визначенням, дорівнює одному біту. \n\nФізичний сенс $I$ --- це міра нашого незнання. Іншими словами, $I$ --- це та інформація, яку ми  можемо  одержати,  вирішивши  завдання.  У  прикладі  (тіло  при  абсолютному  нулі температури),  що  розглядається  вище,  міра  нашого  незнання  рівна  нулю,  оскільки  $P = 1$. Після проведення досвіду ми одержуємо нульову інформацію $I = 0$, оскільки все було відомо до  досвіду.  Якщо  розглядати  тіло  при  кінцевих  температурах,  то  до  проведення  досвіду число мікростанів, а отже, і $P$ дуже велике. Після проведення досвіду ми одержуємо велику інформацію, оскільки нам стають відомими координати і імпульси всіх частинок.\n\nАналогія  між  кількістю  інформації  і  ентропією $S$,  визначуваною  з  принципу Больцмана,  очевидна.  Досить покласти  множник  $K$  рівним  постійній  Больцмана  $k_{B}$ і використовувати  натуральний  логарифм.  Саме  з  цієї  причини  величину $I$  називають інформаційною ентропією. Інформаційна ентропія (кількість інформації) була визначена по аналогії із звичайною ентропією, і вона має властивості, характерні для звичайній ентропії: адитивність, екстремальні  властивості  і  т.д.  Проте  ототожнювати  звичайну  ентропію  з інформаційною не можна, оскільки неясно, яке відношення має другий закон термодинаміки до інформації. Нагадаємо, що екстенсивна величина --- ця така характеристика  системи,  яка  росте зі збільшенням  розмірів  системи,  тобто,  якщо  наша  система  складається  з  двох  *незалежних* підсистем $А$ і $В$, то ентропію всієї системи можна одержати складанням ентропій підсистем:\n\n$$ \nS(A + B) = S(A) + S(B). \n$$\n\nСаме ця властивість і означає екстенсивність, або адитивність, ентропії.\n\n## Хід роботи\n\nРозглянемо, яким чином ми використовувати ентропійні показники в якості індикаторів або індикаторів-передвісників кризових подій. Перш за все імпортуємо необхідні модулі для подальшої роботи:\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nУ даній роботі виконаємо розрахунки на прикладі одного з найважливіших фондових індексів Японії --- Nikkei 225. Індекс обчислюється шляхом визначення простого середнього арифметичного значення цін акцій 225 провідних компаній, які входять до першої секції Токійської фондової біржі. Для отримання значень індексу скористаємось бібліотекою `yfinance`. Значення розглядатимемо за весь період, тому початкову та кінцеву дати вказувати не будемо.  \n\n::: {.callout-warning}\n## Увага\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо \n\n:::\n\n---\n\n---\n\nВиводимо досліджуваний ряд:\n\nДля приведення ряду до стандартизованого вигляду або прибутковостей визначимо функцію `transformations()`:\n\nДля побудови пари часових рядів визначимо функцію `plot_pair()`:\n\n### Апроксимаційна ентропія (ApEn)\n\nЕнтропія  подібності  (Approximate  Entropy,  *ApEn*)  є \"статистикою регулярності\" [@pincus1991regularity;@app_en],  що  визначає  можливість передбачувати  флуктуації  в  часових рядах.  Інтуїтивно  вона  означає,  що  наявність  повторюваних  шаблонів  (послідовностей певної  довжини,  побудованих  із  чисел  ряду,  що  слідують  одне  за  іншим)  флуктуацій  у часовому ряді призводить до більшої передбачуваності часового ряду порівняно із рядами, де  повторюваності  шаблонів  немає.  Порівняно  велике  значення  *ApEn*  показує  ймовірність того,  що  подібні  між  собою  шаблони  спостережень  не  будуть  слідувати  один  за  одним. Іншими  словами,  часовий  ряд,  що  містить  велику  кількість  повторюваних  шаблонів,  має порівняно  мале  значення  *ApEn*,  а  значення  *ApEn*  для  менш  передбачуваного  (більш складного) процесу є більшим. \n\nПри розрахунку *ApEn* для даного часового ряду  $S_{N}$, що складається із $N$ значень $t(1),t(2),t(3),...,t(N)\\,$ вибираються два параметри, $m$ та $r$. Перший з цих параметрів, $m$,  вказує на довжину  шаблона,  а  другий  ---  $r$ ---  визначає  критерій  подібності.  Досліджуються підпослідовності елементів часового ряду  $S_{N}$, що складаються з $m$ чисел, взятих, починаючи з номера $i$, і називаються векторами  $p_{m} (i)$. Два вектори (шаблони),  $p_{m}(i)$ та  $p_{m}(j)$, будуть подібними, якщо всі різниці пар їх відповідних координат є меншими за значення $r$, тобто якщо \n\n$$ \n| t(i+k) - t(j+k) | < r \\quad \\textrm{для} \\quad 0 \\leq k < m. \n$$\n\nДля  розглядуваної  множини  $P_{m}$ всіх  векторів  довжини  $m$ часового  ряду  $S_{N}$ можна розрахувати значення\n\n$$ \nC_{im}(r) = n_{im}(r) \\Big/ (N-m+1), \n$$\n\nде  $n_{im}(r)$  ---  кількість  векторів  у  $P_{m}$,  що  подібні  вектору  $p_{m}(i)$  (враховуючи  вибраний критерій подібності $r$). Значення $C_{im}(r)$ є часткою векторів довжини $m$, що мають схожість із вектором  такої  ж  довжини,  елементи  якого  починаються  з  номера  $i$.  Для  даного  часового ряду  обраховуються  значення  $C_{im}(r)$ для  кожного  вектора  у  $P_{m}$,  після  чого  знаходиться середнє значення $C_{m}(r)$, що відображає розповсюдженість подібних векторів довжини $m$ у ряду $S_{N}$.  Безпосередньо  ентропія  подібності  для  часового  ряду  $S_{N}$ з  використанням  векторів довжини $m$ та критерію подібності $r$ визначається за формулою:\n\n$$ \nApEn(S_{N}, m, r) = \\ln{C_{m}(r)} - \\ln{C_{m+1}(r)}. \n$$\n\nТобто,  як  натуральний  логарифм  відношення  повторюваності  векторів  довжиною  $m$ до повторюваності векторів довжиною $m+1$. \n\nТаким  чином,  якщо  знайдуться  подібні  вектори  у  часовому  ряді,  *ApEn*  оцінить логарифмічну  ймовірність  того,  що  наступні  інтервали  після  кожного  із  векторів  будуть відрізнятись. Менші значення *ApEn* відповідають більшій ймовірності того, що за векторами слідують подібні їм. Якщо часовий ряд дуже нерегулярний --- наявність подібних векторів не може бути передбачуваною і значення *ApEn* є порівняно великим. \n\nЗауважимо, що *ApEn* є нестійкою до вхідних даних характеристикою, оскільки досить сильно залежить від параметрів $m$ та $r$.\n\nЗберігаємо значення апроксимаційної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\nЯк можна бачити з представленого рисунку ([@fig-n225-wind-apen]), апроксимаційна ентропія падає у кризові та передкризові моменти часу. Це говорить те, що середнє значення кореляційного інтегралу отриманого для фазового простору розмірністю $d_{E+1}$ не сильно відрізняється від того, що було отримано для фазового простору розмірністю $d_{E}$. Тобто, при реконструкції простору в різних вимірах, усі точки просто знаходяться достатньо близько один до одного, не дивлячись на геометричні перетворення атрактора фондового ринку. Це вказує на досить високий ступінь кореляцій цінових флуктуацій індексу N225 і спрямованість трейдерів ринку по одному тренду. \n\n### Нечітка ентропія (FuzzEn)\n\nОднією з модифікацій ентропії Шеннона є нечітка ентропія (*FuzzEn*) [@fuzz_en_approx_1;@XIE20087140]. Цей підхід виключає самоподібність між досліджуваними векторами, і замість функції Гевісайда, яка видає або 0, або 1 для схожих векторів, використовується нечітка функція належності, яка у випадку *FuzzEn* буде асоціювати схожість між двома векторами з реальним значенням у діапазоні $[0, 1]$. Різницю можна побачити на етапі побудови вектора вкладень, де для реконструйованих векторів ми виконуємо детрендування:\n\n$$\n\\vec{X}(i) = \\left\\{ x(i), x(i+1),..., x(i+d_E - 1) - x_{0}(i) \\right\\}, \\,\\, i=1,...,N-d_{E}+1, \n$$\n\n$x_{0}(i) = (d_{E})^{-1} \\sum_{j=0}^{d_{E}-1}x(i+j)$. Далі, для послідовних вбудованих векторів знаходиться відстань \n\n$$\nd\\left[ \\vec{X}(i), \\vec{X}(j) \\right] = \\max{\\left| \\vec{X}(i) - \\vec{X}(j) \\right|}, i\\geq 1, \\, j \\leq N - d_E + 1.\n$$\n\nУ класичній *ApEn* значення відстаней пропускаються через функцію Гевісайда. Нечітка модифікація використовує функції належності для вимірювання належності однієї траєкторії до іншої: \n\n$$\nD_{i, j} = \\mu\\left( d\\left[\\vec{X}(i), \\vec{X}(j) \\right]\\right),\n$$\n\n$\\mu = \\exp\\left( -x^{r_2} \\big/ r_1 \\right)$, а $r_1$ і $r_2$ ширина та градієнт експоненціальної функції.\n\nДалі, обчислюється наступна функція, що подібна до кореляційного інтегралу в класичній *ApEn*:\n\n$$\n\\phi^{d_E} = \\frac{1}{(N-d_{E}+1)}\\sum_{i=1}^{N-d_{E}+1}\\left( \\frac{1}{N-d_{E}}\\sum_{j=1, j \\neq i}^{N-d_{E}} D_{i, j} \\right).\n$$\n\nНарешті, \n\n$$\nFuzzEn(\\vec{X}, d_{E}) = -\\left[ \\ln{\\phi^{d_{E}+1} - \\ln{\\phi^{d_{E}}}} \\right].\n$$\n\n\nЗберігаємо значення нечіткої ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\n[@fig-n225-wind-fuzzen] демонструє спадку динаміку нечіткої ентропії в кризові та передкризові періоди, яка працює по аналогії до апроксимаційної ентропії. Із цього випливає, що нечітка ентропія також вказує на зростання корельованості системи та зростання її трендостійкості через однонаправленність думок трейдерів щодо подальшої динаміки фондового ринку.\n\n### Ентропія шаблонів (SampEn)\n\nПри розрахунку *ApEn* беруться до уваги подібності певного вектора $p_n(i)$ до самого себе, що використовується для уникнення можливого значення $\\ln{0}$ при відсутності подібних до даного векторів. Однак, вказана особливість призводить до нівелювання двох важливих характеристик у ентропії подібності:\n\n- *ApEn* сильно залежить від довжини розглядуваного шаблона (вектора) і є нижчою, ніж очікується, для векторів малої розмірності;\n- *ApEn* не враховує відносну щільність даних.\n\nЦе означає, що коли значення *ApEn* для одного ряду є більшим, ніж для іншого, то воно повинно залишатись таким (проте не є) для будь-яких можливих початкових умов. Такий висновок тим більш важливий, оскільки *ApEn* рекомендується в якості міри порівняння двох наборів даних різними авторами.\n\nВраховуючи вказані обмеження, розроблена для розрахунку інша характеристика, --- **ентропія шаблонів** (Sample Entropy, *SampEn*) [@samp_en].\n\nПри розрахунку *SampEn*, на відміну від алгоритму *ApEn*, додаються дві умови:\n\n- не враховується подібність вектора самому собі;\n- при розрахунку значень умовних ймовірностей SampEn не використовується довжина векторів.\n\nНа основі аналізу вищезазначеного можна зробити висновок про те, що *SampEn*:\n\n- більше, ніж *ApEn*, відповідає теорії випадкових чисел для ряду із відомою функцією щільності розподілу;\n- зберігає відносну щільність, в той час як *ApEn* втрачає дану характеристику;\n- додає значно меншу помилку до розрахованого значення у випадку використання векторів малої розмірності.\n\nЗберігаємо значення ентропії шаблонів до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\n[@fig-n225-wind-sampen] демонструє, що ентропія шаблонів спадає в передкризові періоди фондового ринку, що вказує на зростання корельованості траєкторій реконструйованого фазового простору індексу N225. Це говорить про те, що ринок у передкрахові періоди стає більш упорядкованим і трендостійким. \n\n### Ентропія перестановок (PEn)\n\nЕнтропія перестановок (*PEn*) --- це міра з теорії хаосу, запропонована Бандтом і Помпе [@perm_en], і характеризується концептуальною простотою і швидкістю обчислень. Ідея *PEn* базується на звичайній ентропії Шеннона, але використовує патерни перестановок --- порядкові відношення між значеннями системи. Порівняно з іншими мірами складності має певні переваги, такі як стійкість до шуму та інваріантність до нелінійних монотонних перетворень [@kantz2004nonlinear].\n\nЯк і в попередніх типах ентропії, ми реконструюємо часовий ряд із $N$ значень з фіксованою розмірністю вбудовування $d_E$ та часовою затримкою $\\tau$, за вкладеною матрицею формуємо часові векторні послідовності\n\n$$\n\\vec{X}(i) = \\left\\{ x(i), x(i+1),..., x(i+[d_E - 1]\\tau) \\right\\}, \n$$\n\nі в результаті отримується $N - [d_E - 1]\\tau$ векторів. \n\nКожен елемент $\\vec{X}(i)$ перетворюється в числові ранги відповідно до їх порядку. Наприклад, для $d_E = 2$ і $\\tau = 1$ та часового ряду $\\vec{X}(i) = (-0.1, 0.4, 3.2, 12.0, 6.5)$, вбудована матриця матиме такі пари: $\\vec{X}(1) = {-0.1, 0.4}$, $\\vec{X}(2) = {0.4, 3.2}$, $\\vec{X}(3) = {3.2, 12.0}$, $\\vec{X}(4) = {12.0, 6.5}$. \n\nДалі ми формуємо порядкові послідовності відповідно до їх числового порядку. Такі вектори як $\\vec{X}(1), \\vec{X}(2), \\vec{X}(3)$ задовольняють умові $x(i) < x(i+1)$ і один вектор $\\vec{X}(4)$ задовольняє умові $x(i) > x(i+1)$. Можна розглянути $d_E!$ можливих перестановок порядку $d_E$. У нашому прикладі є лише $2!$ шаблони: $\\pi_1 = {0, 1}, \\pi_2 = {1, 0}$.\n\nДля кожного шаблону ми визначаємо його відносну частоту:\n\n$$\np(\\pi) = \\#\\left\\{\\vec{X}(i) \\,\\, \\text{має шаблон} \\,\\, \\pi \\right\\} \\Big/ (N - [d_E - 1]\\tau).\n$$\n\nІмовірність знаходження вектора по шаблону $\\pi_1$ дорівнює $3/4$ і по шаблону $\\pi_2$ дорівнює $1/4$, тобто, ми формуємо розподіл імовірностей $P = \\left\\{ p(\\pi_{1}),...,p(\\pi_{d_E}) \\right\\}$. Нарешті, даний вид ентропії може бути розрахований у той самий спосіб, що й ентрпія Шеннона:\n\n$$\nPEn(\\vec{X}, d_E) = -\\sum_{i=1}^{d_E}p(\\pi_i)\\ln{p(\\pi_i)}.\n$$\n\nДля зручності *PEn* нормалізується згідно наступного рівняння:\n\n$$\n\\overline{PEn(\\vec{X}, d_E)} = PEn(\\vec{X}, d_E) \\Big/ PEn_{max},\n$$\n\nде $PEn_{max} = \\ln{D!}$, а нормалізована ентропія перестановок знаходиться в діапазоні $0 \\leq \\overline{PEn(\\vec{X}, d_E)} \\leq 1$. \n\nЗберігаємо значення пермутаційної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВізуалізуємо результат для ентропії перестановок:\n\nНа представленому рисунку ([@fig-n225-wind-permen]) видно, що $PEn$ спадає в кризові та передкризові періоди на фондовому ринку. Це вказує на зростання ймовірності появи одного конкретного патерна для подальшої динаміки ринку, а отже й кількості очікуваної інформації при аналізі флуктуацій індексу N225. \n\n### Eнтропія сингулярного розкладу (SVDEn)\n\nЕнтропію сингулярного розкладу (*SVDEn*) [@roberts1999temporal] можна інтуїтивно розглядати як показник того, скільки власних векторів потрібно для адекватного пояснення набору даних. Іншими словами, вона вимірює багатство ознак: чим вища *SVDEn*, тим більше ортогональних векторів потрібно для адекватного пояснення стану простору. Подібно до інформаційого показника Фішера, *SVDEn* базується на розкладанні сингулярного значення реконструйованого методом часових затримок сигналу.\n\nЗберігаємо значення розподільної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\n[@fig-n225-wind-svden] показує, що ентропія сингулярного розкладу спадає у (перед)кризові періоди, що говорить про зростання кореляцій на фондовому ринку. Оскільки $SVDEn$ базується на розподілі власних векторів, можна зробити припущення, що в передкризові моменти часу динамікою ринку керують один або декілька власних векторів, які і є рушійною складновою досліджуваного індексу. \n\n### Дисперсійна ентропія (DispEn)\n\nДля заданого одновимірного сигналу довжини $N$: $x=\\{x_1, x_2,...,x_N\\}$, алгоритм дисперсійної ентропії (*DispEn*) включає 4 основних кроки [@disp]:\n\n1) Спочатку $x_j \\, (j=1,2,...N)$ відображаються на $c$ класів, позначених від $1$ до $c$. Для цього існує ряд лінійних та нелінійних підходів. Хоча лінійний алгоритм відображення є найшвидшим, коли максимальні та/або мінімальні значення часового ряду набагато більші або менші за середнє/медіанне значення сигналу, більшість значень $x_i$ віднесено лише до кількох класів. Таким чином, ми спочатку використовуємо нормальну кумулятивну функцію розподілу (NCDF) для відображення $x$ в $y=\\{y_1,y_2,...,y_N\\}$ від $0$ до $1$. Далі виконується лінійний алгоритм присвоєння кожному $y_j$ цілого числа від $1$ до $c$. Для цього для кожного члена відображеного сигналу ми використовуємо $z_{j}^{c} = \\text{round}(y_{j} + 0.5)$, де $z_{j}^{c}$ показує $j$-й член класифікованого часового ряду, а округлення передбачає або збільшення, або зменшення числа до наступної цифри. Варто зазначити, що цей крок можна виконати за допомогою інших лінійних та нелінійних методів відображення.\n2) Кожен вектор $\\mathbf{z}_{i}^{d_E, c}$ з розмірністю $d_E$ та часовою затримкою $\\tau$ має вид $\\mathbf{z}_{i}^{d_E, c} = \\{ z_{i}^{c}, z_{i+\\tau}^{c},...,z_{i+(d_E-1)\\tau}^{c} \\}$, $i=1,...,N-(d_E-1)\\tau$ і проектується на дисперсійний шаблон $\\pi_{v_0, v_1,...,v_{d_E-1}}$, де $z_{i}^{c}=v_{0}, z_{i+\\tau}^{c}=v_{1},...,z_{i+(d_{E}-1)\\tau}^{c}=v_{d_{E}-1}$. Кількість можливих дисперсійних шаблонів, що може бути присвоєна кожному вектору $\\mathbf{z}_{i}^{d_E, c}$, дорівнює $c^{m}$, оскільки сигнал має $d_E$ елементів і кожному елементу може бути присвоєно ціле значення від $1$ до $c$. \n3) Для всіх потенційних $c^{m}$ дисперсійних шаблонів розраховується відносна частота:\n\n   $$\n      p(\\pi_{v_0, v_1,...,v_{d_{E}-1}}) = \\frac{\\#\\left\\{i | i \\leq N-(d_E-1)\\tau, \\mathbf{z}_{i}^{d_E, c} \\,\\, \\text{має шаблон} \\,\\, \\pi_{v_0, v_1,...,v_{d_{E}-1}} \\right\\} }{N-(d_{E}-1)\\tau}.\n   $$\n\n4) Нарешті, опираючись на формулу ентропії Шеннона, *DispEn* розраховується як \n\n   $$\n      DispEn(\\mathbf{x}, d_E, c, \\tau) = -\\sum_{\\pi = 1}^{c^{d_E}}p(\\pi_{v_0, v_1,...,v_{d_{E}-1}})\\ln{(\\pi_{v_0, v_1,...,v_{d_{E}-1}})}.\n   $$\n\nКоли всі можливі дисперсійні шаблони мають однакову ймовірність, отримуємо найбільше значення *DispEn*, яке має величину $\\ln{c^{d_E}}$. І навпаки, якщо тільки одне $p(\\pi_{v_0, v_1,...,v_{d_{E}-1}})$ відрізняється від нуля (абсолютно регулярний/передбачуваний сигнал), отримуємо найменше значення *DispEn*.\n\nЗберігаємо значення дисперсійної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\n[@fig-n225-wind-dispen] демонструє, що дисперсійна ентропія падає напередодні крахових періодів. Особливо помітним це предстає для крахів 1970, 1990, 2010 та 2020. Це говорить про те, що розподіл дисперсійних шаблонів стає зміщенним, що й відзеркалюється у спаді ентропії. Це також вказує на періодизацію ринку. Для періодів, коли дисперсійна ентропія максимальна, очікування трейдерів також залишаються різносторонніми, що робить ринок більш непередбачуваним.  \n\n### Спектральна ентропія (SpEn)\n\nСпектральна ентропія (*SE* або *SpEn*) [@CrepeauIsaacson+1991+137+152] розглядає нормовану щільність спектра потужності (PSD) сигналу в частотній області як розподіл імовірностей і обчислює його ентропію Шеннона:\n\n$$\nSpEn = -\\sum P(f)\\log_{2}[P(f)]. \n$$\n\nСигнал з однією частотною складовою (наприклад, чиста синусоїда) має найменшу ентропію. З іншого боку, сигнал з усіма частотними компонентами однакової потужності (білий шум) дає найбільшу ентропію.\n\nЗберігаємо значення спектральної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\nЗ [@fig-n225-wind-spen] видно, що спектральна ентропія починає падати в передкризовий період, що говорить про зміщення спектра потужності в конкретну область частот. Це вказує на періодизацію динаміки ринку, що у свою чергу вказує на зростання кореляцій і трендостійкості ринку. \n\n## Висновок\n\nНа прикладі ентропійних мір складності перевірено гіпотезу про зв'язок мір складності та кризових явищ, висунуту на основі теорії складних систем. У рамках алгоритму ковзного вікна за набором ентропійних показників було показано, що фінансові крахи характеризуються зміною складності: у передкризовий період, як правило, ми можемо спостерігати упорядкування системи, а в кризовий та післякризовий періоди зростання хаотичності. Порівняння ентропійних характеристик відкриває можливість передчасної ідентифікації та попередження ня кризових явищ у системах різної природи та складності. \n\nТаким чином, представлені індикатори-передвісники кризових явищ, теоретично, дозволяють обійти потребу в значних обчислювальних ресурсах і досить дискусійних методів прогнозування цінових коливань і їх трендів.\n\n## Завдання для самостійної роботи\n\n1. Проведіть порівняльний аналіз ентропійних показників для тестових часових рядів\n2. Побудуйте часові ряди, які у визначених точках характеризуються кризами і покажіть, чи є (і які саме) ентропійні міри індикаторами, або передвісниками кризових явищ\n3. Зробіть загальні висновки і оформіть звіт\n","srcMarkdownNoYaml":"\n\n\n\n\n**Тема.** Ентропійний аналіз складних систем\n\n**Мета.** Навчитись  розраховувати  значення  різних  типів  ентропії  часового  ряду  та досліджувати динаміку зміни її значень для оцінки якості прогнозів часових рядів\n\n\n\n## Теоретичні відомості\n\nПитання динаміки розвитку і функціонування складних систем може розглядатись у двох варіантах:\n\n- як дослідження шумової активності;\n- як детерміністичного випадку з певним ступенем порядку. \n\nОстанніми роками було використано кілька підходів для ідентифікації механізмів, що \nлежать в основі еволюції складних. Особливо корисні результати \nбуло  отримано  при  їх  дослідженні  методами  теорії  випадкових  матриць,  моно-  та \nмультифрактального аналізу, теорії хаосу з реконструкцією траєкторії системи  у фазовому \nпросторі,  рекурентного аналізу тощо.  Ми  розглянули  ці  методи  у \nпопередніх роботах. Однак, застосування деяких із методів висуває вимоги до стаціонарності \nдосліджуваних даних, потребує довгих часових рядів та комплексного обчислення кількох \nпараметрів. \n\nІншим широко відомим підходом моделювання особливостей складних систем є обчислення характеристик різних видів ентропії. \n\nКонцепція термодинамічної ентропії як міри хаосу системи добре відома у фізиці, однак, останніми роками поняття ентропії було застосоване до складних систем інших об’єктів (біологічних, економічних, соціальних тощо). Так, один із найбільш часто використовуваних методів визначення ентропії базується на обчисленні спектру потужності Фур'є та застосовується для вивчення часових рядів різної природи. Проте, використання дискретного перетворення Фур’є для аналізу часових рядів має свої недоліки, зокрема, на результати впливає нестаціонарність рядів, варіювання їх довжини від сотень до сотень тисяч, та обмеження самого методу (незмінність частотно-часових характеристик протягом всього часу функціонування системи). Тому виникає питання про розрахунок значень ентропії за допомогою інших методів.\n\nВведемо поняття ентропії, скориставшись інформацією, яку можна знайти у [Вікіпедії](https://uk.wikipedia.org/wiki/Ентропія).\n\n**Термодинамічна ентропія**  $S$,  часто  просто  іменована  **ентропія**,  в  хімії  і термодинаміці є мірою кількості енергії у  фізичній системі, яка не може бути використана для виконання роботи. Вона також є мірою безладдя, присутнього в системі. \n\nПоняття  ентропії  була  вперше  введено  у  1865  році  Рудольфом  Клаузіусом [@clausius1867mechanical].  Він визначив  зміну  ентропії  термодинамічної  системи  при  оборотному  процесі  як  відношення зміни загальної кількості тепла $\\Delta Q$ до величини абсолютної температури $T$: \n\n$$ \n\\Delta S = \\Delta Q / T. \n$$\n\nРудольф Клаузіус дав величині $S$ ім'я \"*ентропія*\", що походить від грецького слова τρoπή, \n\"<i>зміна</i>\" (зміна,  перетворення). \n\nУ 1877 році, Людвіг  Больцман [@Boltzmann1970] зрозумів, що  ентропія системи може відноситися до кількості  можливих \"мікростанів\"  (мікроскопічних  станів),  що  узгоджуються  з  їх термодинамічними  властивостями.  Розглянемо,  наприклад,  ідеальний  газ  у  посудині. Мікростан визначений як позиції і імпульси кожного атома, що становить систему. Зв'язність пред'являє до нас вимоги розглядати тільки ті мікростани, для яких: (i) місце розташування всіх частин обмежене границями судини, (ii) для отримання загальної енергії газу кінетичні енергії атомів підсумовуються. Больцман постулював що \n\n$$ \nS = k_{B}\\ln{\\Omega}, \n$$ \n\nде  константу  $k_{B} = 1,38 \\cdot 10^{-23} Дж/К$   ми  знаємо  тепер  як  сталу  Больцмана, a $\\Omega$  є  числом мікростанів,  які  можливі  в  наявному  макроскопічному  стані.  Цей  постулат,  відомий  як принцип  Больцмана,  може  бути  оцінений  як  початок  статистичної  механіки, що описує термодинамічні системи з використанням статистичної поведінки компонентів. Принцип Больцмана зв'язує мікроскопічні властивості системи ($\\Omega$) з однією з її  термодинамічних властивостей ($S$). \n\nЗгідно  визначенню  Больцмана,  ентропія  є  просто  функцією  стану.  Більш  того, оскільки  ($\\Omega$)  може  бути  тільки  натуральним числом,  ентропія  повинна  бути додатною --- виходячи з властивостей логарифма.\n\nУ  випадку  дискретних  станів  квантової  механіки  кількість  станів  підраховується звичайним  чином.  У  рамках  класичної  механіки  мікроскопічний  стан  системи  описується координатами $q_{i}$ й імпульсами $p_{i}$ окремих частинок, які пробігають неперервні значення. У такому випадку\n\n$$ \nS = k_{B}\\ln\\frac{1}{( 2\\pi\\hbar )^{s}} \\int \\prod_{i=1}^{s} dq_{i}dp_{i}, \n$$\n\nде $s$ --- число незалежних координат, $\\hbar$ --- приведена стала Планка, а інтегрування проводиться по області фазового простору, який відповідає певному макроскопічному стану. \n\nКлод  Шеннон  [@shannon]  запропонував  формулу  для  оцінки  невизначеності кодової інформації в каналах зв'язку, звану ентропією Шеннона:\n\n$$ \nS = -k\\sum_{i=1}^{n}p_{i}\\ln{p_{i}}, \n$$\n\nде $p_{i}$ --- вірогідність того, що символ $i$ зустрічається в коді, який містить $N$ символів, $k$ --- розмірний множник.  \n\nЗв'язок між ентропією і інформацією можна прослідкувати на наступному прикладі. Розглянемо тіло при абсолютному нулі температури, і хай ми маємо повну інформацію про координати і імпульси кожної частинки. Для простоти покладемо, що імпульси всіх частинок рівні нулю. В цьому випадку термодинамічна ймовірність рівна одиниці, а ентропія --- нулю. При  кінцевих  температурах  ентропія  в  рівновазі  досягає  максимуму.  Можна  зміряти  всі макропараметри, що характеризують даний макростан. Проте ми практично нічого не знаємо про мікростан системи. Точніше кажучи, ми знаємо, що даний макростан можна реалізувати за допомогою дуже великого числа мікростанів. Таким чином, нульовій ентропії відповідає повна інформація (ступінь незнання рівний нулю), а максимальної ентропії --- повне незнання мікростанів (ступінь незнання максимальний).\n\nУ  теорії  інформації  ентропія  (інформаційна  ентропія)  визначається  як  кількість інформації. Нехай $P$ --- апріорна вірогідність деякої події (ймовірність до проведення досвіду), а  $P_{1}$ --- ймовірність цієї події після проведення досвіду. Для простоти вважатимемо, що $P_{1} = 1$. За  Шенноном,  кількість  інформації  $I$,  яка  дає  точну  відповідь  (після  проведення експерименту) \n\n$$ \nI = K \\log{P}. \n$$  \n\nЦя кількість інформації, за визначенням, дорівнює одному біту. \n\nФізичний сенс $I$ --- це міра нашого незнання. Іншими словами, $I$ --- це та інформація, яку ми  можемо  одержати,  вирішивши  завдання.  У  прикладі  (тіло  при  абсолютному  нулі температури),  що  розглядається  вище,  міра  нашого  незнання  рівна  нулю,  оскільки  $P = 1$. Після проведення досвіду ми одержуємо нульову інформацію $I = 0$, оскільки все було відомо до  досвіду.  Якщо  розглядати  тіло  при  кінцевих  температурах,  то  до  проведення  досвіду число мікростанів, а отже, і $P$ дуже велике. Після проведення досвіду ми одержуємо велику інформацію, оскільки нам стають відомими координати і імпульси всіх частинок.\n\nАналогія  між  кількістю  інформації  і  ентропією $S$,  визначуваною  з  принципу Больцмана,  очевидна.  Досить покласти  множник  $K$  рівним  постійній  Больцмана  $k_{B}$ і використовувати  натуральний  логарифм.  Саме  з  цієї  причини  величину $I$  називають інформаційною ентропією. Інформаційна ентропія (кількість інформації) була визначена по аналогії із звичайною ентропією, і вона має властивості, характерні для звичайній ентропії: адитивність, екстремальні  властивості  і  т.д.  Проте  ототожнювати  звичайну  ентропію  з інформаційною не можна, оскільки неясно, яке відношення має другий закон термодинаміки до інформації. Нагадаємо, що екстенсивна величина --- ця така характеристика  системи,  яка  росте зі збільшенням  розмірів  системи,  тобто,  якщо  наша  система  складається  з  двох  *незалежних* підсистем $А$ і $В$, то ентропію всієї системи можна одержати складанням ентропій підсистем:\n\n$$ \nS(A + B) = S(A) + S(B). \n$$\n\nСаме ця властивість і означає екстенсивність, або адитивність, ентропії.\n\n## Хід роботи\n\nРозглянемо, яким чином ми використовувати ентропійні показники в якості індикаторів або індикаторів-передвісників кризових подій. Перш за все імпортуємо необхідні модулі для подальшої роботи:\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nУ даній роботі виконаємо розрахунки на прикладі одного з найважливіших фондових індексів Японії --- Nikkei 225. Індекс обчислюється шляхом визначення простого середнього арифметичного значення цін акцій 225 провідних компаній, які входять до першої секції Токійської фондової біржі. Для отримання значень індексу скористаємось бібліотекою `yfinance`. Значення розглядатимемо за весь період, тому початкову та кінцеву дати вказувати не будемо.  \n\n::: {.callout-warning}\n## Увага\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо \n\n:::\n\n---\n\n---\n\nВиводимо досліджуваний ряд:\n\nДля приведення ряду до стандартизованого вигляду або прибутковостей визначимо функцію `transformations()`:\n\nДля побудови пари часових рядів визначимо функцію `plot_pair()`:\n\n### Апроксимаційна ентропія (ApEn)\n\nЕнтропія  подібності  (Approximate  Entropy,  *ApEn*)  є \"статистикою регулярності\" [@pincus1991regularity;@app_en],  що  визначає  можливість передбачувати  флуктуації  в  часових рядах.  Інтуїтивно  вона  означає,  що  наявність  повторюваних  шаблонів  (послідовностей певної  довжини,  побудованих  із  чисел  ряду,  що  слідують  одне  за  іншим)  флуктуацій  у часовому ряді призводить до більшої передбачуваності часового ряду порівняно із рядами, де  повторюваності  шаблонів  немає.  Порівняно  велике  значення  *ApEn*  показує  ймовірність того,  що  подібні  між  собою  шаблони  спостережень  не  будуть  слідувати  один  за  одним. Іншими  словами,  часовий  ряд,  що  містить  велику  кількість  повторюваних  шаблонів,  має порівняно  мале  значення  *ApEn*,  а  значення  *ApEn*  для  менш  передбачуваного  (більш складного) процесу є більшим. \n\nПри розрахунку *ApEn* для даного часового ряду  $S_{N}$, що складається із $N$ значень $t(1),t(2),t(3),...,t(N)\\,$ вибираються два параметри, $m$ та $r$. Перший з цих параметрів, $m$,  вказує на довжину  шаблона,  а  другий  ---  $r$ ---  визначає  критерій  подібності.  Досліджуються підпослідовності елементів часового ряду  $S_{N}$, що складаються з $m$ чисел, взятих, починаючи з номера $i$, і називаються векторами  $p_{m} (i)$. Два вектори (шаблони),  $p_{m}(i)$ та  $p_{m}(j)$, будуть подібними, якщо всі різниці пар їх відповідних координат є меншими за значення $r$, тобто якщо \n\n$$ \n| t(i+k) - t(j+k) | < r \\quad \\textrm{для} \\quad 0 \\leq k < m. \n$$\n\nДля  розглядуваної  множини  $P_{m}$ всіх  векторів  довжини  $m$ часового  ряду  $S_{N}$ можна розрахувати значення\n\n$$ \nC_{im}(r) = n_{im}(r) \\Big/ (N-m+1), \n$$\n\nде  $n_{im}(r)$  ---  кількість  векторів  у  $P_{m}$,  що  подібні  вектору  $p_{m}(i)$  (враховуючи  вибраний критерій подібності $r$). Значення $C_{im}(r)$ є часткою векторів довжини $m$, що мають схожість із вектором  такої  ж  довжини,  елементи  якого  починаються  з  номера  $i$.  Для  даного  часового ряду  обраховуються  значення  $C_{im}(r)$ для  кожного  вектора  у  $P_{m}$,  після  чого  знаходиться середнє значення $C_{m}(r)$, що відображає розповсюдженість подібних векторів довжини $m$ у ряду $S_{N}$.  Безпосередньо  ентропія  подібності  для  часового  ряду  $S_{N}$ з  використанням  векторів довжини $m$ та критерію подібності $r$ визначається за формулою:\n\n$$ \nApEn(S_{N}, m, r) = \\ln{C_{m}(r)} - \\ln{C_{m+1}(r)}. \n$$\n\nТобто,  як  натуральний  логарифм  відношення  повторюваності  векторів  довжиною  $m$ до повторюваності векторів довжиною $m+1$. \n\nТаким  чином,  якщо  знайдуться  подібні  вектори  у  часовому  ряді,  *ApEn*  оцінить логарифмічну  ймовірність  того,  що  наступні  інтервали  після  кожного  із  векторів  будуть відрізнятись. Менші значення *ApEn* відповідають більшій ймовірності того, що за векторами слідують подібні їм. Якщо часовий ряд дуже нерегулярний --- наявність подібних векторів не може бути передбачуваною і значення *ApEn* є порівняно великим. \n\nЗауважимо, що *ApEn* є нестійкою до вхідних даних характеристикою, оскільки досить сильно залежить від параметрів $m$ та $r$.\n\nЗберігаємо значення апроксимаційної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\nЯк можна бачити з представленого рисунку ([@fig-n225-wind-apen]), апроксимаційна ентропія падає у кризові та передкризові моменти часу. Це говорить те, що середнє значення кореляційного інтегралу отриманого для фазового простору розмірністю $d_{E+1}$ не сильно відрізняється від того, що було отримано для фазового простору розмірністю $d_{E}$. Тобто, при реконструкції простору в різних вимірах, усі точки просто знаходяться достатньо близько один до одного, не дивлячись на геометричні перетворення атрактора фондового ринку. Це вказує на досить високий ступінь кореляцій цінових флуктуацій індексу N225 і спрямованість трейдерів ринку по одному тренду. \n\n### Нечітка ентропія (FuzzEn)\n\nОднією з модифікацій ентропії Шеннона є нечітка ентропія (*FuzzEn*) [@fuzz_en_approx_1;@XIE20087140]. Цей підхід виключає самоподібність між досліджуваними векторами, і замість функції Гевісайда, яка видає або 0, або 1 для схожих векторів, використовується нечітка функція належності, яка у випадку *FuzzEn* буде асоціювати схожість між двома векторами з реальним значенням у діапазоні $[0, 1]$. Різницю можна побачити на етапі побудови вектора вкладень, де для реконструйованих векторів ми виконуємо детрендування:\n\n$$\n\\vec{X}(i) = \\left\\{ x(i), x(i+1),..., x(i+d_E - 1) - x_{0}(i) \\right\\}, \\,\\, i=1,...,N-d_{E}+1, \n$$\n\n$x_{0}(i) = (d_{E})^{-1} \\sum_{j=0}^{d_{E}-1}x(i+j)$. Далі, для послідовних вбудованих векторів знаходиться відстань \n\n$$\nd\\left[ \\vec{X}(i), \\vec{X}(j) \\right] = \\max{\\left| \\vec{X}(i) - \\vec{X}(j) \\right|}, i\\geq 1, \\, j \\leq N - d_E + 1.\n$$\n\nУ класичній *ApEn* значення відстаней пропускаються через функцію Гевісайда. Нечітка модифікація використовує функції належності для вимірювання належності однієї траєкторії до іншої: \n\n$$\nD_{i, j} = \\mu\\left( d\\left[\\vec{X}(i), \\vec{X}(j) \\right]\\right),\n$$\n\n$\\mu = \\exp\\left( -x^{r_2} \\big/ r_1 \\right)$, а $r_1$ і $r_2$ ширина та градієнт експоненціальної функції.\n\nДалі, обчислюється наступна функція, що подібна до кореляційного інтегралу в класичній *ApEn*:\n\n$$\n\\phi^{d_E} = \\frac{1}{(N-d_{E}+1)}\\sum_{i=1}^{N-d_{E}+1}\\left( \\frac{1}{N-d_{E}}\\sum_{j=1, j \\neq i}^{N-d_{E}} D_{i, j} \\right).\n$$\n\nНарешті, \n\n$$\nFuzzEn(\\vec{X}, d_{E}) = -\\left[ \\ln{\\phi^{d_{E}+1} - \\ln{\\phi^{d_{E}}}} \\right].\n$$\n\n\nЗберігаємо значення нечіткої ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\n[@fig-n225-wind-fuzzen] демонструє спадку динаміку нечіткої ентропії в кризові та передкризові періоди, яка працює по аналогії до апроксимаційної ентропії. Із цього випливає, що нечітка ентропія також вказує на зростання корельованості системи та зростання її трендостійкості через однонаправленність думок трейдерів щодо подальшої динаміки фондового ринку.\n\n### Ентропія шаблонів (SampEn)\n\nПри розрахунку *ApEn* беруться до уваги подібності певного вектора $p_n(i)$ до самого себе, що використовується для уникнення можливого значення $\\ln{0}$ при відсутності подібних до даного векторів. Однак, вказана особливість призводить до нівелювання двох важливих характеристик у ентропії подібності:\n\n- *ApEn* сильно залежить від довжини розглядуваного шаблона (вектора) і є нижчою, ніж очікується, для векторів малої розмірності;\n- *ApEn* не враховує відносну щільність даних.\n\nЦе означає, що коли значення *ApEn* для одного ряду є більшим, ніж для іншого, то воно повинно залишатись таким (проте не є) для будь-яких можливих початкових умов. Такий висновок тим більш важливий, оскільки *ApEn* рекомендується в якості міри порівняння двох наборів даних різними авторами.\n\nВраховуючи вказані обмеження, розроблена для розрахунку інша характеристика, --- **ентропія шаблонів** (Sample Entropy, *SampEn*) [@samp_en].\n\nПри розрахунку *SampEn*, на відміну від алгоритму *ApEn*, додаються дві умови:\n\n- не враховується подібність вектора самому собі;\n- при розрахунку значень умовних ймовірностей SampEn не використовується довжина векторів.\n\nНа основі аналізу вищезазначеного можна зробити висновок про те, що *SampEn*:\n\n- більше, ніж *ApEn*, відповідає теорії випадкових чисел для ряду із відомою функцією щільності розподілу;\n- зберігає відносну щільність, в той час як *ApEn* втрачає дану характеристику;\n- додає значно меншу помилку до розрахованого значення у випадку використання векторів малої розмірності.\n\nЗберігаємо значення ентропії шаблонів до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\n[@fig-n225-wind-sampen] демонструє, що ентропія шаблонів спадає в передкризові періоди фондового ринку, що вказує на зростання корельованості траєкторій реконструйованого фазового простору індексу N225. Це говорить про те, що ринок у передкрахові періоди стає більш упорядкованим і трендостійким. \n\n### Ентропія перестановок (PEn)\n\nЕнтропія перестановок (*PEn*) --- це міра з теорії хаосу, запропонована Бандтом і Помпе [@perm_en], і характеризується концептуальною простотою і швидкістю обчислень. Ідея *PEn* базується на звичайній ентропії Шеннона, але використовує патерни перестановок --- порядкові відношення між значеннями системи. Порівняно з іншими мірами складності має певні переваги, такі як стійкість до шуму та інваріантність до нелінійних монотонних перетворень [@kantz2004nonlinear].\n\nЯк і в попередніх типах ентропії, ми реконструюємо часовий ряд із $N$ значень з фіксованою розмірністю вбудовування $d_E$ та часовою затримкою $\\tau$, за вкладеною матрицею формуємо часові векторні послідовності\n\n$$\n\\vec{X}(i) = \\left\\{ x(i), x(i+1),..., x(i+[d_E - 1]\\tau) \\right\\}, \n$$\n\nі в результаті отримується $N - [d_E - 1]\\tau$ векторів. \n\nКожен елемент $\\vec{X}(i)$ перетворюється в числові ранги відповідно до їх порядку. Наприклад, для $d_E = 2$ і $\\tau = 1$ та часового ряду $\\vec{X}(i) = (-0.1, 0.4, 3.2, 12.0, 6.5)$, вбудована матриця матиме такі пари: $\\vec{X}(1) = {-0.1, 0.4}$, $\\vec{X}(2) = {0.4, 3.2}$, $\\vec{X}(3) = {3.2, 12.0}$, $\\vec{X}(4) = {12.0, 6.5}$. \n\nДалі ми формуємо порядкові послідовності відповідно до їх числового порядку. Такі вектори як $\\vec{X}(1), \\vec{X}(2), \\vec{X}(3)$ задовольняють умові $x(i) < x(i+1)$ і один вектор $\\vec{X}(4)$ задовольняє умові $x(i) > x(i+1)$. Можна розглянути $d_E!$ можливих перестановок порядку $d_E$. У нашому прикладі є лише $2!$ шаблони: $\\pi_1 = {0, 1}, \\pi_2 = {1, 0}$.\n\nДля кожного шаблону ми визначаємо його відносну частоту:\n\n$$\np(\\pi) = \\#\\left\\{\\vec{X}(i) \\,\\, \\text{має шаблон} \\,\\, \\pi \\right\\} \\Big/ (N - [d_E - 1]\\tau).\n$$\n\nІмовірність знаходження вектора по шаблону $\\pi_1$ дорівнює $3/4$ і по шаблону $\\pi_2$ дорівнює $1/4$, тобто, ми формуємо розподіл імовірностей $P = \\left\\{ p(\\pi_{1}),...,p(\\pi_{d_E}) \\right\\}$. Нарешті, даний вид ентропії може бути розрахований у той самий спосіб, що й ентрпія Шеннона:\n\n$$\nPEn(\\vec{X}, d_E) = -\\sum_{i=1}^{d_E}p(\\pi_i)\\ln{p(\\pi_i)}.\n$$\n\nДля зручності *PEn* нормалізується згідно наступного рівняння:\n\n$$\n\\overline{PEn(\\vec{X}, d_E)} = PEn(\\vec{X}, d_E) \\Big/ PEn_{max},\n$$\n\nде $PEn_{max} = \\ln{D!}$, а нормалізована ентропія перестановок знаходиться в діапазоні $0 \\leq \\overline{PEn(\\vec{X}, d_E)} \\leq 1$. \n\nЗберігаємо значення пермутаційної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВізуалізуємо результат для ентропії перестановок:\n\nНа представленому рисунку ([@fig-n225-wind-permen]) видно, що $PEn$ спадає в кризові та передкризові періоди на фондовому ринку. Це вказує на зростання ймовірності появи одного конкретного патерна для подальшої динаміки ринку, а отже й кількості очікуваної інформації при аналізі флуктуацій індексу N225. \n\n### Eнтропія сингулярного розкладу (SVDEn)\n\nЕнтропію сингулярного розкладу (*SVDEn*) [@roberts1999temporal] можна інтуїтивно розглядати як показник того, скільки власних векторів потрібно для адекватного пояснення набору даних. Іншими словами, вона вимірює багатство ознак: чим вища *SVDEn*, тим більше ортогональних векторів потрібно для адекватного пояснення стану простору. Подібно до інформаційого показника Фішера, *SVDEn* базується на розкладанні сингулярного значення реконструйованого методом часових затримок сигналу.\n\nЗберігаємо значення розподільної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\n[@fig-n225-wind-svden] показує, що ентропія сингулярного розкладу спадає у (перед)кризові періоди, що говорить про зростання кореляцій на фондовому ринку. Оскільки $SVDEn$ базується на розподілі власних векторів, можна зробити припущення, що в передкризові моменти часу динамікою ринку керують один або декілька власних векторів, які і є рушійною складновою досліджуваного індексу. \n\n### Дисперсійна ентропія (DispEn)\n\nДля заданого одновимірного сигналу довжини $N$: $x=\\{x_1, x_2,...,x_N\\}$, алгоритм дисперсійної ентропії (*DispEn*) включає 4 основних кроки [@disp]:\n\n1) Спочатку $x_j \\, (j=1,2,...N)$ відображаються на $c$ класів, позначених від $1$ до $c$. Для цього існує ряд лінійних та нелінійних підходів. Хоча лінійний алгоритм відображення є найшвидшим, коли максимальні та/або мінімальні значення часового ряду набагато більші або менші за середнє/медіанне значення сигналу, більшість значень $x_i$ віднесено лише до кількох класів. Таким чином, ми спочатку використовуємо нормальну кумулятивну функцію розподілу (NCDF) для відображення $x$ в $y=\\{y_1,y_2,...,y_N\\}$ від $0$ до $1$. Далі виконується лінійний алгоритм присвоєння кожному $y_j$ цілого числа від $1$ до $c$. Для цього для кожного члена відображеного сигналу ми використовуємо $z_{j}^{c} = \\text{round}(y_{j} + 0.5)$, де $z_{j}^{c}$ показує $j$-й член класифікованого часового ряду, а округлення передбачає або збільшення, або зменшення числа до наступної цифри. Варто зазначити, що цей крок можна виконати за допомогою інших лінійних та нелінійних методів відображення.\n2) Кожен вектор $\\mathbf{z}_{i}^{d_E, c}$ з розмірністю $d_E$ та часовою затримкою $\\tau$ має вид $\\mathbf{z}_{i}^{d_E, c} = \\{ z_{i}^{c}, z_{i+\\tau}^{c},...,z_{i+(d_E-1)\\tau}^{c} \\}$, $i=1,...,N-(d_E-1)\\tau$ і проектується на дисперсійний шаблон $\\pi_{v_0, v_1,...,v_{d_E-1}}$, де $z_{i}^{c}=v_{0}, z_{i+\\tau}^{c}=v_{1},...,z_{i+(d_{E}-1)\\tau}^{c}=v_{d_{E}-1}$. Кількість можливих дисперсійних шаблонів, що може бути присвоєна кожному вектору $\\mathbf{z}_{i}^{d_E, c}$, дорівнює $c^{m}$, оскільки сигнал має $d_E$ елементів і кожному елементу може бути присвоєно ціле значення від $1$ до $c$. \n3) Для всіх потенційних $c^{m}$ дисперсійних шаблонів розраховується відносна частота:\n\n   $$\n      p(\\pi_{v_0, v_1,...,v_{d_{E}-1}}) = \\frac{\\#\\left\\{i | i \\leq N-(d_E-1)\\tau, \\mathbf{z}_{i}^{d_E, c} \\,\\, \\text{має шаблон} \\,\\, \\pi_{v_0, v_1,...,v_{d_{E}-1}} \\right\\} }{N-(d_{E}-1)\\tau}.\n   $$\n\n4) Нарешті, опираючись на формулу ентропії Шеннона, *DispEn* розраховується як \n\n   $$\n      DispEn(\\mathbf{x}, d_E, c, \\tau) = -\\sum_{\\pi = 1}^{c^{d_E}}p(\\pi_{v_0, v_1,...,v_{d_{E}-1}})\\ln{(\\pi_{v_0, v_1,...,v_{d_{E}-1}})}.\n   $$\n\nКоли всі можливі дисперсійні шаблони мають однакову ймовірність, отримуємо найбільше значення *DispEn*, яке має величину $\\ln{c^{d_E}}$. І навпаки, якщо тільки одне $p(\\pi_{v_0, v_1,...,v_{d_{E}-1}})$ відрізняється від нуля (абсолютно регулярний/передбачуваний сигнал), отримуємо найменше значення *DispEn*.\n\nЗберігаємо значення дисперсійної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\n[@fig-n225-wind-dispen] демонструє, що дисперсійна ентропія падає напередодні крахових періодів. Особливо помітним це предстає для крахів 1970, 1990, 2010 та 2020. Це говорить про те, що розподіл дисперсійних шаблонів стає зміщенним, що й відзеркалюється у спаді ентропії. Це також вказує на періодизацію ринку. Для періодів, коли дисперсійна ентропія максимальна, очікування трейдерів також залишаються різносторонніми, що робить ринок більш непередбачуваним.  \n\n### Спектральна ентропія (SpEn)\n\nСпектральна ентропія (*SE* або *SpEn*) [@CrepeauIsaacson+1991+137+152] розглядає нормовану щільність спектра потужності (PSD) сигналу в частотній області як розподіл імовірностей і обчислює його ентропію Шеннона:\n\n$$\nSpEn = -\\sum P(f)\\log_{2}[P(f)]. \n$$\n\nСигнал з однією частотною складовою (наприклад, чиста синусоїда) має найменшу ентропію. З іншого боку, сигнал з усіма частотними компонентами однакової потужності (білий шум) дає найбільшу ентропію.\n\nЗберігаємо значення спектральної ентропії до текстового файлу:\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nВиводимо результат:\n\nЗ [@fig-n225-wind-spen] видно, що спектральна ентропія починає падати в передкризовий період, що говорить про зміщення спектра потужності в конкретну область частот. Це вказує на періодизацію динаміки ринку, що у свою чергу вказує на зростання кореляцій і трендостійкості ринку. \n\n## Висновок\n\nНа прикладі ентропійних мір складності перевірено гіпотезу про зв'язок мір складності та кризових явищ, висунуту на основі теорії складних систем. У рамках алгоритму ковзного вікна за набором ентропійних показників було показано, що фінансові крахи характеризуються зміною складності: у передкризовий період, як правило, ми можемо спостерігати упорядкування системи, а в кризовий та післякризовий періоди зростання хаотичності. Порівняння ентропійних характеристик відкриває можливість передчасної ідентифікації та попередження ня кризових явищ у системах різної природи та складності. \n\nТаким чином, представлені індикатори-передвісники кризових явищ, теоретично, дозволяють обійти потребу в значних обчислювальних ресурсах і досить дискусійних методів прогнозування цінових коливань і їх трендів.\n\n## Завдання для самостійної роботи\n\n1. Проведіть порівняльний аналіз ентропійних показників для тестових часових рядів\n2. Побудуйте часові ряди, які у визначених точках характеризуються кризами і покажіть, чи є (і які саме) ентропійні міри індикаторами, або передвісниками кризових явищ\n3. Зробіть загальні висновки і оформіть звіт\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"highlight-style":"arrow","css":["style.css"],"output-file":"lab_5.html"},"language":{"toc-title-document":"Зміст","toc-title-website":"На цій сторінці","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Анотація","section-title-appendices":"Додатки","section-title-footnotes":"Зноски","section-title-references":"Використана література","section-title-reuse":"Повторне використання","section-title-copyright":"Copyright","section-title-citation":"Цитата","appendix-attribution-cite-as":"Будь-ласка, цитуйте цю роботу як:","appendix-attribution-bibtex":"BibTeX:","appendix-view-license":"View License","title-block-author-single":"Автор","title-block-author-plural":"Автори","title-block-affiliation-single":"Приналежність","title-block-affiliation-plural":"Приналежності","title-block-published":"Дата публікації","title-block-modified":"Змінено","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Розгорнути код","code-tools-hide-all-code":"Приховати код","code-tools-view-source":"Переглянути код","code-tools-source-code":"Вихідний код","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Копіювати","copy-button-tooltip-success":"Скопійовано!","repo-action-links-edit":"Редагувати сторінку","repo-action-links-source":"Переглянути код","repo-action-links-issue":"Повідомити про проблему","back-to-top":"Back to top","search-no-results-text":"Пошук не дав результату","search-matching-documents-text":"Результати пошуку","search-copy-link-title":"Скопіюйте посилання для пошуку","search-hide-matches-text":"Приховати додаткові результати","search-more-match-text":"Додатковий результат у цьому документі","search-more-matches-text":"Додаткові результати у цьому документі","search-clear-button-title":"Очистити","search-text-placeholder":"","search-detached-cancel-button-title":"Скасувати","search-submit-button-title":"Надіслати","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рис.","crossref-tbl-title":"Таблиця","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лема","crossref-cor-title":"Наслідок","crossref-prp-title":"Твердження","crossref-cnj-title":"Гіпотеза","crossref-def-title":"Визначення","crossref-exm-title":"Приклад","crossref-exr-title":"Завдання","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Додаток","crossref-sec-prefix":"Розділ","crossref-eq-prefix":"Рівняння","crossref-lof-title":"Список Рисунків","crossref-lot-title":"Список Таблиць","crossref-lol-title":"Список Каталогів","environment-proof-title":"Доведення","environment-remark-title":"Зауваження","environment-solution-title":"Рішення","listing-page-order-by":"Сортувати по","listing-page-order-by-default":"попередньо вибраний","listing-page-order-by-date-asc":"Найновіші","listing-page-order-by-date-desc":"Найстріші","listing-page-order-by-number-desc":"За спаданням","listing-page-order-by-number-asc":"За зростанням","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Опис","listing-page-field-author":"Автор","listing-page-field-filename":"Ім'я файлу","listing-page-field-filemodified":"Змінено","listing-page-field-subtitle":"Підзаголовок","listing-page-field-readingtime":"Час читання","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Категорії","listing-page-minutes-compact":"{0} хвилин","listing-page-category-all":"Все","listing-page-no-matches":"Немає відповідних елементів","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.25","bibliography":["references_main.bib"],"csl":"physical-review-b.csl","callout-appearance":"default","grid":{"body-width":"1050px"},"page-layout":"full","theme":{"light":"cosmo","dark":"superhero"},"title":"Лабораторна робота № 5"},"extensions":{"book":{"multiFile":true}}},"docx":{"identifier":{"display-name":"MS Word","target-format":"docx","base-format":"docx"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"docx","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"page-width":6.5},"pandoc":{"default-image-extension":"png","to":"docx","toc":true,"number-sections":true,"output-file":"lab_5.docx"},"language":{"toc-title-document":"Зміст","toc-title-website":"На цій сторінці","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Анотація","section-title-appendices":"Додатки","section-title-footnotes":"Зноски","section-title-references":"Використана література","section-title-reuse":"Повторне використання","section-title-copyright":"Copyright","section-title-citation":"Цитата","appendix-attribution-cite-as":"Будь-ласка, цитуйте цю роботу як:","appendix-attribution-bibtex":"BibTeX:","appendix-view-license":"View License","title-block-author-single":"Автор","title-block-author-plural":"Автори","title-block-affiliation-single":"Приналежність","title-block-affiliation-plural":"Приналежності","title-block-published":"Дата публікації","title-block-modified":"Змінено","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Розгорнути код","code-tools-hide-all-code":"Приховати код","code-tools-view-source":"Переглянути код","code-tools-source-code":"Вихідний код","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Копіювати","copy-button-tooltip-success":"Скопійовано!","repo-action-links-edit":"Редагувати сторінку","repo-action-links-source":"Переглянути код","repo-action-links-issue":"Повідомити про проблему","back-to-top":"Back to top","search-no-results-text":"Пошук не дав результату","search-matching-documents-text":"Результати пошуку","search-copy-link-title":"Скопіюйте посилання для пошуку","search-hide-matches-text":"Приховати додаткові результати","search-more-match-text":"Додатковий результат у цьому документі","search-more-matches-text":"Додаткові результати у цьому документі","search-clear-button-title":"Очистити","search-text-placeholder":"","search-detached-cancel-button-title":"Скасувати","search-submit-button-title":"Надіслати","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рис.","crossref-tbl-title":"Таблиця","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лема","crossref-cor-title":"Наслідок","crossref-prp-title":"Твердження","crossref-cnj-title":"Гіпотеза","crossref-def-title":"Визначення","crossref-exm-title":"Приклад","crossref-exr-title":"Завдання","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Додаток","crossref-sec-prefix":"Розділ","crossref-eq-prefix":"Рівняння","crossref-lof-title":"Список Рисунків","crossref-lot-title":"Список Таблиць","crossref-lol-title":"Список Каталогів","environment-proof-title":"Доведення","environment-remark-title":"Зауваження","environment-solution-title":"Рішення","listing-page-order-by":"Сортувати по","listing-page-order-by-default":"попередньо вибраний","listing-page-order-by-date-asc":"Найновіші","listing-page-order-by-date-desc":"Найстріші","listing-page-order-by-number-desc":"За спаданням","listing-page-order-by-number-asc":"За зростанням","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Опис","listing-page-field-author":"Автор","listing-page-field-filename":"Ім'я файлу","listing-page-field-filemodified":"Змінено","listing-page-field-subtitle":"Підзаголовок","listing-page-field-readingtime":"Час читання","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Категорії","listing-page-minutes-compact":"{0} хвилин","listing-page-category-all":"Все","listing-page-no-matches":"Немає відповідних елементів","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"bibliography":["references_main.bib"],"csl":"physical-review-b.csl","callout-appearance":"default","fontsize":14,"title":"Лабораторна робота № 5"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","docx"]}